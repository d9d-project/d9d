{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"d9d is a distributed training framework built on top of PyTorch 2.0. It aims to be hackable, modular, and efficient, designed to scale from single-GPU debugging to massive clusters running 6D-Parallelism. Installation Just use your favourite package manager: pip install d9d poetry add d9d uv add d9d Extras d9d[aim] : Aim experiment tracker integration. d9d[visualization] : Plotting libraries required to some advanced visualization functionality. d9d[moe] : Efficient Mixture of Experts GPU kernels. You should build and install some dependencies manually before installation: DeepEP , grouped-gemm . d9d[cce] : Efficient Fused Cross-Entropy kernels. You should build and install some dependencies manually before installation: Cut Cross Entropy . Why another framework? Distributed training frameworks such as Megatron-LM are monolithic in the way you run a script from the command line to train any of a set of predefined models, using predefined regimes. While powerful, these systems can be difficult to hack and integrate into novel research workflows. Their focus is often on providing a complete, end-to-end solution, which can limit flexibility for experimentally-driven research. Conversely, creating your own distributed training solution from scratch is tricky. You have to implement many low-level components (like distributed checkpoints and synchronization) that are identical across setups, and manually tackle common performance bottlenecks. d9d was designed to fill the gap between monolithic frameworks and homebrew setups, providing a modular yet effective solution for distributed training. What d9d is and isn't In terms of core concept : IS a pluggable framework for implementing distributed training regimes for your deep learning models. IS built on clear interfaces and building blocks that may be composed and implemented in your own way. IS NOT an all-in-one CLI platform for setting up pre-training and post-training like torchtitan , Megatron-LM , or torchforge . In terms of codebase & engineering : IS built on a strong engineering foundation : We enforce strict type-checking and rigorous linting to catch errors before execution. IS reliable: The framework is backed by a suite of over 450 tests , covering unit logic, integration flows, and End-to-End distributed scenarios. IS eager to use performance hacks (like DeepEp or custom kernels) if they improve MFU, even if they aren't PyTorch-native. IS NOT for legacy setups: We do not maintain backward compatibility with older PyTorch versions or hardware. We prioritize simplicity and modern APIs (like DTensor ). Key Philosophies To achieve the balance between hackability and performance, d9d adheres to specific design principles: Composition over Monoliths : We avoid \"God Classes\" like DistributedDataParallel or ParallelDims that assume ownership of the entire execution loop. Instead, we provide composable and extendable APIs. For instance, specific horizontal parallelism strategies for specific layers ( parallelize_replicate , parallelize_expert_parallel , ...). White-Box Modelling : We encourage standard PyTorch code. Models are not wrapped in obscure metadata specifications; they are standard nn.Module s that implement lightweight protocols. Pragmatic Efficiency : While we prefer native PyTorch, we are eager to integrate non-native solutions if they improve MFU. For example, we implement MoE using DeepEp communications, reindexing kernels from Megatron-LM , and efficient grouped-GEMM implementations. Graph-Based State Management : Our IO system treats model checkpoints as directed acyclic graphs. This allows you to transform architectures (e.g., merging q , k , v into qkv ) on-the-fly while streaming from disk, without massive memory overhead. DTensors : We mandate that distributed parameters be represented as torch.distributed.tensor.DTensor . This simplifies checkpointing by making them topology-aware automatically. We leverage modern PyTorch 2.0 APIs ( DeviceMesh ) as much as possible. Documentation Please navigate through our Table of Contents - you can safely read everything from top to bottom. Examples Qwen3-MoE Pretraining An example showing causal LM pretraing for the Qwen3-MoE model. WIP: MoE load balancing is currently work in progress. Link .","title":"Home"},{"location":"#installation","text":"Just use your favourite package manager: pip install d9d poetry add d9d uv add d9d","title":"Installation"},{"location":"#extras","text":"d9d[aim] : Aim experiment tracker integration. d9d[visualization] : Plotting libraries required to some advanced visualization functionality. d9d[moe] : Efficient Mixture of Experts GPU kernels. You should build and install some dependencies manually before installation: DeepEP , grouped-gemm . d9d[cce] : Efficient Fused Cross-Entropy kernels. You should build and install some dependencies manually before installation: Cut Cross Entropy .","title":"Extras"},{"location":"#why-another-framework","text":"Distributed training frameworks such as Megatron-LM are monolithic in the way you run a script from the command line to train any of a set of predefined models, using predefined regimes. While powerful, these systems can be difficult to hack and integrate into novel research workflows. Their focus is often on providing a complete, end-to-end solution, which can limit flexibility for experimentally-driven research. Conversely, creating your own distributed training solution from scratch is tricky. You have to implement many low-level components (like distributed checkpoints and synchronization) that are identical across setups, and manually tackle common performance bottlenecks. d9d was designed to fill the gap between monolithic frameworks and homebrew setups, providing a modular yet effective solution for distributed training.","title":"Why another framework?"},{"location":"#what-d9d-is-and-isnt","text":"In terms of core concept : IS a pluggable framework for implementing distributed training regimes for your deep learning models. IS built on clear interfaces and building blocks that may be composed and implemented in your own way. IS NOT an all-in-one CLI platform for setting up pre-training and post-training like torchtitan , Megatron-LM , or torchforge . In terms of codebase & engineering : IS built on a strong engineering foundation : We enforce strict type-checking and rigorous linting to catch errors before execution. IS reliable: The framework is backed by a suite of over 450 tests , covering unit logic, integration flows, and End-to-End distributed scenarios. IS eager to use performance hacks (like DeepEp or custom kernels) if they improve MFU, even if they aren't PyTorch-native. IS NOT for legacy setups: We do not maintain backward compatibility with older PyTorch versions or hardware. We prioritize simplicity and modern APIs (like DTensor ).","title":"What d9d is and isn't"},{"location":"#key-philosophies","text":"To achieve the balance between hackability and performance, d9d adheres to specific design principles: Composition over Monoliths : We avoid \"God Classes\" like DistributedDataParallel or ParallelDims that assume ownership of the entire execution loop. Instead, we provide composable and extendable APIs. For instance, specific horizontal parallelism strategies for specific layers ( parallelize_replicate , parallelize_expert_parallel , ...). White-Box Modelling : We encourage standard PyTorch code. Models are not wrapped in obscure metadata specifications; they are standard nn.Module s that implement lightweight protocols. Pragmatic Efficiency : While we prefer native PyTorch, we are eager to integrate non-native solutions if they improve MFU. For example, we implement MoE using DeepEp communications, reindexing kernels from Megatron-LM , and efficient grouped-GEMM implementations. Graph-Based State Management : Our IO system treats model checkpoints as directed acyclic graphs. This allows you to transform architectures (e.g., merging q , k , v into qkv ) on-the-fly while streaming from disk, without massive memory overhead. DTensors : We mandate that distributed parameters be represented as torch.distributed.tensor.DTensor . This simplifies checkpointing by making them topology-aware automatically. We leverage modern PyTorch 2.0 APIs ( DeviceMesh ) as much as possible.","title":"Key Philosophies"},{"location":"#documentation","text":"Please navigate through our Table of Contents - you can safely read everything from top to bottom.","title":"Documentation"},{"location":"#examples","text":"","title":"Examples"},{"location":"#qwen3-moe-pretraining","text":"An example showing causal LM pretraing for the Qwen3-MoE model. WIP: MoE load balancing is currently work in progress. Link .","title":"Qwen3-MoE Pretraining"},{"location":"toc/","text":"\ud83c\udf10 Distributed Core The foundational primitives managing the cluster. Distributed Context : The Source of Truth for topology. Understanding DeviceMesh domains ( dense , expert , batch ). \ud83d\ude80 Execution Engine How to configure and run jobs. Training Loop : The lifecycle of the Trainer , dependency injection, and execution flow. Inference Loop : The lifecycle of distributed Inference and forward-only execution. Configuration : Pydantic schemas for configuring jobs, batching, and logging. Interfaces (Providers & Tasks) : How to inject your custom Model, Dataset, and Step logic (Train & Infer). \ud83d\udcbe Data & State Managing data loading and model checkpoints. Model State Mapper : The graph-based transformation engine for checkpoints (transform architectures on-the-fly). Model State I/O : Streaming reader/writers for checkpoints. Datasets : Distributed-aware dataset wrappers and smart bucketing. \ud83e\udde0 Modeling & Architecture Building blocks for modern LLMs. Model Design : Principles for creating compatible models. Model Catalogue : Models available directly in d9d. Building Blocks : Attention (FlashSDPA, GQA) Mixture of Experts (Sparse Experts, Routers, DeepEP integration) Heads & Embeddings (Split Vocab support) Positional Embeddings (RoPE) FFN (SwiGLU) \u26a1 Parallelism Strategies for distributing computations. Horizontal Parallelism : Data Parallelism, Fully-Sharded Data Parallelism, Expert Parallelism, Tensor Parallelism. Pipeline Parallelism : Vertical scaling, schedules (1F1B, ZeroBubble), and cross-stage communication. Distributed Operations : Utilities for gathering var-length tensors and objects. PyTree Sharding : Splitting complex nested structures across ranks. \ud83d\udd27 Fine-Tuning (PEFT) Parameter-Efficient Fine-Tuning framework. Overview : Injection lifecycle and state mapping. Methods : LoRA , Full Tune , and Method Stacking . \ud83d\udcc8 Optimization & Metrics Metrics : Distributed-aware statistic accumulation. Implemented Metrics : Ready-to-use metric implementations. Custom Metrics : Implementing custom metrics. Experiment Tracking : Integration with logging backends (WandB, Aim). Piecewise Scheduler : Composable LR schedules and Visualization . Stochastic Optimizers : Low-precision training using stochastic rounding. \u2699\ufe0f Internals Deep dive into the engine room. AutoGrad Extensions : How we do split-backward for Pipeline Parallel. Pipelining Internals : How the VM and Schedules work. Gradient Sync : Custom backward hooks for overlapping comms. Gradient Norm & Clipping : Correct global norm calculation across hybrid meshes. Metric Collection : Custom overlapped metric synchronization & computation. Pipeline State : Context switching between Global and Microbatch scopes. Determinism . Profiling .","title":"Table of Contents"},{"location":"toc/#distributed-core","text":"The foundational primitives managing the cluster. Distributed Context : The Source of Truth for topology. Understanding DeviceMesh domains ( dense , expert , batch ).","title":"\ud83c\udf10 Distributed Core"},{"location":"toc/#execution-engine","text":"How to configure and run jobs. Training Loop : The lifecycle of the Trainer , dependency injection, and execution flow. Inference Loop : The lifecycle of distributed Inference and forward-only execution. Configuration : Pydantic schemas for configuring jobs, batching, and logging. Interfaces (Providers & Tasks) : How to inject your custom Model, Dataset, and Step logic (Train & Infer).","title":"\ud83d\ude80 Execution Engine"},{"location":"toc/#data-state","text":"Managing data loading and model checkpoints. Model State Mapper : The graph-based transformation engine for checkpoints (transform architectures on-the-fly). Model State I/O : Streaming reader/writers for checkpoints. Datasets : Distributed-aware dataset wrappers and smart bucketing.","title":"\ud83d\udcbe Data &amp; State"},{"location":"toc/#modeling-architecture","text":"Building blocks for modern LLMs. Model Design : Principles for creating compatible models. Model Catalogue : Models available directly in d9d. Building Blocks : Attention (FlashSDPA, GQA) Mixture of Experts (Sparse Experts, Routers, DeepEP integration) Heads & Embeddings (Split Vocab support) Positional Embeddings (RoPE) FFN (SwiGLU)","title":"\ud83e\udde0 Modeling &amp; Architecture"},{"location":"toc/#parallelism","text":"Strategies for distributing computations. Horizontal Parallelism : Data Parallelism, Fully-Sharded Data Parallelism, Expert Parallelism, Tensor Parallelism. Pipeline Parallelism : Vertical scaling, schedules (1F1B, ZeroBubble), and cross-stage communication. Distributed Operations : Utilities for gathering var-length tensors and objects. PyTree Sharding : Splitting complex nested structures across ranks.","title":"\u26a1 Parallelism"},{"location":"toc/#fine-tuning-peft","text":"Parameter-Efficient Fine-Tuning framework. Overview : Injection lifecycle and state mapping. Methods : LoRA , Full Tune , and Method Stacking .","title":"\ud83d\udd27 Fine-Tuning (PEFT)"},{"location":"toc/#optimization-metrics","text":"Metrics : Distributed-aware statistic accumulation. Implemented Metrics : Ready-to-use metric implementations. Custom Metrics : Implementing custom metrics. Experiment Tracking : Integration with logging backends (WandB, Aim). Piecewise Scheduler : Composable LR schedules and Visualization . Stochastic Optimizers : Low-precision training using stochastic rounding.","title":"\ud83d\udcc8 Optimization &amp; Metrics"},{"location":"toc/#internals","text":"Deep dive into the engine room. AutoGrad Extensions : How we do split-backward for Pipeline Parallel. Pipelining Internals : How the VM and Schedules work. Gradient Sync : Custom backward hooks for overlapping comms. Gradient Norm & Clipping : Correct global norm calculation across hybrid meshes. Metric Collection : Custom overlapped metric synchronization & computation. Pipeline State : Context switching between Global and Microbatch scopes. Determinism . Profiling .","title":"\u2699\ufe0f Internals"},{"location":"0_loop/0_index/","text":"Overview The d9d.loop package provides the execution engine for distributed training. The d9d Trainer separates the definition of the job (Models, Tasks, Data) from the execution of the job (Synchronization, Checkpointing, Profiling). This allows the same code to run on a single GPU or a 1000-GPU Pipeline Parallel cluster without modifications. Configuration & Construction To ensure reproducibility, the Trainer is not instantiated directly with loose objects. It is built using the TrainingConfigurator and the dependency injection pattern. TrainingConfigurator This class binds the Infrastructure Configuration , Job Configuration , and User Logic (Providers) into a Trainer object with prepared TrainJobState . from d9d.loop.run import TrainingConfigurator trainer = TrainingConfigurator ( mesh = mesh_params , # Physical cluster layout parameters = config , # Logic configuration (batch size, etc) # --- User Logic --- model_provider =... , # How to build the model task_provider =... , # How to compute loss data_provider =... , # How to load data optimizer_provider =... , # How to optimize lr_scheduler_provider =... # LR scheduler ) . configure () The Configuration Lifecycle The TrainingConfigurator.configure() method does: Distributed Context Initialization : Constructs the global DistributedContext , therefore initializing all the required NCCL process groups and DeviceMesh es. Seeding : Sets distributed seeds using the configured base_seed . This ensures model initialization and other initial states are deterministic. More info . Task Instantiation : Instantiates the TrainTask object using specified TrainTaskProvider . Data Loader Construction : Calls the DatasetProvider to get the dataset and wraps it into a DataLoader . The DataLoader will move all the Tensor data to this worker's device automatically . Model Materialization : The ModelStageFactory runs. This is the heavy lifting of initialization: Meta Init : ModelProvider creates the model on the meta device (no memory usage). Parallelization : ModelProvider applies DTensor sharding/replication to parameters. Materialization : Empty tensors are allocated on the actual GPU. Wait : Hard barrier to ensure all ranks allocated memory successfully. Parameter Reset : model.reset_parameters() is called to generate random weights on GPU. Source Loading (Optional) : If configured, a pretrained checkpoint (e.g., from HF) is streamed into the model using ModelStateMapper . Optimizer and LR Scheduler Setup : OptimizerFactory iterates over the model parameters. Calls OptimizerProvider and LRSchedulerProvider . State Assembly : All components (including internal ones) are packed into the TrainJobState . The Trainer is instantiated with this state and returned. Training To run a train job, just call the .train() method on a Trainer object that is returned by configuration process. The Training Lifecycle The Trainer.train() method orchestrates the following lifecycle. It is critical to understand this flow when debugging distributed issues or checking for side effects. 1. Initialization & Recovery Before the loop starts: Global Synchronization : The trainer waits for all ranks to come online ( barrier ). State Loading : The StateCheckpointer checks the filesystem. If a checkpoint exists, it loads it into all the Stateful objects inside its _state . If no checkpoint exists, it starts from the first step. Context Entry : The trainer enters several context managers: UI : Renders a progress bar. Logging : Initiates a new run in selected experiment tracker and dumps run hyperparameters there. More info . Garbage Collector : Disables automatic Python garbage collection. Profiler : Starts torch.profiler hooks. More info . Gradient Manager : Sets up backward hooks for synchronizing gradient states by all-reduce. Gradient Clipper : Looks for model parameters which gradients will be registered for clipping. 2. The Step Loop For every global step ( step ), the trainer performs the following actions in strict order: Microbatch Execution The DataLoader yields a \"Batch Group\" containing \\(N\\) microbatches (calculated automatically based on BatchingConfig ). We delegate to the TrainTask for mapping data before feeding it into the model. The gradients will be accumulated locally using either regular multiple forward-backward calls if pipeline parallelism is disabled, either using our internal pipelining API . We delegate to TrainTask to compute loss values between forward and backward passes. Last gradient accumulation triggers all-reduce synchronization. Communications may start overlapping here. We delegate to TrainTask to accumulate local metrics (e.g., token counts, accuracy) into the Metric state. Metric Synchronization Metric Sync Trigger : JobLogger triggers an async reduction of all metrics across the world. More info . Gradient Synchronization Wait & Scale : The GradientManager waits for all backward hooks to finish. It synchronizes the total weighted loss across the world to determine the scaling factor, then divides all gradients by this factor (essential for correct averaging when batch sizes vary due to masking/packing). More info . Gradient Clipping The GradientClipper calculates the global L2 norm of all parameters. If max_norm is set, gradients are modified in-place. The total norm is logged. More info . Optimization Step : The Optimizer updates model parameters. Schedule : The LRScheduler updates the learning rate for the next step. Zero Grad : The GradientManager clears gradients for the next iteration. Logging & Maintenance Log : Metrics are finalized and written to the tracker. GC : ManualGarbageCollector runs if the current step matches the GC period. Advance : The Stepper increments the step count. Checkpointing If the current step matches checkpointing.period_steps , checkpointing is triggered. This acts as a global barrier. 3. Finalization Task-specific : We delegate to the TrainTask to do its specific finalization work. API Reference d9d.loop.run.TrainingConfigurator Orchestrates the assembly of the distributed training environment. This class binds the infrastructure configuration (DeviceMesh), the training parameters (TrainerConfig), and the user-defined logic (Providers) to create a fully initialized state object capable of running the training loop. Source code in d9d/loop/run/train.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 class TrainingConfigurator : \"\"\" Orchestrates the assembly of the distributed training environment. This class binds the infrastructure configuration (DeviceMesh), the training parameters (TrainerConfig), and the user-defined logic (Providers) to create a fully initialized state object capable of running the training loop. \"\"\" def __init__ ( self , mesh : DeviceMeshParameters , parameters : TrainerConfig , task_provider : TrainTaskProvider , model_provider : ModelProvider , data_provider : DatasetProvider , optimizer_provider : OptimizerProvider , lr_scheduler_provider : LRSchedulerProvider , ): \"\"\" Constructs a configurator capable of building the full training state. Args: mesh: Definition of the distributed device mesh topology. parameters: The global configuration object for the trainer. task_provider: Factory for creating the training task logic. model_provider: Factory for defining and creating model stages. data_provider: Factory for providing training datasets. optimizer_provider: Factory for creating the optimizer. lr_scheduler_provider: Factory for creating the learning rate scheduler. \"\"\" self . _mesh = mesh self . _parameters = parameters self . _task_provider = task_provider self . _model_provider = model_provider self . _data_provider = data_provider self . _optimizer_provider = optimizer_provider self . _lr_scheduler_provider = lr_scheduler_provider def _build_new_training_state ( self ) -> TrainJobState : dist_context = self . _mesh . build () set_seeds ( dist_context , seed = self . _parameters . determinism . base_seed ) timeout_manager = TimeoutManager ( dist_context = dist_context , config = self . _parameters . timeout ) timeout_manager . set_init () task = self . _task_provider ( TrainTaskProviderContext ( dist_context = dist_context )) batch_maths = BatchMaths ( dist_context = dist_context , config_batching = self . _parameters . batching , config_pipelining = self . _parameters . pipelining , ) data_loader_factory = DataLoaderFactory ( dist_context = dist_context , provider = self . _data_provider , config_data_loading = self . _parameters . data_loading , batch_maths = batch_maths , ) data_loader_train = data_loader_factory . build_dataloader_for_train_job () stepper = Stepper ( initial_step = 1 , total_steps = len ( data_loader_train )) pipeline_state_handler = PipelineStateHandler ( sharding_spec = {}, num_shards = batch_maths . num_microbatches_pipelining ) loss_computer = LossComputer ( state = pipeline_state_handler , task = task , stepper = stepper ) schedule , modules = ModelStageFactory ( model_provider = self . _model_provider , dist_context = dist_context , config_model = self . _parameters . model_stage_factory , config_pipelining = self . _parameters . pipelining , batch_maths = batch_maths , pipeline_callback = loss_computer , ) . build_pipeline_and_modules () metrics = ComposeMetric ( task . create_metrics ( CreateMetricsContext ()) . metrics ) task_operator = TrainTaskOperator ( dist_context = dist_context , task = task , pipeline = schedule , pipeline_state = pipeline_state_handler , metrics = metrics , ) grad_clipper = GradientClipper ( dist_context = dist_context , tracked_modules = modules , config = self . _parameters . gradient_clipping , stepper = stepper , ) optimizer , scheduler = OptimizerFactory ( dist_context = dist_context , tracked_modules = modules , optimizer_provider = self . _optimizer_provider , stepper = stepper , lr_scheduler_provider = self . _lr_scheduler_provider , ) . build_optimizer_and_scheduler () gc = ManualGarbageCollector ( dist_ctx = dist_context , config = self . _parameters . gc , step = stepper ) checkpointer = StateCheckpointer ( dist_context = dist_context , stepper = stepper , config = self . _parameters . checkpointing , gc = gc , run_name = self . _parameters . run . name , ) profiler = JobProfiler ( dist_context = dist_context , stepper = stepper , config = self . _parameters . profiling ) exporter = ModelStageExporter ( model_provider = self . _model_provider , dist_context = dist_context , modules = modules ) gradient_manager = GradientManager ( dist_context = dist_context , tracked_modules = modules , batch_maths = batch_maths , config = self . _parameters . gradient_manager , ) job_logger = JobLogger ( dist_context = dist_context , config = self . _parameters . logging , metrics = metrics , stepper = stepper , run_config = self . _parameters . run , additional_hparams = { \"task\" : task . dump_hparams (), \"model\" : self . _model_provider . dump_hparams ()}, ) return TrainJobState ( dist_context = dist_context , data_loader = data_loader_train , stepper = stepper , tracked_modules = modules , garbage_collector = gc , batch_maths = batch_maths , checkpointer = checkpointer , optimizer = optimizer , task = task , lr_scheduler = scheduler , gradient_clipper = grad_clipper , profiler = profiler , exporter = exporter , metrics = metrics , logger = job_logger , gradient_manager = gradient_manager , timeout_manager = timeout_manager , task_operator = task_operator , ) def configure ( self ) -> \"Trainer\" : \"\"\" Instantiates all training components and returns a configured Trainer. This method triggers the creation of the distributed context, sets seeds, builds the model, optimizer, data loaders, and attaches all auxiliary components (logging, profiling, checkpointing). Returns: Trainer: A ready-to-use trainer instance encapsulating the job state. \"\"\" state = self . _build_new_training_state () return Trainer ( state ) __init__ ( mesh , parameters , task_provider , model_provider , data_provider , optimizer_provider , lr_scheduler_provider ) Constructs a configurator capable of building the full training state. Parameters: Name Type Description Default mesh DeviceMeshParameters Definition of the distributed device mesh topology. required parameters TrainerConfig The global configuration object for the trainer. required task_provider TrainTaskProvider Factory for creating the training task logic. required model_provider ModelProvider Factory for defining and creating model stages. required data_provider DatasetProvider Factory for providing training datasets. required optimizer_provider OptimizerProvider Factory for creating the optimizer. required lr_scheduler_provider LRSchedulerProvider Factory for creating the learning rate scheduler. required Source code in d9d/loop/run/train.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def __init__ ( self , mesh : DeviceMeshParameters , parameters : TrainerConfig , task_provider : TrainTaskProvider , model_provider : ModelProvider , data_provider : DatasetProvider , optimizer_provider : OptimizerProvider , lr_scheduler_provider : LRSchedulerProvider , ): \"\"\" Constructs a configurator capable of building the full training state. Args: mesh: Definition of the distributed device mesh topology. parameters: The global configuration object for the trainer. task_provider: Factory for creating the training task logic. model_provider: Factory for defining and creating model stages. data_provider: Factory for providing training datasets. optimizer_provider: Factory for creating the optimizer. lr_scheduler_provider: Factory for creating the learning rate scheduler. \"\"\" self . _mesh = mesh self . _parameters = parameters self . _task_provider = task_provider self . _model_provider = model_provider self . _data_provider = data_provider self . _optimizer_provider = optimizer_provider self . _lr_scheduler_provider = lr_scheduler_provider configure () Instantiates all training components and returns a configured Trainer. This method triggers the creation of the distributed context, sets seeds, builds the model, optimizer, data loaders, and attaches all auxiliary components (logging, profiling, checkpointing). Returns: Name Type Description Trainer Trainer A ready-to-use trainer instance encapsulating the job state. Source code in d9d/loop/run/train.py 196 197 198 199 200 201 202 203 204 205 206 207 208 209 def configure ( self ) -> \"Trainer\" : \"\"\" Instantiates all training components and returns a configured Trainer. This method triggers the creation of the distributed context, sets seeds, builds the model, optimizer, data loaders, and attaches all auxiliary components (logging, profiling, checkpointing). Returns: Trainer: A ready-to-use trainer instance encapsulating the job state. \"\"\" state = self . _build_new_training_state () return Trainer ( state ) d9d.loop.run.Trainer The main execution engine for running a distributed training job. This class manages the training loop, lifecycle events, distributed synchronization, and periodic side-effects (logging, checkpointing). Source code in d9d/loop/run/train.py 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 class Trainer : \"\"\" The main execution engine for running a distributed training job. This class manages the training loop, lifecycle events, distributed synchronization, and periodic side-effects (logging, checkpointing). \"\"\" def __init__ ( self , state : TrainJobState ): \"\"\" Constructs a Trainer from a pre-built job state. Args: state: The encapsulated state object containing all initialized components (model, optimizer, dist_context, etc.). \"\"\" self . _state = state def train ( self ): \"\"\" Executes the full training workflow. \"\"\" self . _state . dist_context . logger . info ( \"Waiting for the world to start training\" ) self . _state . dist_context . wait_world () self . _state . dist_context . logger . info ( \"Trying to load last checkpoint before doing anything else\" ) self . _state . checkpointer . load_last_checkpoint ( self . _state ) if self . _state . stepper . current_step >= self . _state . stepper . total_steps : self . _state . dist_context . logger . info ( \"Already trained fully, will do nothing\" ) return self . _state . dist_context . wait_world () with ( tqdm ( desc = \"Training\" , total = self . _state . stepper . total_steps , disable = not self . _state . dist_context . is_local_main_process , initial = self . _state . stepper . current_step , ) as bar , self . _state . logger . new_run () as run , self . _state . garbage_collector as gc , self . _state . profiler . open () as profiler , self . _state . gradient_manager . install (), self . _state . gradient_clipper . install (), self . _state . logger . install (), ): run . set_context ({ \"stage\" : \"train\" }) for batch_group in self . _state . data_loader : run . set_step ( self . _state . stepper . current_step ) for batch in batch_group : # we do both forward and backward passes # since GradientManager is installed - it should start performing # synchronization overlapping grad sync with compute loss = self . _state . task_operator . forward_backward ( batch ) # add loss for grad manager - it want it for grad reduction if loss is not None : self . _state . gradient_manager . add_loss_with_weight ( loss . loss , loss . loss_weight ) # metrics were successfully accumulated during forward passes - we can schedule their synchronization self . _state . logger . trigger_sync () # wait for gradient synchronization finishes and scale them self . _state . gradient_manager . sync_and_scale () # clip grads after they are synced across world self . _state . gradient_clipper . clip_and_log ( run ) # optimize (it won't sync grads - they are already Replicate-d) self . _state . optimizer . step () # update LR self . _state . lr_scheduler . step () # log everything self . _state . logger . log ( run , loss_value = self . _state . gradient_manager . compute_global_loss ()) # reset grads self . _state . gradient_manager . zero_grad () gc . collect_periodic () self . _state . stepper . step () bar . update () # checkpoint at the end of the step self . _state . checkpointer . checkpoint_if_needed ( self . _state ) if profiler : profiler . step () self . _state . timeout_manager . set_periodic () self . _state . task . finalize ( FinalizeContext ()) def export ( self , export_to : Path , load_checkpoint : bool ): \"\"\" Exports the current model state to the specified directory. This handles the distributed saving logic, allowing the model to be reconstituted later or used for inference. Args: export_to: The directory path where the model artifacts will be saved. load_checkpoint: If True, attempts to load the latest checkpoint into the model before exporting. \"\"\" if load_checkpoint : self . _state . checkpointer . load_last_checkpoint ( self . _state ) self . _state . exporter . export ( export_to ) __init__ ( state ) Constructs a Trainer from a pre-built job state. Parameters: Name Type Description Default state TrainJobState The encapsulated state object containing all initialized components (model, optimizer, dist_context, etc.). required Source code in d9d/loop/run/train.py 220 221 222 223 224 225 226 227 228 def __init__ ( self , state : TrainJobState ): \"\"\" Constructs a Trainer from a pre-built job state. Args: state: The encapsulated state object containing all initialized components (model, optimizer, dist_context, etc.). \"\"\" self . _state = state export ( export_to , load_checkpoint ) Exports the current model state to the specified directory. This handles the distributed saving logic, allowing the model to be reconstituted later or used for inference. Parameters: Name Type Description Default export_to Path The directory path where the model artifacts will be saved. required load_checkpoint bool If True, attempts to load the latest checkpoint into the model before exporting. required Source code in d9d/loop/run/train.py 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 def export ( self , export_to : Path , load_checkpoint : bool ): \"\"\" Exports the current model state to the specified directory. This handles the distributed saving logic, allowing the model to be reconstituted later or used for inference. Args: export_to: The directory path where the model artifacts will be saved. load_checkpoint: If True, attempts to load the latest checkpoint into the model before exporting. \"\"\" if load_checkpoint : self . _state . checkpointer . load_last_checkpoint ( self . _state ) self . _state . exporter . export ( export_to ) train () Executes the full training workflow. Source code in d9d/loop/run/train.py 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 def train ( self ): \"\"\" Executes the full training workflow. \"\"\" self . _state . dist_context . logger . info ( \"Waiting for the world to start training\" ) self . _state . dist_context . wait_world () self . _state . dist_context . logger . info ( \"Trying to load last checkpoint before doing anything else\" ) self . _state . checkpointer . load_last_checkpoint ( self . _state ) if self . _state . stepper . current_step >= self . _state . stepper . total_steps : self . _state . dist_context . logger . info ( \"Already trained fully, will do nothing\" ) return self . _state . dist_context . wait_world () with ( tqdm ( desc = \"Training\" , total = self . _state . stepper . total_steps , disable = not self . _state . dist_context . is_local_main_process , initial = self . _state . stepper . current_step , ) as bar , self . _state . logger . new_run () as run , self . _state . garbage_collector as gc , self . _state . profiler . open () as profiler , self . _state . gradient_manager . install (), self . _state . gradient_clipper . install (), self . _state . logger . install (), ): run . set_context ({ \"stage\" : \"train\" }) for batch_group in self . _state . data_loader : run . set_step ( self . _state . stepper . current_step ) for batch in batch_group : # we do both forward and backward passes # since GradientManager is installed - it should start performing # synchronization overlapping grad sync with compute loss = self . _state . task_operator . forward_backward ( batch ) # add loss for grad manager - it want it for grad reduction if loss is not None : self . _state . gradient_manager . add_loss_with_weight ( loss . loss , loss . loss_weight ) # metrics were successfully accumulated during forward passes - we can schedule their synchronization self . _state . logger . trigger_sync () # wait for gradient synchronization finishes and scale them self . _state . gradient_manager . sync_and_scale () # clip grads after they are synced across world self . _state . gradient_clipper . clip_and_log ( run ) # optimize (it won't sync grads - they are already Replicate-d) self . _state . optimizer . step () # update LR self . _state . lr_scheduler . step () # log everything self . _state . logger . log ( run , loss_value = self . _state . gradient_manager . compute_global_loss ()) # reset grads self . _state . gradient_manager . zero_grad () gc . collect_periodic () self . _state . stepper . step () bar . update () # checkpoint at the end of the step self . _state . checkpointer . checkpoint_if_needed ( self . _state ) if profiler : profiler . step () self . _state . timeout_manager . set_periodic () self . _state . task . finalize ( FinalizeContext ())","title":"Training Loop"},{"location":"0_loop/0_index/#overview","text":"The d9d.loop package provides the execution engine for distributed training. The d9d Trainer separates the definition of the job (Models, Tasks, Data) from the execution of the job (Synchronization, Checkpointing, Profiling). This allows the same code to run on a single GPU or a 1000-GPU Pipeline Parallel cluster without modifications.","title":"Overview"},{"location":"0_loop/0_index/#configuration-construction","text":"To ensure reproducibility, the Trainer is not instantiated directly with loose objects. It is built using the TrainingConfigurator and the dependency injection pattern.","title":"Configuration &amp; Construction"},{"location":"0_loop/0_index/#trainingconfigurator","text":"This class binds the Infrastructure Configuration , Job Configuration , and User Logic (Providers) into a Trainer object with prepared TrainJobState . from d9d.loop.run import TrainingConfigurator trainer = TrainingConfigurator ( mesh = mesh_params , # Physical cluster layout parameters = config , # Logic configuration (batch size, etc) # --- User Logic --- model_provider =... , # How to build the model task_provider =... , # How to compute loss data_provider =... , # How to load data optimizer_provider =... , # How to optimize lr_scheduler_provider =... # LR scheduler ) . configure ()","title":"TrainingConfigurator"},{"location":"0_loop/0_index/#the-configuration-lifecycle","text":"The TrainingConfigurator.configure() method does: Distributed Context Initialization : Constructs the global DistributedContext , therefore initializing all the required NCCL process groups and DeviceMesh es. Seeding : Sets distributed seeds using the configured base_seed . This ensures model initialization and other initial states are deterministic. More info . Task Instantiation : Instantiates the TrainTask object using specified TrainTaskProvider . Data Loader Construction : Calls the DatasetProvider to get the dataset and wraps it into a DataLoader . The DataLoader will move all the Tensor data to this worker's device automatically . Model Materialization : The ModelStageFactory runs. This is the heavy lifting of initialization: Meta Init : ModelProvider creates the model on the meta device (no memory usage). Parallelization : ModelProvider applies DTensor sharding/replication to parameters. Materialization : Empty tensors are allocated on the actual GPU. Wait : Hard barrier to ensure all ranks allocated memory successfully. Parameter Reset : model.reset_parameters() is called to generate random weights on GPU. Source Loading (Optional) : If configured, a pretrained checkpoint (e.g., from HF) is streamed into the model using ModelStateMapper . Optimizer and LR Scheduler Setup : OptimizerFactory iterates over the model parameters. Calls OptimizerProvider and LRSchedulerProvider . State Assembly : All components (including internal ones) are packed into the TrainJobState . The Trainer is instantiated with this state and returned.","title":"The Configuration Lifecycle"},{"location":"0_loop/0_index/#training","text":"To run a train job, just call the .train() method on a Trainer object that is returned by configuration process.","title":"Training"},{"location":"0_loop/0_index/#the-training-lifecycle","text":"The Trainer.train() method orchestrates the following lifecycle. It is critical to understand this flow when debugging distributed issues or checking for side effects.","title":"The Training Lifecycle"},{"location":"0_loop/0_index/#1-initialization-recovery","text":"Before the loop starts: Global Synchronization : The trainer waits for all ranks to come online ( barrier ). State Loading : The StateCheckpointer checks the filesystem. If a checkpoint exists, it loads it into all the Stateful objects inside its _state . If no checkpoint exists, it starts from the first step. Context Entry : The trainer enters several context managers: UI : Renders a progress bar. Logging : Initiates a new run in selected experiment tracker and dumps run hyperparameters there. More info . Garbage Collector : Disables automatic Python garbage collection. Profiler : Starts torch.profiler hooks. More info . Gradient Manager : Sets up backward hooks for synchronizing gradient states by all-reduce. Gradient Clipper : Looks for model parameters which gradients will be registered for clipping.","title":"1. Initialization &amp; Recovery"},{"location":"0_loop/0_index/#2-the-step-loop","text":"For every global step ( step ), the trainer performs the following actions in strict order: Microbatch Execution The DataLoader yields a \"Batch Group\" containing \\(N\\) microbatches (calculated automatically based on BatchingConfig ). We delegate to the TrainTask for mapping data before feeding it into the model. The gradients will be accumulated locally using either regular multiple forward-backward calls if pipeline parallelism is disabled, either using our internal pipelining API . We delegate to TrainTask to compute loss values between forward and backward passes. Last gradient accumulation triggers all-reduce synchronization. Communications may start overlapping here. We delegate to TrainTask to accumulate local metrics (e.g., token counts, accuracy) into the Metric state. Metric Synchronization Metric Sync Trigger : JobLogger triggers an async reduction of all metrics across the world. More info . Gradient Synchronization Wait & Scale : The GradientManager waits for all backward hooks to finish. It synchronizes the total weighted loss across the world to determine the scaling factor, then divides all gradients by this factor (essential for correct averaging when batch sizes vary due to masking/packing). More info . Gradient Clipping The GradientClipper calculates the global L2 norm of all parameters. If max_norm is set, gradients are modified in-place. The total norm is logged. More info . Optimization Step : The Optimizer updates model parameters. Schedule : The LRScheduler updates the learning rate for the next step. Zero Grad : The GradientManager clears gradients for the next iteration. Logging & Maintenance Log : Metrics are finalized and written to the tracker. GC : ManualGarbageCollector runs if the current step matches the GC period. Advance : The Stepper increments the step count. Checkpointing If the current step matches checkpointing.period_steps , checkpointing is triggered. This acts as a global barrier.","title":"2. The Step Loop"},{"location":"0_loop/0_index/#3-finalization","text":"Task-specific : We delegate to the TrainTask to do its specific finalization work.","title":"3. Finalization"},{"location":"0_loop/0_index/#api-reference","text":"","title":"API Reference"},{"location":"0_loop/0_index/#d9d.loop.run.TrainingConfigurator","text":"Orchestrates the assembly of the distributed training environment. This class binds the infrastructure configuration (DeviceMesh), the training parameters (TrainerConfig), and the user-defined logic (Providers) to create a fully initialized state object capable of running the training loop. Source code in d9d/loop/run/train.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 class TrainingConfigurator : \"\"\" Orchestrates the assembly of the distributed training environment. This class binds the infrastructure configuration (DeviceMesh), the training parameters (TrainerConfig), and the user-defined logic (Providers) to create a fully initialized state object capable of running the training loop. \"\"\" def __init__ ( self , mesh : DeviceMeshParameters , parameters : TrainerConfig , task_provider : TrainTaskProvider , model_provider : ModelProvider , data_provider : DatasetProvider , optimizer_provider : OptimizerProvider , lr_scheduler_provider : LRSchedulerProvider , ): \"\"\" Constructs a configurator capable of building the full training state. Args: mesh: Definition of the distributed device mesh topology. parameters: The global configuration object for the trainer. task_provider: Factory for creating the training task logic. model_provider: Factory for defining and creating model stages. data_provider: Factory for providing training datasets. optimizer_provider: Factory for creating the optimizer. lr_scheduler_provider: Factory for creating the learning rate scheduler. \"\"\" self . _mesh = mesh self . _parameters = parameters self . _task_provider = task_provider self . _model_provider = model_provider self . _data_provider = data_provider self . _optimizer_provider = optimizer_provider self . _lr_scheduler_provider = lr_scheduler_provider def _build_new_training_state ( self ) -> TrainJobState : dist_context = self . _mesh . build () set_seeds ( dist_context , seed = self . _parameters . determinism . base_seed ) timeout_manager = TimeoutManager ( dist_context = dist_context , config = self . _parameters . timeout ) timeout_manager . set_init () task = self . _task_provider ( TrainTaskProviderContext ( dist_context = dist_context )) batch_maths = BatchMaths ( dist_context = dist_context , config_batching = self . _parameters . batching , config_pipelining = self . _parameters . pipelining , ) data_loader_factory = DataLoaderFactory ( dist_context = dist_context , provider = self . _data_provider , config_data_loading = self . _parameters . data_loading , batch_maths = batch_maths , ) data_loader_train = data_loader_factory . build_dataloader_for_train_job () stepper = Stepper ( initial_step = 1 , total_steps = len ( data_loader_train )) pipeline_state_handler = PipelineStateHandler ( sharding_spec = {}, num_shards = batch_maths . num_microbatches_pipelining ) loss_computer = LossComputer ( state = pipeline_state_handler , task = task , stepper = stepper ) schedule , modules = ModelStageFactory ( model_provider = self . _model_provider , dist_context = dist_context , config_model = self . _parameters . model_stage_factory , config_pipelining = self . _parameters . pipelining , batch_maths = batch_maths , pipeline_callback = loss_computer , ) . build_pipeline_and_modules () metrics = ComposeMetric ( task . create_metrics ( CreateMetricsContext ()) . metrics ) task_operator = TrainTaskOperator ( dist_context = dist_context , task = task , pipeline = schedule , pipeline_state = pipeline_state_handler , metrics = metrics , ) grad_clipper = GradientClipper ( dist_context = dist_context , tracked_modules = modules , config = self . _parameters . gradient_clipping , stepper = stepper , ) optimizer , scheduler = OptimizerFactory ( dist_context = dist_context , tracked_modules = modules , optimizer_provider = self . _optimizer_provider , stepper = stepper , lr_scheduler_provider = self . _lr_scheduler_provider , ) . build_optimizer_and_scheduler () gc = ManualGarbageCollector ( dist_ctx = dist_context , config = self . _parameters . gc , step = stepper ) checkpointer = StateCheckpointer ( dist_context = dist_context , stepper = stepper , config = self . _parameters . checkpointing , gc = gc , run_name = self . _parameters . run . name , ) profiler = JobProfiler ( dist_context = dist_context , stepper = stepper , config = self . _parameters . profiling ) exporter = ModelStageExporter ( model_provider = self . _model_provider , dist_context = dist_context , modules = modules ) gradient_manager = GradientManager ( dist_context = dist_context , tracked_modules = modules , batch_maths = batch_maths , config = self . _parameters . gradient_manager , ) job_logger = JobLogger ( dist_context = dist_context , config = self . _parameters . logging , metrics = metrics , stepper = stepper , run_config = self . _parameters . run , additional_hparams = { \"task\" : task . dump_hparams (), \"model\" : self . _model_provider . dump_hparams ()}, ) return TrainJobState ( dist_context = dist_context , data_loader = data_loader_train , stepper = stepper , tracked_modules = modules , garbage_collector = gc , batch_maths = batch_maths , checkpointer = checkpointer , optimizer = optimizer , task = task , lr_scheduler = scheduler , gradient_clipper = grad_clipper , profiler = profiler , exporter = exporter , metrics = metrics , logger = job_logger , gradient_manager = gradient_manager , timeout_manager = timeout_manager , task_operator = task_operator , ) def configure ( self ) -> \"Trainer\" : \"\"\" Instantiates all training components and returns a configured Trainer. This method triggers the creation of the distributed context, sets seeds, builds the model, optimizer, data loaders, and attaches all auxiliary components (logging, profiling, checkpointing). Returns: Trainer: A ready-to-use trainer instance encapsulating the job state. \"\"\" state = self . _build_new_training_state () return Trainer ( state )","title":"TrainingConfigurator"},{"location":"0_loop/0_index/#d9d.loop.run.TrainingConfigurator.__init__","text":"Constructs a configurator capable of building the full training state. Parameters: Name Type Description Default mesh DeviceMeshParameters Definition of the distributed device mesh topology. required parameters TrainerConfig The global configuration object for the trainer. required task_provider TrainTaskProvider Factory for creating the training task logic. required model_provider ModelProvider Factory for defining and creating model stages. required data_provider DatasetProvider Factory for providing training datasets. required optimizer_provider OptimizerProvider Factory for creating the optimizer. required lr_scheduler_provider LRSchedulerProvider Factory for creating the learning rate scheduler. required Source code in d9d/loop/run/train.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def __init__ ( self , mesh : DeviceMeshParameters , parameters : TrainerConfig , task_provider : TrainTaskProvider , model_provider : ModelProvider , data_provider : DatasetProvider , optimizer_provider : OptimizerProvider , lr_scheduler_provider : LRSchedulerProvider , ): \"\"\" Constructs a configurator capable of building the full training state. Args: mesh: Definition of the distributed device mesh topology. parameters: The global configuration object for the trainer. task_provider: Factory for creating the training task logic. model_provider: Factory for defining and creating model stages. data_provider: Factory for providing training datasets. optimizer_provider: Factory for creating the optimizer. lr_scheduler_provider: Factory for creating the learning rate scheduler. \"\"\" self . _mesh = mesh self . _parameters = parameters self . _task_provider = task_provider self . _model_provider = model_provider self . _data_provider = data_provider self . _optimizer_provider = optimizer_provider self . _lr_scheduler_provider = lr_scheduler_provider","title":"__init__"},{"location":"0_loop/0_index/#d9d.loop.run.TrainingConfigurator.configure","text":"Instantiates all training components and returns a configured Trainer. This method triggers the creation of the distributed context, sets seeds, builds the model, optimizer, data loaders, and attaches all auxiliary components (logging, profiling, checkpointing). Returns: Name Type Description Trainer Trainer A ready-to-use trainer instance encapsulating the job state. Source code in d9d/loop/run/train.py 196 197 198 199 200 201 202 203 204 205 206 207 208 209 def configure ( self ) -> \"Trainer\" : \"\"\" Instantiates all training components and returns a configured Trainer. This method triggers the creation of the distributed context, sets seeds, builds the model, optimizer, data loaders, and attaches all auxiliary components (logging, profiling, checkpointing). Returns: Trainer: A ready-to-use trainer instance encapsulating the job state. \"\"\" state = self . _build_new_training_state () return Trainer ( state )","title":"configure"},{"location":"0_loop/0_index/#d9d.loop.run.Trainer","text":"The main execution engine for running a distributed training job. This class manages the training loop, lifecycle events, distributed synchronization, and periodic side-effects (logging, checkpointing). Source code in d9d/loop/run/train.py 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 class Trainer : \"\"\" The main execution engine for running a distributed training job. This class manages the training loop, lifecycle events, distributed synchronization, and periodic side-effects (logging, checkpointing). \"\"\" def __init__ ( self , state : TrainJobState ): \"\"\" Constructs a Trainer from a pre-built job state. Args: state: The encapsulated state object containing all initialized components (model, optimizer, dist_context, etc.). \"\"\" self . _state = state def train ( self ): \"\"\" Executes the full training workflow. \"\"\" self . _state . dist_context . logger . info ( \"Waiting for the world to start training\" ) self . _state . dist_context . wait_world () self . _state . dist_context . logger . info ( \"Trying to load last checkpoint before doing anything else\" ) self . _state . checkpointer . load_last_checkpoint ( self . _state ) if self . _state . stepper . current_step >= self . _state . stepper . total_steps : self . _state . dist_context . logger . info ( \"Already trained fully, will do nothing\" ) return self . _state . dist_context . wait_world () with ( tqdm ( desc = \"Training\" , total = self . _state . stepper . total_steps , disable = not self . _state . dist_context . is_local_main_process , initial = self . _state . stepper . current_step , ) as bar , self . _state . logger . new_run () as run , self . _state . garbage_collector as gc , self . _state . profiler . open () as profiler , self . _state . gradient_manager . install (), self . _state . gradient_clipper . install (), self . _state . logger . install (), ): run . set_context ({ \"stage\" : \"train\" }) for batch_group in self . _state . data_loader : run . set_step ( self . _state . stepper . current_step ) for batch in batch_group : # we do both forward and backward passes # since GradientManager is installed - it should start performing # synchronization overlapping grad sync with compute loss = self . _state . task_operator . forward_backward ( batch ) # add loss for grad manager - it want it for grad reduction if loss is not None : self . _state . gradient_manager . add_loss_with_weight ( loss . loss , loss . loss_weight ) # metrics were successfully accumulated during forward passes - we can schedule their synchronization self . _state . logger . trigger_sync () # wait for gradient synchronization finishes and scale them self . _state . gradient_manager . sync_and_scale () # clip grads after they are synced across world self . _state . gradient_clipper . clip_and_log ( run ) # optimize (it won't sync grads - they are already Replicate-d) self . _state . optimizer . step () # update LR self . _state . lr_scheduler . step () # log everything self . _state . logger . log ( run , loss_value = self . _state . gradient_manager . compute_global_loss ()) # reset grads self . _state . gradient_manager . zero_grad () gc . collect_periodic () self . _state . stepper . step () bar . update () # checkpoint at the end of the step self . _state . checkpointer . checkpoint_if_needed ( self . _state ) if profiler : profiler . step () self . _state . timeout_manager . set_periodic () self . _state . task . finalize ( FinalizeContext ()) def export ( self , export_to : Path , load_checkpoint : bool ): \"\"\" Exports the current model state to the specified directory. This handles the distributed saving logic, allowing the model to be reconstituted later or used for inference. Args: export_to: The directory path where the model artifacts will be saved. load_checkpoint: If True, attempts to load the latest checkpoint into the model before exporting. \"\"\" if load_checkpoint : self . _state . checkpointer . load_last_checkpoint ( self . _state ) self . _state . exporter . export ( export_to )","title":"Trainer"},{"location":"0_loop/0_index/#d9d.loop.run.Trainer.__init__","text":"Constructs a Trainer from a pre-built job state. Parameters: Name Type Description Default state TrainJobState The encapsulated state object containing all initialized components (model, optimizer, dist_context, etc.). required Source code in d9d/loop/run/train.py 220 221 222 223 224 225 226 227 228 def __init__ ( self , state : TrainJobState ): \"\"\" Constructs a Trainer from a pre-built job state. Args: state: The encapsulated state object containing all initialized components (model, optimizer, dist_context, etc.). \"\"\" self . _state = state","title":"__init__"},{"location":"0_loop/0_index/#d9d.loop.run.Trainer.export","text":"Exports the current model state to the specified directory. This handles the distributed saving logic, allowing the model to be reconstituted later or used for inference. Parameters: Name Type Description Default export_to Path The directory path where the model artifacts will be saved. required load_checkpoint bool If True, attempts to load the latest checkpoint into the model before exporting. required Source code in d9d/loop/run/train.py 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 def export ( self , export_to : Path , load_checkpoint : bool ): \"\"\" Exports the current model state to the specified directory. This handles the distributed saving logic, allowing the model to be reconstituted later or used for inference. Args: export_to: The directory path where the model artifacts will be saved. load_checkpoint: If True, attempts to load the latest checkpoint into the model before exporting. \"\"\" if load_checkpoint : self . _state . checkpointer . load_last_checkpoint ( self . _state ) self . _state . exporter . export ( export_to )","title":"export"},{"location":"0_loop/0_index/#d9d.loop.run.Trainer.train","text":"Executes the full training workflow. Source code in d9d/loop/run/train.py 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 def train ( self ): \"\"\" Executes the full training workflow. \"\"\" self . _state . dist_context . logger . info ( \"Waiting for the world to start training\" ) self . _state . dist_context . wait_world () self . _state . dist_context . logger . info ( \"Trying to load last checkpoint before doing anything else\" ) self . _state . checkpointer . load_last_checkpoint ( self . _state ) if self . _state . stepper . current_step >= self . _state . stepper . total_steps : self . _state . dist_context . logger . info ( \"Already trained fully, will do nothing\" ) return self . _state . dist_context . wait_world () with ( tqdm ( desc = \"Training\" , total = self . _state . stepper . total_steps , disable = not self . _state . dist_context . is_local_main_process , initial = self . _state . stepper . current_step , ) as bar , self . _state . logger . new_run () as run , self . _state . garbage_collector as gc , self . _state . profiler . open () as profiler , self . _state . gradient_manager . install (), self . _state . gradient_clipper . install (), self . _state . logger . install (), ): run . set_context ({ \"stage\" : \"train\" }) for batch_group in self . _state . data_loader : run . set_step ( self . _state . stepper . current_step ) for batch in batch_group : # we do both forward and backward passes # since GradientManager is installed - it should start performing # synchronization overlapping grad sync with compute loss = self . _state . task_operator . forward_backward ( batch ) # add loss for grad manager - it want it for grad reduction if loss is not None : self . _state . gradient_manager . add_loss_with_weight ( loss . loss , loss . loss_weight ) # metrics were successfully accumulated during forward passes - we can schedule their synchronization self . _state . logger . trigger_sync () # wait for gradient synchronization finishes and scale them self . _state . gradient_manager . sync_and_scale () # clip grads after they are synced across world self . _state . gradient_clipper . clip_and_log ( run ) # optimize (it won't sync grads - they are already Replicate-d) self . _state . optimizer . step () # update LR self . _state . lr_scheduler . step () # log everything self . _state . logger . log ( run , loss_value = self . _state . gradient_manager . compute_global_loss ()) # reset grads self . _state . gradient_manager . zero_grad () gc . collect_periodic () self . _state . stepper . step () bar . update () # checkpoint at the end of the step self . _state . checkpointer . checkpoint_if_needed ( self . _state ) if profiler : profiler . step () self . _state . timeout_manager . set_periodic () self . _state . task . finalize ( FinalizeContext ())","title":"train"},{"location":"0_loop/1_inference_loop/","text":"Overview The d9d.loop package provides the execution engine not only for training but also for high-scale distributed inference. The inference engine shares the same core philosophy as the Trainer: separating the definition of the job from the execution . Configuration & Construction The inference environment is assembled using the InferenceConfigurator . InferenceConfigurator This class binds the infrastructure and user logic into a ready-to-execute Inference object. from d9d.loop.run import InferenceConfigurator inference = InferenceConfigurator ( mesh = mesh_params , # Physical cluster layout parameters = config , # Logic configuration (batch size, etc) model_provider =... , # Same provider used in training task_provider =... , # Inference-specific logic (e.g., generation) data_provider =... , # Validation/Test dataset ) . configure () The Configuration Lifecycle The InferenceConfigurator.configure() method performs a setup sequence similar to training, but optimized for forward-only execution: Distributed Context Initialization : Constructs the global DistributedContext . Seeding : Sets distributed seeds. Determinism is crucial in inference for reproducible sampling or validation splits. Task Instantiation : Instantiates the InferenceTask . This defines how inputs are processed and what to do with the outputs (e.g., writing to a JSONL file). Data Loader Construction : Creates a distributed DataLoader that handles sharding the inference dataset across ranks. Model Materialization : The ModelStageFactory runs to build the model. Note : This reuses the exact same ModelProvider as training. State Assembly : Components are packed into InferenceJobState . The Inference engine is instantiated. Inference Execution To run the job, call the .infer() method on the configured object. The Inference Lifecycle The Inference.infer() method orchestrates the execution flow. It is designed to be lean and memory-efficient. 1. Initialization & Recovery Before the loop starts: Mode Switching : Enables torch.inference_mode() . This disables gradient calculation globally, saving significant memory. Sets all model modules to .eval() mode (affecting Dropout, BatchNorm, etc.). State Loading : The StateCheckpointer loads the model weights from the specified checkpoint. If the job was interrupted previously, it also restores the Stepper and DataLoader state to resume exactly where it left off. Context Entry : Enters UI, Garbage Collector, and Profiler contexts. 2. The Step Loop For every step: Microbatch Execution : The DataLoader yields a batch group. The InferenceTaskOperator manages the execution. Data is fed through the model. Unlike training, no backward pass is performed. Maintenance : GC : ManualGarbageCollector runs periodically to ensure peak memory usage is controlled. Advance : The Stepper increments. Checkpointing : If configured, the system saves the progress of the inference job. This allows restarting a long-running generation job on a massive dataset without re-processing the first half. 3. Finalization Task-specific : Calls InferenceTask.finalize() . This is typically used to close file handles (e.g., flushing the final lines of a generated dataset to disk). API Reference d9d.loop.run.InferenceConfigurator Orchestrates the assembly of the distributed inference environment. This class binds the infrastructure configuration (DeviceMesh), the inference parameters, and the user-defined logic (Providers) to create a fully initialized state object capable of running the inference loop. Source code in d9d/loop/run/inference.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 class InferenceConfigurator : \"\"\" Orchestrates the assembly of the distributed inference environment. This class binds the infrastructure configuration (DeviceMesh), the inference parameters, and the user-defined logic (Providers) to create a fully initialized state object capable of running the inference loop. \"\"\" def __init__ ( self , mesh : DeviceMeshParameters , parameters : InferenceConfig , task_provider : InferenceTaskProvider , model_provider : ModelProvider , data_provider : DatasetProvider , ): \"\"\" Constructs a configurator capable of building the full inference state. Args: mesh: Definition of the distributed device mesh topology. parameters: The global configuration object for inference. task_provider: Factory for creating the inference task logic. model_provider: Factory for defining and creating model stages. data_provider: Factory for providing inference datasets. \"\"\" self . _mesh = mesh self . _parameters = parameters self . _task_provider = task_provider self . _model_provider = model_provider self . _data_provider = data_provider def _build_new_state ( self ) -> InferenceJobState : dist_context = self . _mesh . build () pipelining_config = PipeliningConfig ( schedule = PipelineScheduleInferenceConfig ()) set_seeds ( dist_context , seed = self . _parameters . determinism . base_seed ) timeout_manager = TimeoutManager ( dist_context = dist_context , config = self . _parameters . timeout ) timeout_manager . set_init () task = self . _task_provider ( InferenceTaskProviderContext ( dist_context = dist_context )) batch_maths = BatchMaths ( dist_context = dist_context , config_batching = self . _parameters . batching , config_pipelining = pipelining_config ) data_loader_factory = DataLoaderFactory ( dist_context = dist_context , provider = self . _data_provider , config_data_loading = self . _parameters . data_loading , batch_maths = batch_maths , ) data_loader_infer = data_loader_factory . build_dataloader_for_infer_job () stepper = Stepper ( initial_step = 1 , total_steps = len ( data_loader_infer )) pipeline_state_handler = PipelineStateHandler ( sharding_spec = {}, num_shards = batch_maths . num_microbatches_pipelining ) processor = InferenceProcessor ( state = pipeline_state_handler , task = task ) schedule , modules = ModelStageFactory ( model_provider = self . _model_provider , dist_context = dist_context , config_model = self . _parameters . model_stage_factory , config_pipelining = pipelining_config , batch_maths = batch_maths , pipeline_callback = processor , ) . build_pipeline_and_modules () task_operator = InferenceTaskOperator ( dist_context = dist_context , task = task , pipeline = schedule , pipeline_state = pipeline_state_handler ) gc = ManualGarbageCollector ( dist_ctx = dist_context , config = self . _parameters . gc , step = stepper ) checkpointer = StateCheckpointer ( dist_context = dist_context , stepper = stepper , config = self . _parameters . checkpointing , gc = gc , run_name = None ) profiler = JobProfiler ( dist_context = dist_context , stepper = stepper , config = self . _parameters . profiling ) return InferenceJobState ( dist_context = dist_context , data_loader = data_loader_infer , stepper = stepper , tracked_modules = modules , garbage_collector = gc , batch_maths = batch_maths , checkpointer = checkpointer , task = task , profiler = profiler , timeout_manager = timeout_manager , task_operator = task_operator , ) def configure ( self ) -> \"Inference\" : \"\"\" Instantiates all inference components and returns a configured Inference engine. This method triggers the creation of the distributed context, sets seeds, builds the model, data loaders, and attaches all auxiliary components. Returns: Inference: A ready-to-use inference engine instance encapsulating the job state. \"\"\" state = self . _build_new_state () return Inference ( state ) __init__ ( mesh , parameters , task_provider , model_provider , data_provider ) Constructs a configurator capable of building the full inference state. Parameters: Name Type Description Default mesh DeviceMeshParameters Definition of the distributed device mesh topology. required parameters InferenceConfig The global configuration object for inference. required task_provider InferenceTaskProvider Factory for creating the inference task logic. required model_provider ModelProvider Factory for defining and creating model stages. required data_provider DatasetProvider Factory for providing inference datasets. required Source code in d9d/loop/run/inference.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def __init__ ( self , mesh : DeviceMeshParameters , parameters : InferenceConfig , task_provider : InferenceTaskProvider , model_provider : ModelProvider , data_provider : DatasetProvider , ): \"\"\" Constructs a configurator capable of building the full inference state. Args: mesh: Definition of the distributed device mesh topology. parameters: The global configuration object for inference. task_provider: Factory for creating the inference task logic. model_provider: Factory for defining and creating model stages. data_provider: Factory for providing inference datasets. \"\"\" self . _mesh = mesh self . _parameters = parameters self . _task_provider = task_provider self . _model_provider = model_provider self . _data_provider = data_provider configure () Instantiates all inference components and returns a configured Inference engine. This method triggers the creation of the distributed context, sets seeds, builds the model, data loaders, and attaches all auxiliary components. Returns: Name Type Description Inference Inference A ready-to-use inference engine instance encapsulating the job state. Source code in d9d/loop/run/inference.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def configure ( self ) -> \"Inference\" : \"\"\" Instantiates all inference components and returns a configured Inference engine. This method triggers the creation of the distributed context, sets seeds, builds the model, data loaders, and attaches all auxiliary components. Returns: Inference: A ready-to-use inference engine instance encapsulating the job state. \"\"\" state = self . _build_new_state () return Inference ( state ) d9d.loop.run.Inference The main execution engine for running a distributed inference job. This class manages the inference loop, lifecycle events, distributed synchronization, and periodic side-effects (profiling, checkpointing). It ensures the model is in evaluation mode and runs within a torch.inference_mode context. Source code in d9d/loop/run/inference.py 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 class Inference : \"\"\" The main execution engine for running a distributed inference job. This class manages the inference loop, lifecycle events, distributed synchronization, and periodic side-effects (profiling, checkpointing). It ensures the model is in evaluation mode and runs within a `torch.inference_mode` context. \"\"\" def __init__ ( self , state : InferenceJobState ): \"\"\" Constructs an Inference engine from a pre-built job state. Args: state: The encapsulated state object containing all initialized components. \"\"\" self . _state = state def _enable_eval_mode ( self ): for module in self . _state . tracked_modules . modules : module . eval () def infer ( self ): \"\"\" Executes the full inference workflow. This method: 1. Waits for world synchronization. 2. Loads the latest checkpoint if available. 3. Iterates through the data loader. 4. Executes the pipeline forward pass for every batch. 5. Handles periodic garbage collection and profiling. 6. Finalizes the task upon completion. \"\"\" with torch . inference_mode (): self . _enable_eval_mode () self . _state . dist_context . logger . info ( \"Waiting for the world to start job\" ) self . _state . dist_context . wait_world () self . _state . dist_context . logger . info ( \"Trying to load last checkpoint before doing anything else\" ) self . _state . checkpointer . load_last_checkpoint ( self . _state ) if self . _state . stepper . current_step >= self . _state . stepper . total_steps : self . _state . dist_context . logger . info ( \"Already ran, will do nothing\" ) return self . _state . dist_context . wait_world () with ( tqdm ( desc = \"Inference\" , total = self . _state . stepper . total_steps , disable = not self . _state . dist_context . is_local_main_process , initial = self . _state . stepper . current_step , ) as bar , self . _state . garbage_collector as gc , self . _state . profiler . open () as profiler , ): for batch_group in self . _state . data_loader : for batch in batch_group : self . _state . task_operator . forward ( batch ) gc . collect_periodic () self . _state . stepper . step () bar . update () # checkpoint at the end of the step self . _state . checkpointer . checkpoint_if_needed ( self . _state ) if profiler : profiler . step () self . _state . timeout_manager . set_periodic () self . _state . task . finalize ( FinalizeContext ()) __init__ ( state ) Constructs an Inference engine from a pre-built job state. Parameters: Name Type Description Default state InferenceJobState The encapsulated state object containing all initialized components. required Source code in d9d/loop/run/inference.py 157 158 159 160 161 162 163 164 165 def __init__ ( self , state : InferenceJobState ): \"\"\" Constructs an Inference engine from a pre-built job state. Args: state: The encapsulated state object containing all initialized components. \"\"\" self . _state = state infer () Executes the full inference workflow. This method: Waits for world synchronization. Loads the latest checkpoint if available. Iterates through the data loader. Executes the pipeline forward pass for every batch. Handles periodic garbage collection and profiling. Finalizes the task upon completion. Source code in d9d/loop/run/inference.py 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 def infer ( self ): \"\"\" Executes the full inference workflow. This method: 1. Waits for world synchronization. 2. Loads the latest checkpoint if available. 3. Iterates through the data loader. 4. Executes the pipeline forward pass for every batch. 5. Handles periodic garbage collection and profiling. 6. Finalizes the task upon completion. \"\"\" with torch . inference_mode (): self . _enable_eval_mode () self . _state . dist_context . logger . info ( \"Waiting for the world to start job\" ) self . _state . dist_context . wait_world () self . _state . dist_context . logger . info ( \"Trying to load last checkpoint before doing anything else\" ) self . _state . checkpointer . load_last_checkpoint ( self . _state ) if self . _state . stepper . current_step >= self . _state . stepper . total_steps : self . _state . dist_context . logger . info ( \"Already ran, will do nothing\" ) return self . _state . dist_context . wait_world () with ( tqdm ( desc = \"Inference\" , total = self . _state . stepper . total_steps , disable = not self . _state . dist_context . is_local_main_process , initial = self . _state . stepper . current_step , ) as bar , self . _state . garbage_collector as gc , self . _state . profiler . open () as profiler , ): for batch_group in self . _state . data_loader : for batch in batch_group : self . _state . task_operator . forward ( batch ) gc . collect_periodic () self . _state . stepper . step () bar . update () # checkpoint at the end of the step self . _state . checkpointer . checkpoint_if_needed ( self . _state ) if profiler : profiler . step () self . _state . timeout_manager . set_periodic () self . _state . task . finalize ( FinalizeContext ())","title":"Inference Loop"},{"location":"0_loop/1_inference_loop/#overview","text":"The d9d.loop package provides the execution engine not only for training but also for high-scale distributed inference. The inference engine shares the same core philosophy as the Trainer: separating the definition of the job from the execution .","title":"Overview"},{"location":"0_loop/1_inference_loop/#configuration-construction","text":"The inference environment is assembled using the InferenceConfigurator .","title":"Configuration &amp; Construction"},{"location":"0_loop/1_inference_loop/#inferenceconfigurator","text":"This class binds the infrastructure and user logic into a ready-to-execute Inference object. from d9d.loop.run import InferenceConfigurator inference = InferenceConfigurator ( mesh = mesh_params , # Physical cluster layout parameters = config , # Logic configuration (batch size, etc) model_provider =... , # Same provider used in training task_provider =... , # Inference-specific logic (e.g., generation) data_provider =... , # Validation/Test dataset ) . configure ()","title":"InferenceConfigurator"},{"location":"0_loop/1_inference_loop/#the-configuration-lifecycle","text":"The InferenceConfigurator.configure() method performs a setup sequence similar to training, but optimized for forward-only execution: Distributed Context Initialization : Constructs the global DistributedContext . Seeding : Sets distributed seeds. Determinism is crucial in inference for reproducible sampling or validation splits. Task Instantiation : Instantiates the InferenceTask . This defines how inputs are processed and what to do with the outputs (e.g., writing to a JSONL file). Data Loader Construction : Creates a distributed DataLoader that handles sharding the inference dataset across ranks. Model Materialization : The ModelStageFactory runs to build the model. Note : This reuses the exact same ModelProvider as training. State Assembly : Components are packed into InferenceJobState . The Inference engine is instantiated.","title":"The Configuration Lifecycle"},{"location":"0_loop/1_inference_loop/#inference-execution","text":"To run the job, call the .infer() method on the configured object.","title":"Inference Execution"},{"location":"0_loop/1_inference_loop/#the-inference-lifecycle","text":"The Inference.infer() method orchestrates the execution flow. It is designed to be lean and memory-efficient.","title":"The Inference Lifecycle"},{"location":"0_loop/1_inference_loop/#1-initialization-recovery","text":"Before the loop starts: Mode Switching : Enables torch.inference_mode() . This disables gradient calculation globally, saving significant memory. Sets all model modules to .eval() mode (affecting Dropout, BatchNorm, etc.). State Loading : The StateCheckpointer loads the model weights from the specified checkpoint. If the job was interrupted previously, it also restores the Stepper and DataLoader state to resume exactly where it left off. Context Entry : Enters UI, Garbage Collector, and Profiler contexts.","title":"1. Initialization &amp; Recovery"},{"location":"0_loop/1_inference_loop/#2-the-step-loop","text":"For every step: Microbatch Execution : The DataLoader yields a batch group. The InferenceTaskOperator manages the execution. Data is fed through the model. Unlike training, no backward pass is performed. Maintenance : GC : ManualGarbageCollector runs periodically to ensure peak memory usage is controlled. Advance : The Stepper increments. Checkpointing : If configured, the system saves the progress of the inference job. This allows restarting a long-running generation job on a massive dataset without re-processing the first half.","title":"2. The Step Loop"},{"location":"0_loop/1_inference_loop/#3-finalization","text":"Task-specific : Calls InferenceTask.finalize() . This is typically used to close file handles (e.g., flushing the final lines of a generated dataset to disk).","title":"3. Finalization"},{"location":"0_loop/1_inference_loop/#api-reference","text":"","title":"API Reference"},{"location":"0_loop/1_inference_loop/#d9d.loop.run.InferenceConfigurator","text":"Orchestrates the assembly of the distributed inference environment. This class binds the infrastructure configuration (DeviceMesh), the inference parameters, and the user-defined logic (Providers) to create a fully initialized state object capable of running the inference loop. Source code in d9d/loop/run/inference.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 class InferenceConfigurator : \"\"\" Orchestrates the assembly of the distributed inference environment. This class binds the infrastructure configuration (DeviceMesh), the inference parameters, and the user-defined logic (Providers) to create a fully initialized state object capable of running the inference loop. \"\"\" def __init__ ( self , mesh : DeviceMeshParameters , parameters : InferenceConfig , task_provider : InferenceTaskProvider , model_provider : ModelProvider , data_provider : DatasetProvider , ): \"\"\" Constructs a configurator capable of building the full inference state. Args: mesh: Definition of the distributed device mesh topology. parameters: The global configuration object for inference. task_provider: Factory for creating the inference task logic. model_provider: Factory for defining and creating model stages. data_provider: Factory for providing inference datasets. \"\"\" self . _mesh = mesh self . _parameters = parameters self . _task_provider = task_provider self . _model_provider = model_provider self . _data_provider = data_provider def _build_new_state ( self ) -> InferenceJobState : dist_context = self . _mesh . build () pipelining_config = PipeliningConfig ( schedule = PipelineScheduleInferenceConfig ()) set_seeds ( dist_context , seed = self . _parameters . determinism . base_seed ) timeout_manager = TimeoutManager ( dist_context = dist_context , config = self . _parameters . timeout ) timeout_manager . set_init () task = self . _task_provider ( InferenceTaskProviderContext ( dist_context = dist_context )) batch_maths = BatchMaths ( dist_context = dist_context , config_batching = self . _parameters . batching , config_pipelining = pipelining_config ) data_loader_factory = DataLoaderFactory ( dist_context = dist_context , provider = self . _data_provider , config_data_loading = self . _parameters . data_loading , batch_maths = batch_maths , ) data_loader_infer = data_loader_factory . build_dataloader_for_infer_job () stepper = Stepper ( initial_step = 1 , total_steps = len ( data_loader_infer )) pipeline_state_handler = PipelineStateHandler ( sharding_spec = {}, num_shards = batch_maths . num_microbatches_pipelining ) processor = InferenceProcessor ( state = pipeline_state_handler , task = task ) schedule , modules = ModelStageFactory ( model_provider = self . _model_provider , dist_context = dist_context , config_model = self . _parameters . model_stage_factory , config_pipelining = pipelining_config , batch_maths = batch_maths , pipeline_callback = processor , ) . build_pipeline_and_modules () task_operator = InferenceTaskOperator ( dist_context = dist_context , task = task , pipeline = schedule , pipeline_state = pipeline_state_handler ) gc = ManualGarbageCollector ( dist_ctx = dist_context , config = self . _parameters . gc , step = stepper ) checkpointer = StateCheckpointer ( dist_context = dist_context , stepper = stepper , config = self . _parameters . checkpointing , gc = gc , run_name = None ) profiler = JobProfiler ( dist_context = dist_context , stepper = stepper , config = self . _parameters . profiling ) return InferenceJobState ( dist_context = dist_context , data_loader = data_loader_infer , stepper = stepper , tracked_modules = modules , garbage_collector = gc , batch_maths = batch_maths , checkpointer = checkpointer , task = task , profiler = profiler , timeout_manager = timeout_manager , task_operator = task_operator , ) def configure ( self ) -> \"Inference\" : \"\"\" Instantiates all inference components and returns a configured Inference engine. This method triggers the creation of the distributed context, sets seeds, builds the model, data loaders, and attaches all auxiliary components. Returns: Inference: A ready-to-use inference engine instance encapsulating the job state. \"\"\" state = self . _build_new_state () return Inference ( state )","title":"InferenceConfigurator"},{"location":"0_loop/1_inference_loop/#d9d.loop.run.InferenceConfigurator.__init__","text":"Constructs a configurator capable of building the full inference state. Parameters: Name Type Description Default mesh DeviceMeshParameters Definition of the distributed device mesh topology. required parameters InferenceConfig The global configuration object for inference. required task_provider InferenceTaskProvider Factory for creating the inference task logic. required model_provider ModelProvider Factory for defining and creating model stages. required data_provider DatasetProvider Factory for providing inference datasets. required Source code in d9d/loop/run/inference.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def __init__ ( self , mesh : DeviceMeshParameters , parameters : InferenceConfig , task_provider : InferenceTaskProvider , model_provider : ModelProvider , data_provider : DatasetProvider , ): \"\"\" Constructs a configurator capable of building the full inference state. Args: mesh: Definition of the distributed device mesh topology. parameters: The global configuration object for inference. task_provider: Factory for creating the inference task logic. model_provider: Factory for defining and creating model stages. data_provider: Factory for providing inference datasets. \"\"\" self . _mesh = mesh self . _parameters = parameters self . _task_provider = task_provider self . _model_provider = model_provider self . _data_provider = data_provider","title":"__init__"},{"location":"0_loop/1_inference_loop/#d9d.loop.run.InferenceConfigurator.configure","text":"Instantiates all inference components and returns a configured Inference engine. This method triggers the creation of the distributed context, sets seeds, builds the model, data loaders, and attaches all auxiliary components. Returns: Name Type Description Inference Inference A ready-to-use inference engine instance encapsulating the job state. Source code in d9d/loop/run/inference.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def configure ( self ) -> \"Inference\" : \"\"\" Instantiates all inference components and returns a configured Inference engine. This method triggers the creation of the distributed context, sets seeds, builds the model, data loaders, and attaches all auxiliary components. Returns: Inference: A ready-to-use inference engine instance encapsulating the job state. \"\"\" state = self . _build_new_state () return Inference ( state )","title":"configure"},{"location":"0_loop/1_inference_loop/#d9d.loop.run.Inference","text":"The main execution engine for running a distributed inference job. This class manages the inference loop, lifecycle events, distributed synchronization, and periodic side-effects (profiling, checkpointing). It ensures the model is in evaluation mode and runs within a torch.inference_mode context. Source code in d9d/loop/run/inference.py 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 class Inference : \"\"\" The main execution engine for running a distributed inference job. This class manages the inference loop, lifecycle events, distributed synchronization, and periodic side-effects (profiling, checkpointing). It ensures the model is in evaluation mode and runs within a `torch.inference_mode` context. \"\"\" def __init__ ( self , state : InferenceJobState ): \"\"\" Constructs an Inference engine from a pre-built job state. Args: state: The encapsulated state object containing all initialized components. \"\"\" self . _state = state def _enable_eval_mode ( self ): for module in self . _state . tracked_modules . modules : module . eval () def infer ( self ): \"\"\" Executes the full inference workflow. This method: 1. Waits for world synchronization. 2. Loads the latest checkpoint if available. 3. Iterates through the data loader. 4. Executes the pipeline forward pass for every batch. 5. Handles periodic garbage collection and profiling. 6. Finalizes the task upon completion. \"\"\" with torch . inference_mode (): self . _enable_eval_mode () self . _state . dist_context . logger . info ( \"Waiting for the world to start job\" ) self . _state . dist_context . wait_world () self . _state . dist_context . logger . info ( \"Trying to load last checkpoint before doing anything else\" ) self . _state . checkpointer . load_last_checkpoint ( self . _state ) if self . _state . stepper . current_step >= self . _state . stepper . total_steps : self . _state . dist_context . logger . info ( \"Already ran, will do nothing\" ) return self . _state . dist_context . wait_world () with ( tqdm ( desc = \"Inference\" , total = self . _state . stepper . total_steps , disable = not self . _state . dist_context . is_local_main_process , initial = self . _state . stepper . current_step , ) as bar , self . _state . garbage_collector as gc , self . _state . profiler . open () as profiler , ): for batch_group in self . _state . data_loader : for batch in batch_group : self . _state . task_operator . forward ( batch ) gc . collect_periodic () self . _state . stepper . step () bar . update () # checkpoint at the end of the step self . _state . checkpointer . checkpoint_if_needed ( self . _state ) if profiler : profiler . step () self . _state . timeout_manager . set_periodic () self . _state . task . finalize ( FinalizeContext ())","title":"Inference"},{"location":"0_loop/1_inference_loop/#d9d.loop.run.Inference.__init__","text":"Constructs an Inference engine from a pre-built job state. Parameters: Name Type Description Default state InferenceJobState The encapsulated state object containing all initialized components. required Source code in d9d/loop/run/inference.py 157 158 159 160 161 162 163 164 165 def __init__ ( self , state : InferenceJobState ): \"\"\" Constructs an Inference engine from a pre-built job state. Args: state: The encapsulated state object containing all initialized components. \"\"\" self . _state = state","title":"__init__"},{"location":"0_loop/1_inference_loop/#d9d.loop.run.Inference.infer","text":"Executes the full inference workflow. This method: Waits for world synchronization. Loads the latest checkpoint if available. Iterates through the data loader. Executes the pipeline forward pass for every batch. Handles periodic garbage collection and profiling. Finalizes the task upon completion. Source code in d9d/loop/run/inference.py 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 def infer ( self ): \"\"\" Executes the full inference workflow. This method: 1. Waits for world synchronization. 2. Loads the latest checkpoint if available. 3. Iterates through the data loader. 4. Executes the pipeline forward pass for every batch. 5. Handles periodic garbage collection and profiling. 6. Finalizes the task upon completion. \"\"\" with torch . inference_mode (): self . _enable_eval_mode () self . _state . dist_context . logger . info ( \"Waiting for the world to start job\" ) self . _state . dist_context . wait_world () self . _state . dist_context . logger . info ( \"Trying to load last checkpoint before doing anything else\" ) self . _state . checkpointer . load_last_checkpoint ( self . _state ) if self . _state . stepper . current_step >= self . _state . stepper . total_steps : self . _state . dist_context . logger . info ( \"Already ran, will do nothing\" ) return self . _state . dist_context . wait_world () with ( tqdm ( desc = \"Inference\" , total = self . _state . stepper . total_steps , disable = not self . _state . dist_context . is_local_main_process , initial = self . _state . stepper . current_step , ) as bar , self . _state . garbage_collector as gc , self . _state . profiler . open () as profiler , ): for batch_group in self . _state . data_loader : for batch in batch_group : self . _state . task_operator . forward ( batch ) gc . collect_periodic () self . _state . stepper . step () bar . update () # checkpoint at the end of the step self . _state . checkpointer . checkpoint_if_needed ( self . _state ) if profiler : profiler . step () self . _state . timeout_manager . set_periodic () self . _state . task . finalize ( FinalizeContext ())","title":"infer"},{"location":"0_loop/config/","text":"The d9d.loop.config package defines the structure for configuring the training job using Pydantic models. This ensures strict validation of configurations (e.g., ensuring global batch size is divisible by microbatch size and DP size). Main Config TrainerConfig The top-level configuration object passed to TrainingConfigurator . d9d.loop.config.TrainerConfig Bases: BaseModel Top-level configuration object defining a complete training job. Attributes: Name Type Description run RunConfig Meta-information about the run (name, ID, tags). batching BatchingConfig Batch sizing strategy. data_loading DataLoadingConfig DataLoader settings. logging JobLoggerConfig Experiment tracking settings. pipelining PipeliningConfig Pipeline Parallelism schedule and settings. If None, pipeline parallelism is disabled. model_stage_factory ModelStageFactoryConfig Model initialization and additional checkpointing logic. determinism DeterminismConfig Random seed settings. gc GarbageCollectionConfig Garbage collection settings. checkpointing CheckpointingConfig Checkpoint saving settings. gradient_clipping GradientClippingConfig Gradient clipping settings. profiling ProfilingConfig | None Profiler settings. gradient_manager GradientManagerConfig Gradient Synchronization Settings. timeout TimeoutConfig Distributed timeout settings. Source code in d9d/loop/config/config.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 class TrainerConfig ( BaseModel ): \"\"\" Top-level configuration object defining a complete training job. Attributes: run: Meta-information about the run (name, ID, tags). batching: Batch sizing strategy. data_loading: DataLoader settings. logging: Experiment tracking settings. pipelining: Pipeline Parallelism schedule and settings. If None, pipeline parallelism is disabled. model_stage_factory: Model initialization and additional checkpointing logic. determinism: Random seed settings. gc: Garbage collection settings. checkpointing: Checkpoint saving settings. gradient_clipping: Gradient clipping settings. profiling: Profiler settings. gradient_manager: Gradient Synchronization Settings. timeout: Distributed timeout settings. \"\"\" run : RunConfig batching : BatchingConfig data_loading : DataLoadingConfig logging : JobLoggerConfig pipelining : PipeliningConfig model_stage_factory : ModelStageFactoryConfig determinism : DeterminismConfig gc : GarbageCollectionConfig checkpointing : CheckpointingConfig gradient_clipping : GradientClippingConfig profiling : ProfilingConfig | None gradient_manager : GradientManagerConfig timeout : TimeoutConfig InferenceConfig The top-level configuration object passed to InferenceConfigurator . d9d.loop.config.InferenceConfig Bases: BaseModel Top-level configuration object defining an inference/evaluation job. Attributes: Name Type Description batching BatchingConfig Batch sizing strategy. data_loading DataLoadingConfig DataLoader settings. model_stage_factory ModelStageFactoryConfig Model initialization logic. determinism DeterminismConfig Random seed settings. gc GarbageCollectionConfig Garbage collection settings. checkpointing CheckpointingConfig Checkpointing settings. profiling ProfilingConfig | None Profiler settings. timeout TimeoutConfig Distributed timeout settings. Source code in d9d/loop/config/config.py 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 class InferenceConfig ( BaseModel ): \"\"\" Top-level configuration object defining an inference/evaluation job. Attributes: batching: Batch sizing strategy. data_loading: DataLoader settings. model_stage_factory: Model initialization logic. determinism: Random seed settings. gc: Garbage collection settings. checkpointing: Checkpointing settings. profiling: Profiler settings. timeout: Distributed timeout settings. \"\"\" batching : BatchingConfig data_loading : DataLoadingConfig model_stage_factory : ModelStageFactoryConfig determinism : DeterminismConfig gc : GarbageCollectionConfig checkpointing : CheckpointingConfig profiling : ProfilingConfig | None timeout : TimeoutConfig Sub-Configurations Diagnostics & Reproducibility d9d.tracker.RunConfig Bases: BaseModel Configuration for initializing a specific logged run. Attributes: Name Type Description name str The display name of the experiment. description str | None An optional description of the experiment. hparams dict [ str , Any ] A dictionary of hyperparameters to log at the start of the run. Source code in d9d/tracker/base.py 69 70 71 72 73 74 75 76 77 78 79 80 81 class RunConfig ( BaseModel ): \"\"\" Configuration for initializing a specific logged run. Attributes: name: The display name of the experiment. description: An optional description of the experiment. hparams: A dictionary of hyperparameters to log at the start of the run. \"\"\" name : str description : str | None hparams : dict [ str , Any ] = Field ( default_factory = dict ) d9d.loop.config.JobLoggerConfig Bases: BaseModel Configuration for experiment tracking and logging. Attributes: Name Type Description period_steps StepActionPeriod How frequently metrics are flushed to the logger. tracker AnyTrackerConfig Logic for the specific tracking backend (e.g., WandB, MLflow, stdout). Source code in d9d/loop/config/config.py 142 143 144 145 146 147 148 149 150 151 152 class JobLoggerConfig ( BaseModel ): \"\"\" Configuration for experiment tracking and logging. Attributes: period_steps: How frequently metrics are flushed to the logger. tracker: Logic for the specific tracking backend (e.g., WandB, MLflow, stdout). \"\"\" period_steps : StepActionPeriod tracker : AnyTrackerConfig d9d.loop.config.ProfilingConfig Bases: BaseModel Configuration for the PyTorch Profiler. Attributes: Name Type Description enabled bool Whether to enable the profiler. traces_dir Path Directory where trace files will be saved. period_steps int Total length of a profiling cycle (wait + warmup + active). warmup_steps int Number of steps to ignore before recording to allow for warming-up. active_steps int Number of steps to actively record traces. Source code in d9d/loop/config/config.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 class ProfilingConfig ( BaseModel ): \"\"\" Configuration for the PyTorch Profiler. Attributes: enabled: Whether to enable the profiler. traces_dir: Directory where trace files will be saved. period_steps: Total length of a profiling cycle (wait + warmup + active). warmup_steps: Number of steps to ignore before recording to allow for warming-up. active_steps: Number of steps to actively record traces. \"\"\" enabled : bool traces_dir : Path period_steps : int warmup_steps : int active_steps : int d9d.loop.config.DeterminismConfig Bases: BaseModel Configuration for reproducibility and random number generation. Attributes: Name Type Description base_seed int The base integer seed used to initialize random number generators (Python, NumPy, PyTorch) across all ranks. Source code in d9d/loop/config/config.py 26 27 28 29 30 31 32 33 34 35 class DeterminismConfig ( BaseModel ): \"\"\" Configuration for reproducibility and random number generation. Attributes: base_seed: The base integer seed used to initialize random number generators (Python, NumPy, PyTorch) across all ranks. \"\"\" base_seed : int Experiment Trackers d9d . tracker . AnyTrackerConfig = Annotated [ AimConfig | NullTrackerConfig , Field ( discriminator = 'provider' )] module-attribute d9d.tracker.provider.null.NullTrackerConfig Bases: BaseModel Configuration for the Null (no-op) tracker. Attributes: Name Type Description provider Literal ['null'] Discriminator field, must be 'null'. Source code in d9d/tracker/provider/null.py 11 12 13 14 15 16 17 18 19 class NullTrackerConfig ( BaseModel ): \"\"\" Configuration for the Null (no-op) tracker. Attributes: provider: Discriminator field, must be 'null'. \"\"\" provider : Literal [ \"null\" ] = \"null\" d9d.tracker.provider.aim.config.AimConfig Bases: BaseModel Configuration for the Aim tracker backend. Attributes: Name Type Description provider Literal ['aim'] Discriminator field, must be 'aim'. repo str Path to the Aim repository directory or URL. log_system_params bool Whether to log system resource usage (CPU/GPU/Memory). capture_terminal_logs bool Whether to capture stdout/stderr. system_tracking_interval int Interval in seconds for system monitoring. Source code in d9d/tracker/provider/aim/config.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 class AimConfig ( BaseModel ): \"\"\" Configuration for the Aim tracker backend. Attributes: provider: Discriminator field, must be 'aim'. repo: Path to the Aim repository directory or URL. log_system_params: Whether to log system resource usage (CPU/GPU/Memory). capture_terminal_logs: Whether to capture stdout/stderr. system_tracking_interval: Interval in seconds for system monitoring. \"\"\" provider : Literal [ \"aim\" ] = \"aim\" repo : str log_system_params : bool = True capture_terminal_logs : bool = True system_tracking_interval : int = 10 Batching & Data d9d.loop.config.BatchingConfig Bases: BaseModel Configuration for batch sizing logic. Attributes: Name Type Description global_batch_size int The total effective batch size across all distributed replicas and gradient accumulation steps. microbatch_size int The distinct batch size fed into the model during a single forward pass on a single device. Source code in d9d/loop/config/config.py 11 12 13 14 15 16 17 18 19 20 21 22 23 class BatchingConfig ( BaseModel ): \"\"\" Configuration for batch sizing logic. Attributes: global_batch_size: The total effective batch size across all distributed replicas and gradient accumulation steps. microbatch_size: The distinct batch size fed into the model during a single forward pass on a single device. \"\"\" global_batch_size : int microbatch_size : int d9d.loop.config.DataLoadingConfig Bases: BaseModel Configuration for PyTorch DataLoaders. Attributes: Name Type Description num_workers int The number of subprocesses to use for data loading. pin_memory bool Whether to copy tensors into CUDA pinned memory before returning them. persistent_workers bool If True, the data loader will not shutdown the worker processes after a dataset has been consumed once. Source code in d9d/loop/config/config.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 class DataLoadingConfig ( BaseModel ): \"\"\" Configuration for PyTorch DataLoaders. Attributes: num_workers: The number of subprocesses to use for data loading. pin_memory: Whether to copy tensors into CUDA pinned memory before returning them. persistent_workers: If True, the data loader will not shutdown the worker processes after a dataset has been consumed once. \"\"\" num_workers : int pin_memory : bool persistent_workers : bool Checkpointing d9d.loop.config.CheckpointingConfig Bases: BaseModel Configuration for saving model snapshots. Attributes: Name Type Description save_dir Path The root directory where checkpoints will be stored. period_steps StepActionPeriod How frequently to save a checkpoint. num_to_keep int | None The maximum number of recent checkpoints to retain. If None, all checkpoints are kept. Source code in d9d/loop/config/config.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 class CheckpointingConfig ( BaseModel ): \"\"\" Configuration for saving model snapshots. Attributes: save_dir: The root directory where checkpoints will be stored. period_steps: How frequently to save a checkpoint. num_to_keep: The maximum number of recent checkpoints to retain. If None, all checkpoints are kept. \"\"\" save_dir : Path period_steps : StepActionPeriod num_to_keep : int | None Model Initialization d9d.loop.config.ModelStageFactoryConfig Bases: BaseModel Configuration for initializing model weights. Attributes: Name Type Description source_checkpoint Path | None Path to an initial checkpoint to load into the model before training starts. If None, random initialization is used. checkpoint_only_trainable_parameters bool If True, only parameters with requires_grad=True will be saved in checkpoints. Useful for PEFT/LoRA. Source code in d9d/loop/config/config.py 92 93 94 95 96 97 98 99 100 101 102 103 104 class ModelStageFactoryConfig ( BaseModel ): \"\"\" Configuration for initializing model weights. Attributes: source_checkpoint: Path to an initial checkpoint to load into the model before training starts. If None, random initialization is used. checkpoint_only_trainable_parameters: If True, only parameters with requires_grad=True will be saved in checkpoints. Useful for PEFT/LoRA. \"\"\" source_checkpoint : Path | None checkpoint_only_trainable_parameters : bool Optimization d9d.loop.config.GradientClippingConfig Bases: BaseModel Configuration for gradient norm clipping. Attributes: Name Type Description max_norm float | None The maximum norm value for gradient clipping. If None, no clipping is performed. log_total_steps StepActionPeriod Frequency at which to log the total gradient norm. Source code in d9d/loop/config/config.py 107 108 109 110 111 112 113 114 115 116 117 118 class GradientClippingConfig ( BaseModel ): \"\"\" Configuration for gradient norm clipping. Attributes: max_norm: The maximum norm value for gradient clipping. If None, no clipping is performed. log_total_steps: Frequency at which to log the total gradient norm. \"\"\" max_norm : float | None log_total_steps : StepActionPeriod d9d.loop.config.GradientManagerConfig Bases: BaseModel Configuration for gradient synchronization. Attributes: Name Type Description grad_dtype str | None The data type to use for storing the gradient. If None, follows the model's dtype. bucket_size_mb int The size of gradient buckets in Megabytes for communication. Source code in d9d/loop/config/config.py 155 156 157 158 159 160 161 162 163 164 165 class GradientManagerConfig ( BaseModel ): \"\"\" Configuration for gradient synchronization. Attributes: grad_dtype: The data type to use for storing the gradient. If None, follows the model's dtype. bucket_size_mb: The size of gradient buckets in Megabytes for communication. \"\"\" grad_dtype : str | None bucket_size_mb : int Infrastructure d9d.loop.config.PipeliningConfig Bases: BaseModel Configuration for pipeline parallelism orchestration. Attributes: Name Type Description schedule AnyPipelineScheduleConfig The specific scheduling strategy configuration used to manage pipeline execution. Source code in d9d/loop/config/config.py 38 39 40 41 42 43 44 45 46 class PipeliningConfig ( BaseModel ): \"\"\" Configuration for pipeline parallelism orchestration. Attributes: schedule: The specific scheduling strategy configuration used to manage pipeline execution. \"\"\" schedule : AnyPipelineScheduleConfig d9d.loop.config.GarbageCollectionConfig Bases: BaseModel Configuration for manual Python garbage collection control. Attributes: Name Type Description period_steps StepActionPeriod How frequently to manually trigger the Python garbage collector. Source code in d9d/loop/config/config.py 49 50 51 52 53 54 55 56 57 class GarbageCollectionConfig ( BaseModel ): \"\"\" Configuration for manual Python garbage collection control. Attributes: period_steps: How frequently to manually trigger the Python garbage collector. \"\"\" period_steps : StepActionPeriod d9d.loop.config.TimeoutConfig Bases: BaseModel Configuration for distributed process group timeouts. Attributes: Name Type Description init_timeout int Timeout in seconds for initializing the process group. step_timeout int Timeout in seconds for individual step communications. Source code in d9d/loop/config/config.py 168 169 170 171 172 173 174 175 176 177 178 class TimeoutConfig ( BaseModel ): \"\"\" Configuration for distributed process group timeouts. Attributes: init_timeout: Timeout in seconds for initializing the process group. step_timeout: Timeout in seconds for individual step communications. \"\"\" init_timeout : int = 10000 step_timeout : int = 100 Types d9d . loop . config . StepActionPeriod = int | StepActionSpecial module-attribute Union type representing a configuration for periodic events. Values int: The period in steps (frequency) at which the event occurs. StepActionSpecial: A special flag indicating end-of-run execution or disabling. d9d.loop.config.StepActionSpecial Bases: StrEnum Special flag values for configuring periodic actions. Attributes: Name Type Description last_step Indicates the action should occur exactly once at the very end of the training run. disable Indicates the action should never occur. Source code in d9d/loop/config/types.py 4 5 6 7 8 9 10 11 12 13 14 15 class StepActionSpecial ( StrEnum ): \"\"\" Special flag values for configuring periodic actions. Attributes: last_step: Indicates the action should occur exactly once at the very end of the training run. disable: Indicates the action should never occur. \"\"\" last_step = \"last_step\" disable = \"disable\"","title":"Configuration"},{"location":"0_loop/config/#main-config","text":"","title":"Main Config"},{"location":"0_loop/config/#trainerconfig","text":"The top-level configuration object passed to TrainingConfigurator .","title":"TrainerConfig"},{"location":"0_loop/config/#d9d.loop.config.TrainerConfig","text":"Bases: BaseModel Top-level configuration object defining a complete training job. Attributes: Name Type Description run RunConfig Meta-information about the run (name, ID, tags). batching BatchingConfig Batch sizing strategy. data_loading DataLoadingConfig DataLoader settings. logging JobLoggerConfig Experiment tracking settings. pipelining PipeliningConfig Pipeline Parallelism schedule and settings. If None, pipeline parallelism is disabled. model_stage_factory ModelStageFactoryConfig Model initialization and additional checkpointing logic. determinism DeterminismConfig Random seed settings. gc GarbageCollectionConfig Garbage collection settings. checkpointing CheckpointingConfig Checkpoint saving settings. gradient_clipping GradientClippingConfig Gradient clipping settings. profiling ProfilingConfig | None Profiler settings. gradient_manager GradientManagerConfig Gradient Synchronization Settings. timeout TimeoutConfig Distributed timeout settings. Source code in d9d/loop/config/config.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 class TrainerConfig ( BaseModel ): \"\"\" Top-level configuration object defining a complete training job. Attributes: run: Meta-information about the run (name, ID, tags). batching: Batch sizing strategy. data_loading: DataLoader settings. logging: Experiment tracking settings. pipelining: Pipeline Parallelism schedule and settings. If None, pipeline parallelism is disabled. model_stage_factory: Model initialization and additional checkpointing logic. determinism: Random seed settings. gc: Garbage collection settings. checkpointing: Checkpoint saving settings. gradient_clipping: Gradient clipping settings. profiling: Profiler settings. gradient_manager: Gradient Synchronization Settings. timeout: Distributed timeout settings. \"\"\" run : RunConfig batching : BatchingConfig data_loading : DataLoadingConfig logging : JobLoggerConfig pipelining : PipeliningConfig model_stage_factory : ModelStageFactoryConfig determinism : DeterminismConfig gc : GarbageCollectionConfig checkpointing : CheckpointingConfig gradient_clipping : GradientClippingConfig profiling : ProfilingConfig | None gradient_manager : GradientManagerConfig timeout : TimeoutConfig","title":"TrainerConfig"},{"location":"0_loop/config/#inferenceconfig","text":"The top-level configuration object passed to InferenceConfigurator .","title":"InferenceConfig"},{"location":"0_loop/config/#d9d.loop.config.InferenceConfig","text":"Bases: BaseModel Top-level configuration object defining an inference/evaluation job. Attributes: Name Type Description batching BatchingConfig Batch sizing strategy. data_loading DataLoadingConfig DataLoader settings. model_stage_factory ModelStageFactoryConfig Model initialization logic. determinism DeterminismConfig Random seed settings. gc GarbageCollectionConfig Garbage collection settings. checkpointing CheckpointingConfig Checkpointing settings. profiling ProfilingConfig | None Profiler settings. timeout TimeoutConfig Distributed timeout settings. Source code in d9d/loop/config/config.py 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 class InferenceConfig ( BaseModel ): \"\"\" Top-level configuration object defining an inference/evaluation job. Attributes: batching: Batch sizing strategy. data_loading: DataLoader settings. model_stage_factory: Model initialization logic. determinism: Random seed settings. gc: Garbage collection settings. checkpointing: Checkpointing settings. profiling: Profiler settings. timeout: Distributed timeout settings. \"\"\" batching : BatchingConfig data_loading : DataLoadingConfig model_stage_factory : ModelStageFactoryConfig determinism : DeterminismConfig gc : GarbageCollectionConfig checkpointing : CheckpointingConfig profiling : ProfilingConfig | None timeout : TimeoutConfig","title":"InferenceConfig"},{"location":"0_loop/config/#sub-configurations","text":"","title":"Sub-Configurations"},{"location":"0_loop/config/#diagnostics-reproducibility","text":"","title":"Diagnostics &amp; Reproducibility"},{"location":"0_loop/config/#d9d.tracker.RunConfig","text":"Bases: BaseModel Configuration for initializing a specific logged run. Attributes: Name Type Description name str The display name of the experiment. description str | None An optional description of the experiment. hparams dict [ str , Any ] A dictionary of hyperparameters to log at the start of the run. Source code in d9d/tracker/base.py 69 70 71 72 73 74 75 76 77 78 79 80 81 class RunConfig ( BaseModel ): \"\"\" Configuration for initializing a specific logged run. Attributes: name: The display name of the experiment. description: An optional description of the experiment. hparams: A dictionary of hyperparameters to log at the start of the run. \"\"\" name : str description : str | None hparams : dict [ str , Any ] = Field ( default_factory = dict )","title":"RunConfig"},{"location":"0_loop/config/#d9d.loop.config.JobLoggerConfig","text":"Bases: BaseModel Configuration for experiment tracking and logging. Attributes: Name Type Description period_steps StepActionPeriod How frequently metrics are flushed to the logger. tracker AnyTrackerConfig Logic for the specific tracking backend (e.g., WandB, MLflow, stdout). Source code in d9d/loop/config/config.py 142 143 144 145 146 147 148 149 150 151 152 class JobLoggerConfig ( BaseModel ): \"\"\" Configuration for experiment tracking and logging. Attributes: period_steps: How frequently metrics are flushed to the logger. tracker: Logic for the specific tracking backend (e.g., WandB, MLflow, stdout). \"\"\" period_steps : StepActionPeriod tracker : AnyTrackerConfig","title":"JobLoggerConfig"},{"location":"0_loop/config/#d9d.loop.config.ProfilingConfig","text":"Bases: BaseModel Configuration for the PyTorch Profiler. Attributes: Name Type Description enabled bool Whether to enable the profiler. traces_dir Path Directory where trace files will be saved. period_steps int Total length of a profiling cycle (wait + warmup + active). warmup_steps int Number of steps to ignore before recording to allow for warming-up. active_steps int Number of steps to actively record traces. Source code in d9d/loop/config/config.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 class ProfilingConfig ( BaseModel ): \"\"\" Configuration for the PyTorch Profiler. Attributes: enabled: Whether to enable the profiler. traces_dir: Directory where trace files will be saved. period_steps: Total length of a profiling cycle (wait + warmup + active). warmup_steps: Number of steps to ignore before recording to allow for warming-up. active_steps: Number of steps to actively record traces. \"\"\" enabled : bool traces_dir : Path period_steps : int warmup_steps : int active_steps : int","title":"ProfilingConfig"},{"location":"0_loop/config/#d9d.loop.config.DeterminismConfig","text":"Bases: BaseModel Configuration for reproducibility and random number generation. Attributes: Name Type Description base_seed int The base integer seed used to initialize random number generators (Python, NumPy, PyTorch) across all ranks. Source code in d9d/loop/config/config.py 26 27 28 29 30 31 32 33 34 35 class DeterminismConfig ( BaseModel ): \"\"\" Configuration for reproducibility and random number generation. Attributes: base_seed: The base integer seed used to initialize random number generators (Python, NumPy, PyTorch) across all ranks. \"\"\" base_seed : int","title":"DeterminismConfig"},{"location":"0_loop/config/#experiment-trackers","text":"","title":"Experiment Trackers"},{"location":"0_loop/config/#d9d.tracker.AnyTrackerConfig","text":"","title":"AnyTrackerConfig"},{"location":"0_loop/config/#d9d.tracker.provider.null.NullTrackerConfig","text":"Bases: BaseModel Configuration for the Null (no-op) tracker. Attributes: Name Type Description provider Literal ['null'] Discriminator field, must be 'null'. Source code in d9d/tracker/provider/null.py 11 12 13 14 15 16 17 18 19 class NullTrackerConfig ( BaseModel ): \"\"\" Configuration for the Null (no-op) tracker. Attributes: provider: Discriminator field, must be 'null'. \"\"\" provider : Literal [ \"null\" ] = \"null\"","title":"NullTrackerConfig"},{"location":"0_loop/config/#d9d.tracker.provider.aim.config.AimConfig","text":"Bases: BaseModel Configuration for the Aim tracker backend. Attributes: Name Type Description provider Literal ['aim'] Discriminator field, must be 'aim'. repo str Path to the Aim repository directory or URL. log_system_params bool Whether to log system resource usage (CPU/GPU/Memory). capture_terminal_logs bool Whether to capture stdout/stderr. system_tracking_interval int Interval in seconds for system monitoring. Source code in d9d/tracker/provider/aim/config.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 class AimConfig ( BaseModel ): \"\"\" Configuration for the Aim tracker backend. Attributes: provider: Discriminator field, must be 'aim'. repo: Path to the Aim repository directory or URL. log_system_params: Whether to log system resource usage (CPU/GPU/Memory). capture_terminal_logs: Whether to capture stdout/stderr. system_tracking_interval: Interval in seconds for system monitoring. \"\"\" provider : Literal [ \"aim\" ] = \"aim\" repo : str log_system_params : bool = True capture_terminal_logs : bool = True system_tracking_interval : int = 10","title":"AimConfig"},{"location":"0_loop/config/#batching-data","text":"","title":"Batching &amp; Data"},{"location":"0_loop/config/#d9d.loop.config.BatchingConfig","text":"Bases: BaseModel Configuration for batch sizing logic. Attributes: Name Type Description global_batch_size int The total effective batch size across all distributed replicas and gradient accumulation steps. microbatch_size int The distinct batch size fed into the model during a single forward pass on a single device. Source code in d9d/loop/config/config.py 11 12 13 14 15 16 17 18 19 20 21 22 23 class BatchingConfig ( BaseModel ): \"\"\" Configuration for batch sizing logic. Attributes: global_batch_size: The total effective batch size across all distributed replicas and gradient accumulation steps. microbatch_size: The distinct batch size fed into the model during a single forward pass on a single device. \"\"\" global_batch_size : int microbatch_size : int","title":"BatchingConfig"},{"location":"0_loop/config/#d9d.loop.config.DataLoadingConfig","text":"Bases: BaseModel Configuration for PyTorch DataLoaders. Attributes: Name Type Description num_workers int The number of subprocesses to use for data loading. pin_memory bool Whether to copy tensors into CUDA pinned memory before returning them. persistent_workers bool If True, the data loader will not shutdown the worker processes after a dataset has been consumed once. Source code in d9d/loop/config/config.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 class DataLoadingConfig ( BaseModel ): \"\"\" Configuration for PyTorch DataLoaders. Attributes: num_workers: The number of subprocesses to use for data loading. pin_memory: Whether to copy tensors into CUDA pinned memory before returning them. persistent_workers: If True, the data loader will not shutdown the worker processes after a dataset has been consumed once. \"\"\" num_workers : int pin_memory : bool persistent_workers : bool","title":"DataLoadingConfig"},{"location":"0_loop/config/#checkpointing","text":"","title":"Checkpointing"},{"location":"0_loop/config/#d9d.loop.config.CheckpointingConfig","text":"Bases: BaseModel Configuration for saving model snapshots. Attributes: Name Type Description save_dir Path The root directory where checkpoints will be stored. period_steps StepActionPeriod How frequently to save a checkpoint. num_to_keep int | None The maximum number of recent checkpoints to retain. If None, all checkpoints are kept. Source code in d9d/loop/config/config.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 class CheckpointingConfig ( BaseModel ): \"\"\" Configuration for saving model snapshots. Attributes: save_dir: The root directory where checkpoints will be stored. period_steps: How frequently to save a checkpoint. num_to_keep: The maximum number of recent checkpoints to retain. If None, all checkpoints are kept. \"\"\" save_dir : Path period_steps : StepActionPeriod num_to_keep : int | None","title":"CheckpointingConfig"},{"location":"0_loop/config/#model-initialization","text":"","title":"Model Initialization"},{"location":"0_loop/config/#d9d.loop.config.ModelStageFactoryConfig","text":"Bases: BaseModel Configuration for initializing model weights. Attributes: Name Type Description source_checkpoint Path | None Path to an initial checkpoint to load into the model before training starts. If None, random initialization is used. checkpoint_only_trainable_parameters bool If True, only parameters with requires_grad=True will be saved in checkpoints. Useful for PEFT/LoRA. Source code in d9d/loop/config/config.py 92 93 94 95 96 97 98 99 100 101 102 103 104 class ModelStageFactoryConfig ( BaseModel ): \"\"\" Configuration for initializing model weights. Attributes: source_checkpoint: Path to an initial checkpoint to load into the model before training starts. If None, random initialization is used. checkpoint_only_trainable_parameters: If True, only parameters with requires_grad=True will be saved in checkpoints. Useful for PEFT/LoRA. \"\"\" source_checkpoint : Path | None checkpoint_only_trainable_parameters : bool","title":"ModelStageFactoryConfig"},{"location":"0_loop/config/#optimization","text":"","title":"Optimization"},{"location":"0_loop/config/#d9d.loop.config.GradientClippingConfig","text":"Bases: BaseModel Configuration for gradient norm clipping. Attributes: Name Type Description max_norm float | None The maximum norm value for gradient clipping. If None, no clipping is performed. log_total_steps StepActionPeriod Frequency at which to log the total gradient norm. Source code in d9d/loop/config/config.py 107 108 109 110 111 112 113 114 115 116 117 118 class GradientClippingConfig ( BaseModel ): \"\"\" Configuration for gradient norm clipping. Attributes: max_norm: The maximum norm value for gradient clipping. If None, no clipping is performed. log_total_steps: Frequency at which to log the total gradient norm. \"\"\" max_norm : float | None log_total_steps : StepActionPeriod","title":"GradientClippingConfig"},{"location":"0_loop/config/#d9d.loop.config.GradientManagerConfig","text":"Bases: BaseModel Configuration for gradient synchronization. Attributes: Name Type Description grad_dtype str | None The data type to use for storing the gradient. If None, follows the model's dtype. bucket_size_mb int The size of gradient buckets in Megabytes for communication. Source code in d9d/loop/config/config.py 155 156 157 158 159 160 161 162 163 164 165 class GradientManagerConfig ( BaseModel ): \"\"\" Configuration for gradient synchronization. Attributes: grad_dtype: The data type to use for storing the gradient. If None, follows the model's dtype. bucket_size_mb: The size of gradient buckets in Megabytes for communication. \"\"\" grad_dtype : str | None bucket_size_mb : int","title":"GradientManagerConfig"},{"location":"0_loop/config/#infrastructure","text":"","title":"Infrastructure"},{"location":"0_loop/config/#d9d.loop.config.PipeliningConfig","text":"Bases: BaseModel Configuration for pipeline parallelism orchestration. Attributes: Name Type Description schedule AnyPipelineScheduleConfig The specific scheduling strategy configuration used to manage pipeline execution. Source code in d9d/loop/config/config.py 38 39 40 41 42 43 44 45 46 class PipeliningConfig ( BaseModel ): \"\"\" Configuration for pipeline parallelism orchestration. Attributes: schedule: The specific scheduling strategy configuration used to manage pipeline execution. \"\"\" schedule : AnyPipelineScheduleConfig","title":"PipeliningConfig"},{"location":"0_loop/config/#d9d.loop.config.GarbageCollectionConfig","text":"Bases: BaseModel Configuration for manual Python garbage collection control. Attributes: Name Type Description period_steps StepActionPeriod How frequently to manually trigger the Python garbage collector. Source code in d9d/loop/config/config.py 49 50 51 52 53 54 55 56 57 class GarbageCollectionConfig ( BaseModel ): \"\"\" Configuration for manual Python garbage collection control. Attributes: period_steps: How frequently to manually trigger the Python garbage collector. \"\"\" period_steps : StepActionPeriod","title":"GarbageCollectionConfig"},{"location":"0_loop/config/#d9d.loop.config.TimeoutConfig","text":"Bases: BaseModel Configuration for distributed process group timeouts. Attributes: Name Type Description init_timeout int Timeout in seconds for initializing the process group. step_timeout int Timeout in seconds for individual step communications. Source code in d9d/loop/config/config.py 168 169 170 171 172 173 174 175 176 177 178 class TimeoutConfig ( BaseModel ): \"\"\" Configuration for distributed process group timeouts. Attributes: init_timeout: Timeout in seconds for initializing the process group. step_timeout: Timeout in seconds for individual step communications. \"\"\" init_timeout : int = 10000 step_timeout : int = 100","title":"TimeoutConfig"},{"location":"0_loop/config/#types","text":"","title":"Types"},{"location":"0_loop/config/#d9d.loop.config.StepActionPeriod","text":"Union type representing a configuration for periodic events. Values int: The period in steps (frequency) at which the event occurs. StepActionSpecial: A special flag indicating end-of-run execution or disabling.","title":"StepActionPeriod"},{"location":"0_loop/config/#d9d.loop.config.StepActionSpecial","text":"Bases: StrEnum Special flag values for configuring periodic actions. Attributes: Name Type Description last_step Indicates the action should occur exactly once at the very end of the training run. disable Indicates the action should never occur. Source code in d9d/loop/config/types.py 4 5 6 7 8 9 10 11 12 13 14 15 class StepActionSpecial ( StrEnum ): \"\"\" Special flag values for configuring periodic actions. Attributes: last_step: Indicates the action should occur exactly once at the very end of the training run. disable: Indicates the action should never occur. \"\"\" last_step = \"last_step\" disable = \"disable\"","title":"StepActionSpecial"},{"location":"0_loop/interfaces/","text":"About The d9d training loop is agnostic to the specific model or data being trained. You interact with the loop by implementing Providers (factories) and Tasks (step logic). For standard use cases (like standard Optimizers), d9d provides Auto implementations that can be configured purely via Pydantic models, avoiding the need to write custom provider classes. User Tasks A Task defines custom logic for a single train/inference step. Each Task may implement Stateful protocol, so you may store some mutable state here. TrainTask It is responsible for logging metrics, mapping batch inputs before they are fed into the model, and for computing the task loss function value. Init : create_metrics(...) , dump_hparams(...) . Lifecycle : build_forward_inputs(...) (will be called once) -> compute_loss(...) (will be called multiple times if pipelining is enabled - once for each pipeline microbatch) -> update_metrics(...) (will be called once). Exit : finalize(...) . State Management : state_dict(...) , load_state_dict(...) . InferenceTask The InferenceTask defines the logic for a single inference step. It is designed to handle the forward-only flow, processing the raw tensors synthesized by the model (e.g., logits, hidden states). Lifecycle : build_forward_inputs(...) (called once) -> process_outputs(...) (called once per pipeline microbatch). Exit : finalize(...) . State Management : state_dict(...) , load_state_dict(...) . Pipeline State You may note that batch is only accessible in build_forward_inputs(...) method, but not in others. Don't worry! There is an object for transferring any state between the Task Lifecycle stages, - it is called PipelineState . ctx . state [ \"target\" ] = torch . tensor ([ 1 , 0 , 1 , 0 ], device = \"cuda\" ) # ... metrics [ \"accuracy\" ] . update ( ctx . state [ \"target\" ]) The pipeline state will automatically shard and unshard data if needed. You may read an additional documentation for its internal behaviour. Example Implementation import torch from d9d.core.dist_context import DistributedContext from d9d.core.types import ScalarTree from d9d.module.block.head import LM_IGNORE_INDEX from d9d.loop.control import * class SFTTask ( TrainTask [ dict [ str , torch . Tensor ]]): def __init__ ( self , dist_ctx : DistributedContext ): self . _dist_ctx = dist_ctx def build_forward_inputs ( self , ctx : BuildForwardInputsContext ) -> BuildForwardInputsResult : # ctx.batch contains the output of the Collator. # Save labels in state for access during loss computation later ctx . state [ \"labels\" ] = ctx . batch [ \"labels\" ] # Return inputs for model.forward() # inputs are only for the first pipeline stage # kwargs are the same for all the pipeline stages return BuildForwardInputsResult ( inputs = { \"input_ids\" : ctx . batch [ \"input_ids\" ] }, kwargs = { \"labels\" : ctx . batch [ \"labels\" ], \"position_ids\" : ctx . batch [ \"position_ids\" ] } ) def dump_hparams ( self ) -> ScalarTree : return super () . dump_hparams () def compute_loss ( self , ctx : ComputeLossContext ) -> ComputeLossResult : # Retrieve log_probs calculated by the model pipeline logps = ctx . pipeline_results [ \"logps\" ] # Calculate number of valid tokens (ignoring the -100 padding) # This is crucial for variable length batches. num_loss_tokens = ( ctx . state [ \"labels\" ] != LM_IGNORE_INDEX ) . sum () # Calculate average loss per valid token total_loss = logps . sum () / num_loss_tokens return ComputeLossResult ( loss = total_loss , # loss_weight is used for gradient accumulation across the distributed world. # If batches have different token counts, we weigh the gradient # by token count to get a mathematical true average over the accumulation steps. loss_weight = num_loss_tokens / 1000 ) API Reference d9d.loop.control.task BaseTask Bases: ABC , Stateful , Generic [ TBatch ] Abstract base class representing a unit of work (Task) in the training/inference loop. Source code in d9d/loop/control/task.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 class BaseTask ( abc . ABC , Stateful , typing . Generic [ TBatch ]): \"\"\"Abstract base class representing a unit of work (Task) in the training/inference loop.\"\"\" @abc . abstractmethod def build_forward_inputs ( self , ctx : BuildForwardInputsContext [ TBatch ]) -> BuildForwardInputsResult : \"\"\" Transforms raw data loaded from the DataLoader into arguments for the model. Args: ctx: Context object. Returns: Result object. \"\"\" ... def state_dict ( self ) -> dict [ str , Any ]: \"\"\" Returns the state dictionary for checkpointing this task. Returns: A dictionary containing the task's state. \"\"\" return {} def load_state_dict ( self , state_dict : dict [ str , Any ]) -> None : \"\"\" Restores the task's state from the provided dictionary. Args: state_dict: The state dictionary to load. \"\"\" # do nothing by default def finalize ( self , ctx : FinalizeContext ) -> None : \"\"\" Performs cleanup or final actions when the task execution finishes. Args: ctx: Context object. \"\"\" build_forward_inputs ( ctx ) abstractmethod Transforms raw data loaded from the DataLoader into arguments for the model. Parameters: Name Type Description Default ctx BuildForwardInputsContext [ TBatch ] Context object. required Returns: Type Description BuildForwardInputsResult Result object. Source code in d9d/loop/control/task.py 65 66 67 68 69 70 71 72 73 74 75 76 77 @abc . abstractmethod def build_forward_inputs ( self , ctx : BuildForwardInputsContext [ TBatch ]) -> BuildForwardInputsResult : \"\"\" Transforms raw data loaded from the DataLoader into arguments for the model. Args: ctx: Context object. Returns: Result object. \"\"\" ... finalize ( ctx ) Performs cleanup or final actions when the task execution finishes. Parameters: Name Type Description Default ctx FinalizeContext Context object. required Source code in d9d/loop/control/task.py 98 99 100 101 102 103 104 def finalize ( self , ctx : FinalizeContext ) -> None : \"\"\" Performs cleanup or final actions when the task execution finishes. Args: ctx: Context object. \"\"\" load_state_dict ( state_dict ) Restores the task's state from the provided dictionary. Parameters: Name Type Description Default state_dict dict [ str , Any ] The state dictionary to load. required Source code in d9d/loop/control/task.py 89 90 91 92 93 94 95 def load_state_dict ( self , state_dict : dict [ str , Any ]) -> None : \"\"\" Restores the task's state from the provided dictionary. Args: state_dict: The state dictionary to load. \"\"\" state_dict () Returns the state dictionary for checkpointing this task. Returns: Type Description dict [ str , Any ] A dictionary containing the task's state. Source code in d9d/loop/control/task.py 79 80 81 82 83 84 85 86 87 def state_dict ( self ) -> dict [ str , Any ]: \"\"\" Returns the state dictionary for checkpointing this task. Returns: A dictionary containing the task's state. \"\"\" return {} BuildForwardInputsContext dataclass Bases: Generic [ TBatch ] Context data to prepare inputs for the model forward pass. Attributes: Name Type Description batch TBatch The raw batch data loaded from the DataLoader object. state PipelineState The current state of the pipeline. You can assign any data to this state object, and it will be accessible during this pipeline step (e.g. when computing loss) Source code in d9d/loop/control/task.py 23 24 25 26 27 28 29 30 31 32 33 34 35 @dataclasses . dataclass ( kw_only = True ) class BuildForwardInputsContext ( typing . Generic [ TBatch ]): \"\"\" Context data to prepare inputs for the model forward pass. Attributes: batch: The raw batch data loaded from the DataLoader object. state: The current state of the pipeline. You can assign any data to this state object, and it will be accessible during this pipeline step (e.g. when computing loss) \"\"\" batch : TBatch state : \"PipelineState\" BuildForwardInputsResult dataclass The result of processing the raw batch into model inputs. Attributes: Name Type Description inputs dict [ str , Tensor ] A dictionary of inputs that are passed to model pipeline as input data (first stage only if using pipeline parallelism). kwargs dict [ str , Any ] A dictionary of keyword arguments passed to each pipeline stage. pipeline_sharding_spec PipelineShardingSpec | None A specification defining how inputs and kwargs should be split into micro-batches for pipeline parallelism. If None, the framework assumes standard behavior where all the non-scalar Tensors and lists are split by 0 dimension. Source code in d9d/loop/control/task.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 @dataclasses . dataclass ( kw_only = True ) class BuildForwardInputsResult : \"\"\" The result of processing the raw batch into model inputs. Attributes: inputs: A dictionary of inputs that are passed to model pipeline as input data (first stage only if using pipeline parallelism). kwargs: A dictionary of keyword arguments passed to each pipeline stage. pipeline_sharding_spec: A specification defining how inputs and kwargs should be split into micro-batches for pipeline parallelism. If None, the framework assumes standard behavior where all the non-scalar Tensors and lists are split by 0 dimension. \"\"\" inputs : dict [ str , torch . Tensor ] kwargs : dict [ str , Any ] pipeline_sharding_spec : PipelineShardingSpec | None = None ComputeLossContext dataclass Context data provided to calculate the loss during training. Attributes: Name Type Description pipeline_results Mapping [ str , Tensor ] The outputs returned by the model's forward pass. state PipelineState The current state of the pipeline. You can assign any data to this state object, and it will be accessible during this pipeline step (e.g. when calculating metrics) stepper Stepper Component tracking the current step. Source code in d9d/loop/control/task.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 @dataclasses . dataclass ( kw_only = True ) class ComputeLossContext : \"\"\" Context data provided to calculate the loss during training. Attributes: pipeline_results: The outputs returned by the model's forward pass. state: The current state of the pipeline. You can assign any data to this state object, and it will be accessible during this pipeline step (e.g. when calculating metrics) stepper: Component tracking the current step. \"\"\" pipeline_results : Mapping [ str , torch . Tensor ] state : \"PipelineState\" stepper : \"Stepper\" ComputeLossResult dataclass The result of the loss computation. Attributes: Name Type Description loss Tensor The scalar tensor representing the loss to be backpropagated. loss_weight Tensor | None The weight to apply to the loss (for synchronizing gradients using weighted mean). None for 1.0. Source code in d9d/loop/control/task.py 124 125 126 127 128 129 130 131 132 133 134 135 136 @dataclasses . dataclass ( kw_only = True ) class ComputeLossResult : \"\"\" The result of the loss computation. Attributes: loss: The scalar tensor representing the loss to be backpropagated. loss_weight: The weight to apply to the loss (for synchronizing gradients using weighted mean). None for 1.0. \"\"\" loss : torch . Tensor loss_weight : torch . Tensor | None CreateMetricsContext dataclass Context data provided to initialize metrics. Source code in d9d/loop/control/task.py 139 140 141 @dataclasses . dataclass ( kw_only = True ) class CreateMetricsContext : \"\"\"Context data provided to initialize metrics.\"\"\" CreateMetricsResult dataclass Result of metric initialization. Attributes: Name Type Description metrics dict [ str , Metric ] A dictionary mapping metric names to Metric instances. Source code in d9d/loop/control/task.py 144 145 146 147 148 149 150 151 152 153 @dataclasses . dataclass ( kw_only = True ) class CreateMetricsResult : \"\"\" Result of metric initialization. Attributes: metrics: A dictionary mapping metric names to Metric instances. \"\"\" metrics : dict [ str , \"Metric\" ] FinalizeContext dataclass Context data provided when the task is being finalized. Source code in d9d/loop/control/task.py 57 58 59 @dataclasses . dataclass ( kw_only = True ) class FinalizeContext : \"\"\"Context data provided when the task is being finalized.\"\"\" InferenceTask Bases: BaseTask , ABC , Generic [ TBatch ] Abstract base class for defining inference-specific logic. Source code in d9d/loop/control/task.py 263 264 265 266 267 268 269 270 271 272 273 274 275 class InferenceTask ( BaseTask , abc . ABC , typing . Generic [ TBatch ]): \"\"\"Abstract base class for defining inference-specific logic.\"\"\" @abc . abstractmethod def process_outputs ( self , ctx : ProcessOutputsContext ): \"\"\" Processes the model outputs (e.g. saving to disk, decoding tokens). Args: ctx: Context containing the model outputs and pipeline state. \"\"\" ... process_outputs ( ctx ) abstractmethod Processes the model outputs (e.g. saving to disk, decoding tokens). Parameters: Name Type Description Default ctx ProcessOutputsContext Context containing the model outputs and pipeline state. required Source code in d9d/loop/control/task.py 266 267 268 269 270 271 272 273 274 275 @abc . abstractmethod def process_outputs ( self , ctx : ProcessOutputsContext ): \"\"\" Processes the model outputs (e.g. saving to disk, decoding tokens). Args: ctx: Context containing the model outputs and pipeline state. \"\"\" ... InferenceTaskProvider Bases: Protocol Protocol for a callable that creates an InferenceTask instance. Source code in d9d/loop/control/task.py 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 @typing . runtime_checkable class InferenceTaskProvider ( Protocol ): \"\"\"Protocol for a callable that creates an InferenceTask instance.\"\"\" def __call__ ( self , ctx : InferenceTaskProviderContext ) -> InferenceTask : \"\"\" Creates and returns a new InferenceTask. Args: ctx: Context providing distributed environment information. Returns: An instantiated InferenceTask. \"\"\" ... __call__ ( ctx ) Creates and returns a new InferenceTask. Parameters: Name Type Description Default ctx InferenceTaskProviderContext Context providing distributed environment information. required Returns: Type Description InferenceTask An instantiated InferenceTask. Source code in d9d/loop/control/task.py 294 295 296 297 298 299 300 301 302 303 304 def __call__ ( self , ctx : InferenceTaskProviderContext ) -> InferenceTask : \"\"\" Creates and returns a new InferenceTask. Args: ctx: Context providing distributed environment information. Returns: An instantiated InferenceTask. \"\"\" ... InferenceTaskProviderContext dataclass Context data provided to the factory creating an InferenceTask. Attributes: Name Type Description dist_context DistributedContext Information about the distributed environment. Source code in d9d/loop/control/task.py 278 279 280 281 282 283 284 285 286 287 @dataclasses . dataclass ( kw_only = True ) class InferenceTaskProviderContext : \"\"\" Context data provided to the factory creating an InferenceTask. Attributes: dist_context: Information about the distributed environment. \"\"\" dist_context : DistributedContext ProcessOutputsContext dataclass Context data provided to process outputs during inference. Attributes: Name Type Description pipeline_results dict [ str , Tensor ] The outputs returned by the model's forward pass. state PipelineState The current state of the pipeline. Source code in d9d/loop/control/task.py 249 250 251 252 253 254 255 256 257 258 259 260 @dataclasses . dataclass ( kw_only = True ) class ProcessOutputsContext : \"\"\" Context data provided to process outputs during inference. Attributes: pipeline_results: The outputs returned by the model's forward pass. state: The current state of the pipeline. \"\"\" pipeline_results : dict [ str , torch . Tensor ] state : \"PipelineState\" TrainTask Bases: BaseTask , ABC , Generic [ TBatch ] Abstract base class for defining training-specific logic. Source code in d9d/loop/control/task.py 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 class TrainTask ( BaseTask , abc . ABC , typing . Generic [ TBatch ]): \"\"\"Abstract base class for defining training-specific logic.\"\"\" @abc . abstractmethod def compute_loss ( self , ctx : ComputeLossContext ) -> ComputeLossResult : \"\"\" Calculates the loss based on model outputs. Args: ctx: Context object. Returns: Result object. \"\"\" ... def create_metrics ( self , ctx : CreateMetricsContext ) -> CreateMetricsResult : \"\"\" Initializes metrics to be tracked during training. Args: ctx: Context object. Returns: Result object. \"\"\" return CreateMetricsResult ( metrics = {}) def update_metrics ( self , ctx : UpdateMetricsContext ): \"\"\" Updates the state of the metrics at the end of training step. Args: ctx: Context object. \"\"\" def dump_hparams ( self ) -> ScalarTree : \"\"\" Exports hyperparameters associated with this task for logging. Returns: A dictionary of hyperparameter names and values. \"\"\" return {} compute_loss ( ctx ) abstractmethod Calculates the loss based on model outputs. Parameters: Name Type Description Default ctx ComputeLossContext Context object. required Returns: Type Description ComputeLossResult Result object. Source code in d9d/loop/control/task.py 173 174 175 176 177 178 179 180 181 182 183 184 185 @abc . abstractmethod def compute_loss ( self , ctx : ComputeLossContext ) -> ComputeLossResult : \"\"\" Calculates the loss based on model outputs. Args: ctx: Context object. Returns: Result object. \"\"\" ... create_metrics ( ctx ) Initializes metrics to be tracked during training. Parameters: Name Type Description Default ctx CreateMetricsContext Context object. required Returns: Type Description CreateMetricsResult Result object. Source code in d9d/loop/control/task.py 187 188 189 190 191 192 193 194 195 196 197 198 def create_metrics ( self , ctx : CreateMetricsContext ) -> CreateMetricsResult : \"\"\" Initializes metrics to be tracked during training. Args: ctx: Context object. Returns: Result object. \"\"\" return CreateMetricsResult ( metrics = {}) dump_hparams () Exports hyperparameters associated with this task for logging. Returns: Type Description ScalarTree A dictionary of hyperparameter names and values. Source code in d9d/loop/control/task.py 208 209 210 211 212 213 214 215 216 def dump_hparams ( self ) -> ScalarTree : \"\"\" Exports hyperparameters associated with this task for logging. Returns: A dictionary of hyperparameter names and values. \"\"\" return {} update_metrics ( ctx ) Updates the state of the metrics at the end of training step. Parameters: Name Type Description Default ctx UpdateMetricsContext Context object. required Source code in d9d/loop/control/task.py 200 201 202 203 204 205 206 def update_metrics ( self , ctx : UpdateMetricsContext ): \"\"\" Updates the state of the metrics at the end of training step. Args: ctx: Context object. \"\"\" TrainTaskProvider Bases: Protocol Protocol that creates a TrainTask instance. Source code in d9d/loop/control/task.py 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 @typing . runtime_checkable class TrainTaskProvider ( Protocol ): \"\"\"Protocol that creates a TrainTask instance.\"\"\" def __call__ ( self , ctx : TrainTaskProviderContext ) -> TrainTask : \"\"\" Creates and returns a new TrainTask. Args: ctx: Context object. Returns: An instantiated TrainTask. \"\"\" ... __call__ ( ctx ) Creates and returns a new TrainTask. Parameters: Name Type Description Default ctx TrainTaskProviderContext Context object. required Returns: Type Description TrainTask An instantiated TrainTask. Source code in d9d/loop/control/task.py 235 236 237 238 239 240 241 242 243 244 245 246 def __call__ ( self , ctx : TrainTaskProviderContext ) -> TrainTask : \"\"\" Creates and returns a new TrainTask. Args: ctx: Context object. Returns: An instantiated TrainTask. \"\"\" ... TrainTaskProviderContext dataclass Context data provided to the factory creating a TrainTask. Attributes: Name Type Description dist_context DistributedContext Information about the distributed environment. Source code in d9d/loop/control/task.py 219 220 221 222 223 224 225 226 227 228 @dataclasses . dataclass ( kw_only = True ) class TrainTaskProviderContext : \"\"\" Context data provided to the factory creating a TrainTask. Attributes: dist_context: Information about the distributed environment. \"\"\" dist_context : DistributedContext UpdateMetricsContext dataclass Context data provided to update metrics after a step. Attributes: Name Type Description state PipelineState The current state of the pipeline. metrics Mapping [ str , Metric ] The dictionary of metrics to be updated. Source code in d9d/loop/control/task.py 156 157 158 159 160 161 162 163 164 165 166 167 @dataclasses . dataclass ( kw_only = True ) class UpdateMetricsContext : \"\"\" Context data provided to update metrics after a step. Attributes: state: The current state of the pipeline. metrics: The dictionary of metrics to be updated. \"\"\" state : \"PipelineState\" metrics : Mapping [ str , \"Metric\" ] Model Definition ModelProvider The ModelProvider controls the lifecycle of the nn.Module . In distributed training, models are rarely just \"instantiated\". They must be initialized, parallelized, and mapped for loading from checkpoint. How to Write a ModelProvider Choose a Model Choose a model from d9d's catalogue or create it by your own. Implement initialize_model_stage(...) Implement the initialize_model_stage(...) method - it should prepare a nn.Module for specified pipeline parallel stage containing model architecture in a target torch.dtype . Note that models are initialized on meta device , so you must not load model weights here. Instead, this function should return a State Mapper that will map model weights on disk to model weights in-memory . You also may apply PEFT methods here and other architectural patches, but make sure you respect the changes they made in returned State Mapper . Implement parallelize_model_stage(...) Implement the parallelize_model_stage(...) method - it should apply Horizontal Parallelism strategy for selected model in-place. If you use one of d9d's models, you may use default strategies for them such as parallelize_qwen3_moe_for_causal_lm ( reference ). For a custom model, please see Horizontal Parallelism docs and reference implementations. Implement prepare_export_model_stage(...) Implement the prepare_export_model_stage(...) method - it should return a State Mapper that converts in-memory model state to that one that will be saved on disk during final export. Basically, it should reverse all the operations of State Mapper produced in initialize_model_stage(...) . Example Implementation from pydantic import BaseModel from d9d.loop.control.model_provider import * from d9d.module.model.qwen3_moe import Qwen3MoEForCausalLM , Qwen3MoEForCausalLMParameters from d9d.module.parallelism.model.qwen3_moe import parallelize_qwen3_moe_for_causal_lm from d9d.module.block.hidden_states_aggregator import HiddenStatesAggregationMode from d9d.model_state.mapper.adapters import identity_mapper_from_module class ModelProviderConfig ( BaseModel ): model : Qwen3MoEForCausalLMParameters # Hyperparameters for Qwen3 MoE checkpointing : bool # Enable gradient checkpointing to save VRAM class ProjectModelProvider ( ModelProvider [ Qwen3MoEForCausalLM ]): def __init__ ( self , config : ModelProviderConfig ): self . _config = config def initialize_model_stage ( self , context : InitializeModelStageContext ) -> InitializeModelStageResult : # Initialize the raw model on Meta device in BF16 precision model = Qwen3MoEForCausalLM ( params = self . _config . model , stage = context . stage , hidden_states_snapshot_mode = HiddenStatesAggregationMode . no , enable_checkpointing = self . _config . checkpointing ) . bfloat16 () return InitializeModelStageResult ( model = model , state_mapper = identity_mapper_from_module ( model ) ) def parallelize_model_stage ( self , context : ParallelizeModelStageContext ): # Applies specific distributed strategies # suited for Qwen3 MoE architecture. # You can apply your own horizontal parallelism strategy here. parallelize_qwen3_moe_for_causal_lm ( dist_context = context . dist_context , stage = context . stage , model = context . model ) def prepare_export_model_stage ( self , context : PrepareExportModelStageContext ) -> PrepareExportModelStageResult : # When exporting, save model weights as-is return PrepareExportModelStageResult ( state_mapper = identity_mapper_from_module ( context . model ) ) def dump_hparams ( self ) -> ScalarTree : return self . _config . model_dump ( mode = \"json\" ) API Reference d9d.loop.control.model_provider InitializeModelStageContext dataclass Context data required for initializing a specific model pipeline stage. Attributes: Name Type Description dist_context DistributedContext The distributed execution context. stage PipelineStageInfo Metadata describing the current pipeline stage being initialized. Source code in d9d/loop/control/model_provider.py 13 14 15 16 17 18 19 20 21 22 23 24 @dataclasses . dataclass ( kw_only = True ) class InitializeModelStageContext : \"\"\" Context data required for initializing a specific model pipeline stage. Attributes: dist_context: The distributed execution context. stage: Metadata describing the current pipeline stage being initialized. \"\"\" dist_context : DistributedContext stage : PipelineStageInfo InitializeModelStageResult dataclass Bases: Generic [ TModel ] The result of initializing a model stage. Attributes: Name Type Description model TModel The PyTorch module. state_mapper ModelStateMapper The mapper defining how to load weights into this module. Source code in d9d/loop/control/model_provider.py 30 31 32 33 34 35 36 37 38 39 40 41 @dataclasses . dataclass ( kw_only = True ) class InitializeModelStageResult ( Generic [ TModel ]): \"\"\" The result of initializing a model stage. Attributes: model: The PyTorch module. state_mapper: The mapper defining how to load weights into this module. \"\"\" model : TModel state_mapper : ModelStateMapper ModelProvider Bases: ABC , Generic [ TModel ] Abstract interface for defining the lifecycle of a distributed model. This provider handles initialization, parallelization (sharding/replication/etc), and export preparation for models within the d9d framework. Source code in d9d/loop/control/model_provider.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 class ModelProvider ( abc . ABC , Generic [ TModel ]): \"\"\" Abstract interface for defining the lifecycle of a distributed model. This provider handles initialization, parallelization (sharding/replication/etc), and export preparation for models within the d9d framework. \"\"\" @abc . abstractmethod def initialize_model_stage ( self , context : InitializeModelStageContext ) -> InitializeModelStageResult [ TModel ]: \"\"\" Initializes the model architecture for a specific pipeline stage. This method is responsible for constructing the `nn.Module` for the requested stage. Construction occurs within a meta-device context; therefore, weights should not be loaded directly here. Instead, a `ModelStateMapper` must be returned to define how weights from a checkpoint map to the newly created module parameters. This allows for architecture modifications, such as injecting LoRA adapters, provided that the returned mapper reflects the new structure. Args: context: Context for this operation. Returns: Result of this operation. \"\"\" ... @abc . abstractmethod def parallelize_model_stage ( self , context : ParallelizeModelStageContext [ TModel ]): \"\"\" Converts the model parameters into distributed tensors (DTensors). Implementations should modify the model in-place. This involves converting standard parameters into DTensors by replicating or sharding them according to the desired parallelism strategies. Args: context: Context for this operation. \"\"\" @abc . abstractmethod def prepare_export_model_stage ( self , context : PrepareExportModelStageContext [ TModel ] ) -> PrepareExportModelStageResult : \"\"\" Prepares the state mapper required for saving the model to disk. This methods defines how the current in-memory model structure maps back to the serialized checkpoint format. Args: context: Context for this operation. Returns: Result of this operation. \"\"\" def dump_hparams ( self ) -> ScalarTree : \"\"\" Exports hyperparameters associated with this model for logging. Returns: A dictionary of hyperparameter names and values. \"\"\" return {} dump_hparams () Exports hyperparameters associated with this model for logging. Returns: Type Description ScalarTree A dictionary of hyperparameter names and values. Source code in d9d/loop/control/model_provider.py 147 148 149 150 151 152 153 154 155 def dump_hparams ( self ) -> ScalarTree : \"\"\" Exports hyperparameters associated with this model for logging. Returns: A dictionary of hyperparameter names and values. \"\"\" return {} initialize_model_stage ( context ) abstractmethod Initializes the model architecture for a specific pipeline stage. This method is responsible for constructing the nn.Module for the requested stage. Construction occurs within a meta-device context; therefore, weights should not be loaded directly here. Instead, a ModelStateMapper must be returned to define how weights from a checkpoint map to the newly created module parameters. This allows for architecture modifications, such as injecting LoRA adapters, provided that the returned mapper reflects the new structure. Parameters: Name Type Description Default context InitializeModelStageContext Context for this operation. required Returns: Type Description InitializeModelStageResult [ TModel ] Result of this operation. Source code in d9d/loop/control/model_provider.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 @abc . abstractmethod def initialize_model_stage ( self , context : InitializeModelStageContext ) -> InitializeModelStageResult [ TModel ]: \"\"\" Initializes the model architecture for a specific pipeline stage. This method is responsible for constructing the `nn.Module` for the requested stage. Construction occurs within a meta-device context; therefore, weights should not be loaded directly here. Instead, a `ModelStateMapper` must be returned to define how weights from a checkpoint map to the newly created module parameters. This allows for architecture modifications, such as injecting LoRA adapters, provided that the returned mapper reflects the new structure. Args: context: Context for this operation. Returns: Result of this operation. \"\"\" ... parallelize_model_stage ( context ) abstractmethod Converts the model parameters into distributed tensors (DTensors). Implementations should modify the model in-place. This involves converting standard parameters into DTensors by replicating or sharding them according to the desired parallelism strategies. Parameters: Name Type Description Default context ParallelizeModelStageContext [ TModel ] Context for this operation. required Source code in d9d/loop/control/model_provider.py 117 118 119 120 121 122 123 124 125 126 127 128 @abc . abstractmethod def parallelize_model_stage ( self , context : ParallelizeModelStageContext [ TModel ]): \"\"\" Converts the model parameters into distributed tensors (DTensors). Implementations should modify the model in-place. This involves converting standard parameters into DTensors by replicating or sharding them according to the desired parallelism strategies. Args: context: Context for this operation. \"\"\" prepare_export_model_stage ( context ) abstractmethod Prepares the state mapper required for saving the model to disk. This methods defines how the current in-memory model structure maps back to the serialized checkpoint format. Parameters: Name Type Description Default context PrepareExportModelStageContext [ TModel ] Context for this operation. required Returns: Type Description PrepareExportModelStageResult Result of this operation. Source code in d9d/loop/control/model_provider.py 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 @abc . abstractmethod def prepare_export_model_stage ( self , context : PrepareExportModelStageContext [ TModel ] ) -> PrepareExportModelStageResult : \"\"\" Prepares the state mapper required for saving the model to disk. This methods defines how the current in-memory model structure maps back to the serialized checkpoint format. Args: context: Context for this operation. Returns: Result of this operation. \"\"\" ParallelizeModelStageContext dataclass Bases: Generic [ TModel ] Context data required for horizontally parallelizing a model stage. Attributes: Name Type Description dist_context DistributedContext The distributed execution context. stage PipelineStageInfo Metadata describing the current pipeline stage. model TModel The PyTorch module to be parallelized. Source code in d9d/loop/control/model_provider.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 @dataclasses . dataclass ( kw_only = True ) class ParallelizeModelStageContext ( Generic [ TModel ]): \"\"\" Context data required for horizontally parallelizing a model stage. Attributes: dist_context: The distributed execution context. stage: Metadata describing the current pipeline stage. model: The PyTorch module to be parallelized. \"\"\" dist_context : DistributedContext stage : PipelineStageInfo model : TModel PrepareExportModelStageContext dataclass Bases: Generic [ TModel ] Context data required for preparing a model stage for export. Attributes: Name Type Description dist_context DistributedContext The distributed execution context. model TModel The PyTorch module to be exported. Source code in d9d/loop/control/model_provider.py 60 61 62 63 64 65 66 67 68 69 70 71 @dataclasses . dataclass ( kw_only = True ) class PrepareExportModelStageContext ( Generic [ TModel ]): \"\"\" Context data required for preparing a model stage for export. Attributes: dist_context: The distributed execution context. model: The PyTorch module to be exported. \"\"\" dist_context : DistributedContext model : TModel PrepareExportModelStageResult dataclass The result of preparing a model stage for export. Attributes: Name Type Description state_mapper ModelStateMapper The mapper defining how model parameters map to disk storage. Source code in d9d/loop/control/model_provider.py 74 75 76 77 78 79 80 81 82 83 @dataclasses . dataclass ( kw_only = True ) class PrepareExportModelStageResult : \"\"\" The result of preparing a model stage for export. Attributes: state_mapper: The mapper defining how model parameters map to disk storage. \"\"\" state_mapper : ModelStateMapper Data Loading DatasetProvider The DatasetProvider is responsible for creating dataset and data collator instances. Distributed-Awareness d9d will not apply sharding to your dataset automatically. You have to configure it manually (optionally applying other dataset wrappers). Please see the Dataset Utilities documentation. Example Implementation from typing import Any , Sequence import torch import datasets from pydantic import BaseModel from tokenizers import Tokenizer from d9d.core.types import TensorTree from d9d.dataset import BufferSortedDataset , shard_dataset_data_parallel , DatasetImplementingSortKeyProtocol from d9d.loop.control.dataset_provider import * class ProjectDataset ( Dataset , DatasetImplementingSortKeyProtocol ): def __init__ ( self , dataset : datasets . Dataset , tokenizer : Tokenizer ): self . _dataset = dataset self . _tokenizer = tokenizer def sort_key ( self , index : int ) -> Any : # Used by BufferSortedDataset to group examples of similar length together. # This minimizes padding overhead in batches. return self . _dataset [ index ][ \"token_counts\" ] def __getitem__ ( self , index : int ) -> TensorTree : return { ... } @classmethod def collate ( cls , batch : Sequence [ dict [ str , torch . Tensor ]]) -> dict [ str , torch . Tensor ]: return { ... } def __len__ ( self ) -> int : return len ( self . _dataset ) class DataConfig ( BaseModel ): dataset : str # HuggingFace dataset path/name split : str # e.g., 'train', 'validation' text_column : str # The column containing the raw text use_samples : int # Limit dataset size for testing/debugging shuffle_seed : int # Distinct seed for shuffling the data tokenizer : str # Path to the tokenizer.json file num_proc : int # Number of CPU processes for data mapping presort_buffer_size : int # Size of buffer for length-based presorting class ProjectDatasetProvider ( DatasetProvider ): def __init__ ( self , config : DataConfig ): self . _config = config @staticmethod def _count_tokens ( item : dict , text_column : str , tokenizer : Tokenizer ) -> dict : return { \"token_counts\" : len ( tokenizer . encode ( item [ text_column ]) . tokens ) } def __call__ ( self , context : InitializeDatasetContext ) -> InitializeDatasetResult : tokenizer = Tokenizer . from_file ( str ( self . _config . tokenizer )) # IMPORTANT: main_process_first ensures that Rank 0 downloads/processes # the dataset and builds the cache first. Ranks 1-N wait, then load from cache. # Prevents race conditions and corruption on the HF cache. with context . dist_context . main_process_first (): data = datasets . load_dataset ( self . _config . dataset , split = self . _config . split ) . take ( self . _config . use_samples ) . shuffle ( self . _config . shuffle_seed ) . map ( self . _count_tokens , num_proc = self . _config . num_proc , fn_kwargs = { \"tokenizer\" : tokenizer , \"text_column\" : self . _config . text_column } ) dataset = ProjectDataset ( data , tokenizer ) # BufferSortedDataset acts as a buffer that shuffles data locally # but outputs batches sorted by length (defined in sort_key above) dataset_buf = BufferSortedDataset ( dataset , buffer_size = self . _config . presort_buffer_size , pack_size = context . batch_maths . global_batch_size , init_seed = self . _config . shuffle_seed ) # Split dataset across data parallel ranks dataset_shard = shard_dataset_data_parallel ( dataset_buf , context . dist_context ) return InitializeDatasetResult ( dataset = dataset_shard , collator = ProjectDataset . collate ) API Reference d9d.loop.control.dataset_provider DatasetProvider Bases: Protocol Protocol that allows users to define how datasets are loaded and collated. Users should subclass this to provide custom data loading logic. Source code in d9d/loop/control/dataset_provider.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 @typing . runtime_checkable class DatasetProvider ( Protocol ): \"\"\"Protocol that allows users to define how datasets are loaded and collated. Users should subclass this to provide custom data loading logic. \"\"\" def __call__ ( self , context : InitializeDatasetContext ) -> InitializeDatasetResult : \"\"\" Initializes the dataset components. It is important that the user must shard the dataset manually, perhaps using `d9d.dataset.ShardedDataset`. Args: context: Context for this operation. Returns: Result of this operation. \"\"\" __call__ ( context ) Initializes the dataset components. It is important that the user must shard the dataset manually, perhaps using d9d.dataset.ShardedDataset . Parameters: Name Type Description Default context InitializeDatasetContext Context for this operation. required Returns: Type Description InitializeDatasetResult Result of this operation. Source code in d9d/loop/control/dataset_provider.py 47 48 49 50 51 52 53 54 55 56 57 58 def __call__ ( self , context : InitializeDatasetContext ) -> InitializeDatasetResult : \"\"\" Initializes the dataset components. It is important that the user must shard the dataset manually, perhaps using `d9d.dataset.ShardedDataset`. Args: context: Context for this operation. Returns: Result of this operation. \"\"\" InitializeDatasetContext dataclass Context data required to initialize a dataset provider. Attributes: Name Type Description dist_context DistributedContext The distributed context containing rank and world size information. batch_maths BatchMaths The batch maths component handling global batch size calculations. Source code in d9d/loop/control/dataset_provider.py 14 15 16 17 18 19 20 21 22 23 24 @dataclasses . dataclass ( kw_only = True ) class InitializeDatasetContext : \"\"\"Context data required to initialize a dataset provider. Attributes: dist_context: The distributed context containing rank and world size information. batch_maths: The batch maths component handling global batch size calculations. \"\"\" dist_context : DistributedContext batch_maths : \"BatchMaths\" InitializeDatasetResult dataclass The result of initializing a dataset provider. Attributes: Name Type Description dataset Dataset The instantiated PyTorch Dataset. collator CollateFn The function used to collate individual samples into a batch. Source code in d9d/loop/control/dataset_provider.py 27 28 29 30 31 32 33 34 35 36 37 @dataclasses . dataclass ( kw_only = True ) class InitializeDatasetResult : \"\"\"The result of initializing a dataset provider. Attributes: dataset: The instantiated PyTorch Dataset. collator: The function used to collate individual samples into a batch. \"\"\" dataset : Dataset collator : CollateFn Optimization & Scheduling Auto Implementations For standard PyTorch usage, d9d includes the d9d.loop.auto package. These providers ingest a Pydantic configuration object and manage the creation of standard optimizers and schedulers. Auto Optimizer Supports AdamW , Adam , SGD , and StochasticAdamW . from d9d.loop.auto import AutoOptimizerProvider , AutoOptimizerConfig provider = AutoOptimizerProvider ( AutoOptimizerConfig . model_validate_json ( '{\"name\": \"adamw\", \"lr\": 1e-4}' ) ) d9d.loop.auto.auto_optimizer AdamOptimizerConfig Bases: BaseAutoOptimizerConfig Configuration for the PyTorch Adam optimizer. Attributes: Name Type Description name Literal ['adam'] Discriminator tag. lr float The learning rate. betas tuple [ float , float ] Coefficients for computing running averages of gradient and its square. eps float Term added to the denominator to improve numerical stability. weight_decay float Weight decay coefficient. decoupled_weight_decay bool Whether to apply decoupled weight decay. amsgrad bool Whether to use the AMSGrad variant. maximize bool Whether to maximize the params based on the objective. Source code in d9d/loop/auto/auto_optimizer.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 class AdamOptimizerConfig ( BaseAutoOptimizerConfig ): \"\"\" Configuration for the PyTorch Adam optimizer. Attributes: name: Discriminator tag. lr: The learning rate. betas: Coefficients for computing running averages of gradient and its square. eps: Term added to the denominator to improve numerical stability. weight_decay: Weight decay coefficient. decoupled_weight_decay: Whether to apply decoupled weight decay. amsgrad: Whether to use the AMSGrad variant. maximize: Whether to maximize the params based on the objective. \"\"\" name : Literal [ \"adam\" ] = \"adam\" lr : float betas : tuple [ float , float ] = ( 0.9 , 0.999 ) eps : float = 1e-8 weight_decay : float = 1e-2 decoupled_weight_decay : bool = False amsgrad : bool = False maximize : bool = False def build ( self , params : Iterable [ nn . Parameter ]) -> Optimizer : \"\"\"Builds fused Adam with the configured parameters.\"\"\" return Adam ( params = params , lr = self . lr , betas = self . betas , eps = self . eps , weight_decay = self . weight_decay , decoupled_weight_decay = self . decoupled_weight_decay , amsgrad = self . amsgrad , maximize = self . maximize , fused = True , ) build ( params ) Builds fused Adam with the configured parameters. Source code in d9d/loop/auto/auto_optimizer.py 129 130 131 132 133 134 135 136 137 138 139 140 141 def build ( self , params : Iterable [ nn . Parameter ]) -> Optimizer : \"\"\"Builds fused Adam with the configured parameters.\"\"\" return Adam ( params = params , lr = self . lr , betas = self . betas , eps = self . eps , weight_decay = self . weight_decay , decoupled_weight_decay = self . decoupled_weight_decay , amsgrad = self . amsgrad , maximize = self . maximize , fused = True , ) AdamWOptimizerConfig Bases: BaseAutoOptimizerConfig Configuration for the PyTorch AdamW optimizer. Attributes: Name Type Description name Literal ['adamw'] Discriminator tag. lr float The learning rate. betas tuple [ float , float ] Coefficients for computing running averages of gradient and its square. eps float Term added to the denominator to improve numerical stability. weight_decay float Weight decay coefficient. amsgrad bool Whether to use the AMSGrad variant. maximize bool Whether to maximize the params based on the objective (as opposed to minimizing). Source code in d9d/loop/auto/auto_optimizer.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 class AdamWOptimizerConfig ( BaseAutoOptimizerConfig ): \"\"\" Configuration for the PyTorch AdamW optimizer. Attributes: name: Discriminator tag. lr: The learning rate. betas: Coefficients for computing running averages of gradient and its square. eps: Term added to the denominator to improve numerical stability. weight_decay: Weight decay coefficient. amsgrad: Whether to use the AMSGrad variant. maximize: Whether to maximize the params based on the objective (as opposed to minimizing). \"\"\" name : Literal [ \"adamw\" ] = \"adamw\" lr : float betas : tuple [ float , float ] = ( 0.9 , 0.999 ) eps : float = 1e-8 weight_decay : float = 1e-2 amsgrad : bool = False maximize : bool = False def build ( self , params : Iterable [ nn . Parameter ]) -> Optimizer : \"\"\"Builds fused AdamW with the configured parameters.\"\"\" return AdamW ( params = params , lr = self . lr , betas = self . betas , eps = self . eps , weight_decay = self . weight_decay , amsgrad = self . amsgrad , maximize = self . maximize , fused = True , ) build ( params ) Builds fused AdamW with the configured parameters. Source code in d9d/loop/auto/auto_optimizer.py 90 91 92 93 94 95 96 97 98 99 100 101 def build ( self , params : Iterable [ nn . Parameter ]) -> Optimizer : \"\"\"Builds fused AdamW with the configured parameters.\"\"\" return AdamW ( params = params , lr = self . lr , betas = self . betas , eps = self . eps , weight_decay = self . weight_decay , amsgrad = self . amsgrad , maximize = self . maximize , fused = True , ) AutoOptimizerProvider Bases: OptimizerProvider OptimizerProvider that builds a PyTorch optimizer based on a configuration object. Source code in d9d/loop/auto/auto_optimizer.py 187 188 189 190 191 192 193 194 195 196 197 class AutoOptimizerProvider ( OptimizerProvider ): \"\"\" OptimizerProvider that builds a PyTorch optimizer based on a configuration object. \"\"\" def __init__ ( self , config : AutoOptimizerConfig ): \"\"\"Constructs the provider with the given configuration.\"\"\" self . _config = config def __call__ ( self , context : InitializeOptimizerStageContext ) -> Optimizer : return self . _config . build ( context . model . parameters ()) __init__ ( config ) Constructs the provider with the given configuration. Source code in d9d/loop/auto/auto_optimizer.py 192 193 194 def __init__ ( self , config : AutoOptimizerConfig ): \"\"\"Constructs the provider with the given configuration.\"\"\" self . _config = config BaseAutoOptimizerConfig Bases: BaseModel , ABC Abstract base class for optimizer configurations. Source code in d9d/loop/auto/auto_optimizer.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 class BaseAutoOptimizerConfig ( BaseModel , ABC ): \"\"\" Abstract base class for optimizer configurations. \"\"\" @abc . abstractmethod def build ( self , params : Iterable [ nn . Parameter ]) -> Optimizer : \"\"\" Creates the PyTorch optimizer instance. Args: params: An iterable of model parameters to optimize. Returns: The instantiated optimizer. \"\"\" ... build ( params ) abstractmethod Creates the PyTorch optimizer instance. Parameters: Name Type Description Default params Iterable [ Parameter ] An iterable of model parameters to optimize. required Returns: Type Description Optimizer The instantiated optimizer. Source code in d9d/loop/auto/auto_optimizer.py 20 21 22 23 24 25 26 27 28 29 30 31 @abc . abstractmethod def build ( self , params : Iterable [ nn . Parameter ]) -> Optimizer : \"\"\" Creates the PyTorch optimizer instance. Args: params: An iterable of model parameters to optimize. Returns: The instantiated optimizer. \"\"\" ... SGDOptimizerConfig Bases: BaseAutoOptimizerConfig Configuration for the PyTorch SGD optimizer. Attributes: Name Type Description name Literal ['sgd'] Discriminator tag. lr float The learning rate. momentum float Momentum factor. dampening float Dampening for momentum. weight_decay float Weight decay (L2 penalty). nesterov bool Enables Nesterov momentum. maximize bool Whether to maximize the params based on the objective. Source code in d9d/loop/auto/auto_optimizer.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 class SGDOptimizerConfig ( BaseAutoOptimizerConfig ): \"\"\" Configuration for the PyTorch SGD optimizer. Attributes: name: Discriminator tag. lr: The learning rate. momentum: Momentum factor. dampening: Dampening for momentum. weight_decay: Weight decay (L2 penalty). nesterov: Enables Nesterov momentum. maximize: Whether to maximize the params based on the objective. \"\"\" name : Literal [ \"sgd\" ] = \"sgd\" lr : float momentum : float = 0 dampening : float = 0 weight_decay : float = 0 nesterov : bool = False maximize : bool = False def build ( self , params : Iterable [ nn . Parameter ]) -> Optimizer : \"\"\"Builds fused SGD with the configured parameters.\"\"\" return SGD ( params , lr = self . lr , momentum = self . momentum , dampening = self . dampening , weight_decay = self . weight_decay , nesterov = self . nesterov , maximize = self . maximize , fused = True , ) build ( params ) Builds fused SGD with the configured parameters. Source code in d9d/loop/auto/auto_optimizer.py 167 168 169 170 171 172 173 174 175 176 177 178 def build ( self , params : Iterable [ nn . Parameter ]) -> Optimizer : \"\"\"Builds fused SGD with the configured parameters.\"\"\" return SGD ( params , lr = self . lr , momentum = self . momentum , dampening = self . dampening , weight_decay = self . weight_decay , nesterov = self . nesterov , maximize = self . maximize , fused = True , ) StochasticAdamWOptimizerConfig Bases: BaseAutoOptimizerConfig Configuration for the Stochastic AdamW optimizer. Attributes: Name Type Description name Literal ['stochastic_adamw'] Discriminator tag. lr float Learning rate. betas tuple [ float , float ] Coefficients used for computing running averages of gradient and its square. eps float Term added to the denominator to improve numerical stability. weight_decay float Weight decay coefficient. state_dtype str Data Type to use for the optimizer states. Source code in d9d/loop/auto/auto_optimizer.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 class StochasticAdamWOptimizerConfig ( BaseAutoOptimizerConfig ): \"\"\" Configuration for the Stochastic AdamW optimizer. Attributes: name: Discriminator tag. lr: Learning rate. betas: Coefficients used for computing running averages of gradient and its square. eps: Term added to the denominator to improve numerical stability. weight_decay: Weight decay coefficient. state_dtype: Data Type to use for the optimizer states. \"\"\" name : Literal [ \"stochastic_adamw\" ] = \"stochastic_adamw\" lr : float betas : tuple [ float , float ] = ( 0.9 , 0.999 ) eps : float = 1e-8 weight_decay : float = 1e-2 state_dtype : str def build ( self , params : Iterable [ nn . Parameter ]) -> Optimizer : \"\"\"Builds StochasticAdamW with the configured parameters.\"\"\" return StochasticAdamW ( params = params , lr = self . lr , betas = self . betas , eps = self . eps , weight_decay = self . weight_decay , state_dtype = getattr ( torch , self . state_dtype ), ) build ( params ) Builds StochasticAdamW with the configured parameters. Source code in d9d/loop/auto/auto_optimizer.py 55 56 57 58 59 60 61 62 63 64 def build ( self , params : Iterable [ nn . Parameter ]) -> Optimizer : \"\"\"Builds StochasticAdamW with the configured parameters.\"\"\" return StochasticAdamW ( params = params , lr = self . lr , betas = self . betas , eps = self . eps , weight_decay = self . weight_decay , state_dtype = getattr ( torch , self . state_dtype ), ) Auto Scheduler Supports Piecewise Linear schedules (warmup, hold, decay). from d9d.loop.auto import AutoLRSchedulerProvider , AutoLRSchedulerConfig cfg = \"\"\" { \"initial_multiplier\": 0.0, \"phases\": [ { \"mode\": \"steps\", \"steps\": 100, \"target_multiplier\": 1.0, \"curve\": { \"type\": \"linear\" } }, { \"mode\": \"rest\", \"target_multiplier\": 0.1, \"curve\": { \"type\": \"cosine\" } } ] } \"\"\" provider = AutoLRSchedulerProvider ( AutoLRSchedulerConfig . model_validate_json ( cfg ) ) d9d.loop.auto.auto_lr_scheduler AutoLRSchedulerProvider Bases: LRSchedulerProvider LRSchedulerProvider that builds a learning rate scheduler based on a configuration object. Source code in d9d/loop/auto/auto_lr_scheduler.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 class AutoLRSchedulerProvider ( LRSchedulerProvider ): \"\"\" LRSchedulerProvider that builds a learning rate scheduler based on a configuration object. \"\"\" def __init__ ( self , config : AutoLRSchedulerConfig ): \"\"\"Constructs the AutoLRSchedulerProvider object.\"\"\" self . _config = config def __call__ ( self , context : InitializeLRSchedulerContext ) -> LRSchedulerProtocol : match self . _config : case PiecewiseConfig (): return piecewise_scheduler_from_config ( self . _config . scheduler , optimizer = context . optimizer , total_steps = context . total_steps ) case _ : raise ValueError ( f \"Unsupported LR scheduler type: { self . _config } \" ) __init__ ( config ) Constructs the AutoLRSchedulerProvider object. Source code in d9d/loop/auto/auto_lr_scheduler.py 32 33 34 35 def __init__ ( self , config : AutoLRSchedulerConfig ): \"\"\"Constructs the AutoLRSchedulerProvider object.\"\"\" self . _config = config PiecewiseConfig Bases: BaseModel Configuration for the piecewise learning rate scheduler. Attributes: Name Type Description name Literal ['piecewise'] Discriminator tag, must be \"piecewise\". scheduler PiecewiseSchedulerConfig Detailed configuration for the piecewise schedule. Source code in d9d/loop/auto/auto_lr_scheduler.py 10 11 12 13 14 15 16 17 18 19 20 21 class PiecewiseConfig ( BaseModel ): \"\"\" Configuration for the piecewise learning rate scheduler. Attributes: name: Discriminator tag, must be \"piecewise\". scheduler: Detailed configuration for the piecewise schedule. \"\"\" name : Literal [ \"piecewise\" ] = \"piecewise\" scheduler : PiecewiseSchedulerConfig Interface If you need a custom optimizer or learning rate scheduler, you implement the OptimizerProvider protocol. d9d.loop.control.optimizer_provider InitializeOptimizerStageContext dataclass Context data required to initialize an optimizer. Attributes: Name Type Description dist_context DistributedContext The distributed context. model Module The model instance for which parameters will be optimized. Source code in d9d/loop/control/optimizer_provider.py 12 13 14 15 16 17 18 19 20 21 22 23 @dataclasses . dataclass ( kw_only = True ) class InitializeOptimizerStageContext : \"\"\" Context data required to initialize an optimizer. Attributes: dist_context: The distributed context. model: The model instance for which parameters will be optimized. \"\"\" dist_context : DistributedContext model : nn . Module OptimizerProvider Bases: Protocol Protocol for defining how optimizers are created for model pipeline stages. Source code in d9d/loop/control/optimizer_provider.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 @typing . runtime_checkable class OptimizerProvider ( Protocol ): \"\"\" Protocol for defining how optimizers are created for model pipeline stages. \"\"\" @abc . abstractmethod def __call__ ( self , context : InitializeOptimizerStageContext ) -> Optimizer : \"\"\" Initializes the optimizer for a specific training stage. Args: context: Context for this operation. Returns: The instantiated PyTorch optimizer. \"\"\" __call__ ( context ) abstractmethod Initializes the optimizer for a specific training stage. Parameters: Name Type Description Default context InitializeOptimizerStageContext Context for this operation. required Returns: Type Description Optimizer The instantiated PyTorch optimizer. Source code in d9d/loop/control/optimizer_provider.py 32 33 34 35 36 37 38 39 40 41 42 @abc . abstractmethod def __call__ ( self , context : InitializeOptimizerStageContext ) -> Optimizer : \"\"\" Initializes the optimizer for a specific training stage. Args: context: Context for this operation. Returns: The instantiated PyTorch optimizer. \"\"\" d9d.loop.control.lr_scheduler_provider InitializeLRSchedulerContext dataclass Context data required to initialize an LR scheduler. Attributes: Name Type Description dist_context DistributedContext The distributed context. total_steps int The total number of training steps. optimizer Optimizer The optimizer instance that the scheduler will control. Source code in d9d/loop/control/lr_scheduler_provider.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 @dataclasses . dataclass ( kw_only = True ) class InitializeLRSchedulerContext : \"\"\" Context data required to initialize an LR scheduler. Attributes: dist_context: The distributed context. total_steps: The total number of training steps. optimizer: The optimizer instance that the scheduler will control. \"\"\" dist_context : DistributedContext total_steps : int optimizer : Optimizer LRSchedulerProvider Bases: Protocol Protocol for defining how Learning Rate schedulers are created. Source code in d9d/loop/control/lr_scheduler_provider.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 @typing . runtime_checkable class LRSchedulerProvider ( Protocol ): \"\"\" Protocol for defining how Learning Rate schedulers are created. \"\"\" @abc . abstractmethod def __call__ ( self , context : InitializeLRSchedulerContext ) -> LRSchedulerProtocol : \"\"\" Initializes the LR scheduler for a specific model pipeline stage. Args: context: Context for this operation. Returns: The instantiated LR scheduler adhering to the protocol. \"\"\" __call__ ( context ) abstractmethod Initializes the LR scheduler for a specific model pipeline stage. Parameters: Name Type Description Default context InitializeLRSchedulerContext Context for this operation. required Returns: Type Description LRSchedulerProtocol The instantiated LR scheduler adhering to the protocol. Source code in d9d/loop/control/lr_scheduler_provider.py 34 35 36 37 38 39 40 41 42 43 44 @abc . abstractmethod def __call__ ( self , context : InitializeLRSchedulerContext ) -> LRSchedulerProtocol : \"\"\" Initializes the LR scheduler for a specific model pipeline stage. Args: context: Context for this operation. Returns: The instantiated LR scheduler adhering to the protocol. \"\"\"","title":"Interfaces & Logic"},{"location":"0_loop/interfaces/#about","text":"The d9d training loop is agnostic to the specific model or data being trained. You interact with the loop by implementing Providers (factories) and Tasks (step logic). For standard use cases (like standard Optimizers), d9d provides Auto implementations that can be configured purely via Pydantic models, avoiding the need to write custom provider classes.","title":"About"},{"location":"0_loop/interfaces/#user-tasks","text":"A Task defines custom logic for a single train/inference step. Each Task may implement Stateful protocol, so you may store some mutable state here.","title":"User Tasks"},{"location":"0_loop/interfaces/#traintask","text":"It is responsible for logging metrics, mapping batch inputs before they are fed into the model, and for computing the task loss function value. Init : create_metrics(...) , dump_hparams(...) . Lifecycle : build_forward_inputs(...) (will be called once) -> compute_loss(...) (will be called multiple times if pipelining is enabled - once for each pipeline microbatch) -> update_metrics(...) (will be called once). Exit : finalize(...) . State Management : state_dict(...) , load_state_dict(...) .","title":"TrainTask"},{"location":"0_loop/interfaces/#inferencetask","text":"The InferenceTask defines the logic for a single inference step. It is designed to handle the forward-only flow, processing the raw tensors synthesized by the model (e.g., logits, hidden states). Lifecycle : build_forward_inputs(...) (called once) -> process_outputs(...) (called once per pipeline microbatch). Exit : finalize(...) . State Management : state_dict(...) , load_state_dict(...) .","title":"InferenceTask"},{"location":"0_loop/interfaces/#pipeline-state","text":"You may note that batch is only accessible in build_forward_inputs(...) method, but not in others. Don't worry! There is an object for transferring any state between the Task Lifecycle stages, - it is called PipelineState . ctx . state [ \"target\" ] = torch . tensor ([ 1 , 0 , 1 , 0 ], device = \"cuda\" ) # ... metrics [ \"accuracy\" ] . update ( ctx . state [ \"target\" ]) The pipeline state will automatically shard and unshard data if needed. You may read an additional documentation for its internal behaviour.","title":"Pipeline State"},{"location":"0_loop/interfaces/#example-implementation","text":"import torch from d9d.core.dist_context import DistributedContext from d9d.core.types import ScalarTree from d9d.module.block.head import LM_IGNORE_INDEX from d9d.loop.control import * class SFTTask ( TrainTask [ dict [ str , torch . Tensor ]]): def __init__ ( self , dist_ctx : DistributedContext ): self . _dist_ctx = dist_ctx def build_forward_inputs ( self , ctx : BuildForwardInputsContext ) -> BuildForwardInputsResult : # ctx.batch contains the output of the Collator. # Save labels in state for access during loss computation later ctx . state [ \"labels\" ] = ctx . batch [ \"labels\" ] # Return inputs for model.forward() # inputs are only for the first pipeline stage # kwargs are the same for all the pipeline stages return BuildForwardInputsResult ( inputs = { \"input_ids\" : ctx . batch [ \"input_ids\" ] }, kwargs = { \"labels\" : ctx . batch [ \"labels\" ], \"position_ids\" : ctx . batch [ \"position_ids\" ] } ) def dump_hparams ( self ) -> ScalarTree : return super () . dump_hparams () def compute_loss ( self , ctx : ComputeLossContext ) -> ComputeLossResult : # Retrieve log_probs calculated by the model pipeline logps = ctx . pipeline_results [ \"logps\" ] # Calculate number of valid tokens (ignoring the -100 padding) # This is crucial for variable length batches. num_loss_tokens = ( ctx . state [ \"labels\" ] != LM_IGNORE_INDEX ) . sum () # Calculate average loss per valid token total_loss = logps . sum () / num_loss_tokens return ComputeLossResult ( loss = total_loss , # loss_weight is used for gradient accumulation across the distributed world. # If batches have different token counts, we weigh the gradient # by token count to get a mathematical true average over the accumulation steps. loss_weight = num_loss_tokens / 1000 )","title":"Example Implementation"},{"location":"0_loop/interfaces/#api-reference","text":"","title":"API Reference"},{"location":"0_loop/interfaces/#d9d.loop.control.task","text":"","title":"task"},{"location":"0_loop/interfaces/#d9d.loop.control.task.BaseTask","text":"Bases: ABC , Stateful , Generic [ TBatch ] Abstract base class representing a unit of work (Task) in the training/inference loop. Source code in d9d/loop/control/task.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 class BaseTask ( abc . ABC , Stateful , typing . Generic [ TBatch ]): \"\"\"Abstract base class representing a unit of work (Task) in the training/inference loop.\"\"\" @abc . abstractmethod def build_forward_inputs ( self , ctx : BuildForwardInputsContext [ TBatch ]) -> BuildForwardInputsResult : \"\"\" Transforms raw data loaded from the DataLoader into arguments for the model. Args: ctx: Context object. Returns: Result object. \"\"\" ... def state_dict ( self ) -> dict [ str , Any ]: \"\"\" Returns the state dictionary for checkpointing this task. Returns: A dictionary containing the task's state. \"\"\" return {} def load_state_dict ( self , state_dict : dict [ str , Any ]) -> None : \"\"\" Restores the task's state from the provided dictionary. Args: state_dict: The state dictionary to load. \"\"\" # do nothing by default def finalize ( self , ctx : FinalizeContext ) -> None : \"\"\" Performs cleanup or final actions when the task execution finishes. Args: ctx: Context object. \"\"\"","title":"BaseTask"},{"location":"0_loop/interfaces/#d9d.loop.control.task.BaseTask.build_forward_inputs","text":"Transforms raw data loaded from the DataLoader into arguments for the model. Parameters: Name Type Description Default ctx BuildForwardInputsContext [ TBatch ] Context object. required Returns: Type Description BuildForwardInputsResult Result object. Source code in d9d/loop/control/task.py 65 66 67 68 69 70 71 72 73 74 75 76 77 @abc . abstractmethod def build_forward_inputs ( self , ctx : BuildForwardInputsContext [ TBatch ]) -> BuildForwardInputsResult : \"\"\" Transforms raw data loaded from the DataLoader into arguments for the model. Args: ctx: Context object. Returns: Result object. \"\"\" ...","title":"build_forward_inputs"},{"location":"0_loop/interfaces/#d9d.loop.control.task.BaseTask.finalize","text":"Performs cleanup or final actions when the task execution finishes. Parameters: Name Type Description Default ctx FinalizeContext Context object. required Source code in d9d/loop/control/task.py 98 99 100 101 102 103 104 def finalize ( self , ctx : FinalizeContext ) -> None : \"\"\" Performs cleanup or final actions when the task execution finishes. Args: ctx: Context object. \"\"\"","title":"finalize"},{"location":"0_loop/interfaces/#d9d.loop.control.task.BaseTask.load_state_dict","text":"Restores the task's state from the provided dictionary. Parameters: Name Type Description Default state_dict dict [ str , Any ] The state dictionary to load. required Source code in d9d/loop/control/task.py 89 90 91 92 93 94 95 def load_state_dict ( self , state_dict : dict [ str , Any ]) -> None : \"\"\" Restores the task's state from the provided dictionary. Args: state_dict: The state dictionary to load. \"\"\"","title":"load_state_dict"},{"location":"0_loop/interfaces/#d9d.loop.control.task.BaseTask.state_dict","text":"Returns the state dictionary for checkpointing this task. Returns: Type Description dict [ str , Any ] A dictionary containing the task's state. Source code in d9d/loop/control/task.py 79 80 81 82 83 84 85 86 87 def state_dict ( self ) -> dict [ str , Any ]: \"\"\" Returns the state dictionary for checkpointing this task. Returns: A dictionary containing the task's state. \"\"\" return {}","title":"state_dict"},{"location":"0_loop/interfaces/#d9d.loop.control.task.BuildForwardInputsContext","text":"Bases: Generic [ TBatch ] Context data to prepare inputs for the model forward pass. Attributes: Name Type Description batch TBatch The raw batch data loaded from the DataLoader object. state PipelineState The current state of the pipeline. You can assign any data to this state object, and it will be accessible during this pipeline step (e.g. when computing loss) Source code in d9d/loop/control/task.py 23 24 25 26 27 28 29 30 31 32 33 34 35 @dataclasses . dataclass ( kw_only = True ) class BuildForwardInputsContext ( typing . Generic [ TBatch ]): \"\"\" Context data to prepare inputs for the model forward pass. Attributes: batch: The raw batch data loaded from the DataLoader object. state: The current state of the pipeline. You can assign any data to this state object, and it will be accessible during this pipeline step (e.g. when computing loss) \"\"\" batch : TBatch state : \"PipelineState\"","title":"BuildForwardInputsContext"},{"location":"0_loop/interfaces/#d9d.loop.control.task.BuildForwardInputsResult","text":"The result of processing the raw batch into model inputs. Attributes: Name Type Description inputs dict [ str , Tensor ] A dictionary of inputs that are passed to model pipeline as input data (first stage only if using pipeline parallelism). kwargs dict [ str , Any ] A dictionary of keyword arguments passed to each pipeline stage. pipeline_sharding_spec PipelineShardingSpec | None A specification defining how inputs and kwargs should be split into micro-batches for pipeline parallelism. If None, the framework assumes standard behavior where all the non-scalar Tensors and lists are split by 0 dimension. Source code in d9d/loop/control/task.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 @dataclasses . dataclass ( kw_only = True ) class BuildForwardInputsResult : \"\"\" The result of processing the raw batch into model inputs. Attributes: inputs: A dictionary of inputs that are passed to model pipeline as input data (first stage only if using pipeline parallelism). kwargs: A dictionary of keyword arguments passed to each pipeline stage. pipeline_sharding_spec: A specification defining how inputs and kwargs should be split into micro-batches for pipeline parallelism. If None, the framework assumes standard behavior where all the non-scalar Tensors and lists are split by 0 dimension. \"\"\" inputs : dict [ str , torch . Tensor ] kwargs : dict [ str , Any ] pipeline_sharding_spec : PipelineShardingSpec | None = None","title":"BuildForwardInputsResult"},{"location":"0_loop/interfaces/#d9d.loop.control.task.ComputeLossContext","text":"Context data provided to calculate the loss during training. Attributes: Name Type Description pipeline_results Mapping [ str , Tensor ] The outputs returned by the model's forward pass. state PipelineState The current state of the pipeline. You can assign any data to this state object, and it will be accessible during this pipeline step (e.g. when calculating metrics) stepper Stepper Component tracking the current step. Source code in d9d/loop/control/task.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 @dataclasses . dataclass ( kw_only = True ) class ComputeLossContext : \"\"\" Context data provided to calculate the loss during training. Attributes: pipeline_results: The outputs returned by the model's forward pass. state: The current state of the pipeline. You can assign any data to this state object, and it will be accessible during this pipeline step (e.g. when calculating metrics) stepper: Component tracking the current step. \"\"\" pipeline_results : Mapping [ str , torch . Tensor ] state : \"PipelineState\" stepper : \"Stepper\"","title":"ComputeLossContext"},{"location":"0_loop/interfaces/#d9d.loop.control.task.ComputeLossResult","text":"The result of the loss computation. Attributes: Name Type Description loss Tensor The scalar tensor representing the loss to be backpropagated. loss_weight Tensor | None The weight to apply to the loss (for synchronizing gradients using weighted mean). None for 1.0. Source code in d9d/loop/control/task.py 124 125 126 127 128 129 130 131 132 133 134 135 136 @dataclasses . dataclass ( kw_only = True ) class ComputeLossResult : \"\"\" The result of the loss computation. Attributes: loss: The scalar tensor representing the loss to be backpropagated. loss_weight: The weight to apply to the loss (for synchronizing gradients using weighted mean). None for 1.0. \"\"\" loss : torch . Tensor loss_weight : torch . Tensor | None","title":"ComputeLossResult"},{"location":"0_loop/interfaces/#d9d.loop.control.task.CreateMetricsContext","text":"Context data provided to initialize metrics. Source code in d9d/loop/control/task.py 139 140 141 @dataclasses . dataclass ( kw_only = True ) class CreateMetricsContext : \"\"\"Context data provided to initialize metrics.\"\"\"","title":"CreateMetricsContext"},{"location":"0_loop/interfaces/#d9d.loop.control.task.CreateMetricsResult","text":"Result of metric initialization. Attributes: Name Type Description metrics dict [ str , Metric ] A dictionary mapping metric names to Metric instances. Source code in d9d/loop/control/task.py 144 145 146 147 148 149 150 151 152 153 @dataclasses . dataclass ( kw_only = True ) class CreateMetricsResult : \"\"\" Result of metric initialization. Attributes: metrics: A dictionary mapping metric names to Metric instances. \"\"\" metrics : dict [ str , \"Metric\" ]","title":"CreateMetricsResult"},{"location":"0_loop/interfaces/#d9d.loop.control.task.FinalizeContext","text":"Context data provided when the task is being finalized. Source code in d9d/loop/control/task.py 57 58 59 @dataclasses . dataclass ( kw_only = True ) class FinalizeContext : \"\"\"Context data provided when the task is being finalized.\"\"\"","title":"FinalizeContext"},{"location":"0_loop/interfaces/#d9d.loop.control.task.InferenceTask","text":"Bases: BaseTask , ABC , Generic [ TBatch ] Abstract base class for defining inference-specific logic. Source code in d9d/loop/control/task.py 263 264 265 266 267 268 269 270 271 272 273 274 275 class InferenceTask ( BaseTask , abc . ABC , typing . Generic [ TBatch ]): \"\"\"Abstract base class for defining inference-specific logic.\"\"\" @abc . abstractmethod def process_outputs ( self , ctx : ProcessOutputsContext ): \"\"\" Processes the model outputs (e.g. saving to disk, decoding tokens). Args: ctx: Context containing the model outputs and pipeline state. \"\"\" ...","title":"InferenceTask"},{"location":"0_loop/interfaces/#d9d.loop.control.task.InferenceTask.process_outputs","text":"Processes the model outputs (e.g. saving to disk, decoding tokens). Parameters: Name Type Description Default ctx ProcessOutputsContext Context containing the model outputs and pipeline state. required Source code in d9d/loop/control/task.py 266 267 268 269 270 271 272 273 274 275 @abc . abstractmethod def process_outputs ( self , ctx : ProcessOutputsContext ): \"\"\" Processes the model outputs (e.g. saving to disk, decoding tokens). Args: ctx: Context containing the model outputs and pipeline state. \"\"\" ...","title":"process_outputs"},{"location":"0_loop/interfaces/#d9d.loop.control.task.InferenceTaskProvider","text":"Bases: Protocol Protocol for a callable that creates an InferenceTask instance. Source code in d9d/loop/control/task.py 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 @typing . runtime_checkable class InferenceTaskProvider ( Protocol ): \"\"\"Protocol for a callable that creates an InferenceTask instance.\"\"\" def __call__ ( self , ctx : InferenceTaskProviderContext ) -> InferenceTask : \"\"\" Creates and returns a new InferenceTask. Args: ctx: Context providing distributed environment information. Returns: An instantiated InferenceTask. \"\"\" ...","title":"InferenceTaskProvider"},{"location":"0_loop/interfaces/#d9d.loop.control.task.InferenceTaskProvider.__call__","text":"Creates and returns a new InferenceTask. Parameters: Name Type Description Default ctx InferenceTaskProviderContext Context providing distributed environment information. required Returns: Type Description InferenceTask An instantiated InferenceTask. Source code in d9d/loop/control/task.py 294 295 296 297 298 299 300 301 302 303 304 def __call__ ( self , ctx : InferenceTaskProviderContext ) -> InferenceTask : \"\"\" Creates and returns a new InferenceTask. Args: ctx: Context providing distributed environment information. Returns: An instantiated InferenceTask. \"\"\" ...","title":"__call__"},{"location":"0_loop/interfaces/#d9d.loop.control.task.InferenceTaskProviderContext","text":"Context data provided to the factory creating an InferenceTask. Attributes: Name Type Description dist_context DistributedContext Information about the distributed environment. Source code in d9d/loop/control/task.py 278 279 280 281 282 283 284 285 286 287 @dataclasses . dataclass ( kw_only = True ) class InferenceTaskProviderContext : \"\"\" Context data provided to the factory creating an InferenceTask. Attributes: dist_context: Information about the distributed environment. \"\"\" dist_context : DistributedContext","title":"InferenceTaskProviderContext"},{"location":"0_loop/interfaces/#d9d.loop.control.task.ProcessOutputsContext","text":"Context data provided to process outputs during inference. Attributes: Name Type Description pipeline_results dict [ str , Tensor ] The outputs returned by the model's forward pass. state PipelineState The current state of the pipeline. Source code in d9d/loop/control/task.py 249 250 251 252 253 254 255 256 257 258 259 260 @dataclasses . dataclass ( kw_only = True ) class ProcessOutputsContext : \"\"\" Context data provided to process outputs during inference. Attributes: pipeline_results: The outputs returned by the model's forward pass. state: The current state of the pipeline. \"\"\" pipeline_results : dict [ str , torch . Tensor ] state : \"PipelineState\"","title":"ProcessOutputsContext"},{"location":"0_loop/interfaces/#d9d.loop.control.task.TrainTask","text":"Bases: BaseTask , ABC , Generic [ TBatch ] Abstract base class for defining training-specific logic. Source code in d9d/loop/control/task.py 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 class TrainTask ( BaseTask , abc . ABC , typing . Generic [ TBatch ]): \"\"\"Abstract base class for defining training-specific logic.\"\"\" @abc . abstractmethod def compute_loss ( self , ctx : ComputeLossContext ) -> ComputeLossResult : \"\"\" Calculates the loss based on model outputs. Args: ctx: Context object. Returns: Result object. \"\"\" ... def create_metrics ( self , ctx : CreateMetricsContext ) -> CreateMetricsResult : \"\"\" Initializes metrics to be tracked during training. Args: ctx: Context object. Returns: Result object. \"\"\" return CreateMetricsResult ( metrics = {}) def update_metrics ( self , ctx : UpdateMetricsContext ): \"\"\" Updates the state of the metrics at the end of training step. Args: ctx: Context object. \"\"\" def dump_hparams ( self ) -> ScalarTree : \"\"\" Exports hyperparameters associated with this task for logging. Returns: A dictionary of hyperparameter names and values. \"\"\" return {}","title":"TrainTask"},{"location":"0_loop/interfaces/#d9d.loop.control.task.TrainTask.compute_loss","text":"Calculates the loss based on model outputs. Parameters: Name Type Description Default ctx ComputeLossContext Context object. required Returns: Type Description ComputeLossResult Result object. Source code in d9d/loop/control/task.py 173 174 175 176 177 178 179 180 181 182 183 184 185 @abc . abstractmethod def compute_loss ( self , ctx : ComputeLossContext ) -> ComputeLossResult : \"\"\" Calculates the loss based on model outputs. Args: ctx: Context object. Returns: Result object. \"\"\" ...","title":"compute_loss"},{"location":"0_loop/interfaces/#d9d.loop.control.task.TrainTask.create_metrics","text":"Initializes metrics to be tracked during training. Parameters: Name Type Description Default ctx CreateMetricsContext Context object. required Returns: Type Description CreateMetricsResult Result object. Source code in d9d/loop/control/task.py 187 188 189 190 191 192 193 194 195 196 197 198 def create_metrics ( self , ctx : CreateMetricsContext ) -> CreateMetricsResult : \"\"\" Initializes metrics to be tracked during training. Args: ctx: Context object. Returns: Result object. \"\"\" return CreateMetricsResult ( metrics = {})","title":"create_metrics"},{"location":"0_loop/interfaces/#d9d.loop.control.task.TrainTask.dump_hparams","text":"Exports hyperparameters associated with this task for logging. Returns: Type Description ScalarTree A dictionary of hyperparameter names and values. Source code in d9d/loop/control/task.py 208 209 210 211 212 213 214 215 216 def dump_hparams ( self ) -> ScalarTree : \"\"\" Exports hyperparameters associated with this task for logging. Returns: A dictionary of hyperparameter names and values. \"\"\" return {}","title":"dump_hparams"},{"location":"0_loop/interfaces/#d9d.loop.control.task.TrainTask.update_metrics","text":"Updates the state of the metrics at the end of training step. Parameters: Name Type Description Default ctx UpdateMetricsContext Context object. required Source code in d9d/loop/control/task.py 200 201 202 203 204 205 206 def update_metrics ( self , ctx : UpdateMetricsContext ): \"\"\" Updates the state of the metrics at the end of training step. Args: ctx: Context object. \"\"\"","title":"update_metrics"},{"location":"0_loop/interfaces/#d9d.loop.control.task.TrainTaskProvider","text":"Bases: Protocol Protocol that creates a TrainTask instance. Source code in d9d/loop/control/task.py 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 @typing . runtime_checkable class TrainTaskProvider ( Protocol ): \"\"\"Protocol that creates a TrainTask instance.\"\"\" def __call__ ( self , ctx : TrainTaskProviderContext ) -> TrainTask : \"\"\" Creates and returns a new TrainTask. Args: ctx: Context object. Returns: An instantiated TrainTask. \"\"\" ...","title":"TrainTaskProvider"},{"location":"0_loop/interfaces/#d9d.loop.control.task.TrainTaskProvider.__call__","text":"Creates and returns a new TrainTask. Parameters: Name Type Description Default ctx TrainTaskProviderContext Context object. required Returns: Type Description TrainTask An instantiated TrainTask. Source code in d9d/loop/control/task.py 235 236 237 238 239 240 241 242 243 244 245 246 def __call__ ( self , ctx : TrainTaskProviderContext ) -> TrainTask : \"\"\" Creates and returns a new TrainTask. Args: ctx: Context object. Returns: An instantiated TrainTask. \"\"\" ...","title":"__call__"},{"location":"0_loop/interfaces/#d9d.loop.control.task.TrainTaskProviderContext","text":"Context data provided to the factory creating a TrainTask. Attributes: Name Type Description dist_context DistributedContext Information about the distributed environment. Source code in d9d/loop/control/task.py 219 220 221 222 223 224 225 226 227 228 @dataclasses . dataclass ( kw_only = True ) class TrainTaskProviderContext : \"\"\" Context data provided to the factory creating a TrainTask. Attributes: dist_context: Information about the distributed environment. \"\"\" dist_context : DistributedContext","title":"TrainTaskProviderContext"},{"location":"0_loop/interfaces/#d9d.loop.control.task.UpdateMetricsContext","text":"Context data provided to update metrics after a step. Attributes: Name Type Description state PipelineState The current state of the pipeline. metrics Mapping [ str , Metric ] The dictionary of metrics to be updated. Source code in d9d/loop/control/task.py 156 157 158 159 160 161 162 163 164 165 166 167 @dataclasses . dataclass ( kw_only = True ) class UpdateMetricsContext : \"\"\" Context data provided to update metrics after a step. Attributes: state: The current state of the pipeline. metrics: The dictionary of metrics to be updated. \"\"\" state : \"PipelineState\" metrics : Mapping [ str , \"Metric\" ]","title":"UpdateMetricsContext"},{"location":"0_loop/interfaces/#model-definition","text":"","title":"Model Definition"},{"location":"0_loop/interfaces/#modelprovider","text":"The ModelProvider controls the lifecycle of the nn.Module . In distributed training, models are rarely just \"instantiated\". They must be initialized, parallelized, and mapped for loading from checkpoint.","title":"ModelProvider"},{"location":"0_loop/interfaces/#how-to-write-a-modelprovider","text":"","title":"How to Write a ModelProvider"},{"location":"0_loop/interfaces/#choose-a-model","text":"Choose a model from d9d's catalogue or create it by your own.","title":"Choose a Model"},{"location":"0_loop/interfaces/#implement-initialize_model_stage","text":"Implement the initialize_model_stage(...) method - it should prepare a nn.Module for specified pipeline parallel stage containing model architecture in a target torch.dtype . Note that models are initialized on meta device , so you must not load model weights here. Instead, this function should return a State Mapper that will map model weights on disk to model weights in-memory . You also may apply PEFT methods here and other architectural patches, but make sure you respect the changes they made in returned State Mapper .","title":"Implement initialize_model_stage(...)"},{"location":"0_loop/interfaces/#implement-parallelize_model_stage","text":"Implement the parallelize_model_stage(...) method - it should apply Horizontal Parallelism strategy for selected model in-place. If you use one of d9d's models, you may use default strategies for them such as parallelize_qwen3_moe_for_causal_lm ( reference ). For a custom model, please see Horizontal Parallelism docs and reference implementations.","title":"Implement parallelize_model_stage(...)"},{"location":"0_loop/interfaces/#implement-prepare_export_model_stage","text":"Implement the prepare_export_model_stage(...) method - it should return a State Mapper that converts in-memory model state to that one that will be saved on disk during final export. Basically, it should reverse all the operations of State Mapper produced in initialize_model_stage(...) .","title":"Implement prepare_export_model_stage(...)"},{"location":"0_loop/interfaces/#example-implementation_1","text":"from pydantic import BaseModel from d9d.loop.control.model_provider import * from d9d.module.model.qwen3_moe import Qwen3MoEForCausalLM , Qwen3MoEForCausalLMParameters from d9d.module.parallelism.model.qwen3_moe import parallelize_qwen3_moe_for_causal_lm from d9d.module.block.hidden_states_aggregator import HiddenStatesAggregationMode from d9d.model_state.mapper.adapters import identity_mapper_from_module class ModelProviderConfig ( BaseModel ): model : Qwen3MoEForCausalLMParameters # Hyperparameters for Qwen3 MoE checkpointing : bool # Enable gradient checkpointing to save VRAM class ProjectModelProvider ( ModelProvider [ Qwen3MoEForCausalLM ]): def __init__ ( self , config : ModelProviderConfig ): self . _config = config def initialize_model_stage ( self , context : InitializeModelStageContext ) -> InitializeModelStageResult : # Initialize the raw model on Meta device in BF16 precision model = Qwen3MoEForCausalLM ( params = self . _config . model , stage = context . stage , hidden_states_snapshot_mode = HiddenStatesAggregationMode . no , enable_checkpointing = self . _config . checkpointing ) . bfloat16 () return InitializeModelStageResult ( model = model , state_mapper = identity_mapper_from_module ( model ) ) def parallelize_model_stage ( self , context : ParallelizeModelStageContext ): # Applies specific distributed strategies # suited for Qwen3 MoE architecture. # You can apply your own horizontal parallelism strategy here. parallelize_qwen3_moe_for_causal_lm ( dist_context = context . dist_context , stage = context . stage , model = context . model ) def prepare_export_model_stage ( self , context : PrepareExportModelStageContext ) -> PrepareExportModelStageResult : # When exporting, save model weights as-is return PrepareExportModelStageResult ( state_mapper = identity_mapper_from_module ( context . model ) ) def dump_hparams ( self ) -> ScalarTree : return self . _config . model_dump ( mode = \"json\" )","title":"Example Implementation"},{"location":"0_loop/interfaces/#api-reference_1","text":"","title":"API Reference"},{"location":"0_loop/interfaces/#d9d.loop.control.model_provider","text":"","title":"model_provider"},{"location":"0_loop/interfaces/#d9d.loop.control.model_provider.InitializeModelStageContext","text":"Context data required for initializing a specific model pipeline stage. Attributes: Name Type Description dist_context DistributedContext The distributed execution context. stage PipelineStageInfo Metadata describing the current pipeline stage being initialized. Source code in d9d/loop/control/model_provider.py 13 14 15 16 17 18 19 20 21 22 23 24 @dataclasses . dataclass ( kw_only = True ) class InitializeModelStageContext : \"\"\" Context data required for initializing a specific model pipeline stage. Attributes: dist_context: The distributed execution context. stage: Metadata describing the current pipeline stage being initialized. \"\"\" dist_context : DistributedContext stage : PipelineStageInfo","title":"InitializeModelStageContext"},{"location":"0_loop/interfaces/#d9d.loop.control.model_provider.InitializeModelStageResult","text":"Bases: Generic [ TModel ] The result of initializing a model stage. Attributes: Name Type Description model TModel The PyTorch module. state_mapper ModelStateMapper The mapper defining how to load weights into this module. Source code in d9d/loop/control/model_provider.py 30 31 32 33 34 35 36 37 38 39 40 41 @dataclasses . dataclass ( kw_only = True ) class InitializeModelStageResult ( Generic [ TModel ]): \"\"\" The result of initializing a model stage. Attributes: model: The PyTorch module. state_mapper: The mapper defining how to load weights into this module. \"\"\" model : TModel state_mapper : ModelStateMapper","title":"InitializeModelStageResult"},{"location":"0_loop/interfaces/#d9d.loop.control.model_provider.ModelProvider","text":"Bases: ABC , Generic [ TModel ] Abstract interface for defining the lifecycle of a distributed model. This provider handles initialization, parallelization (sharding/replication/etc), and export preparation for models within the d9d framework. Source code in d9d/loop/control/model_provider.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 class ModelProvider ( abc . ABC , Generic [ TModel ]): \"\"\" Abstract interface for defining the lifecycle of a distributed model. This provider handles initialization, parallelization (sharding/replication/etc), and export preparation for models within the d9d framework. \"\"\" @abc . abstractmethod def initialize_model_stage ( self , context : InitializeModelStageContext ) -> InitializeModelStageResult [ TModel ]: \"\"\" Initializes the model architecture for a specific pipeline stage. This method is responsible for constructing the `nn.Module` for the requested stage. Construction occurs within a meta-device context; therefore, weights should not be loaded directly here. Instead, a `ModelStateMapper` must be returned to define how weights from a checkpoint map to the newly created module parameters. This allows for architecture modifications, such as injecting LoRA adapters, provided that the returned mapper reflects the new structure. Args: context: Context for this operation. Returns: Result of this operation. \"\"\" ... @abc . abstractmethod def parallelize_model_stage ( self , context : ParallelizeModelStageContext [ TModel ]): \"\"\" Converts the model parameters into distributed tensors (DTensors). Implementations should modify the model in-place. This involves converting standard parameters into DTensors by replicating or sharding them according to the desired parallelism strategies. Args: context: Context for this operation. \"\"\" @abc . abstractmethod def prepare_export_model_stage ( self , context : PrepareExportModelStageContext [ TModel ] ) -> PrepareExportModelStageResult : \"\"\" Prepares the state mapper required for saving the model to disk. This methods defines how the current in-memory model structure maps back to the serialized checkpoint format. Args: context: Context for this operation. Returns: Result of this operation. \"\"\" def dump_hparams ( self ) -> ScalarTree : \"\"\" Exports hyperparameters associated with this model for logging. Returns: A dictionary of hyperparameter names and values. \"\"\" return {}","title":"ModelProvider"},{"location":"0_loop/interfaces/#d9d.loop.control.model_provider.ModelProvider.dump_hparams","text":"Exports hyperparameters associated with this model for logging. Returns: Type Description ScalarTree A dictionary of hyperparameter names and values. Source code in d9d/loop/control/model_provider.py 147 148 149 150 151 152 153 154 155 def dump_hparams ( self ) -> ScalarTree : \"\"\" Exports hyperparameters associated with this model for logging. Returns: A dictionary of hyperparameter names and values. \"\"\" return {}","title":"dump_hparams"},{"location":"0_loop/interfaces/#d9d.loop.control.model_provider.ModelProvider.initialize_model_stage","text":"Initializes the model architecture for a specific pipeline stage. This method is responsible for constructing the nn.Module for the requested stage. Construction occurs within a meta-device context; therefore, weights should not be loaded directly here. Instead, a ModelStateMapper must be returned to define how weights from a checkpoint map to the newly created module parameters. This allows for architecture modifications, such as injecting LoRA adapters, provided that the returned mapper reflects the new structure. Parameters: Name Type Description Default context InitializeModelStageContext Context for this operation. required Returns: Type Description InitializeModelStageResult [ TModel ] Result of this operation. Source code in d9d/loop/control/model_provider.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 @abc . abstractmethod def initialize_model_stage ( self , context : InitializeModelStageContext ) -> InitializeModelStageResult [ TModel ]: \"\"\" Initializes the model architecture for a specific pipeline stage. This method is responsible for constructing the `nn.Module` for the requested stage. Construction occurs within a meta-device context; therefore, weights should not be loaded directly here. Instead, a `ModelStateMapper` must be returned to define how weights from a checkpoint map to the newly created module parameters. This allows for architecture modifications, such as injecting LoRA adapters, provided that the returned mapper reflects the new structure. Args: context: Context for this operation. Returns: Result of this operation. \"\"\" ...","title":"initialize_model_stage"},{"location":"0_loop/interfaces/#d9d.loop.control.model_provider.ModelProvider.parallelize_model_stage","text":"Converts the model parameters into distributed tensors (DTensors). Implementations should modify the model in-place. This involves converting standard parameters into DTensors by replicating or sharding them according to the desired parallelism strategies. Parameters: Name Type Description Default context ParallelizeModelStageContext [ TModel ] Context for this operation. required Source code in d9d/loop/control/model_provider.py 117 118 119 120 121 122 123 124 125 126 127 128 @abc . abstractmethod def parallelize_model_stage ( self , context : ParallelizeModelStageContext [ TModel ]): \"\"\" Converts the model parameters into distributed tensors (DTensors). Implementations should modify the model in-place. This involves converting standard parameters into DTensors by replicating or sharding them according to the desired parallelism strategies. Args: context: Context for this operation. \"\"\"","title":"parallelize_model_stage"},{"location":"0_loop/interfaces/#d9d.loop.control.model_provider.ModelProvider.prepare_export_model_stage","text":"Prepares the state mapper required for saving the model to disk. This methods defines how the current in-memory model structure maps back to the serialized checkpoint format. Parameters: Name Type Description Default context PrepareExportModelStageContext [ TModel ] Context for this operation. required Returns: Type Description PrepareExportModelStageResult Result of this operation. Source code in d9d/loop/control/model_provider.py 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 @abc . abstractmethod def prepare_export_model_stage ( self , context : PrepareExportModelStageContext [ TModel ] ) -> PrepareExportModelStageResult : \"\"\" Prepares the state mapper required for saving the model to disk. This methods defines how the current in-memory model structure maps back to the serialized checkpoint format. Args: context: Context for this operation. Returns: Result of this operation. \"\"\"","title":"prepare_export_model_stage"},{"location":"0_loop/interfaces/#d9d.loop.control.model_provider.ParallelizeModelStageContext","text":"Bases: Generic [ TModel ] Context data required for horizontally parallelizing a model stage. Attributes: Name Type Description dist_context DistributedContext The distributed execution context. stage PipelineStageInfo Metadata describing the current pipeline stage. model TModel The PyTorch module to be parallelized. Source code in d9d/loop/control/model_provider.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 @dataclasses . dataclass ( kw_only = True ) class ParallelizeModelStageContext ( Generic [ TModel ]): \"\"\" Context data required for horizontally parallelizing a model stage. Attributes: dist_context: The distributed execution context. stage: Metadata describing the current pipeline stage. model: The PyTorch module to be parallelized. \"\"\" dist_context : DistributedContext stage : PipelineStageInfo model : TModel","title":"ParallelizeModelStageContext"},{"location":"0_loop/interfaces/#d9d.loop.control.model_provider.PrepareExportModelStageContext","text":"Bases: Generic [ TModel ] Context data required for preparing a model stage for export. Attributes: Name Type Description dist_context DistributedContext The distributed execution context. model TModel The PyTorch module to be exported. Source code in d9d/loop/control/model_provider.py 60 61 62 63 64 65 66 67 68 69 70 71 @dataclasses . dataclass ( kw_only = True ) class PrepareExportModelStageContext ( Generic [ TModel ]): \"\"\" Context data required for preparing a model stage for export. Attributes: dist_context: The distributed execution context. model: The PyTorch module to be exported. \"\"\" dist_context : DistributedContext model : TModel","title":"PrepareExportModelStageContext"},{"location":"0_loop/interfaces/#d9d.loop.control.model_provider.PrepareExportModelStageResult","text":"The result of preparing a model stage for export. Attributes: Name Type Description state_mapper ModelStateMapper The mapper defining how model parameters map to disk storage. Source code in d9d/loop/control/model_provider.py 74 75 76 77 78 79 80 81 82 83 @dataclasses . dataclass ( kw_only = True ) class PrepareExportModelStageResult : \"\"\" The result of preparing a model stage for export. Attributes: state_mapper: The mapper defining how model parameters map to disk storage. \"\"\" state_mapper : ModelStateMapper","title":"PrepareExportModelStageResult"},{"location":"0_loop/interfaces/#data-loading","text":"","title":"Data Loading"},{"location":"0_loop/interfaces/#datasetprovider","text":"The DatasetProvider is responsible for creating dataset and data collator instances.","title":"DatasetProvider"},{"location":"0_loop/interfaces/#distributed-awareness","text":"d9d will not apply sharding to your dataset automatically. You have to configure it manually (optionally applying other dataset wrappers). Please see the Dataset Utilities documentation.","title":"Distributed-Awareness"},{"location":"0_loop/interfaces/#example-implementation_2","text":"from typing import Any , Sequence import torch import datasets from pydantic import BaseModel from tokenizers import Tokenizer from d9d.core.types import TensorTree from d9d.dataset import BufferSortedDataset , shard_dataset_data_parallel , DatasetImplementingSortKeyProtocol from d9d.loop.control.dataset_provider import * class ProjectDataset ( Dataset , DatasetImplementingSortKeyProtocol ): def __init__ ( self , dataset : datasets . Dataset , tokenizer : Tokenizer ): self . _dataset = dataset self . _tokenizer = tokenizer def sort_key ( self , index : int ) -> Any : # Used by BufferSortedDataset to group examples of similar length together. # This minimizes padding overhead in batches. return self . _dataset [ index ][ \"token_counts\" ] def __getitem__ ( self , index : int ) -> TensorTree : return { ... } @classmethod def collate ( cls , batch : Sequence [ dict [ str , torch . Tensor ]]) -> dict [ str , torch . Tensor ]: return { ... } def __len__ ( self ) -> int : return len ( self . _dataset ) class DataConfig ( BaseModel ): dataset : str # HuggingFace dataset path/name split : str # e.g., 'train', 'validation' text_column : str # The column containing the raw text use_samples : int # Limit dataset size for testing/debugging shuffle_seed : int # Distinct seed for shuffling the data tokenizer : str # Path to the tokenizer.json file num_proc : int # Number of CPU processes for data mapping presort_buffer_size : int # Size of buffer for length-based presorting class ProjectDatasetProvider ( DatasetProvider ): def __init__ ( self , config : DataConfig ): self . _config = config @staticmethod def _count_tokens ( item : dict , text_column : str , tokenizer : Tokenizer ) -> dict : return { \"token_counts\" : len ( tokenizer . encode ( item [ text_column ]) . tokens ) } def __call__ ( self , context : InitializeDatasetContext ) -> InitializeDatasetResult : tokenizer = Tokenizer . from_file ( str ( self . _config . tokenizer )) # IMPORTANT: main_process_first ensures that Rank 0 downloads/processes # the dataset and builds the cache first. Ranks 1-N wait, then load from cache. # Prevents race conditions and corruption on the HF cache. with context . dist_context . main_process_first (): data = datasets . load_dataset ( self . _config . dataset , split = self . _config . split ) . take ( self . _config . use_samples ) . shuffle ( self . _config . shuffle_seed ) . map ( self . _count_tokens , num_proc = self . _config . num_proc , fn_kwargs = { \"tokenizer\" : tokenizer , \"text_column\" : self . _config . text_column } ) dataset = ProjectDataset ( data , tokenizer ) # BufferSortedDataset acts as a buffer that shuffles data locally # but outputs batches sorted by length (defined in sort_key above) dataset_buf = BufferSortedDataset ( dataset , buffer_size = self . _config . presort_buffer_size , pack_size = context . batch_maths . global_batch_size , init_seed = self . _config . shuffle_seed ) # Split dataset across data parallel ranks dataset_shard = shard_dataset_data_parallel ( dataset_buf , context . dist_context ) return InitializeDatasetResult ( dataset = dataset_shard , collator = ProjectDataset . collate )","title":"Example Implementation"},{"location":"0_loop/interfaces/#api-reference_2","text":"","title":"API Reference"},{"location":"0_loop/interfaces/#d9d.loop.control.dataset_provider","text":"","title":"dataset_provider"},{"location":"0_loop/interfaces/#d9d.loop.control.dataset_provider.DatasetProvider","text":"Bases: Protocol Protocol that allows users to define how datasets are loaded and collated. Users should subclass this to provide custom data loading logic. Source code in d9d/loop/control/dataset_provider.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 @typing . runtime_checkable class DatasetProvider ( Protocol ): \"\"\"Protocol that allows users to define how datasets are loaded and collated. Users should subclass this to provide custom data loading logic. \"\"\" def __call__ ( self , context : InitializeDatasetContext ) -> InitializeDatasetResult : \"\"\" Initializes the dataset components. It is important that the user must shard the dataset manually, perhaps using `d9d.dataset.ShardedDataset`. Args: context: Context for this operation. Returns: Result of this operation. \"\"\"","title":"DatasetProvider"},{"location":"0_loop/interfaces/#d9d.loop.control.dataset_provider.DatasetProvider.__call__","text":"Initializes the dataset components. It is important that the user must shard the dataset manually, perhaps using d9d.dataset.ShardedDataset . Parameters: Name Type Description Default context InitializeDatasetContext Context for this operation. required Returns: Type Description InitializeDatasetResult Result of this operation. Source code in d9d/loop/control/dataset_provider.py 47 48 49 50 51 52 53 54 55 56 57 58 def __call__ ( self , context : InitializeDatasetContext ) -> InitializeDatasetResult : \"\"\" Initializes the dataset components. It is important that the user must shard the dataset manually, perhaps using `d9d.dataset.ShardedDataset`. Args: context: Context for this operation. Returns: Result of this operation. \"\"\"","title":"__call__"},{"location":"0_loop/interfaces/#d9d.loop.control.dataset_provider.InitializeDatasetContext","text":"Context data required to initialize a dataset provider. Attributes: Name Type Description dist_context DistributedContext The distributed context containing rank and world size information. batch_maths BatchMaths The batch maths component handling global batch size calculations. Source code in d9d/loop/control/dataset_provider.py 14 15 16 17 18 19 20 21 22 23 24 @dataclasses . dataclass ( kw_only = True ) class InitializeDatasetContext : \"\"\"Context data required to initialize a dataset provider. Attributes: dist_context: The distributed context containing rank and world size information. batch_maths: The batch maths component handling global batch size calculations. \"\"\" dist_context : DistributedContext batch_maths : \"BatchMaths\"","title":"InitializeDatasetContext"},{"location":"0_loop/interfaces/#d9d.loop.control.dataset_provider.InitializeDatasetResult","text":"The result of initializing a dataset provider. Attributes: Name Type Description dataset Dataset The instantiated PyTorch Dataset. collator CollateFn The function used to collate individual samples into a batch. Source code in d9d/loop/control/dataset_provider.py 27 28 29 30 31 32 33 34 35 36 37 @dataclasses . dataclass ( kw_only = True ) class InitializeDatasetResult : \"\"\"The result of initializing a dataset provider. Attributes: dataset: The instantiated PyTorch Dataset. collator: The function used to collate individual samples into a batch. \"\"\" dataset : Dataset collator : CollateFn","title":"InitializeDatasetResult"},{"location":"0_loop/interfaces/#optimization-scheduling","text":"","title":"Optimization &amp; Scheduling"},{"location":"0_loop/interfaces/#auto-implementations","text":"For standard PyTorch usage, d9d includes the d9d.loop.auto package. These providers ingest a Pydantic configuration object and manage the creation of standard optimizers and schedulers.","title":"Auto Implementations"},{"location":"0_loop/interfaces/#auto-optimizer","text":"Supports AdamW , Adam , SGD , and StochasticAdamW . from d9d.loop.auto import AutoOptimizerProvider , AutoOptimizerConfig provider = AutoOptimizerProvider ( AutoOptimizerConfig . model_validate_json ( '{\"name\": \"adamw\", \"lr\": 1e-4}' ) )","title":"Auto Optimizer"},{"location":"0_loop/interfaces/#d9d.loop.auto.auto_optimizer","text":"","title":"auto_optimizer"},{"location":"0_loop/interfaces/#d9d.loop.auto.auto_optimizer.AdamOptimizerConfig","text":"Bases: BaseAutoOptimizerConfig Configuration for the PyTorch Adam optimizer. Attributes: Name Type Description name Literal ['adam'] Discriminator tag. lr float The learning rate. betas tuple [ float , float ] Coefficients for computing running averages of gradient and its square. eps float Term added to the denominator to improve numerical stability. weight_decay float Weight decay coefficient. decoupled_weight_decay bool Whether to apply decoupled weight decay. amsgrad bool Whether to use the AMSGrad variant. maximize bool Whether to maximize the params based on the objective. Source code in d9d/loop/auto/auto_optimizer.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 class AdamOptimizerConfig ( BaseAutoOptimizerConfig ): \"\"\" Configuration for the PyTorch Adam optimizer. Attributes: name: Discriminator tag. lr: The learning rate. betas: Coefficients for computing running averages of gradient and its square. eps: Term added to the denominator to improve numerical stability. weight_decay: Weight decay coefficient. decoupled_weight_decay: Whether to apply decoupled weight decay. amsgrad: Whether to use the AMSGrad variant. maximize: Whether to maximize the params based on the objective. \"\"\" name : Literal [ \"adam\" ] = \"adam\" lr : float betas : tuple [ float , float ] = ( 0.9 , 0.999 ) eps : float = 1e-8 weight_decay : float = 1e-2 decoupled_weight_decay : bool = False amsgrad : bool = False maximize : bool = False def build ( self , params : Iterable [ nn . Parameter ]) -> Optimizer : \"\"\"Builds fused Adam with the configured parameters.\"\"\" return Adam ( params = params , lr = self . lr , betas = self . betas , eps = self . eps , weight_decay = self . weight_decay , decoupled_weight_decay = self . decoupled_weight_decay , amsgrad = self . amsgrad , maximize = self . maximize , fused = True , )","title":"AdamOptimizerConfig"},{"location":"0_loop/interfaces/#d9d.loop.auto.auto_optimizer.AdamOptimizerConfig.build","text":"Builds fused Adam with the configured parameters. Source code in d9d/loop/auto/auto_optimizer.py 129 130 131 132 133 134 135 136 137 138 139 140 141 def build ( self , params : Iterable [ nn . Parameter ]) -> Optimizer : \"\"\"Builds fused Adam with the configured parameters.\"\"\" return Adam ( params = params , lr = self . lr , betas = self . betas , eps = self . eps , weight_decay = self . weight_decay , decoupled_weight_decay = self . decoupled_weight_decay , amsgrad = self . amsgrad , maximize = self . maximize , fused = True , )","title":"build"},{"location":"0_loop/interfaces/#d9d.loop.auto.auto_optimizer.AdamWOptimizerConfig","text":"Bases: BaseAutoOptimizerConfig Configuration for the PyTorch AdamW optimizer. Attributes: Name Type Description name Literal ['adamw'] Discriminator tag. lr float The learning rate. betas tuple [ float , float ] Coefficients for computing running averages of gradient and its square. eps float Term added to the denominator to improve numerical stability. weight_decay float Weight decay coefficient. amsgrad bool Whether to use the AMSGrad variant. maximize bool Whether to maximize the params based on the objective (as opposed to minimizing). Source code in d9d/loop/auto/auto_optimizer.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 class AdamWOptimizerConfig ( BaseAutoOptimizerConfig ): \"\"\" Configuration for the PyTorch AdamW optimizer. Attributes: name: Discriminator tag. lr: The learning rate. betas: Coefficients for computing running averages of gradient and its square. eps: Term added to the denominator to improve numerical stability. weight_decay: Weight decay coefficient. amsgrad: Whether to use the AMSGrad variant. maximize: Whether to maximize the params based on the objective (as opposed to minimizing). \"\"\" name : Literal [ \"adamw\" ] = \"adamw\" lr : float betas : tuple [ float , float ] = ( 0.9 , 0.999 ) eps : float = 1e-8 weight_decay : float = 1e-2 amsgrad : bool = False maximize : bool = False def build ( self , params : Iterable [ nn . Parameter ]) -> Optimizer : \"\"\"Builds fused AdamW with the configured parameters.\"\"\" return AdamW ( params = params , lr = self . lr , betas = self . betas , eps = self . eps , weight_decay = self . weight_decay , amsgrad = self . amsgrad , maximize = self . maximize , fused = True , )","title":"AdamWOptimizerConfig"},{"location":"0_loop/interfaces/#d9d.loop.auto.auto_optimizer.AdamWOptimizerConfig.build","text":"Builds fused AdamW with the configured parameters. Source code in d9d/loop/auto/auto_optimizer.py 90 91 92 93 94 95 96 97 98 99 100 101 def build ( self , params : Iterable [ nn . Parameter ]) -> Optimizer : \"\"\"Builds fused AdamW with the configured parameters.\"\"\" return AdamW ( params = params , lr = self . lr , betas = self . betas , eps = self . eps , weight_decay = self . weight_decay , amsgrad = self . amsgrad , maximize = self . maximize , fused = True , )","title":"build"},{"location":"0_loop/interfaces/#d9d.loop.auto.auto_optimizer.AutoOptimizerProvider","text":"Bases: OptimizerProvider OptimizerProvider that builds a PyTorch optimizer based on a configuration object. Source code in d9d/loop/auto/auto_optimizer.py 187 188 189 190 191 192 193 194 195 196 197 class AutoOptimizerProvider ( OptimizerProvider ): \"\"\" OptimizerProvider that builds a PyTorch optimizer based on a configuration object. \"\"\" def __init__ ( self , config : AutoOptimizerConfig ): \"\"\"Constructs the provider with the given configuration.\"\"\" self . _config = config def __call__ ( self , context : InitializeOptimizerStageContext ) -> Optimizer : return self . _config . build ( context . model . parameters ())","title":"AutoOptimizerProvider"},{"location":"0_loop/interfaces/#d9d.loop.auto.auto_optimizer.AutoOptimizerProvider.__init__","text":"Constructs the provider with the given configuration. Source code in d9d/loop/auto/auto_optimizer.py 192 193 194 def __init__ ( self , config : AutoOptimizerConfig ): \"\"\"Constructs the provider with the given configuration.\"\"\" self . _config = config","title":"__init__"},{"location":"0_loop/interfaces/#d9d.loop.auto.auto_optimizer.BaseAutoOptimizerConfig","text":"Bases: BaseModel , ABC Abstract base class for optimizer configurations. Source code in d9d/loop/auto/auto_optimizer.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 class BaseAutoOptimizerConfig ( BaseModel , ABC ): \"\"\" Abstract base class for optimizer configurations. \"\"\" @abc . abstractmethod def build ( self , params : Iterable [ nn . Parameter ]) -> Optimizer : \"\"\" Creates the PyTorch optimizer instance. Args: params: An iterable of model parameters to optimize. Returns: The instantiated optimizer. \"\"\" ...","title":"BaseAutoOptimizerConfig"},{"location":"0_loop/interfaces/#d9d.loop.auto.auto_optimizer.BaseAutoOptimizerConfig.build","text":"Creates the PyTorch optimizer instance. Parameters: Name Type Description Default params Iterable [ Parameter ] An iterable of model parameters to optimize. required Returns: Type Description Optimizer The instantiated optimizer. Source code in d9d/loop/auto/auto_optimizer.py 20 21 22 23 24 25 26 27 28 29 30 31 @abc . abstractmethod def build ( self , params : Iterable [ nn . Parameter ]) -> Optimizer : \"\"\" Creates the PyTorch optimizer instance. Args: params: An iterable of model parameters to optimize. Returns: The instantiated optimizer. \"\"\" ...","title":"build"},{"location":"0_loop/interfaces/#d9d.loop.auto.auto_optimizer.SGDOptimizerConfig","text":"Bases: BaseAutoOptimizerConfig Configuration for the PyTorch SGD optimizer. Attributes: Name Type Description name Literal ['sgd'] Discriminator tag. lr float The learning rate. momentum float Momentum factor. dampening float Dampening for momentum. weight_decay float Weight decay (L2 penalty). nesterov bool Enables Nesterov momentum. maximize bool Whether to maximize the params based on the objective. Source code in d9d/loop/auto/auto_optimizer.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 class SGDOptimizerConfig ( BaseAutoOptimizerConfig ): \"\"\" Configuration for the PyTorch SGD optimizer. Attributes: name: Discriminator tag. lr: The learning rate. momentum: Momentum factor. dampening: Dampening for momentum. weight_decay: Weight decay (L2 penalty). nesterov: Enables Nesterov momentum. maximize: Whether to maximize the params based on the objective. \"\"\" name : Literal [ \"sgd\" ] = \"sgd\" lr : float momentum : float = 0 dampening : float = 0 weight_decay : float = 0 nesterov : bool = False maximize : bool = False def build ( self , params : Iterable [ nn . Parameter ]) -> Optimizer : \"\"\"Builds fused SGD with the configured parameters.\"\"\" return SGD ( params , lr = self . lr , momentum = self . momentum , dampening = self . dampening , weight_decay = self . weight_decay , nesterov = self . nesterov , maximize = self . maximize , fused = True , )","title":"SGDOptimizerConfig"},{"location":"0_loop/interfaces/#d9d.loop.auto.auto_optimizer.SGDOptimizerConfig.build","text":"Builds fused SGD with the configured parameters. Source code in d9d/loop/auto/auto_optimizer.py 167 168 169 170 171 172 173 174 175 176 177 178 def build ( self , params : Iterable [ nn . Parameter ]) -> Optimizer : \"\"\"Builds fused SGD with the configured parameters.\"\"\" return SGD ( params , lr = self . lr , momentum = self . momentum , dampening = self . dampening , weight_decay = self . weight_decay , nesterov = self . nesterov , maximize = self . maximize , fused = True , )","title":"build"},{"location":"0_loop/interfaces/#d9d.loop.auto.auto_optimizer.StochasticAdamWOptimizerConfig","text":"Bases: BaseAutoOptimizerConfig Configuration for the Stochastic AdamW optimizer. Attributes: Name Type Description name Literal ['stochastic_adamw'] Discriminator tag. lr float Learning rate. betas tuple [ float , float ] Coefficients used for computing running averages of gradient and its square. eps float Term added to the denominator to improve numerical stability. weight_decay float Weight decay coefficient. state_dtype str Data Type to use for the optimizer states. Source code in d9d/loop/auto/auto_optimizer.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 class StochasticAdamWOptimizerConfig ( BaseAutoOptimizerConfig ): \"\"\" Configuration for the Stochastic AdamW optimizer. Attributes: name: Discriminator tag. lr: Learning rate. betas: Coefficients used for computing running averages of gradient and its square. eps: Term added to the denominator to improve numerical stability. weight_decay: Weight decay coefficient. state_dtype: Data Type to use for the optimizer states. \"\"\" name : Literal [ \"stochastic_adamw\" ] = \"stochastic_adamw\" lr : float betas : tuple [ float , float ] = ( 0.9 , 0.999 ) eps : float = 1e-8 weight_decay : float = 1e-2 state_dtype : str def build ( self , params : Iterable [ nn . Parameter ]) -> Optimizer : \"\"\"Builds StochasticAdamW with the configured parameters.\"\"\" return StochasticAdamW ( params = params , lr = self . lr , betas = self . betas , eps = self . eps , weight_decay = self . weight_decay , state_dtype = getattr ( torch , self . state_dtype ), )","title":"StochasticAdamWOptimizerConfig"},{"location":"0_loop/interfaces/#d9d.loop.auto.auto_optimizer.StochasticAdamWOptimizerConfig.build","text":"Builds StochasticAdamW with the configured parameters. Source code in d9d/loop/auto/auto_optimizer.py 55 56 57 58 59 60 61 62 63 64 def build ( self , params : Iterable [ nn . Parameter ]) -> Optimizer : \"\"\"Builds StochasticAdamW with the configured parameters.\"\"\" return StochasticAdamW ( params = params , lr = self . lr , betas = self . betas , eps = self . eps , weight_decay = self . weight_decay , state_dtype = getattr ( torch , self . state_dtype ), )","title":"build"},{"location":"0_loop/interfaces/#auto-scheduler","text":"Supports Piecewise Linear schedules (warmup, hold, decay). from d9d.loop.auto import AutoLRSchedulerProvider , AutoLRSchedulerConfig cfg = \"\"\" { \"initial_multiplier\": 0.0, \"phases\": [ { \"mode\": \"steps\", \"steps\": 100, \"target_multiplier\": 1.0, \"curve\": { \"type\": \"linear\" } }, { \"mode\": \"rest\", \"target_multiplier\": 0.1, \"curve\": { \"type\": \"cosine\" } } ] } \"\"\" provider = AutoLRSchedulerProvider ( AutoLRSchedulerConfig . model_validate_json ( cfg ) )","title":"Auto Scheduler"},{"location":"0_loop/interfaces/#d9d.loop.auto.auto_lr_scheduler","text":"","title":"auto_lr_scheduler"},{"location":"0_loop/interfaces/#d9d.loop.auto.auto_lr_scheduler.AutoLRSchedulerProvider","text":"Bases: LRSchedulerProvider LRSchedulerProvider that builds a learning rate scheduler based on a configuration object. Source code in d9d/loop/auto/auto_lr_scheduler.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 class AutoLRSchedulerProvider ( LRSchedulerProvider ): \"\"\" LRSchedulerProvider that builds a learning rate scheduler based on a configuration object. \"\"\" def __init__ ( self , config : AutoLRSchedulerConfig ): \"\"\"Constructs the AutoLRSchedulerProvider object.\"\"\" self . _config = config def __call__ ( self , context : InitializeLRSchedulerContext ) -> LRSchedulerProtocol : match self . _config : case PiecewiseConfig (): return piecewise_scheduler_from_config ( self . _config . scheduler , optimizer = context . optimizer , total_steps = context . total_steps ) case _ : raise ValueError ( f \"Unsupported LR scheduler type: { self . _config } \" )","title":"AutoLRSchedulerProvider"},{"location":"0_loop/interfaces/#d9d.loop.auto.auto_lr_scheduler.AutoLRSchedulerProvider.__init__","text":"Constructs the AutoLRSchedulerProvider object. Source code in d9d/loop/auto/auto_lr_scheduler.py 32 33 34 35 def __init__ ( self , config : AutoLRSchedulerConfig ): \"\"\"Constructs the AutoLRSchedulerProvider object.\"\"\" self . _config = config","title":"__init__"},{"location":"0_loop/interfaces/#d9d.loop.auto.auto_lr_scheduler.PiecewiseConfig","text":"Bases: BaseModel Configuration for the piecewise learning rate scheduler. Attributes: Name Type Description name Literal ['piecewise'] Discriminator tag, must be \"piecewise\". scheduler PiecewiseSchedulerConfig Detailed configuration for the piecewise schedule. Source code in d9d/loop/auto/auto_lr_scheduler.py 10 11 12 13 14 15 16 17 18 19 20 21 class PiecewiseConfig ( BaseModel ): \"\"\" Configuration for the piecewise learning rate scheduler. Attributes: name: Discriminator tag, must be \"piecewise\". scheduler: Detailed configuration for the piecewise schedule. \"\"\" name : Literal [ \"piecewise\" ] = \"piecewise\" scheduler : PiecewiseSchedulerConfig","title":"PiecewiseConfig"},{"location":"0_loop/interfaces/#interface","text":"If you need a custom optimizer or learning rate scheduler, you implement the OptimizerProvider protocol.","title":"Interface"},{"location":"0_loop/interfaces/#d9d.loop.control.optimizer_provider","text":"","title":"optimizer_provider"},{"location":"0_loop/interfaces/#d9d.loop.control.optimizer_provider.InitializeOptimizerStageContext","text":"Context data required to initialize an optimizer. Attributes: Name Type Description dist_context DistributedContext The distributed context. model Module The model instance for which parameters will be optimized. Source code in d9d/loop/control/optimizer_provider.py 12 13 14 15 16 17 18 19 20 21 22 23 @dataclasses . dataclass ( kw_only = True ) class InitializeOptimizerStageContext : \"\"\" Context data required to initialize an optimizer. Attributes: dist_context: The distributed context. model: The model instance for which parameters will be optimized. \"\"\" dist_context : DistributedContext model : nn . Module","title":"InitializeOptimizerStageContext"},{"location":"0_loop/interfaces/#d9d.loop.control.optimizer_provider.OptimizerProvider","text":"Bases: Protocol Protocol for defining how optimizers are created for model pipeline stages. Source code in d9d/loop/control/optimizer_provider.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 @typing . runtime_checkable class OptimizerProvider ( Protocol ): \"\"\" Protocol for defining how optimizers are created for model pipeline stages. \"\"\" @abc . abstractmethod def __call__ ( self , context : InitializeOptimizerStageContext ) -> Optimizer : \"\"\" Initializes the optimizer for a specific training stage. Args: context: Context for this operation. Returns: The instantiated PyTorch optimizer. \"\"\"","title":"OptimizerProvider"},{"location":"0_loop/interfaces/#d9d.loop.control.optimizer_provider.OptimizerProvider.__call__","text":"Initializes the optimizer for a specific training stage. Parameters: Name Type Description Default context InitializeOptimizerStageContext Context for this operation. required Returns: Type Description Optimizer The instantiated PyTorch optimizer. Source code in d9d/loop/control/optimizer_provider.py 32 33 34 35 36 37 38 39 40 41 42 @abc . abstractmethod def __call__ ( self , context : InitializeOptimizerStageContext ) -> Optimizer : \"\"\" Initializes the optimizer for a specific training stage. Args: context: Context for this operation. Returns: The instantiated PyTorch optimizer. \"\"\"","title":"__call__"},{"location":"0_loop/interfaces/#d9d.loop.control.lr_scheduler_provider","text":"","title":"lr_scheduler_provider"},{"location":"0_loop/interfaces/#d9d.loop.control.lr_scheduler_provider.InitializeLRSchedulerContext","text":"Context data required to initialize an LR scheduler. Attributes: Name Type Description dist_context DistributedContext The distributed context. total_steps int The total number of training steps. optimizer Optimizer The optimizer instance that the scheduler will control. Source code in d9d/loop/control/lr_scheduler_provider.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 @dataclasses . dataclass ( kw_only = True ) class InitializeLRSchedulerContext : \"\"\" Context data required to initialize an LR scheduler. Attributes: dist_context: The distributed context. total_steps: The total number of training steps. optimizer: The optimizer instance that the scheduler will control. \"\"\" dist_context : DistributedContext total_steps : int optimizer : Optimizer","title":"InitializeLRSchedulerContext"},{"location":"0_loop/interfaces/#d9d.loop.control.lr_scheduler_provider.LRSchedulerProvider","text":"Bases: Protocol Protocol for defining how Learning Rate schedulers are created. Source code in d9d/loop/control/lr_scheduler_provider.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 @typing . runtime_checkable class LRSchedulerProvider ( Protocol ): \"\"\" Protocol for defining how Learning Rate schedulers are created. \"\"\" @abc . abstractmethod def __call__ ( self , context : InitializeLRSchedulerContext ) -> LRSchedulerProtocol : \"\"\" Initializes the LR scheduler for a specific model pipeline stage. Args: context: Context for this operation. Returns: The instantiated LR scheduler adhering to the protocol. \"\"\"","title":"LRSchedulerProvider"},{"location":"0_loop/interfaces/#d9d.loop.control.lr_scheduler_provider.LRSchedulerProvider.__call__","text":"Initializes the LR scheduler for a specific model pipeline stage. Parameters: Name Type Description Default context InitializeLRSchedulerContext Context for this operation. required Returns: Type Description LRSchedulerProtocol The instantiated LR scheduler adhering to the protocol. Source code in d9d/loop/control/lr_scheduler_provider.py 34 35 36 37 38 39 40 41 42 43 44 @abc . abstractmethod def __call__ ( self , context : InitializeLRSchedulerContext ) -> LRSchedulerProtocol : \"\"\" Initializes the LR scheduler for a specific model pipeline stage. Args: context: Context for this operation. Returns: The instantiated LR scheduler adhering to the protocol. \"\"\"","title":"__call__"},{"location":"core/autograd_extensions/","text":"About The d9d.core.autograd package provides utilities to exert fine-grained control over PyTorch's automatic differentiation engine. The Global Grad Context Why The primary purpose of so-called Global Grad Context is to solve specific limitations in torch.autograd.Function regarding partial backward passes, which are critical for advanced distributed training schedules like Zero-Bubble Pipeline Parallelism. In standard PyTorch operations (like torch.matmul ), the autograd engine is highly optimized. If you perform a backward pass specifying only a subset of inputs (e.g., torch.autograd.backward(..., inputs=[activations]) ), PyTorch will intelligently skip computing gradients for parameters (weights) to save compute. However, custom torch.autograd.Function implementations do not share this intelligence. PyTorch sets ctx.needs_input_grad to True for every input that has requires_grad=True , regardless of whether that specific edge is actually being computed in the current backward() call. This behavior makes it impossible to implement split-backward pipeline schedules (where activation gradients and weight gradients are computed at different times) using custom operations (like GroupedGEMM) without performing redundant calculations. For more details, see PyTorch Issue #174017 . How it Works To bypass this limitation, d9d introduces the GlobalGradContext . It acts as a side-channel state manager that allows the training loop to explicitly signal its intent to the custom operators. Orchestrator : The training loop sets the context (e.g., \"I only want Input gradients now\"). Operator : The custom backward checks this context. Even if PyTorch says needs_input_grad=True , the operator will verify with GlobalGradContext before computation. Usage In Custom Autograd Functions When writing a custom operation, you must tag your gradients with a semantic GradDirection and check the context before computation. import torch from d9d.core.autograd import GLOBAL_GRAD_CONTEXT , GradDirection class MyCustomOp ( torch . autograd . Function ): @staticmethod def forward ( ctx , inputs , weight ): # Save which direction 'inputs' and 'weight' correspond to ctx . dir_inputs = GradDirection . inputs ctx . dir_weight = GradDirection . weight ctx . save_for_backward ( inputs , weight ) return torch . matmul ( inputs , weight ) @staticmethod def backward ( ctx , grad_output ): inputs , weight = ctx . saved_tensors grad_input = grad_weight = None # Check 1: Does PyTorch need it? AND Check 2: Does Context allow it? # Calculate Input Gradients (Activation) if ctx . needs_input_grad [ 0 ] and GLOBAL_GRAD_CONTEXT . check_direction ( ctx . dir_inputs ): grad_input = torch . matmul ( grad_output , weight . t ()) # Calculate Weight Gradients if ctx . needs_input_grad [ 1 ] and GLOBAL_GRAD_CONTEXT . check_direction ( ctx . dir_weight ): grad_weight = torch . matmul ( inputs . t (), grad_output ) return grad_input , grad_weight In Training Loops By default, the GLOBAL_GRAD_CONTEXT is set to compute both input and weight gradients. The d9d pipelining API configures it for split-backward automatically. So, if you use the Trainer , everything will work out of the box . If you use your own training loop implementation - you have to configure the context manually. API Reference d9d.core.autograd GLOBAL_GRAD_CONTEXT = GlobalGradContext () module-attribute The singleton instance of GlobalGradContext. This should be used by custom autograd functions to check GLOBAL_GRAD_CONTEXT.check_direction() during their backward pass. GlobalGradContext Global state manager for controlling gradient computation in custom autograd functions. This context addresses a limitation in PyTorch where custom torch.autograd.Function implementations set ctx.needs_input_grad to True for all edges requiring grad, even during partial backward passes (e.g., torch.autograd.backward(inputs=...) ). For additional information on this limitation, please refer to a related issue . This class allows: For the training code - to explicitly signal which gradient edges (inputs vs weights) should currently be computed, allowing custom ops to skip unnecessary computations. For module code - to check whether it's required to compute a gradient edge. Source code in d9d/core/autograd/grad_context.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 class GlobalGradContext : \"\"\" Global state manager for controlling gradient computation in custom autograd functions. This context addresses a limitation in PyTorch where custom `torch.autograd.Function` implementations set `ctx.needs_input_grad` to True for all edges requiring grad, even during partial backward passes (e.g., `torch.autograd.backward(inputs=...)`). For additional information on this limitation, please refer to a [related issue](https://github.com/pytorch/pytorch/issues/174017). This class allows: 1. For the training code - to explicitly signal which gradient edges (inputs vs weights) should currently be computed, allowing custom ops to skip unnecessary computations. 2. For module code - to check whether it's required to compute a gradient edge. \"\"\" def __init__ ( self ): \"\"\"Constructs a GlobalGradContext object with all directions enabled by default.\"\"\" # both directions by default self . _enabled_directions : set [ GradDirection ] = { GradDirection . inputs , GradDirection . weight } def check_direction ( self , direction : GradDirection | None ) -> bool : \"\"\" Checks if the gradient calculation for the given direction is currently enabled. Args: direction: The direction to check (inputs or weights). If None, returns True. Returns: True if the direction is enabled or None is passed, False otherwise. \"\"\" if direction is None : return True return direction in self . _enabled_directions @contextmanager def with_directions ( self , * directions : GradDirection ): \"\"\" Context manager that sets the enabled gradient directions. This overrides the current state for the duration of the context and restores the previous state afterwards. Args: *directions: The gradient directions to enable. \"\"\" prev_directions = self . _enabled_directions self . _enabled_directions = set ( directions ) yield self . _enabled_directions = prev_directions __init__ () Constructs a GlobalGradContext object with all directions enabled by default. Source code in d9d/core/autograd/grad_context.py 39 40 41 42 43 def __init__ ( self ): \"\"\"Constructs a GlobalGradContext object with all directions enabled by default.\"\"\" # both directions by default self . _enabled_directions : set [ GradDirection ] = { GradDirection . inputs , GradDirection . weight } check_direction ( direction ) Checks if the gradient calculation for the given direction is currently enabled. Parameters: Name Type Description Default direction GradDirection | None The direction to check (inputs or weights). If None, returns True. required Returns: Type Description bool True if the direction is enabled or None is passed, False otherwise. Source code in d9d/core/autograd/grad_context.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def check_direction ( self , direction : GradDirection | None ) -> bool : \"\"\" Checks if the gradient calculation for the given direction is currently enabled. Args: direction: The direction to check (inputs or weights). If None, returns True. Returns: True if the direction is enabled or None is passed, False otherwise. \"\"\" if direction is None : return True return direction in self . _enabled_directions with_directions ( * directions ) Context manager that sets the enabled gradient directions. This overrides the current state for the duration of the context and restores the previous state afterwards. Parameters: Name Type Description Default *directions GradDirection The gradient directions to enable. () Source code in d9d/core/autograd/grad_context.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 @contextmanager def with_directions ( self , * directions : GradDirection ): \"\"\" Context manager that sets the enabled gradient directions. This overrides the current state for the duration of the context and restores the previous state afterwards. Args: *directions: The gradient directions to enable. \"\"\" prev_directions = self . _enabled_directions self . _enabled_directions = set ( directions ) yield self . _enabled_directions = prev_directions GradDirection Bases: StrEnum Enum representing the specific gradient edges to compute. This is used to manually control gradient flow in custom autograd functions during split backward passes. Attributes: Name Type Description inputs Mark gradient edge as pointing to the module's inputs (activations). weight Mark gradient edge as pointing to the module's parameters (weights). Source code in d9d/core/autograd/grad_context.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class GradDirection ( StrEnum ): \"\"\" Enum representing the specific gradient edges to compute. This is used to manually control gradient flow in custom autograd functions during split backward passes. Attributes: inputs: Mark gradient edge as pointing to the module's inputs (activations). weight: Mark gradient edge as pointing to the module's parameters (weights). \"\"\" inputs = \"inputs\" weight = \"weights\"","title":"Autograd Extensions"},{"location":"core/autograd_extensions/#about","text":"The d9d.core.autograd package provides utilities to exert fine-grained control over PyTorch's automatic differentiation engine.","title":"About"},{"location":"core/autograd_extensions/#the-global-grad-context","text":"","title":"The Global Grad Context"},{"location":"core/autograd_extensions/#why","text":"The primary purpose of so-called Global Grad Context is to solve specific limitations in torch.autograd.Function regarding partial backward passes, which are critical for advanced distributed training schedules like Zero-Bubble Pipeline Parallelism. In standard PyTorch operations (like torch.matmul ), the autograd engine is highly optimized. If you perform a backward pass specifying only a subset of inputs (e.g., torch.autograd.backward(..., inputs=[activations]) ), PyTorch will intelligently skip computing gradients for parameters (weights) to save compute. However, custom torch.autograd.Function implementations do not share this intelligence. PyTorch sets ctx.needs_input_grad to True for every input that has requires_grad=True , regardless of whether that specific edge is actually being computed in the current backward() call. This behavior makes it impossible to implement split-backward pipeline schedules (where activation gradients and weight gradients are computed at different times) using custom operations (like GroupedGEMM) without performing redundant calculations. For more details, see PyTorch Issue #174017 .","title":"Why"},{"location":"core/autograd_extensions/#how-it-works","text":"To bypass this limitation, d9d introduces the GlobalGradContext . It acts as a side-channel state manager that allows the training loop to explicitly signal its intent to the custom operators. Orchestrator : The training loop sets the context (e.g., \"I only want Input gradients now\"). Operator : The custom backward checks this context. Even if PyTorch says needs_input_grad=True , the operator will verify with GlobalGradContext before computation.","title":"How it Works"},{"location":"core/autograd_extensions/#usage","text":"","title":"Usage"},{"location":"core/autograd_extensions/#in-custom-autograd-functions","text":"When writing a custom operation, you must tag your gradients with a semantic GradDirection and check the context before computation. import torch from d9d.core.autograd import GLOBAL_GRAD_CONTEXT , GradDirection class MyCustomOp ( torch . autograd . Function ): @staticmethod def forward ( ctx , inputs , weight ): # Save which direction 'inputs' and 'weight' correspond to ctx . dir_inputs = GradDirection . inputs ctx . dir_weight = GradDirection . weight ctx . save_for_backward ( inputs , weight ) return torch . matmul ( inputs , weight ) @staticmethod def backward ( ctx , grad_output ): inputs , weight = ctx . saved_tensors grad_input = grad_weight = None # Check 1: Does PyTorch need it? AND Check 2: Does Context allow it? # Calculate Input Gradients (Activation) if ctx . needs_input_grad [ 0 ] and GLOBAL_GRAD_CONTEXT . check_direction ( ctx . dir_inputs ): grad_input = torch . matmul ( grad_output , weight . t ()) # Calculate Weight Gradients if ctx . needs_input_grad [ 1 ] and GLOBAL_GRAD_CONTEXT . check_direction ( ctx . dir_weight ): grad_weight = torch . matmul ( inputs . t (), grad_output ) return grad_input , grad_weight","title":"In Custom Autograd Functions"},{"location":"core/autograd_extensions/#in-training-loops","text":"By default, the GLOBAL_GRAD_CONTEXT is set to compute both input and weight gradients. The d9d pipelining API configures it for split-backward automatically. So, if you use the Trainer , everything will work out of the box . If you use your own training loop implementation - you have to configure the context manually.","title":"In Training Loops"},{"location":"core/autograd_extensions/#api-reference","text":"","title":"API Reference"},{"location":"core/autograd_extensions/#d9d.core.autograd","text":"","title":"autograd"},{"location":"core/autograd_extensions/#d9d.core.autograd.GLOBAL_GRAD_CONTEXT","text":"The singleton instance of GlobalGradContext. This should be used by custom autograd functions to check GLOBAL_GRAD_CONTEXT.check_direction() during their backward pass.","title":"GLOBAL_GRAD_CONTEXT"},{"location":"core/autograd_extensions/#d9d.core.autograd.GlobalGradContext","text":"Global state manager for controlling gradient computation in custom autograd functions. This context addresses a limitation in PyTorch where custom torch.autograd.Function implementations set ctx.needs_input_grad to True for all edges requiring grad, even during partial backward passes (e.g., torch.autograd.backward(inputs=...) ). For additional information on this limitation, please refer to a related issue . This class allows: For the training code - to explicitly signal which gradient edges (inputs vs weights) should currently be computed, allowing custom ops to skip unnecessary computations. For module code - to check whether it's required to compute a gradient edge. Source code in d9d/core/autograd/grad_context.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 class GlobalGradContext : \"\"\" Global state manager for controlling gradient computation in custom autograd functions. This context addresses a limitation in PyTorch where custom `torch.autograd.Function` implementations set `ctx.needs_input_grad` to True for all edges requiring grad, even during partial backward passes (e.g., `torch.autograd.backward(inputs=...)`). For additional information on this limitation, please refer to a [related issue](https://github.com/pytorch/pytorch/issues/174017). This class allows: 1. For the training code - to explicitly signal which gradient edges (inputs vs weights) should currently be computed, allowing custom ops to skip unnecessary computations. 2. For module code - to check whether it's required to compute a gradient edge. \"\"\" def __init__ ( self ): \"\"\"Constructs a GlobalGradContext object with all directions enabled by default.\"\"\" # both directions by default self . _enabled_directions : set [ GradDirection ] = { GradDirection . inputs , GradDirection . weight } def check_direction ( self , direction : GradDirection | None ) -> bool : \"\"\" Checks if the gradient calculation for the given direction is currently enabled. Args: direction: The direction to check (inputs or weights). If None, returns True. Returns: True if the direction is enabled or None is passed, False otherwise. \"\"\" if direction is None : return True return direction in self . _enabled_directions @contextmanager def with_directions ( self , * directions : GradDirection ): \"\"\" Context manager that sets the enabled gradient directions. This overrides the current state for the duration of the context and restores the previous state afterwards. Args: *directions: The gradient directions to enable. \"\"\" prev_directions = self . _enabled_directions self . _enabled_directions = set ( directions ) yield self . _enabled_directions = prev_directions","title":"GlobalGradContext"},{"location":"core/autograd_extensions/#d9d.core.autograd.GlobalGradContext.__init__","text":"Constructs a GlobalGradContext object with all directions enabled by default. Source code in d9d/core/autograd/grad_context.py 39 40 41 42 43 def __init__ ( self ): \"\"\"Constructs a GlobalGradContext object with all directions enabled by default.\"\"\" # both directions by default self . _enabled_directions : set [ GradDirection ] = { GradDirection . inputs , GradDirection . weight }","title":"__init__"},{"location":"core/autograd_extensions/#d9d.core.autograd.GlobalGradContext.check_direction","text":"Checks if the gradient calculation for the given direction is currently enabled. Parameters: Name Type Description Default direction GradDirection | None The direction to check (inputs or weights). If None, returns True. required Returns: Type Description bool True if the direction is enabled or None is passed, False otherwise. Source code in d9d/core/autograd/grad_context.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def check_direction ( self , direction : GradDirection | None ) -> bool : \"\"\" Checks if the gradient calculation for the given direction is currently enabled. Args: direction: The direction to check (inputs or weights). If None, returns True. Returns: True if the direction is enabled or None is passed, False otherwise. \"\"\" if direction is None : return True return direction in self . _enabled_directions","title":"check_direction"},{"location":"core/autograd_extensions/#d9d.core.autograd.GlobalGradContext.with_directions","text":"Context manager that sets the enabled gradient directions. This overrides the current state for the duration of the context and restores the previous state afterwards. Parameters: Name Type Description Default *directions GradDirection The gradient directions to enable. () Source code in d9d/core/autograd/grad_context.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 @contextmanager def with_directions ( self , * directions : GradDirection ): \"\"\" Context manager that sets the enabled gradient directions. This overrides the current state for the duration of the context and restores the previous state afterwards. Args: *directions: The gradient directions to enable. \"\"\" prev_directions = self . _enabled_directions self . _enabled_directions = set ( directions ) yield self . _enabled_directions = prev_directions","title":"with_directions"},{"location":"core/autograd_extensions/#d9d.core.autograd.GradDirection","text":"Bases: StrEnum Enum representing the specific gradient edges to compute. This is used to manually control gradient flow in custom autograd functions during split backward passes. Attributes: Name Type Description inputs Mark gradient edge as pointing to the module's inputs (activations). weight Mark gradient edge as pointing to the module's parameters (weights). Source code in d9d/core/autograd/grad_context.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class GradDirection ( StrEnum ): \"\"\" Enum representing the specific gradient edges to compute. This is used to manually control gradient flow in custom autograd functions during split backward passes. Attributes: inputs: Mark gradient edge as pointing to the module's inputs (activations). weight: Mark gradient edge as pointing to the module's parameters (weights). \"\"\" inputs = \"inputs\" weight = \"weights\"","title":"GradDirection"},{"location":"core/dist_context/","text":"About The d9d.core.dist_context package is the Source of Truth for the distributed execution environment. In large-scale model training, ensuring that every rank agrees on the topology, global rank mapping, and communication groups is critical. This package provides the DistributedContext class, which serves as the central repository for this configuration. It is extremely important to use this context for all distributed assertions (e.g., \"Am I the main process?\", \"Which rank is my pipeline peer?\") rather than checking raw os.environ variables or initializing ad-hoc process groups, which can lead to silent inconsistencies. Comparison with Other Frameworks The problem of managing distributed topology is solved in a different ways across different distributed training frameworks. Megatron-LM ( parallel_state ) Megatron-LM manages topology via a module often called mpu (Model Parallel Unit) or core.parallel_state . Megatron historically relies on global variables and manual rank arithmetic. To find a peer rank, developers often write code involving modulo operations (e.g., rank % tp_size ). This is flexible but error-prone and brittle. HuggingFace Accelerate ( PartialState ) Accelerate uses a class called PartialState to abstract the environment. We find Accelerate's utility methods quite useful. d9d implements similar helpers, such as wait_world() (similar to wait_for_everyone() ) and properties like is_main_process or is_local_main_process . PartialState is primarily designed for \"Flat\" Data Parallelism (DDP/FSDP) and does not support complex multidimensional parallelisms natively. PartialState is implemented as a Singleton. Instantiating it anywhere in the code returns the exact same global state. This makes flow of dependencies unclear and also could lead to initialization of your ProcessGroups and distributed environment in unexpected places in your code. TorchTitan ( ParallelDims ) TorchTitan is the most similar framework to d9d in spirit, as both are built on top of native PyTorch 2.x DeviceMesh abstractions. However, ParallelDims in TorchTitan is more like a mesh factory rather than global distributed environment controller. d9d ( DistributedContext ) d9d positions DistributedContext as the explicit controller for managing all the distributed environment. DistributedContext is a standard object that is instantiated and passed explicitly to dependent components. This ensures that the initialization of process groups happens exactly when and where the developer intends, making the initialization flow transparent. It replaces manual rank arithmetic with formalized and native to PyTorch DeviceMesh abstractions. Functionally, it elevates the mesh system into an active runtime controller. It bundles timeout management, context-aware logging, and node-level synchronization. DeviceMesh Domains Modern architectures require different parallelism strategies for different parts of the model (e.g., standard dense layers vs. Mixture-of-Experts layers). d9d handles this by abstracting these strategies into specific DeviceMesh Domains . The underlying physical GPUs are immutable, but how we view them changes depending on what we are working with (distributing MoE layers, Dense layers, distributing input batch). DeviceMesh object for specific domain is retrieved via dist_ctx.mesh_for(domain_name) . Demonstration Video: For better understanding domains, we have prepared a quick demonstration video on YouTube . Regular Domain ( regular ) Identifier : REGULAR_DOMAIN or \"regular\" Purpose : The most granular mesh view for fully decomposed parallelism. Used for setting up logging and seeding. Dimensions : pp : Pipeline Parallel dp_replicate : Data Parallel (DDP style) dp_shard : Data Parallel (FSDP style) cp_shard : Context Parallel (FSDP style) cp_replicate : Context Parallel (DDP style) tp : Tensor Parallelism Expert Domain ( expert ) Identifier : EXPERT_DOMAIN or \"expert\" Purpose : Mesh view optimized for distributing MoE (Mixture of Experts) layers. It is intended that sparse expert layers should be sharded across ep_shard dimension and replicated across ep_replicate dimension. Dimensions : pp : Pipeline Parallel ep_replicate : Combined Replication Dimension ( (DP * CP) // EP ) ep_shard : Expert Parallel Dimension Dense Domain ( dense ) Identifier : DENSE_DOMAIN or \"dense\" Purpose : Mesh view for distributing dense layers. Dimensions : pp : Pipeline Parallel dp_replicate : Data Parallel for replication using HSDP dp_cp_shard : Merged Data and Context Parallel dimension for sharding using HSDP cp_replicate : Context Parallel for replication tp : Tensor Parallel Batch Domain ( batch ) Identifier : BATCH_DOMAIN or \"batch\" Purpose : Mesh view for distributing batch tensor and setting up DataLoader sharding. Dimensions : pp : Pipeline Parallel dp : Data Parallel cp : Context Parallel tp : Tensor Parallel Flat Domain ( flat ) Identifier : FLAT_DOMAIN or \"flat\" Purpose : Mesh view with a single dimension. Dimensions : world : World Size Usage Initialization The system is usually initialized via DeviceMeshParameters . from d9d.core.dist_context import DeviceMeshParameters # Define the topology params = DeviceMeshParameters ( pipeline_parallel = 2 , data_parallel_replicate = 8 , data_parallel_shard = 1 , context_parallel_replicate = 1 , context_parallel_shard = 1 , expert_parallel = 8 , tensor_parallel = 1 ) dist_ctx = params . build () Accessing DeviceMesh Domains from torch.distributed import DeviceMesh from d9d.core.dist_context import DistributedContext , DENSE_DOMAIN dist_ctx : DistributedContext = ... mesh_dense : DeviceMesh = dist_ctx . mesh_for ( DENSE_DOMAIN ) Rank Utilities Accessing rank information. if dist_ctx . is_main_process : print ( \"I am Global Rank 0 (Master)\" ) if dist_ctx . is_local_main_process : print ( \"I am Rank 0 on this specific node\" ) # Synchronize dist_ctx . wait_world () Context Managers Control execution flow across ranks. # Ensure only one process per node downloads a file with dist_ctx . local_main_process_first (): if dist_ctx . is_local_main_process : download_dataset () # Others wait here implicitly # All resume together d9d.core.dist_context This package configures the distributed environment and device meshes. DeviceMeshParameters Bases: BaseModel Configuration parameters for initializing Distributed Device Meshes. Attributes: Name Type Description pipeline_parallel int Degree of pipeline parallelism (PP). data_parallel_replicate int Degree of data parallel replication (DDP). data_parallel_shard int Degree of data parallel sharding (FSDP). context_parallel_replicate int Degree of context parallel (CP) replication. context_parallel_shard int Degree of context parallel (FSCP) sharding. tensor_parallel int Degree of tensor parallelism (TP). expert_parallel int Degree of expert parallelism (EP/MoE). Source code in d9d/core/dist_context/params.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 class DeviceMeshParameters ( BaseModel ): \"\"\" Configuration parameters for initializing Distributed Device Meshes. Attributes: pipeline_parallel: Degree of pipeline parallelism (PP). data_parallel_replicate: Degree of data parallel replication (DDP). data_parallel_shard: Degree of data parallel sharding (FSDP). context_parallel_replicate: Degree of context parallel (CP) replication. context_parallel_shard: Degree of context parallel (FSCP) sharding. tensor_parallel: Degree of tensor parallelism (TP). expert_parallel: Degree of expert parallelism (EP/MoE). \"\"\" model_config = ConfigDict ( frozen = True ) pipeline_parallel : int = 1 data_parallel_replicate : int = 1 data_parallel_shard : int = 1 context_parallel_replicate : int = 1 context_parallel_shard : int = 1 tensor_parallel : int = 1 expert_parallel : int = 1 @property def has_pipeline_parallel ( self ) -> bool : \"\"\"Checks if pipeline parallelism is enabled (degree > 1).\"\"\" return self . pipeline_parallel > 1 @property def has_data_parallel_replicate ( self ) -> bool : \"\"\"Checks if data parallel replication is enabled (degree > 1).\"\"\" return self . data_parallel_replicate > 1 @property def has_data_parallel_shard ( self ) -> bool : \"\"\"Checks if data parallel sharding is enabled (degree > 1).\"\"\" return self . data_parallel_shard > 1 @property def has_context_parallel_replicate ( self ) -> bool : return self . context_parallel_replicate > 1 @property def has_context_parallel_shard ( self ) -> bool : return self . context_parallel_shard > 1 @property def has_tensor_parallel ( self ) -> bool : return self . tensor_parallel > 1 @property def has_expert_parallel ( self ) -> bool : \"\"\"Checks if expert parallelism is enabled (degree > 1).\"\"\" return self . expert_parallel > 1 @property def is_distributed ( self ) -> bool : \"\"\"Checks if any form of parallelism is enabled.\"\"\" return ( self . has_pipeline_parallel or self . has_data_parallel_replicate or self . has_data_parallel_shard or self . has_context_parallel_shard or self . has_context_parallel_replicate or self . has_expert_parallel or self . has_tensor_parallel ) @model_validator ( mode = \"after\" ) def _check_ep_divisibility ( self ) -> Self : \"\"\"Validates that DP/CP/TP dimensions can support the requested EP/ETP degrees.\"\"\" dp_cp_tp_degree = ( self . data_parallel_shard * self . data_parallel_replicate * self . context_parallel_shard * self . context_parallel_replicate * self . tensor_parallel ) ep_degree = self . expert_parallel if dp_cp_tp_degree % ep_degree != 0 : raise ValueError ( f \"Total data/context/tensor parallelism degree ( { dp_cp_tp_degree } ) must be divisible by \" f \"total expert parallelism degree ( { ep_degree } ).\" ) return self def build ( self , log_level : int = logging . INFO ) -> \"DistributedContext\" : \"\"\" Initializes the DistributedContext using these parameters. Returns: A new DistributedContext instance containing the initialized device meshes. \"\"\" return DistributedContext ( self , log_level ) has_data_parallel_replicate property Checks if data parallel replication is enabled (degree > 1). has_data_parallel_shard property Checks if data parallel sharding is enabled (degree > 1). has_expert_parallel property Checks if expert parallelism is enabled (degree > 1). has_pipeline_parallel property Checks if pipeline parallelism is enabled (degree > 1). is_distributed property Checks if any form of parallelism is enabled. build ( log_level = logging . INFO ) Initializes the DistributedContext using these parameters. Returns: Type Description DistributedContext A new DistributedContext instance containing the initialized device meshes. Source code in d9d/core/dist_context/params.py 105 106 107 108 109 110 111 112 113 def build ( self , log_level : int = logging . INFO ) -> \"DistributedContext\" : \"\"\" Initializes the DistributedContext using these parameters. Returns: A new DistributedContext instance containing the initialized device meshes. \"\"\" return DistributedContext ( self , log_level ) DistributedContext Acts as the single source of truth for the distributed execution environment. It acts as the central repository for the distributed configuration, managing the creation and synchronization of PyTorch DeviceMeshes for different domains (Regular domain, Expert Parallel domain, ...). All assertions regarding rank placement, group memberships, and parallel topology must be derived from this context to ensure consistency. Source code in d9d/core/dist_context/configured.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 class DistributedContext : \"\"\" Acts as the single source of truth for the distributed execution environment. It acts as the central repository for the distributed configuration, managing the creation and synchronization of PyTorch DeviceMeshes for different domains (Regular domain, Expert Parallel domain, ...). All assertions regarding rank placement, group memberships, and parallel topology must be derived from this context to ensure consistency. \"\"\" def __init__ ( self , params : \"DeviceMeshParameters\" , log_level : int ): self . _params = params if params . is_distributed : meshes = _build_mesh_domains ( params ) regular_mesh = meshes [ REGULAR_DOMAIN ] self . _meshes = meshes self . _num_nodes = regular_mesh . size () // torch . cuda . device_count () self . _logger = build_dist_logger ( f \"pp: { regular_mesh . get_local_rank ( 'pp' ) } -\" f \"dpr: { regular_mesh . get_local_rank ( 'dp_replicate' ) } -\" f \"dps: { regular_mesh . get_local_rank ( 'dp_shard' ) } -\" f \"cps: { regular_mesh . get_local_rank ( 'cp_shard' ) } -\" f \"cpr: { regular_mesh . get_local_rank ( 'cp_replicate' ) } -\" f \"tp: { regular_mesh . get_local_rank ( 'tp' ) } \" , level = log_level , ) else : self . _meshes = {} self . _num_nodes = 1 self . _logger = build_dist_logger ( \"local\" , level = log_level ) self . _local_rank = int ( os . environ . get ( \"LOCAL_RANK\" , \"0\" )) self . _global_rank = int ( os . environ . get ( \"RANK\" , \"0\" )) self . _node_rank = self . _global_rank // torch . cuda . device_count () self . _master_addr = _resolve_master_addr () self . _current_device = torch . device ( \"cuda\" ) torch . cuda . set_device ( self . _local_rank ) @property def logger ( self ) -> logging . Logger : \"\"\"Returns the logger instance configured for distributed logging.\"\"\" return self . _logger def mesh_for ( self , domain : str ) -> DeviceMesh : \"\"\" Returns the device mesh view associated with a specific logical domain. Available Domains and Dimensions: * `regular` (`REGULAR_DOMAIN`): The most granular mesh for fully decomposed parallelism. Dimensions: ``('pp', 'dp_replicate', 'dp_shard', 'cp_shard', 'cp_replicate', 'tp')`` * `expert` (`EXPERT_DOMAIN`): Mesh optimized for distributing MoE (Mixture of Experts) layers. Dimensions: ``('pp', 'replicate', 'ep')`` * `dense` (`DENSE_DOMAIN`): Mesh optimized for distributing dense layers. Dimensions: ``('pp', 'dp_replicate', 'dp_cp_shard', 'cp_replicate', 'tp')`` * `batch` (`BATCH_DOMAIN`): Mesh optimized for distributing input data. Dimensions: ``('pp', 'dp', 'cp', 'tp')`` * `flat` (`FLAT_DOMAIN`): Mesh containing a single dimension with all the processes. Dimensions: ``('world')`` Args: domain: The name of the domain to retrieve. Returns: The PyTorch DeviceMesh configured for the requested domain. Raises: ValueError: If the specified domain does not exist. \"\"\" if domain not in self . _meshes : raise ValueError ( f \"Domain { domain } does not exist\" ) return self . _meshes [ domain ] @property def is_main_process ( self ) -> bool : \"\"\"Checks if the current process is the global rank 0.\"\"\" return self . _global_rank == 0 @property def is_local_main_process ( self ) -> bool : \"\"\"Checks if the current process is the rank 0 on the specific node.\"\"\" return self . _local_rank == 0 def wait_world ( self ): \"\"\"Blocks process execution until all ranks reach this point.\"\"\" if self . _params . is_distributed : torch . distributed . barrier ( device_ids = [ torch . cuda . current_device ()]) torch . cuda . synchronize () def set_timeout ( self , timeout_seconds : float ): \"\"\" Updates the NCCL/process group timeout for all underlying meshes. Args: timeout_seconds: New timeout duration in seconds. \"\"\" if not self . _params . is_distributed : # does nothing for local setups return self . logger . info ( f \"Setting global timeout to { timeout_seconds } seconds\" ) self . wait_world () groups : list [ torch . distributed . ProcessGroup | None ] = [ None ] for mesh in self . _meshes . values (): for dim in range ( mesh . ndim ): groups . append ( mesh . get_group ( dim )) for group in groups : torch . distributed . distributed_c10d . _set_pg_timeout ( datetime . timedelta ( seconds = timeout_seconds ), group ) # noqa: SLF001 @contextmanager def local_main_process_first ( self ): \"\"\" Context manager that executes the block on the local main process first. Other local ranks wait at the entrance. The local main process waits at the exit to synchronize before continuing. \"\"\" if not self . is_local_main_process : self . wait_world () yield if self . is_local_main_process : self . wait_world () @contextmanager def main_process_first ( self ): \"\"\" Context manager that executes the block on the global main process first. All other ranks wait at the entrance. The global main process waits at the exit to synchronize before continuing. \"\"\" if not self . is_main_process : self . wait_world () yield if self . is_main_process : self . wait_world () @property def current_device ( self ) -> torch . device : \"\"\"Returns the CUDA device associated with this rank.\"\"\" return self . _current_device @property def mesh_params ( self ) -> \"DeviceMeshParameters\" : \"\"\"Returns the parameters used to initialize this context.\"\"\" return self . _params @property def master_addr ( self ) -> str : \"\"\"Returns the IP address or domain name of the master node.\"\"\" return self . _master_addr @property def node_rank ( self ) -> int : \"\"\"Returns the index of the node this process is running on.\"\"\" return self . _node_rank @property def num_nodes ( self ) -> int : \"\"\"Returns the total number of nodes in the cluster.\"\"\" return self . _num_nodes current_device property Returns the CUDA device associated with this rank. is_local_main_process property Checks if the current process is the rank 0 on the specific node. is_main_process property Checks if the current process is the global rank 0. logger property Returns the logger instance configured for distributed logging. master_addr property Returns the IP address or domain name of the master node. mesh_params property Returns the parameters used to initialize this context. node_rank property Returns the index of the node this process is running on. num_nodes property Returns the total number of nodes in the cluster. local_main_process_first () Context manager that executes the block on the local main process first. Other local ranks wait at the entrance. The local main process waits at the exit to synchronize before continuing. Source code in d9d/core/dist_context/configured.py 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 @contextmanager def local_main_process_first ( self ): \"\"\" Context manager that executes the block on the local main process first. Other local ranks wait at the entrance. The local main process waits at the exit to synchronize before continuing. \"\"\" if not self . is_local_main_process : self . wait_world () yield if self . is_local_main_process : self . wait_world () main_process_first () Context manager that executes the block on the global main process first. All other ranks wait at the entrance. The global main process waits at the exit to synchronize before continuing. Source code in d9d/core/dist_context/configured.py 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 @contextmanager def main_process_first ( self ): \"\"\" Context manager that executes the block on the global main process first. All other ranks wait at the entrance. The global main process waits at the exit to synchronize before continuing. \"\"\" if not self . is_main_process : self . wait_world () yield if self . is_main_process : self . wait_world () mesh_for ( domain ) Returns the device mesh view associated with a specific logical domain. Available Domains and Dimensions regular ( REGULAR_DOMAIN ): The most granular mesh for fully decomposed parallelism. Dimensions: ('pp', 'dp_replicate', 'dp_shard', 'cp_shard', 'cp_replicate', 'tp') expert ( EXPERT_DOMAIN ): Mesh optimized for distributing MoE (Mixture of Experts) layers. Dimensions: ('pp', 'replicate', 'ep') dense ( DENSE_DOMAIN ): Mesh optimized for distributing dense layers. Dimensions: ('pp', 'dp_replicate', 'dp_cp_shard', 'cp_replicate', 'tp') batch ( BATCH_DOMAIN ): Mesh optimized for distributing input data. Dimensions: ('pp', 'dp', 'cp', 'tp') flat ( FLAT_DOMAIN ): Mesh containing a single dimension with all the processes. Dimensions: ('world') Parameters: Name Type Description Default domain str The name of the domain to retrieve. required Returns: Type Description DeviceMesh The PyTorch DeviceMesh configured for the requested domain. Raises: Type Description ValueError If the specified domain does not exist. Source code in d9d/core/dist_context/configured.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def mesh_for ( self , domain : str ) -> DeviceMesh : \"\"\" Returns the device mesh view associated with a specific logical domain. Available Domains and Dimensions: * `regular` (`REGULAR_DOMAIN`): The most granular mesh for fully decomposed parallelism. Dimensions: ``('pp', 'dp_replicate', 'dp_shard', 'cp_shard', 'cp_replicate', 'tp')`` * `expert` (`EXPERT_DOMAIN`): Mesh optimized for distributing MoE (Mixture of Experts) layers. Dimensions: ``('pp', 'replicate', 'ep')`` * `dense` (`DENSE_DOMAIN`): Mesh optimized for distributing dense layers. Dimensions: ``('pp', 'dp_replicate', 'dp_cp_shard', 'cp_replicate', 'tp')`` * `batch` (`BATCH_DOMAIN`): Mesh optimized for distributing input data. Dimensions: ``('pp', 'dp', 'cp', 'tp')`` * `flat` (`FLAT_DOMAIN`): Mesh containing a single dimension with all the processes. Dimensions: ``('world')`` Args: domain: The name of the domain to retrieve. Returns: The PyTorch DeviceMesh configured for the requested domain. Raises: ValueError: If the specified domain does not exist. \"\"\" if domain not in self . _meshes : raise ValueError ( f \"Domain { domain } does not exist\" ) return self . _meshes [ domain ] set_timeout ( timeout_seconds ) Updates the NCCL/process group timeout for all underlying meshes. Parameters: Name Type Description Default timeout_seconds float New timeout duration in seconds. required Source code in d9d/core/dist_context/configured.py 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def set_timeout ( self , timeout_seconds : float ): \"\"\" Updates the NCCL/process group timeout for all underlying meshes. Args: timeout_seconds: New timeout duration in seconds. \"\"\" if not self . _params . is_distributed : # does nothing for local setups return self . logger . info ( f \"Setting global timeout to { timeout_seconds } seconds\" ) self . wait_world () groups : list [ torch . distributed . ProcessGroup | None ] = [ None ] for mesh in self . _meshes . values (): for dim in range ( mesh . ndim ): groups . append ( mesh . get_group ( dim )) for group in groups : torch . distributed . distributed_c10d . _set_pg_timeout ( datetime . timedelta ( seconds = timeout_seconds ), group ) # noqa: SLF001 wait_world () Blocks process execution until all ranks reach this point. Source code in d9d/core/dist_context/configured.py 126 127 128 129 130 131 def wait_world ( self ): \"\"\"Blocks process execution until all ranks reach this point.\"\"\" if self . _params . is_distributed : torch . distributed . barrier ( device_ids = [ torch . cuda . current_device ()]) torch . cuda . synchronize () build_dist_logger ( qualifier , level ) Configures and returns a logger instance for d9d. The logger is configured to write to stdout with a formatter that includes the provided rank qualifier, allowing for easier debugging in distributed logs. Parameters: Name Type Description Default qualifier str A string identifying the current rank's position in the mesh. required level int Log level to set by default required Returns: Type Description Logger A configured logging.Logger instance. Source code in d9d/core/dist_context/log.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def build_dist_logger ( qualifier : str , level : int ) -> logging . Logger : \"\"\" Configures and returns a logger instance for d9d. The logger is configured to write to stdout with a formatter that includes the provided rank qualifier, allowing for easier debugging in distributed logs. Args: qualifier: A string identifying the current rank's position in the mesh. level: Log level to set by default Returns: A configured logging.Logger instance. \"\"\" dist_logger = logging . getLogger ( \"d9d\" ) dist_logger . setLevel ( level ) dist_logger . handlers . clear () ch = logging . StreamHandler ( sys . stdout ) ch . setLevel ( level ) formatter = logging . Formatter ( f \"[d9d] [ { qualifier } ] %(asctime)s - %(levelname)s - %(message)s\" ) ch . setFormatter ( formatter ) dist_logger . addHandler ( ch ) return dist_logger","title":"Distributed Context"},{"location":"core/dist_context/#about","text":"The d9d.core.dist_context package is the Source of Truth for the distributed execution environment. In large-scale model training, ensuring that every rank agrees on the topology, global rank mapping, and communication groups is critical. This package provides the DistributedContext class, which serves as the central repository for this configuration. It is extremely important to use this context for all distributed assertions (e.g., \"Am I the main process?\", \"Which rank is my pipeline peer?\") rather than checking raw os.environ variables or initializing ad-hoc process groups, which can lead to silent inconsistencies.","title":"About"},{"location":"core/dist_context/#comparison-with-other-frameworks","text":"The problem of managing distributed topology is solved in a different ways across different distributed training frameworks.","title":"Comparison with Other Frameworks"},{"location":"core/dist_context/#megatron-lm-parallel_state","text":"Megatron-LM manages topology via a module often called mpu (Model Parallel Unit) or core.parallel_state . Megatron historically relies on global variables and manual rank arithmetic. To find a peer rank, developers often write code involving modulo operations (e.g., rank % tp_size ). This is flexible but error-prone and brittle.","title":"Megatron-LM (parallel_state)"},{"location":"core/dist_context/#huggingface-accelerate-partialstate","text":"Accelerate uses a class called PartialState to abstract the environment. We find Accelerate's utility methods quite useful. d9d implements similar helpers, such as wait_world() (similar to wait_for_everyone() ) and properties like is_main_process or is_local_main_process . PartialState is primarily designed for \"Flat\" Data Parallelism (DDP/FSDP) and does not support complex multidimensional parallelisms natively. PartialState is implemented as a Singleton. Instantiating it anywhere in the code returns the exact same global state. This makes flow of dependencies unclear and also could lead to initialization of your ProcessGroups and distributed environment in unexpected places in your code.","title":"HuggingFace Accelerate (PartialState)"},{"location":"core/dist_context/#torchtitan-paralleldims","text":"TorchTitan is the most similar framework to d9d in spirit, as both are built on top of native PyTorch 2.x DeviceMesh abstractions. However, ParallelDims in TorchTitan is more like a mesh factory rather than global distributed environment controller.","title":"TorchTitan (ParallelDims)"},{"location":"core/dist_context/#d9d-distributedcontext","text":"d9d positions DistributedContext as the explicit controller for managing all the distributed environment. DistributedContext is a standard object that is instantiated and passed explicitly to dependent components. This ensures that the initialization of process groups happens exactly when and where the developer intends, making the initialization flow transparent. It replaces manual rank arithmetic with formalized and native to PyTorch DeviceMesh abstractions. Functionally, it elevates the mesh system into an active runtime controller. It bundles timeout management, context-aware logging, and node-level synchronization.","title":"d9d (DistributedContext)"},{"location":"core/dist_context/#devicemesh-domains","text":"Modern architectures require different parallelism strategies for different parts of the model (e.g., standard dense layers vs. Mixture-of-Experts layers). d9d handles this by abstracting these strategies into specific DeviceMesh Domains . The underlying physical GPUs are immutable, but how we view them changes depending on what we are working with (distributing MoE layers, Dense layers, distributing input batch). DeviceMesh object for specific domain is retrieved via dist_ctx.mesh_for(domain_name) . Demonstration Video: For better understanding domains, we have prepared a quick demonstration video on YouTube .","title":"DeviceMesh Domains"},{"location":"core/dist_context/#regular-domain-regular","text":"Identifier : REGULAR_DOMAIN or \"regular\" Purpose : The most granular mesh view for fully decomposed parallelism. Used for setting up logging and seeding. Dimensions : pp : Pipeline Parallel dp_replicate : Data Parallel (DDP style) dp_shard : Data Parallel (FSDP style) cp_shard : Context Parallel (FSDP style) cp_replicate : Context Parallel (DDP style) tp : Tensor Parallelism","title":"Regular Domain (regular)"},{"location":"core/dist_context/#expert-domain-expert","text":"Identifier : EXPERT_DOMAIN or \"expert\" Purpose : Mesh view optimized for distributing MoE (Mixture of Experts) layers. It is intended that sparse expert layers should be sharded across ep_shard dimension and replicated across ep_replicate dimension. Dimensions : pp : Pipeline Parallel ep_replicate : Combined Replication Dimension ( (DP * CP) // EP ) ep_shard : Expert Parallel Dimension","title":"Expert Domain (expert)"},{"location":"core/dist_context/#dense-domain-dense","text":"Identifier : DENSE_DOMAIN or \"dense\" Purpose : Mesh view for distributing dense layers. Dimensions : pp : Pipeline Parallel dp_replicate : Data Parallel for replication using HSDP dp_cp_shard : Merged Data and Context Parallel dimension for sharding using HSDP cp_replicate : Context Parallel for replication tp : Tensor Parallel","title":"Dense Domain (dense)"},{"location":"core/dist_context/#batch-domain-batch","text":"Identifier : BATCH_DOMAIN or \"batch\" Purpose : Mesh view for distributing batch tensor and setting up DataLoader sharding. Dimensions : pp : Pipeline Parallel dp : Data Parallel cp : Context Parallel tp : Tensor Parallel","title":"Batch Domain (batch)"},{"location":"core/dist_context/#flat-domain-flat","text":"Identifier : FLAT_DOMAIN or \"flat\" Purpose : Mesh view with a single dimension. Dimensions : world : World Size","title":"Flat Domain (flat)"},{"location":"core/dist_context/#usage","text":"","title":"Usage"},{"location":"core/dist_context/#initialization","text":"The system is usually initialized via DeviceMeshParameters . from d9d.core.dist_context import DeviceMeshParameters # Define the topology params = DeviceMeshParameters ( pipeline_parallel = 2 , data_parallel_replicate = 8 , data_parallel_shard = 1 , context_parallel_replicate = 1 , context_parallel_shard = 1 , expert_parallel = 8 , tensor_parallel = 1 ) dist_ctx = params . build ()","title":"Initialization"},{"location":"core/dist_context/#accessing-devicemesh-domains","text":"from torch.distributed import DeviceMesh from d9d.core.dist_context import DistributedContext , DENSE_DOMAIN dist_ctx : DistributedContext = ... mesh_dense : DeviceMesh = dist_ctx . mesh_for ( DENSE_DOMAIN )","title":"Accessing DeviceMesh Domains"},{"location":"core/dist_context/#rank-utilities","text":"Accessing rank information. if dist_ctx . is_main_process : print ( \"I am Global Rank 0 (Master)\" ) if dist_ctx . is_local_main_process : print ( \"I am Rank 0 on this specific node\" ) # Synchronize dist_ctx . wait_world ()","title":"Rank Utilities"},{"location":"core/dist_context/#context-managers","text":"Control execution flow across ranks. # Ensure only one process per node downloads a file with dist_ctx . local_main_process_first (): if dist_ctx . is_local_main_process : download_dataset () # Others wait here implicitly # All resume together","title":"Context Managers"},{"location":"core/dist_context/#d9d.core.dist_context","text":"This package configures the distributed environment and device meshes.","title":"dist_context"},{"location":"core/dist_context/#d9d.core.dist_context.DeviceMeshParameters","text":"Bases: BaseModel Configuration parameters for initializing Distributed Device Meshes. Attributes: Name Type Description pipeline_parallel int Degree of pipeline parallelism (PP). data_parallel_replicate int Degree of data parallel replication (DDP). data_parallel_shard int Degree of data parallel sharding (FSDP). context_parallel_replicate int Degree of context parallel (CP) replication. context_parallel_shard int Degree of context parallel (FSCP) sharding. tensor_parallel int Degree of tensor parallelism (TP). expert_parallel int Degree of expert parallelism (EP/MoE). Source code in d9d/core/dist_context/params.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 class DeviceMeshParameters ( BaseModel ): \"\"\" Configuration parameters for initializing Distributed Device Meshes. Attributes: pipeline_parallel: Degree of pipeline parallelism (PP). data_parallel_replicate: Degree of data parallel replication (DDP). data_parallel_shard: Degree of data parallel sharding (FSDP). context_parallel_replicate: Degree of context parallel (CP) replication. context_parallel_shard: Degree of context parallel (FSCP) sharding. tensor_parallel: Degree of tensor parallelism (TP). expert_parallel: Degree of expert parallelism (EP/MoE). \"\"\" model_config = ConfigDict ( frozen = True ) pipeline_parallel : int = 1 data_parallel_replicate : int = 1 data_parallel_shard : int = 1 context_parallel_replicate : int = 1 context_parallel_shard : int = 1 tensor_parallel : int = 1 expert_parallel : int = 1 @property def has_pipeline_parallel ( self ) -> bool : \"\"\"Checks if pipeline parallelism is enabled (degree > 1).\"\"\" return self . pipeline_parallel > 1 @property def has_data_parallel_replicate ( self ) -> bool : \"\"\"Checks if data parallel replication is enabled (degree > 1).\"\"\" return self . data_parallel_replicate > 1 @property def has_data_parallel_shard ( self ) -> bool : \"\"\"Checks if data parallel sharding is enabled (degree > 1).\"\"\" return self . data_parallel_shard > 1 @property def has_context_parallel_replicate ( self ) -> bool : return self . context_parallel_replicate > 1 @property def has_context_parallel_shard ( self ) -> bool : return self . context_parallel_shard > 1 @property def has_tensor_parallel ( self ) -> bool : return self . tensor_parallel > 1 @property def has_expert_parallel ( self ) -> bool : \"\"\"Checks if expert parallelism is enabled (degree > 1).\"\"\" return self . expert_parallel > 1 @property def is_distributed ( self ) -> bool : \"\"\"Checks if any form of parallelism is enabled.\"\"\" return ( self . has_pipeline_parallel or self . has_data_parallel_replicate or self . has_data_parallel_shard or self . has_context_parallel_shard or self . has_context_parallel_replicate or self . has_expert_parallel or self . has_tensor_parallel ) @model_validator ( mode = \"after\" ) def _check_ep_divisibility ( self ) -> Self : \"\"\"Validates that DP/CP/TP dimensions can support the requested EP/ETP degrees.\"\"\" dp_cp_tp_degree = ( self . data_parallel_shard * self . data_parallel_replicate * self . context_parallel_shard * self . context_parallel_replicate * self . tensor_parallel ) ep_degree = self . expert_parallel if dp_cp_tp_degree % ep_degree != 0 : raise ValueError ( f \"Total data/context/tensor parallelism degree ( { dp_cp_tp_degree } ) must be divisible by \" f \"total expert parallelism degree ( { ep_degree } ).\" ) return self def build ( self , log_level : int = logging . INFO ) -> \"DistributedContext\" : \"\"\" Initializes the DistributedContext using these parameters. Returns: A new DistributedContext instance containing the initialized device meshes. \"\"\" return DistributedContext ( self , log_level )","title":"DeviceMeshParameters"},{"location":"core/dist_context/#d9d.core.dist_context.DeviceMeshParameters.has_data_parallel_replicate","text":"Checks if data parallel replication is enabled (degree > 1).","title":"has_data_parallel_replicate"},{"location":"core/dist_context/#d9d.core.dist_context.DeviceMeshParameters.has_data_parallel_shard","text":"Checks if data parallel sharding is enabled (degree > 1).","title":"has_data_parallel_shard"},{"location":"core/dist_context/#d9d.core.dist_context.DeviceMeshParameters.has_expert_parallel","text":"Checks if expert parallelism is enabled (degree > 1).","title":"has_expert_parallel"},{"location":"core/dist_context/#d9d.core.dist_context.DeviceMeshParameters.has_pipeline_parallel","text":"Checks if pipeline parallelism is enabled (degree > 1).","title":"has_pipeline_parallel"},{"location":"core/dist_context/#d9d.core.dist_context.DeviceMeshParameters.is_distributed","text":"Checks if any form of parallelism is enabled.","title":"is_distributed"},{"location":"core/dist_context/#d9d.core.dist_context.DeviceMeshParameters.build","text":"Initializes the DistributedContext using these parameters. Returns: Type Description DistributedContext A new DistributedContext instance containing the initialized device meshes. Source code in d9d/core/dist_context/params.py 105 106 107 108 109 110 111 112 113 def build ( self , log_level : int = logging . INFO ) -> \"DistributedContext\" : \"\"\" Initializes the DistributedContext using these parameters. Returns: A new DistributedContext instance containing the initialized device meshes. \"\"\" return DistributedContext ( self , log_level )","title":"build"},{"location":"core/dist_context/#d9d.core.dist_context.DistributedContext","text":"Acts as the single source of truth for the distributed execution environment. It acts as the central repository for the distributed configuration, managing the creation and synchronization of PyTorch DeviceMeshes for different domains (Regular domain, Expert Parallel domain, ...). All assertions regarding rank placement, group memberships, and parallel topology must be derived from this context to ensure consistency. Source code in d9d/core/dist_context/configured.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 class DistributedContext : \"\"\" Acts as the single source of truth for the distributed execution environment. It acts as the central repository for the distributed configuration, managing the creation and synchronization of PyTorch DeviceMeshes for different domains (Regular domain, Expert Parallel domain, ...). All assertions regarding rank placement, group memberships, and parallel topology must be derived from this context to ensure consistency. \"\"\" def __init__ ( self , params : \"DeviceMeshParameters\" , log_level : int ): self . _params = params if params . is_distributed : meshes = _build_mesh_domains ( params ) regular_mesh = meshes [ REGULAR_DOMAIN ] self . _meshes = meshes self . _num_nodes = regular_mesh . size () // torch . cuda . device_count () self . _logger = build_dist_logger ( f \"pp: { regular_mesh . get_local_rank ( 'pp' ) } -\" f \"dpr: { regular_mesh . get_local_rank ( 'dp_replicate' ) } -\" f \"dps: { regular_mesh . get_local_rank ( 'dp_shard' ) } -\" f \"cps: { regular_mesh . get_local_rank ( 'cp_shard' ) } -\" f \"cpr: { regular_mesh . get_local_rank ( 'cp_replicate' ) } -\" f \"tp: { regular_mesh . get_local_rank ( 'tp' ) } \" , level = log_level , ) else : self . _meshes = {} self . _num_nodes = 1 self . _logger = build_dist_logger ( \"local\" , level = log_level ) self . _local_rank = int ( os . environ . get ( \"LOCAL_RANK\" , \"0\" )) self . _global_rank = int ( os . environ . get ( \"RANK\" , \"0\" )) self . _node_rank = self . _global_rank // torch . cuda . device_count () self . _master_addr = _resolve_master_addr () self . _current_device = torch . device ( \"cuda\" ) torch . cuda . set_device ( self . _local_rank ) @property def logger ( self ) -> logging . Logger : \"\"\"Returns the logger instance configured for distributed logging.\"\"\" return self . _logger def mesh_for ( self , domain : str ) -> DeviceMesh : \"\"\" Returns the device mesh view associated with a specific logical domain. Available Domains and Dimensions: * `regular` (`REGULAR_DOMAIN`): The most granular mesh for fully decomposed parallelism. Dimensions: ``('pp', 'dp_replicate', 'dp_shard', 'cp_shard', 'cp_replicate', 'tp')`` * `expert` (`EXPERT_DOMAIN`): Mesh optimized for distributing MoE (Mixture of Experts) layers. Dimensions: ``('pp', 'replicate', 'ep')`` * `dense` (`DENSE_DOMAIN`): Mesh optimized for distributing dense layers. Dimensions: ``('pp', 'dp_replicate', 'dp_cp_shard', 'cp_replicate', 'tp')`` * `batch` (`BATCH_DOMAIN`): Mesh optimized for distributing input data. Dimensions: ``('pp', 'dp', 'cp', 'tp')`` * `flat` (`FLAT_DOMAIN`): Mesh containing a single dimension with all the processes. Dimensions: ``('world')`` Args: domain: The name of the domain to retrieve. Returns: The PyTorch DeviceMesh configured for the requested domain. Raises: ValueError: If the specified domain does not exist. \"\"\" if domain not in self . _meshes : raise ValueError ( f \"Domain { domain } does not exist\" ) return self . _meshes [ domain ] @property def is_main_process ( self ) -> bool : \"\"\"Checks if the current process is the global rank 0.\"\"\" return self . _global_rank == 0 @property def is_local_main_process ( self ) -> bool : \"\"\"Checks if the current process is the rank 0 on the specific node.\"\"\" return self . _local_rank == 0 def wait_world ( self ): \"\"\"Blocks process execution until all ranks reach this point.\"\"\" if self . _params . is_distributed : torch . distributed . barrier ( device_ids = [ torch . cuda . current_device ()]) torch . cuda . synchronize () def set_timeout ( self , timeout_seconds : float ): \"\"\" Updates the NCCL/process group timeout for all underlying meshes. Args: timeout_seconds: New timeout duration in seconds. \"\"\" if not self . _params . is_distributed : # does nothing for local setups return self . logger . info ( f \"Setting global timeout to { timeout_seconds } seconds\" ) self . wait_world () groups : list [ torch . distributed . ProcessGroup | None ] = [ None ] for mesh in self . _meshes . values (): for dim in range ( mesh . ndim ): groups . append ( mesh . get_group ( dim )) for group in groups : torch . distributed . distributed_c10d . _set_pg_timeout ( datetime . timedelta ( seconds = timeout_seconds ), group ) # noqa: SLF001 @contextmanager def local_main_process_first ( self ): \"\"\" Context manager that executes the block on the local main process first. Other local ranks wait at the entrance. The local main process waits at the exit to synchronize before continuing. \"\"\" if not self . is_local_main_process : self . wait_world () yield if self . is_local_main_process : self . wait_world () @contextmanager def main_process_first ( self ): \"\"\" Context manager that executes the block on the global main process first. All other ranks wait at the entrance. The global main process waits at the exit to synchronize before continuing. \"\"\" if not self . is_main_process : self . wait_world () yield if self . is_main_process : self . wait_world () @property def current_device ( self ) -> torch . device : \"\"\"Returns the CUDA device associated with this rank.\"\"\" return self . _current_device @property def mesh_params ( self ) -> \"DeviceMeshParameters\" : \"\"\"Returns the parameters used to initialize this context.\"\"\" return self . _params @property def master_addr ( self ) -> str : \"\"\"Returns the IP address or domain name of the master node.\"\"\" return self . _master_addr @property def node_rank ( self ) -> int : \"\"\"Returns the index of the node this process is running on.\"\"\" return self . _node_rank @property def num_nodes ( self ) -> int : \"\"\"Returns the total number of nodes in the cluster.\"\"\" return self . _num_nodes","title":"DistributedContext"},{"location":"core/dist_context/#d9d.core.dist_context.DistributedContext.current_device","text":"Returns the CUDA device associated with this rank.","title":"current_device"},{"location":"core/dist_context/#d9d.core.dist_context.DistributedContext.is_local_main_process","text":"Checks if the current process is the rank 0 on the specific node.","title":"is_local_main_process"},{"location":"core/dist_context/#d9d.core.dist_context.DistributedContext.is_main_process","text":"Checks if the current process is the global rank 0.","title":"is_main_process"},{"location":"core/dist_context/#d9d.core.dist_context.DistributedContext.logger","text":"Returns the logger instance configured for distributed logging.","title":"logger"},{"location":"core/dist_context/#d9d.core.dist_context.DistributedContext.master_addr","text":"Returns the IP address or domain name of the master node.","title":"master_addr"},{"location":"core/dist_context/#d9d.core.dist_context.DistributedContext.mesh_params","text":"Returns the parameters used to initialize this context.","title":"mesh_params"},{"location":"core/dist_context/#d9d.core.dist_context.DistributedContext.node_rank","text":"Returns the index of the node this process is running on.","title":"node_rank"},{"location":"core/dist_context/#d9d.core.dist_context.DistributedContext.num_nodes","text":"Returns the total number of nodes in the cluster.","title":"num_nodes"},{"location":"core/dist_context/#d9d.core.dist_context.DistributedContext.local_main_process_first","text":"Context manager that executes the block on the local main process first. Other local ranks wait at the entrance. The local main process waits at the exit to synchronize before continuing. Source code in d9d/core/dist_context/configured.py 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 @contextmanager def local_main_process_first ( self ): \"\"\" Context manager that executes the block on the local main process first. Other local ranks wait at the entrance. The local main process waits at the exit to synchronize before continuing. \"\"\" if not self . is_local_main_process : self . wait_world () yield if self . is_local_main_process : self . wait_world ()","title":"local_main_process_first"},{"location":"core/dist_context/#d9d.core.dist_context.DistributedContext.main_process_first","text":"Context manager that executes the block on the global main process first. All other ranks wait at the entrance. The global main process waits at the exit to synchronize before continuing. Source code in d9d/core/dist_context/configured.py 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 @contextmanager def main_process_first ( self ): \"\"\" Context manager that executes the block on the global main process first. All other ranks wait at the entrance. The global main process waits at the exit to synchronize before continuing. \"\"\" if not self . is_main_process : self . wait_world () yield if self . is_main_process : self . wait_world ()","title":"main_process_first"},{"location":"core/dist_context/#d9d.core.dist_context.DistributedContext.mesh_for","text":"Returns the device mesh view associated with a specific logical domain. Available Domains and Dimensions regular ( REGULAR_DOMAIN ): The most granular mesh for fully decomposed parallelism. Dimensions: ('pp', 'dp_replicate', 'dp_shard', 'cp_shard', 'cp_replicate', 'tp') expert ( EXPERT_DOMAIN ): Mesh optimized for distributing MoE (Mixture of Experts) layers. Dimensions: ('pp', 'replicate', 'ep') dense ( DENSE_DOMAIN ): Mesh optimized for distributing dense layers. Dimensions: ('pp', 'dp_replicate', 'dp_cp_shard', 'cp_replicate', 'tp') batch ( BATCH_DOMAIN ): Mesh optimized for distributing input data. Dimensions: ('pp', 'dp', 'cp', 'tp') flat ( FLAT_DOMAIN ): Mesh containing a single dimension with all the processes. Dimensions: ('world') Parameters: Name Type Description Default domain str The name of the domain to retrieve. required Returns: Type Description DeviceMesh The PyTorch DeviceMesh configured for the requested domain. Raises: Type Description ValueError If the specified domain does not exist. Source code in d9d/core/dist_context/configured.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def mesh_for ( self , domain : str ) -> DeviceMesh : \"\"\" Returns the device mesh view associated with a specific logical domain. Available Domains and Dimensions: * `regular` (`REGULAR_DOMAIN`): The most granular mesh for fully decomposed parallelism. Dimensions: ``('pp', 'dp_replicate', 'dp_shard', 'cp_shard', 'cp_replicate', 'tp')`` * `expert` (`EXPERT_DOMAIN`): Mesh optimized for distributing MoE (Mixture of Experts) layers. Dimensions: ``('pp', 'replicate', 'ep')`` * `dense` (`DENSE_DOMAIN`): Mesh optimized for distributing dense layers. Dimensions: ``('pp', 'dp_replicate', 'dp_cp_shard', 'cp_replicate', 'tp')`` * `batch` (`BATCH_DOMAIN`): Mesh optimized for distributing input data. Dimensions: ``('pp', 'dp', 'cp', 'tp')`` * `flat` (`FLAT_DOMAIN`): Mesh containing a single dimension with all the processes. Dimensions: ``('world')`` Args: domain: The name of the domain to retrieve. Returns: The PyTorch DeviceMesh configured for the requested domain. Raises: ValueError: If the specified domain does not exist. \"\"\" if domain not in self . _meshes : raise ValueError ( f \"Domain { domain } does not exist\" ) return self . _meshes [ domain ]","title":"mesh_for"},{"location":"core/dist_context/#d9d.core.dist_context.DistributedContext.set_timeout","text":"Updates the NCCL/process group timeout for all underlying meshes. Parameters: Name Type Description Default timeout_seconds float New timeout duration in seconds. required Source code in d9d/core/dist_context/configured.py 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def set_timeout ( self , timeout_seconds : float ): \"\"\" Updates the NCCL/process group timeout for all underlying meshes. Args: timeout_seconds: New timeout duration in seconds. \"\"\" if not self . _params . is_distributed : # does nothing for local setups return self . logger . info ( f \"Setting global timeout to { timeout_seconds } seconds\" ) self . wait_world () groups : list [ torch . distributed . ProcessGroup | None ] = [ None ] for mesh in self . _meshes . values (): for dim in range ( mesh . ndim ): groups . append ( mesh . get_group ( dim )) for group in groups : torch . distributed . distributed_c10d . _set_pg_timeout ( datetime . timedelta ( seconds = timeout_seconds ), group ) # noqa: SLF001","title":"set_timeout"},{"location":"core/dist_context/#d9d.core.dist_context.DistributedContext.wait_world","text":"Blocks process execution until all ranks reach this point. Source code in d9d/core/dist_context/configured.py 126 127 128 129 130 131 def wait_world ( self ): \"\"\"Blocks process execution until all ranks reach this point.\"\"\" if self . _params . is_distributed : torch . distributed . barrier ( device_ids = [ torch . cuda . current_device ()]) torch . cuda . synchronize ()","title":"wait_world"},{"location":"core/dist_context/#d9d.core.dist_context.build_dist_logger","text":"Configures and returns a logger instance for d9d. The logger is configured to write to stdout with a formatter that includes the provided rank qualifier, allowing for easier debugging in distributed logs. Parameters: Name Type Description Default qualifier str A string identifying the current rank's position in the mesh. required level int Log level to set by default required Returns: Type Description Logger A configured logging.Logger instance. Source code in d9d/core/dist_context/log.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def build_dist_logger ( qualifier : str , level : int ) -> logging . Logger : \"\"\" Configures and returns a logger instance for d9d. The logger is configured to write to stdout with a formatter that includes the provided rank qualifier, allowing for easier debugging in distributed logs. Args: qualifier: A string identifying the current rank's position in the mesh. level: Log level to set by default Returns: A configured logging.Logger instance. \"\"\" dist_logger = logging . getLogger ( \"d9d\" ) dist_logger . setLevel ( level ) dist_logger . handlers . clear () ch = logging . StreamHandler ( sys . stdout ) ch . setLevel ( level ) formatter = logging . Formatter ( f \"[d9d] [ { qualifier } ] %(asctime)s - %(levelname)s - %(message)s\" ) ch . setFormatter ( formatter ) dist_logger . addHandler ( ch ) return dist_logger","title":"build_dist_logger"},{"location":"core/dist_ops/","text":"About The d9d.core.dist_ops package provides high-level wrappers around torch.distributed collective operations. While PyTorch's native distributed library is powerful, it often requires significant boilerplate code - specifically the manual pre-allocation of output buffers (e.g., creating a list of empty tensors for all_gather ). d9d simplifies this by handling buffer allocation automatically. It also introduces specialized operators for handling Variadic Shapes , allowing ranks to exchange tensors even when they do not know the incoming tensor shapes beforehand. Usage Examples Gathering Tensors Gathering tensors of identical shapes from all ranks. d9d automatically allocates buffers for this operation. import torch from d9d.core.dist_context import DistributedContext , REGULAR_DOMAIN from d9d.core.dist_ops import all_gather # Setup ctx : DistributedContext = ... group = ctx . mesh_for ( REGULAR_DOMAIN ) . get_group () rank = ctx . mesh_for ( REGULAR_DOMAIN ) . get_rank () # Each rank has a tensor of the same shape (e.g., [2, 2]) # but different values local_tensor = torch . ones (( 2 , 2 ), device = \"cuda\" ) * rank # Gather gathered_tensors = all_gather ( local_tensor , group = group ) for i , t in enumerate ( gathered_tensors ): print ( f \"From rank { i } : { t } \" ) Gathering Tensors with Variadic Shapes Gathering tensors where dimensions differ across ranks. import torch from d9d.core.dist_context import DistributedContext , REGULAR_DOMAIN from d9d.core.dist_ops import all_gather_variadic_shape # Setup ctx : DistributedContext = ... group = ctx . mesh_for ( REGULAR_DOMAIN ) . get_group () rank = ctx . mesh_for ( REGULAR_DOMAIN ) . get_rank () # Rank 0 has shape [1], Rank 1 has shape [2], ... local_tensor = torch . randn (( rank + 1 ,), device = \"cuda\" ) # Gather # The system automatically handles the shape mismatch gathered_tensors = all_gather_variadic_shape ( local_tensor , group = group ) for i , t in enumerate ( gathered_tensors ): print ( f \"Rank { i } sent shape: { t . shape } \" ) Object Communication Sending arbitrary Python objects between ranks. These objects must be picklable. import torch.distributed as dist from d9d.core.dist_context import DistributedContext , REGULAR_DOMAIN from d9d.core.dist_ops import all_gather_object # Setup ctx : DistributedContext = ... group = ctx . mesh_for ( REGULAR_DOMAIN ) . get_group () rank = ctx . mesh_for ( REGULAR_DOMAIN ) . get_rank () # Local data my_metadata = { \"rank\" : rank , \"the-strongest\" : \"satoru-gojo\" } # Gather results = all_gather_object ( my_metadata , group = group ) for data in results : print ( f \"Rank { data [ 'rank' ] } sent { data } \" ) d9d.core.dist_ops This module provides high-level wrappers around torch.distributed collective operations. all_gather ( tensor , group , async_op = False ) Gathers tensors from the whole process group to all ranks. This function assumes that tensors on all ranks have the same shape and dtype as the tensor on the current rank. It automatically allocates the output buffer list. Parameters: Name Type Description Default tensor Tensor The local tensor to send. required group ProcessGroup The process group to work on. required async_op bool Whether the operation should be asynchronous. False Returns: Type Description list [ Tensor ] | tuple [ list [ Tensor ], Work ] If async_op is False: A list of gathered tensors. list [ Tensor ] | tuple [ list [ Tensor ], Work ] If async_op is True: A tuple containing (buffer_list, work_handle). Source code in d9d/core/dist_ops/tensor.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def all_gather ( tensor : torch . Tensor , group : dist . ProcessGroup , async_op : bool = False ) -> list [ torch . Tensor ] | tuple [ list [ torch . Tensor ], dist . Work ]: \"\"\" Gathers tensors from the whole process group to all ranks. This function assumes that tensors on all ranks have the same shape and dtype as the tensor on the current rank. It automatically allocates the output buffer list. Args: tensor: The local tensor to send. group: The process group to work on. async_op: Whether the operation should be asynchronous. Returns: If async_op is False: A list of gathered tensors. If async_op is True: A tuple containing (buffer_list, work_handle). \"\"\" save_list = [ torch . empty_like ( tensor ) for _ in range ( group . size ())] work = dist . all_gather ( save_list , tensor , group = group , async_op = async_op ) if async_op : return save_list , work else : return save_list all_gather_object ( obj , group ) Gathers picklable objects from the whole process group to all ranks. This acts as a wrapper around torch.distributed.all_gather_object that automatically initializes the output buffer list on all ranks. Parameters: Name Type Description Default obj T The local object to send. Must be picklable. required group ProcessGroup The process group to work on. required Returns: Type Description list [ T ] A list of objects containing the data gathered from all ranks. Source code in d9d/core/dist_ops/object.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def all_gather_object ( obj : T , group : dist . ProcessGroup ) -> list [ T ]: \"\"\" Gathers picklable objects from the whole process group to all ranks. This acts as a wrapper around torch.distributed.all_gather_object that automatically initializes the output buffer list on all ranks. Args: obj: The local object to send. Must be picklable. group: The process group to work on. Returns: A list of objects containing the data gathered from all ranks. \"\"\" # We initialize with None, but we cast to list[T] because we know # dist.gather_object will populate these slots with actual objects. save_list = cast ( list [ T ], [ None for _ in range ( group . size ())]) dist . all_gather_object ( save_list , obj , group = group ) return save_list all_gather_variadic_shape ( tensor , group , async_op = False ) Gathers tensors of different shapes from the whole process group to all ranks. Unlike standard all_gather, this function first communicates the shape of the tensor on every rank allowing for dynamic sizing. Parameters: Name Type Description Default tensor Tensor The local tensor to send. required group ProcessGroup The process group to work on. required async_op bool Whether the final data gathering should be asynchronous. Note that shape gathering is always synchronous. False Returns: Type Description list [ Tensor ] | tuple [ list [ Tensor ], Work ] If async_op is False: A list of gathered tensors of varying shapes. list [ Tensor ] | tuple [ list [ Tensor ], Work ] If async_op is True: A tuple containing (buffer_list, work_handle). Source code in d9d/core/dist_ops/tensor.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def all_gather_variadic_shape ( tensor : torch . Tensor , group : dist . ProcessGroup , async_op : bool = False ) -> list [ torch . Tensor ] | tuple [ list [ torch . Tensor ], dist . Work ]: \"\"\" Gathers tensors of different shapes from the whole process group to all ranks. Unlike standard all_gather, this function first communicates the shape of the tensor on every rank allowing for dynamic sizing. Args: tensor: The local tensor to send. group: The process group to work on. async_op: Whether the final data gathering should be asynchronous. Note that shape gathering is always synchronous. Returns: If async_op is False: A list of gathered tensors of varying shapes. If async_op is True: A tuple containing (buffer_list, work_handle). \"\"\" all_shape = _all_gather_shapes ( tensor , group ) all_result = [ torch . empty ( tuple ( shape ), dtype = tensor . dtype , device = tensor . device ) for shape in all_shape ] all_result_wait = dist . all_gather ( all_result , tensor , group = group , async_op = async_op ) if async_op : return all_result , all_result_wait else : return all_result gather ( tensor , group , group_dst , async_op = False ) Gathers tensors from the process group to a specific destination rank. This function assumes that tensors on all ranks have the same shape and dtype as the tensor on the current rank. It automatically allocates the output buffer list on the destination. Parameters: Name Type Description Default tensor Tensor The local tensor to send. required group ProcessGroup The process group to work on. required group_dst int The rank within the group that will receive the tensors. required async_op bool Whether the operation should be asynchronous. False Returns: Type Description list [ Tensor ] | tuple [ list [ Tensor ] | None, Work ] | None If async_op is False: A list of tensors on the destination rank, None elsewhere. list [ Tensor ] | tuple [ list [ Tensor ] | None, Work ] | None If async_op is True: A tuple containing (buffer_list, work_handle). Source code in d9d/core/dist_ops/tensor.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def gather ( tensor : torch . Tensor , group : dist . ProcessGroup , group_dst : int , async_op : bool = False ) -> list [ torch . Tensor ] | tuple [ list [ torch . Tensor ] | None , dist . Work ] | None : \"\"\" Gathers tensors from the process group to a specific destination rank. This function assumes that tensors on all ranks have the same shape and dtype as the tensor on the current rank. It automatically allocates the output buffer list on the destination. Args: tensor: The local tensor to send. group: The process group to work on. group_dst: The rank within the group that will receive the tensors. async_op: Whether the operation should be asynchronous. Returns: If async_op is False: A list of tensors on the destination rank, None elsewhere. If async_op is True: A tuple containing (buffer_list, work_handle). \"\"\" if group . rank () == group_dst : save_list = [ torch . empty_like ( tensor ) for _ in range ( group . size ())] else : save_list = None work = dist . gather ( tensor , save_list , group = group , group_dst = group_dst , async_op = async_op ) if async_op : return save_list , work else : return save_list gather_object ( obj , group , group_dst ) Gathers picklable objects from the whole process group to a specific destination rank. This acts as a wrapper around torch.distributed.gather_object that automatically initializes the output buffer list on the destination rank. Parameters: Name Type Description Default obj T The local object to send. Must be picklable. required group ProcessGroup The process group to work on. required group_dst int The rank within the group that will receive the objects. required Returns: Type Description list [ T ] | None A list of objects from all ranks on the destination rank; None on other ranks. Source code in d9d/core/dist_ops/object.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def gather_object ( obj : T , group : dist . ProcessGroup , group_dst : int ) -> list [ T ] | None : \"\"\" Gathers picklable objects from the whole process group to a specific destination rank. This acts as a wrapper around torch.distributed.gather_object that automatically initializes the output buffer list on the destination rank. Args: obj: The local object to send. Must be picklable. group: The process group to work on. group_dst: The rank within the group that will receive the objects. Returns: A list of objects from all ranks on the destination rank; None on other ranks. \"\"\" if group . rank () == group_dst : # We initialize with None, but we cast to list[T] because we know # dist.gather_object will populate these slots with actual objects. save_list = cast ( list [ T ], [ None for _ in range ( group . size ())]) else : save_list = None dist . gather_object ( obj , save_list , group = group , group_dst = group_dst ) return save_list gather_variadic_shape ( tensor , group , group_dst ) Gathers tensors of different shapes from the process group to a specific rank. This function coordinates shape exchange and uses point-to-point communication (isend/irecv) to gather tensors that may differ in shape across ranks. Currently, does not support async_op. Parameters: Name Type Description Default tensor Tensor The local tensor to send. required group ProcessGroup The process group to work on. required group_dst int The rank within the group that will receive the tensors. required Returns: Type Description list [ Tensor ] | None A list of tensors of varying shapes on the destination rank; None on other ranks. Source code in d9d/core/dist_ops/tensor.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 def gather_variadic_shape ( tensor : torch . Tensor , group : dist . ProcessGroup , group_dst : int ) -> list [ torch . Tensor ] | None : \"\"\" Gathers tensors of different shapes from the process group to a specific rank. This function coordinates shape exchange and uses point-to-point communication (isend/irecv) to gather tensors that may differ in shape across ranks. Currently, does not support async_op. Args: tensor: The local tensor to send. group: The process group to work on. group_dst: The rank within the group that will receive the tensors. Returns: A list of tensors of varying shapes on the destination rank; None on other ranks. \"\"\" is_current_dst = group . rank () == group_dst all_shape = _all_gather_shapes ( tensor , group ) if is_current_dst : all_recv_futures : list [ dist . Work ] = [] all_result : list [ torch . Tensor ] = cast ( list [ torch . Tensor ], [ None for _ in range ( group . size ())]) for group_src_i in range ( group . size ()): if group_src_i == group_dst : all_result [ group_src_i ] = tensor continue all_result [ group_src_i ] = torch . empty ( tuple ( all_shape [ group_src_i ]), dtype = tensor . dtype , device = tensor . device ) all_recv_future = dist . irecv ( all_result [ group_src_i ], group = group , group_src = group_src_i ) all_recv_future = cast ( dist . Work , all_recv_future ) # we know we are on dst rank all_recv_futures . append ( all_recv_future ) for recv_future in all_recv_futures : recv_future . wait () return all_result else : dist . isend ( tensor = tensor , group = group , group_dst = group_dst ) return None","title":"Distributed Operations"},{"location":"core/dist_ops/#about","text":"The d9d.core.dist_ops package provides high-level wrappers around torch.distributed collective operations. While PyTorch's native distributed library is powerful, it often requires significant boilerplate code - specifically the manual pre-allocation of output buffers (e.g., creating a list of empty tensors for all_gather ). d9d simplifies this by handling buffer allocation automatically. It also introduces specialized operators for handling Variadic Shapes , allowing ranks to exchange tensors even when they do not know the incoming tensor shapes beforehand.","title":"About"},{"location":"core/dist_ops/#usage-examples","text":"","title":"Usage Examples"},{"location":"core/dist_ops/#gathering-tensors","text":"Gathering tensors of identical shapes from all ranks. d9d automatically allocates buffers for this operation. import torch from d9d.core.dist_context import DistributedContext , REGULAR_DOMAIN from d9d.core.dist_ops import all_gather # Setup ctx : DistributedContext = ... group = ctx . mesh_for ( REGULAR_DOMAIN ) . get_group () rank = ctx . mesh_for ( REGULAR_DOMAIN ) . get_rank () # Each rank has a tensor of the same shape (e.g., [2, 2]) # but different values local_tensor = torch . ones (( 2 , 2 ), device = \"cuda\" ) * rank # Gather gathered_tensors = all_gather ( local_tensor , group = group ) for i , t in enumerate ( gathered_tensors ): print ( f \"From rank { i } : { t } \" )","title":"Gathering Tensors"},{"location":"core/dist_ops/#gathering-tensors-with-variadic-shapes","text":"Gathering tensors where dimensions differ across ranks. import torch from d9d.core.dist_context import DistributedContext , REGULAR_DOMAIN from d9d.core.dist_ops import all_gather_variadic_shape # Setup ctx : DistributedContext = ... group = ctx . mesh_for ( REGULAR_DOMAIN ) . get_group () rank = ctx . mesh_for ( REGULAR_DOMAIN ) . get_rank () # Rank 0 has shape [1], Rank 1 has shape [2], ... local_tensor = torch . randn (( rank + 1 ,), device = \"cuda\" ) # Gather # The system automatically handles the shape mismatch gathered_tensors = all_gather_variadic_shape ( local_tensor , group = group ) for i , t in enumerate ( gathered_tensors ): print ( f \"Rank { i } sent shape: { t . shape } \" )","title":"Gathering Tensors with Variadic Shapes"},{"location":"core/dist_ops/#object-communication","text":"Sending arbitrary Python objects between ranks. These objects must be picklable. import torch.distributed as dist from d9d.core.dist_context import DistributedContext , REGULAR_DOMAIN from d9d.core.dist_ops import all_gather_object # Setup ctx : DistributedContext = ... group = ctx . mesh_for ( REGULAR_DOMAIN ) . get_group () rank = ctx . mesh_for ( REGULAR_DOMAIN ) . get_rank () # Local data my_metadata = { \"rank\" : rank , \"the-strongest\" : \"satoru-gojo\" } # Gather results = all_gather_object ( my_metadata , group = group ) for data in results : print ( f \"Rank { data [ 'rank' ] } sent { data } \" )","title":"Object Communication"},{"location":"core/dist_ops/#d9d.core.dist_ops","text":"This module provides high-level wrappers around torch.distributed collective operations.","title":"dist_ops"},{"location":"core/dist_ops/#d9d.core.dist_ops.all_gather","text":"Gathers tensors from the whole process group to all ranks. This function assumes that tensors on all ranks have the same shape and dtype as the tensor on the current rank. It automatically allocates the output buffer list. Parameters: Name Type Description Default tensor Tensor The local tensor to send. required group ProcessGroup The process group to work on. required async_op bool Whether the operation should be asynchronous. False Returns: Type Description list [ Tensor ] | tuple [ list [ Tensor ], Work ] If async_op is False: A list of gathered tensors. list [ Tensor ] | tuple [ list [ Tensor ], Work ] If async_op is True: A tuple containing (buffer_list, work_handle). Source code in d9d/core/dist_ops/tensor.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def all_gather ( tensor : torch . Tensor , group : dist . ProcessGroup , async_op : bool = False ) -> list [ torch . Tensor ] | tuple [ list [ torch . Tensor ], dist . Work ]: \"\"\" Gathers tensors from the whole process group to all ranks. This function assumes that tensors on all ranks have the same shape and dtype as the tensor on the current rank. It automatically allocates the output buffer list. Args: tensor: The local tensor to send. group: The process group to work on. async_op: Whether the operation should be asynchronous. Returns: If async_op is False: A list of gathered tensors. If async_op is True: A tuple containing (buffer_list, work_handle). \"\"\" save_list = [ torch . empty_like ( tensor ) for _ in range ( group . size ())] work = dist . all_gather ( save_list , tensor , group = group , async_op = async_op ) if async_op : return save_list , work else : return save_list","title":"all_gather"},{"location":"core/dist_ops/#d9d.core.dist_ops.all_gather_object","text":"Gathers picklable objects from the whole process group to all ranks. This acts as a wrapper around torch.distributed.all_gather_object that automatically initializes the output buffer list on all ranks. Parameters: Name Type Description Default obj T The local object to send. Must be picklable. required group ProcessGroup The process group to work on. required Returns: Type Description list [ T ] A list of objects containing the data gathered from all ranks. Source code in d9d/core/dist_ops/object.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def all_gather_object ( obj : T , group : dist . ProcessGroup ) -> list [ T ]: \"\"\" Gathers picklable objects from the whole process group to all ranks. This acts as a wrapper around torch.distributed.all_gather_object that automatically initializes the output buffer list on all ranks. Args: obj: The local object to send. Must be picklable. group: The process group to work on. Returns: A list of objects containing the data gathered from all ranks. \"\"\" # We initialize with None, but we cast to list[T] because we know # dist.gather_object will populate these slots with actual objects. save_list = cast ( list [ T ], [ None for _ in range ( group . size ())]) dist . all_gather_object ( save_list , obj , group = group ) return save_list","title":"all_gather_object"},{"location":"core/dist_ops/#d9d.core.dist_ops.all_gather_variadic_shape","text":"Gathers tensors of different shapes from the whole process group to all ranks. Unlike standard all_gather, this function first communicates the shape of the tensor on every rank allowing for dynamic sizing. Parameters: Name Type Description Default tensor Tensor The local tensor to send. required group ProcessGroup The process group to work on. required async_op bool Whether the final data gathering should be asynchronous. Note that shape gathering is always synchronous. False Returns: Type Description list [ Tensor ] | tuple [ list [ Tensor ], Work ] If async_op is False: A list of gathered tensors of varying shapes. list [ Tensor ] | tuple [ list [ Tensor ], Work ] If async_op is True: A tuple containing (buffer_list, work_handle). Source code in d9d/core/dist_ops/tensor.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def all_gather_variadic_shape ( tensor : torch . Tensor , group : dist . ProcessGroup , async_op : bool = False ) -> list [ torch . Tensor ] | tuple [ list [ torch . Tensor ], dist . Work ]: \"\"\" Gathers tensors of different shapes from the whole process group to all ranks. Unlike standard all_gather, this function first communicates the shape of the tensor on every rank allowing for dynamic sizing. Args: tensor: The local tensor to send. group: The process group to work on. async_op: Whether the final data gathering should be asynchronous. Note that shape gathering is always synchronous. Returns: If async_op is False: A list of gathered tensors of varying shapes. If async_op is True: A tuple containing (buffer_list, work_handle). \"\"\" all_shape = _all_gather_shapes ( tensor , group ) all_result = [ torch . empty ( tuple ( shape ), dtype = tensor . dtype , device = tensor . device ) for shape in all_shape ] all_result_wait = dist . all_gather ( all_result , tensor , group = group , async_op = async_op ) if async_op : return all_result , all_result_wait else : return all_result","title":"all_gather_variadic_shape"},{"location":"core/dist_ops/#d9d.core.dist_ops.gather","text":"Gathers tensors from the process group to a specific destination rank. This function assumes that tensors on all ranks have the same shape and dtype as the tensor on the current rank. It automatically allocates the output buffer list on the destination. Parameters: Name Type Description Default tensor Tensor The local tensor to send. required group ProcessGroup The process group to work on. required group_dst int The rank within the group that will receive the tensors. required async_op bool Whether the operation should be asynchronous. False Returns: Type Description list [ Tensor ] | tuple [ list [ Tensor ] | None, Work ] | None If async_op is False: A list of tensors on the destination rank, None elsewhere. list [ Tensor ] | tuple [ list [ Tensor ] | None, Work ] | None If async_op is True: A tuple containing (buffer_list, work_handle). Source code in d9d/core/dist_ops/tensor.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def gather ( tensor : torch . Tensor , group : dist . ProcessGroup , group_dst : int , async_op : bool = False ) -> list [ torch . Tensor ] | tuple [ list [ torch . Tensor ] | None , dist . Work ] | None : \"\"\" Gathers tensors from the process group to a specific destination rank. This function assumes that tensors on all ranks have the same shape and dtype as the tensor on the current rank. It automatically allocates the output buffer list on the destination. Args: tensor: The local tensor to send. group: The process group to work on. group_dst: The rank within the group that will receive the tensors. async_op: Whether the operation should be asynchronous. Returns: If async_op is False: A list of tensors on the destination rank, None elsewhere. If async_op is True: A tuple containing (buffer_list, work_handle). \"\"\" if group . rank () == group_dst : save_list = [ torch . empty_like ( tensor ) for _ in range ( group . size ())] else : save_list = None work = dist . gather ( tensor , save_list , group = group , group_dst = group_dst , async_op = async_op ) if async_op : return save_list , work else : return save_list","title":"gather"},{"location":"core/dist_ops/#d9d.core.dist_ops.gather_object","text":"Gathers picklable objects from the whole process group to a specific destination rank. This acts as a wrapper around torch.distributed.gather_object that automatically initializes the output buffer list on the destination rank. Parameters: Name Type Description Default obj T The local object to send. Must be picklable. required group ProcessGroup The process group to work on. required group_dst int The rank within the group that will receive the objects. required Returns: Type Description list [ T ] | None A list of objects from all ranks on the destination rank; None on other ranks. Source code in d9d/core/dist_ops/object.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def gather_object ( obj : T , group : dist . ProcessGroup , group_dst : int ) -> list [ T ] | None : \"\"\" Gathers picklable objects from the whole process group to a specific destination rank. This acts as a wrapper around torch.distributed.gather_object that automatically initializes the output buffer list on the destination rank. Args: obj: The local object to send. Must be picklable. group: The process group to work on. group_dst: The rank within the group that will receive the objects. Returns: A list of objects from all ranks on the destination rank; None on other ranks. \"\"\" if group . rank () == group_dst : # We initialize with None, but we cast to list[T] because we know # dist.gather_object will populate these slots with actual objects. save_list = cast ( list [ T ], [ None for _ in range ( group . size ())]) else : save_list = None dist . gather_object ( obj , save_list , group = group , group_dst = group_dst ) return save_list","title":"gather_object"},{"location":"core/dist_ops/#d9d.core.dist_ops.gather_variadic_shape","text":"Gathers tensors of different shapes from the process group to a specific rank. This function coordinates shape exchange and uses point-to-point communication (isend/irecv) to gather tensors that may differ in shape across ranks. Currently, does not support async_op. Parameters: Name Type Description Default tensor Tensor The local tensor to send. required group ProcessGroup The process group to work on. required group_dst int The rank within the group that will receive the tensors. required Returns: Type Description list [ Tensor ] | None A list of tensors of varying shapes on the destination rank; None on other ranks. Source code in d9d/core/dist_ops/tensor.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 def gather_variadic_shape ( tensor : torch . Tensor , group : dist . ProcessGroup , group_dst : int ) -> list [ torch . Tensor ] | None : \"\"\" Gathers tensors of different shapes from the process group to a specific rank. This function coordinates shape exchange and uses point-to-point communication (isend/irecv) to gather tensors that may differ in shape across ranks. Currently, does not support async_op. Args: tensor: The local tensor to send. group: The process group to work on. group_dst: The rank within the group that will receive the tensors. Returns: A list of tensors of varying shapes on the destination rank; None on other ranks. \"\"\" is_current_dst = group . rank () == group_dst all_shape = _all_gather_shapes ( tensor , group ) if is_current_dst : all_recv_futures : list [ dist . Work ] = [] all_result : list [ torch . Tensor ] = cast ( list [ torch . Tensor ], [ None for _ in range ( group . size ())]) for group_src_i in range ( group . size ()): if group_src_i == group_dst : all_result [ group_src_i ] = tensor continue all_result [ group_src_i ] = torch . empty ( tuple ( all_shape [ group_src_i ]), dtype = tensor . dtype , device = tensor . device ) all_recv_future = dist . irecv ( all_result [ group_src_i ], group = group , group_src = group_src_i ) all_recv_future = cast ( dist . Work , all_recv_future ) # we know we are on dst rank all_recv_futures . append ( all_recv_future ) for recv_future in all_recv_futures : recv_future . wait () return all_result else : dist . isend ( tensor = tensor , group = group , group_dst = group_dst ) return None","title":"gather_variadic_shape"},{"location":"core/sharding/","text":"About The d9d.core.sharding package provides utilities for splitting and reconstructing complex nested structures (PyTrees) of PyTorch Tensors and Python Lists. Sharding Spec A Sharding Spec is a PyTree that mirrors the structure of your data (e.g., a State Dict). Structure : Mirrors the data hierarchy. The spec structure is used to traverse the data; sharding operations flatten the data tree up to the leaves defined in the spec. Leaves : d9d.core.sharding.SpecShard(dim, do_stack=False) : Tensors : The tensor is split along dimension dim . Lists : The list is split into chunks. dim must be 0 . do_stack : If True , tensors are unbound/stacked (reducing/increasing dimensionality). If False (default), tensors are split/concatenated (preserving dimensionality). d9d.core.sharding.SpecReplicate (or None ): The item is replicated (kept as-is/not split) across all shards. Helper functions like shard_spec_on_dim allow generating these specs automatically. d9d.core.sharding SpecReplicate dataclass Specifies that a leaf node should be replicated across all shards. Source code in d9d/core/sharding/spec.py 6 7 8 9 10 @dataclasses . dataclass ( slots = True , frozen = True ) class SpecReplicate : \"\"\" Specifies that a leaf node should be replicated across all shards. \"\"\" SpecShard dataclass Specifies that a leaf node should be split along a specific dimension. Attributes: Name Type Description dim int The dimension to split. do_stack bool If True, sharding will squeeze the sharded dimension (it should be exactly the num_shards length) Source code in d9d/core/sharding/spec.py 13 14 15 16 17 18 19 20 21 22 23 24 @dataclasses . dataclass ( slots = True , frozen = True ) class SpecShard : \"\"\" Specifies that a leaf node should be split along a specific dimension. Attributes: dim: The dimension to split. do_stack: If True, sharding will squeeze the sharded dimension (it should be exactly the num_shards length) \"\"\" dim : int do_stack : bool = False shard_spec_nothing ( tree ) Creates a sharding specification where no sharding is performed. This effectively clones the tree structure but replaces every leaf with SpecReplicate. Parameters: Name Type Description Default tree PyTree [ Any ] The input PyTree structure. required Returns: Type Description ShardingSpec A new PyTree matching the input structure, containing strictly SpecReplicate for all leaves. Source code in d9d/core/sharding/auto_spec.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def shard_spec_nothing ( tree : PyTree [ Any ]) -> ShardingSpec : \"\"\" Creates a sharding specification where no sharding is performed. This effectively clones the tree structure but replaces every leaf with SpecReplicate. Args: tree: The input PyTree structure. Returns: A new PyTree matching the input structure, containing strictly SpecReplicate for all leaves. \"\"\" return pytree . tree_map ( lambda _ : SpecReplicate (), tree , is_leaf = lambda x : isinstance ( x , ( torch . Tensor , list ))) shard_spec_on_dim ( tree , dim ) Creates a sharding specification to split all tensors in the tree on a specific dimension. Iterates over the input tree: * If a leaf is a Tensor with enough dimensions, it is mapped to a SpecShard(dim) object. * If a leaf is a list, it is mapped to a SpecShard(0) object (only dim=0 is allowed for lists). * Other types and 0-dim tensors are mapped to SpecReplicate. Parameters: Name Type Description Default tree PyTree [ Any ] The input PyTree structure. required dim int The dimension index to shard eligible tensors on. required Returns: Type Description ShardingSpec A new PyTree matching the input structure, containing SpecShard or SpecReplicate objects. Raises: Type Description ValueError If a tensor exists in the tree with rank less than or equal to 'dim'. Source code in d9d/core/sharding/auto_spec.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def shard_spec_on_dim ( tree : PyTree [ Any ], dim : int ) -> ShardingSpec : \"\"\" Creates a sharding specification to split all tensors in the tree on a specific dimension. Iterates over the input tree: * If a leaf is a Tensor with enough dimensions, it is mapped to a SpecShard(dim) object. * If a leaf is a list, it is mapped to a SpecShard(0) object (only dim=0 is allowed for lists). * Other types and 0-dim tensors are mapped to SpecReplicate. Args: tree: The input PyTree structure. dim: The dimension index to shard eligible tensors on. Returns: A new PyTree matching the input structure, containing SpecShard or SpecReplicate objects. Raises: ValueError: If a tensor exists in the tree with rank less than or equal to 'dim'. \"\"\" return pytree . tree_map ( lambda x : _tree_item_to_shard ( x , dim ), tree , is_leaf = lambda x : isinstance ( x , ( torch . Tensor , list )) ) shard_tree ( tree , sharding_spec , num_shards , enforce_even_split ) Shards a PyTree into a tuple of PyTrees, one for each shard rank. This function takes a single global data structure and splits it into num_shards structures. If a spec leaf is a SpecShard(dim) , the tensor or list is split along that dimension, and the i -th slice goes to the i -th output tree. If a spec leaf is SpecReplicate , the item is replicated (reference copy) to all output trees. Parameters: Name Type Description Default tree TSameTree The structure containing tensors to be sharded. required sharding_spec ShardingSpec A structure matching 'tree' containing SpecShard or SpecReplicate objects. required num_shards int The total number of shards to split the tensors into. required enforce_even_split bool If True, raises a ValueError if a tensor's dimension size is not perfectly divisible by num_shards . required Returns: Type Description TSameTree A tuple of length num_shards . Each element is a PyTree matching ... the structure of the input tree , containing the local data for tuple [ TSameTree , ...] that specific rank. Raises: Type Description ValueError If tree structures do not match, or valid sharding conditions are not met. Source code in d9d/core/sharding/shard.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 def shard_tree ( tree : TSameTree , sharding_spec : ShardingSpec , num_shards : int , enforce_even_split : bool ) -> tuple [ TSameTree , ... ]: \"\"\" Shards a PyTree into a tuple of PyTrees, one for each shard rank. This function takes a single global data structure and splits it into `num_shards` structures. * If a spec leaf is a ``SpecShard(dim)``, the tensor or list is split along that dimension, and the ``i``-th slice goes to the ``i``-th output tree. * If a spec leaf is ``SpecReplicate``, the item is replicated (reference copy) to all output trees. Args: tree: The structure containing tensors to be sharded. sharding_spec: A structure matching 'tree' containing ``SpecShard`` or ``SpecReplicate`` objects. num_shards: The total number of shards to split the tensors into. enforce_even_split: If True, raises a ValueError if a tensor's dimension size is not perfectly divisible by ``num_shards``. Returns: A tuple of length ``num_shards``. Each element is a PyTree matching the structure of the input ``tree``, containing the local data for that specific rank. Raises: ValueError: If tree structures do not match, or valid sharding conditions are not met. \"\"\" flat_spec , spec_struct = pytree . tree_flatten ( sharding_spec ) try : flat_tree = spec_struct . flatten_up_to ( tree ) except ( ValueError , TypeError ) as e : raise ValueError ( \"Tree structure does not match sharding spec\" ) from e sharded_leaves_per_node = [ _shard_leaf_to_list ( item , spec , num_shards , enforce_even_split ) for item , spec in zip ( flat_tree , flat_spec , strict = True ) ] rank_leaves = list ( zip ( * sharded_leaves_per_node , strict = True )) return tuple ( spec_struct . unflatten ( leaves ) for leaves in rank_leaves ) unshard_tree ( sharded_trees , sharding_spec ) Combines a sequence of PyTrees (one per rank) into a single global PyTree. This is the inverse of shard_tree . It iterates over the provided trees, gathering corresponding leaves from each rank. If the spec for a leaf is SpecShard(dim) , the tensors from all ranks are concatenated (or stacked if do_stack=True ) along that dimension. If the spec is SpecReplicate , it assumes the data is replicated and takes the item from the first rank. Parameters: Name Type Description Default sharded_trees Sequence [ TSameTree ] A sequence (list or tuple) of PyTrees. There must be one tree for each shard rank, and they must all share the same structure as sharding_spec . required sharding_spec ShardingSpec A structure matching the input trees containing SpecShard or SpecReplicate objects. required Returns: Type Description TSameTree A single PyTree where distinct shards have been merged into full tensors. Raises: Type Description ValueError If sharded_trees is empty, or if unit structures do not match the spec. Source code in d9d/core/sharding/unshard.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def unshard_tree ( sharded_trees : Sequence [ TSameTree ], sharding_spec : ShardingSpec ) -> TSameTree : \"\"\" Combines a sequence of PyTrees (one per rank) into a single global PyTree. This is the inverse of ``shard_tree``. It iterates over the provided trees, gathering corresponding leaves from each rank. * If the spec for a leaf is ``SpecShard(dim)``, the tensors from all ranks are concatenated (or stacked if ``do_stack=True``) along that dimension. * If the spec is ``SpecReplicate``, it assumes the data is replicated and takes the item from the first rank. Args: sharded_trees: A sequence (list or tuple) of PyTrees. There must be one tree for each shard rank, and they must all share the same structure as ``sharding_spec``. sharding_spec: A structure matching the input trees containing ``SpecShard`` or ``SpecReplicate`` objects. Returns: A single PyTree where distinct shards have been merged into full tensors. Raises: ValueError: If ``sharded_trees`` is empty, or if unit structures do not match the spec. \"\"\" if not sharded_trees : raise ValueError ( \"sharded_trees sequence cannot be empty\" ) flat_spec , spec_struct = pytree . tree_flatten ( sharding_spec ) flat_shards_per_rank = [] for i , tree in enumerate ( sharded_trees ): try : leaves = spec_struct . flatten_up_to ( tree ) except ( ValueError , TypeError ) as e : raise ValueError ( f \"Structure mismatch at shard { i } : tree does not match sharding spec structure\" ) from e flat_shards_per_rank . append ( leaves ) grouped_leaves = list ( zip ( * flat_shards_per_rank , strict = True )) reconstructed_leaves = [ _unshard_leaf_from_group ( group , spec ) for group , spec in zip ( grouped_leaves , flat_spec , strict = True ) ] return spec_struct . unflatten ( reconstructed_leaves )","title":"PyTree Sharding"},{"location":"core/sharding/#about","text":"The d9d.core.sharding package provides utilities for splitting and reconstructing complex nested structures (PyTrees) of PyTorch Tensors and Python Lists.","title":"About"},{"location":"core/sharding/#sharding-spec","text":"A Sharding Spec is a PyTree that mirrors the structure of your data (e.g., a State Dict). Structure : Mirrors the data hierarchy. The spec structure is used to traverse the data; sharding operations flatten the data tree up to the leaves defined in the spec. Leaves : d9d.core.sharding.SpecShard(dim, do_stack=False) : Tensors : The tensor is split along dimension dim . Lists : The list is split into chunks. dim must be 0 . do_stack : If True , tensors are unbound/stacked (reducing/increasing dimensionality). If False (default), tensors are split/concatenated (preserving dimensionality). d9d.core.sharding.SpecReplicate (or None ): The item is replicated (kept as-is/not split) across all shards. Helper functions like shard_spec_on_dim allow generating these specs automatically.","title":"Sharding Spec"},{"location":"core/sharding/#d9d.core.sharding","text":"","title":"sharding"},{"location":"core/sharding/#d9d.core.sharding.SpecReplicate","text":"Specifies that a leaf node should be replicated across all shards. Source code in d9d/core/sharding/spec.py 6 7 8 9 10 @dataclasses . dataclass ( slots = True , frozen = True ) class SpecReplicate : \"\"\" Specifies that a leaf node should be replicated across all shards. \"\"\"","title":"SpecReplicate"},{"location":"core/sharding/#d9d.core.sharding.SpecShard","text":"Specifies that a leaf node should be split along a specific dimension. Attributes: Name Type Description dim int The dimension to split. do_stack bool If True, sharding will squeeze the sharded dimension (it should be exactly the num_shards length) Source code in d9d/core/sharding/spec.py 13 14 15 16 17 18 19 20 21 22 23 24 @dataclasses . dataclass ( slots = True , frozen = True ) class SpecShard : \"\"\" Specifies that a leaf node should be split along a specific dimension. Attributes: dim: The dimension to split. do_stack: If True, sharding will squeeze the sharded dimension (it should be exactly the num_shards length) \"\"\" dim : int do_stack : bool = False","title":"SpecShard"},{"location":"core/sharding/#d9d.core.sharding.shard_spec_nothing","text":"Creates a sharding specification where no sharding is performed. This effectively clones the tree structure but replaces every leaf with SpecReplicate. Parameters: Name Type Description Default tree PyTree [ Any ] The input PyTree structure. required Returns: Type Description ShardingSpec A new PyTree matching the input structure, containing strictly SpecReplicate for all leaves. Source code in d9d/core/sharding/auto_spec.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def shard_spec_nothing ( tree : PyTree [ Any ]) -> ShardingSpec : \"\"\" Creates a sharding specification where no sharding is performed. This effectively clones the tree structure but replaces every leaf with SpecReplicate. Args: tree: The input PyTree structure. Returns: A new PyTree matching the input structure, containing strictly SpecReplicate for all leaves. \"\"\" return pytree . tree_map ( lambda _ : SpecReplicate (), tree , is_leaf = lambda x : isinstance ( x , ( torch . Tensor , list )))","title":"shard_spec_nothing"},{"location":"core/sharding/#d9d.core.sharding.shard_spec_on_dim","text":"Creates a sharding specification to split all tensors in the tree on a specific dimension. Iterates over the input tree: * If a leaf is a Tensor with enough dimensions, it is mapped to a SpecShard(dim) object. * If a leaf is a list, it is mapped to a SpecShard(0) object (only dim=0 is allowed for lists). * Other types and 0-dim tensors are mapped to SpecReplicate. Parameters: Name Type Description Default tree PyTree [ Any ] The input PyTree structure. required dim int The dimension index to shard eligible tensors on. required Returns: Type Description ShardingSpec A new PyTree matching the input structure, containing SpecShard or SpecReplicate objects. Raises: Type Description ValueError If a tensor exists in the tree with rank less than or equal to 'dim'. Source code in d9d/core/sharding/auto_spec.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def shard_spec_on_dim ( tree : PyTree [ Any ], dim : int ) -> ShardingSpec : \"\"\" Creates a sharding specification to split all tensors in the tree on a specific dimension. Iterates over the input tree: * If a leaf is a Tensor with enough dimensions, it is mapped to a SpecShard(dim) object. * If a leaf is a list, it is mapped to a SpecShard(0) object (only dim=0 is allowed for lists). * Other types and 0-dim tensors are mapped to SpecReplicate. Args: tree: The input PyTree structure. dim: The dimension index to shard eligible tensors on. Returns: A new PyTree matching the input structure, containing SpecShard or SpecReplicate objects. Raises: ValueError: If a tensor exists in the tree with rank less than or equal to 'dim'. \"\"\" return pytree . tree_map ( lambda x : _tree_item_to_shard ( x , dim ), tree , is_leaf = lambda x : isinstance ( x , ( torch . Tensor , list )) )","title":"shard_spec_on_dim"},{"location":"core/sharding/#d9d.core.sharding.shard_tree","text":"Shards a PyTree into a tuple of PyTrees, one for each shard rank. This function takes a single global data structure and splits it into num_shards structures. If a spec leaf is a SpecShard(dim) , the tensor or list is split along that dimension, and the i -th slice goes to the i -th output tree. If a spec leaf is SpecReplicate , the item is replicated (reference copy) to all output trees. Parameters: Name Type Description Default tree TSameTree The structure containing tensors to be sharded. required sharding_spec ShardingSpec A structure matching 'tree' containing SpecShard or SpecReplicate objects. required num_shards int The total number of shards to split the tensors into. required enforce_even_split bool If True, raises a ValueError if a tensor's dimension size is not perfectly divisible by num_shards . required Returns: Type Description TSameTree A tuple of length num_shards . Each element is a PyTree matching ... the structure of the input tree , containing the local data for tuple [ TSameTree , ...] that specific rank. Raises: Type Description ValueError If tree structures do not match, or valid sharding conditions are not met. Source code in d9d/core/sharding/shard.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 def shard_tree ( tree : TSameTree , sharding_spec : ShardingSpec , num_shards : int , enforce_even_split : bool ) -> tuple [ TSameTree , ... ]: \"\"\" Shards a PyTree into a tuple of PyTrees, one for each shard rank. This function takes a single global data structure and splits it into `num_shards` structures. * If a spec leaf is a ``SpecShard(dim)``, the tensor or list is split along that dimension, and the ``i``-th slice goes to the ``i``-th output tree. * If a spec leaf is ``SpecReplicate``, the item is replicated (reference copy) to all output trees. Args: tree: The structure containing tensors to be sharded. sharding_spec: A structure matching 'tree' containing ``SpecShard`` or ``SpecReplicate`` objects. num_shards: The total number of shards to split the tensors into. enforce_even_split: If True, raises a ValueError if a tensor's dimension size is not perfectly divisible by ``num_shards``. Returns: A tuple of length ``num_shards``. Each element is a PyTree matching the structure of the input ``tree``, containing the local data for that specific rank. Raises: ValueError: If tree structures do not match, or valid sharding conditions are not met. \"\"\" flat_spec , spec_struct = pytree . tree_flatten ( sharding_spec ) try : flat_tree = spec_struct . flatten_up_to ( tree ) except ( ValueError , TypeError ) as e : raise ValueError ( \"Tree structure does not match sharding spec\" ) from e sharded_leaves_per_node = [ _shard_leaf_to_list ( item , spec , num_shards , enforce_even_split ) for item , spec in zip ( flat_tree , flat_spec , strict = True ) ] rank_leaves = list ( zip ( * sharded_leaves_per_node , strict = True )) return tuple ( spec_struct . unflatten ( leaves ) for leaves in rank_leaves )","title":"shard_tree"},{"location":"core/sharding/#d9d.core.sharding.unshard_tree","text":"Combines a sequence of PyTrees (one per rank) into a single global PyTree. This is the inverse of shard_tree . It iterates over the provided trees, gathering corresponding leaves from each rank. If the spec for a leaf is SpecShard(dim) , the tensors from all ranks are concatenated (or stacked if do_stack=True ) along that dimension. If the spec is SpecReplicate , it assumes the data is replicated and takes the item from the first rank. Parameters: Name Type Description Default sharded_trees Sequence [ TSameTree ] A sequence (list or tuple) of PyTrees. There must be one tree for each shard rank, and they must all share the same structure as sharding_spec . required sharding_spec ShardingSpec A structure matching the input trees containing SpecShard or SpecReplicate objects. required Returns: Type Description TSameTree A single PyTree where distinct shards have been merged into full tensors. Raises: Type Description ValueError If sharded_trees is empty, or if unit structures do not match the spec. Source code in d9d/core/sharding/unshard.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def unshard_tree ( sharded_trees : Sequence [ TSameTree ], sharding_spec : ShardingSpec ) -> TSameTree : \"\"\" Combines a sequence of PyTrees (one per rank) into a single global PyTree. This is the inverse of ``shard_tree``. It iterates over the provided trees, gathering corresponding leaves from each rank. * If the spec for a leaf is ``SpecShard(dim)``, the tensors from all ranks are concatenated (or stacked if ``do_stack=True``) along that dimension. * If the spec is ``SpecReplicate``, it assumes the data is replicated and takes the item from the first rank. Args: sharded_trees: A sequence (list or tuple) of PyTrees. There must be one tree for each shard rank, and they must all share the same structure as ``sharding_spec``. sharding_spec: A structure matching the input trees containing ``SpecShard`` or ``SpecReplicate`` objects. Returns: A single PyTree where distinct shards have been merged into full tensors. Raises: ValueError: If ``sharded_trees`` is empty, or if unit structures do not match the spec. \"\"\" if not sharded_trees : raise ValueError ( \"sharded_trees sequence cannot be empty\" ) flat_spec , spec_struct = pytree . tree_flatten ( sharding_spec ) flat_shards_per_rank = [] for i , tree in enumerate ( sharded_trees ): try : leaves = spec_struct . flatten_up_to ( tree ) except ( ValueError , TypeError ) as e : raise ValueError ( f \"Structure mismatch at shard { i } : tree does not match sharding spec structure\" ) from e flat_shards_per_rank . append ( leaves ) grouped_leaves = list ( zip ( * flat_shards_per_rank , strict = True )) reconstructed_leaves = [ _unshard_leaf_from_group ( group , spec ) for group , spec in zip ( grouped_leaves , flat_spec , strict = True ) ] return spec_struct . unflatten ( reconstructed_leaves )","title":"unshard_tree"},{"location":"core/types/","text":"About The d9d.core.types package gathers common Type Aliases used throughout the framework. The d9d.core.protocol package defines standard interfaces (Protocols) for standard PyTorch components used in the distributed training loop. d9d.core.types Common type definitions used throughout the framework. CollateFn = Callable [[ Sequence [ TDataTree ]], TDataTree ] module-attribute Type alias for a function that collates a sequence of samples into a batch. The function receives a sequence of individual data point structures (PyTrees) and is responsible for stacking or merging them into a single batched structure. PyTree = TLeaf | list [ 'PyTree[TLeaf]' ] | dict [ str , 'PyTree[TLeaf]' ] | tuple [ 'PyTree[TLeaf]' , ... ] module-attribute A recursive type definition representing a tree of data. This type alias covers standard Python containers (dictionaries, lists, tuples) nested arbitrarily deep, terminating in a leaf node of type TLeaf . This is commonly used for handling nested state dictionaries or arguments passed to functions that support recursive traversal (similar to torch.utils._pytree ). ScalarTree = PyTree [ str | float | int | bool ] module-attribute A recursive tree structure where the leaf nodes are python scalars (str, float, int). TensorTree = PyTree [ torch . Tensor ] module-attribute A recursive tree structure where the leaf nodes are PyTorch Tensors. d9d.core.protocol Package providing protocol definitions for standard PyTorch objects. LRSchedulerProtocol Bases: Protocol Protocol defining an interface for a Learning Rate Scheduler. This protocol ensures that the wrapped scheduler supports stepping and state checkpointing via the Stateful interface. Source code in d9d/core/protocol/training.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 @runtime_checkable class LRSchedulerProtocol ( Protocol ): \"\"\" Protocol defining an interface for a Learning Rate Scheduler. This protocol ensures that the wrapped scheduler supports stepping and state checkpointing via the Stateful interface. \"\"\" def step ( self ): \"\"\"Performs a single learning rate scheduling step.\"\"\" ... def state_dict ( self ) -> dict [ str , Any ]: \"\"\" Objects should return their state_dict representation as a dictionary. The output of this function will be checkpointed, and later restored in `load_state_dict()`. Returns: Dict: The objects state dict \"\"\" def load_state_dict ( self , state_dict : dict [ str , Any ]) -> None : \"\"\" Restore the object's state from the provided state_dict. Args: state_dict: The state dict to restore from \"\"\" load_state_dict ( state_dict ) Restore the object's state from the provided state_dict. Parameters: Name Type Description Default state_dict dict [ str , Any ] The state dict to restore from required Source code in d9d/core/protocol/training.py 62 63 64 65 66 67 68 def load_state_dict ( self , state_dict : dict [ str , Any ]) -> None : \"\"\" Restore the object's state from the provided state_dict. Args: state_dict: The state dict to restore from \"\"\" state_dict () Objects should return their state_dict representation as a dictionary. The output of this function will be checkpointed, and later restored in load_state_dict() . Returns: Name Type Description Dict dict [ str , Any ] The objects state dict Source code in d9d/core/protocol/training.py 52 53 54 55 56 57 58 59 60 def state_dict ( self ) -> dict [ str , Any ]: \"\"\" Objects should return their state_dict representation as a dictionary. The output of this function will be checkpointed, and later restored in `load_state_dict()`. Returns: Dict: The objects state dict \"\"\" step () Performs a single learning rate scheduling step. Source code in d9d/core/protocol/training.py 47 48 49 50 def step ( self ): \"\"\"Performs a single learning rate scheduling step.\"\"\" ... OptimizerProtocol Bases: Protocol Protocol defining an interface for standard PyTorch Optimizer object. This protocol ensures that the wrapped optimizer supports standard API and state checkpointing via the Stateful interface. Source code in d9d/core/protocol/training.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 @runtime_checkable class OptimizerProtocol ( Protocol ): \"\"\" Protocol defining an interface for standard PyTorch Optimizer object. This protocol ensures that the wrapped optimizer supports standard API and state checkpointing via the Stateful interface. \"\"\" def step ( self ): \"\"\"Performs a single optimization step.\"\"\" def zero_grad ( self ): \"\"\"Sets the gradients of all optimized tensors to zero.\"\"\" def state_dict ( self ) -> dict [ str , Any ]: \"\"\" Objects should return their state_dict representation as a dictionary. The output of this function will be checkpointed, and later restored in `load_state_dict()`. Returns: Dict: The objects state dict \"\"\" def load_state_dict ( self , state_dict : dict [ str , Any ]) -> None : \"\"\" Restore the object's state from the provided state_dict. Args: state_dict: The state dict to restore from \"\"\" load_state_dict ( state_dict ) Restore the object's state from the provided state_dict. Parameters: Name Type Description Default state_dict dict [ str , Any ] The state dict to restore from required Source code in d9d/core/protocol/training.py 29 30 31 32 33 34 35 def load_state_dict ( self , state_dict : dict [ str , Any ]) -> None : \"\"\" Restore the object's state from the provided state_dict. Args: state_dict: The state dict to restore from \"\"\" state_dict () Objects should return their state_dict representation as a dictionary. The output of this function will be checkpointed, and later restored in load_state_dict() . Returns: Name Type Description Dict dict [ str , Any ] The objects state dict Source code in d9d/core/protocol/training.py 19 20 21 22 23 24 25 26 27 def state_dict ( self ) -> dict [ str , Any ]: \"\"\" Objects should return their state_dict representation as a dictionary. The output of this function will be checkpointed, and later restored in `load_state_dict()`. Returns: Dict: The objects state dict \"\"\" step () Performs a single optimization step. Source code in d9d/core/protocol/training.py 13 14 def step ( self ): \"\"\"Performs a single optimization step.\"\"\" zero_grad () Sets the gradients of all optimized tensors to zero. Source code in d9d/core/protocol/training.py 16 17 def zero_grad ( self ): \"\"\"Sets the gradients of all optimized tensors to zero.\"\"\"","title":"Typing Extensions"},{"location":"core/types/#about","text":"The d9d.core.types package gathers common Type Aliases used throughout the framework. The d9d.core.protocol package defines standard interfaces (Protocols) for standard PyTorch components used in the distributed training loop.","title":"About"},{"location":"core/types/#d9d.core.types","text":"Common type definitions used throughout the framework.","title":"types"},{"location":"core/types/#d9d.core.types.CollateFn","text":"Type alias for a function that collates a sequence of samples into a batch. The function receives a sequence of individual data point structures (PyTrees) and is responsible for stacking or merging them into a single batched structure.","title":"CollateFn"},{"location":"core/types/#d9d.core.types.PyTree","text":"A recursive type definition representing a tree of data. This type alias covers standard Python containers (dictionaries, lists, tuples) nested arbitrarily deep, terminating in a leaf node of type TLeaf . This is commonly used for handling nested state dictionaries or arguments passed to functions that support recursive traversal (similar to torch.utils._pytree ).","title":"PyTree"},{"location":"core/types/#d9d.core.types.ScalarTree","text":"A recursive tree structure where the leaf nodes are python scalars (str, float, int).","title":"ScalarTree"},{"location":"core/types/#d9d.core.types.TensorTree","text":"A recursive tree structure where the leaf nodes are PyTorch Tensors.","title":"TensorTree"},{"location":"core/types/#d9d.core.protocol","text":"Package providing protocol definitions for standard PyTorch objects.","title":"protocol"},{"location":"core/types/#d9d.core.protocol.LRSchedulerProtocol","text":"Bases: Protocol Protocol defining an interface for a Learning Rate Scheduler. This protocol ensures that the wrapped scheduler supports stepping and state checkpointing via the Stateful interface. Source code in d9d/core/protocol/training.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 @runtime_checkable class LRSchedulerProtocol ( Protocol ): \"\"\" Protocol defining an interface for a Learning Rate Scheduler. This protocol ensures that the wrapped scheduler supports stepping and state checkpointing via the Stateful interface. \"\"\" def step ( self ): \"\"\"Performs a single learning rate scheduling step.\"\"\" ... def state_dict ( self ) -> dict [ str , Any ]: \"\"\" Objects should return their state_dict representation as a dictionary. The output of this function will be checkpointed, and later restored in `load_state_dict()`. Returns: Dict: The objects state dict \"\"\" def load_state_dict ( self , state_dict : dict [ str , Any ]) -> None : \"\"\" Restore the object's state from the provided state_dict. Args: state_dict: The state dict to restore from \"\"\"","title":"LRSchedulerProtocol"},{"location":"core/types/#d9d.core.protocol.LRSchedulerProtocol.load_state_dict","text":"Restore the object's state from the provided state_dict. Parameters: Name Type Description Default state_dict dict [ str , Any ] The state dict to restore from required Source code in d9d/core/protocol/training.py 62 63 64 65 66 67 68 def load_state_dict ( self , state_dict : dict [ str , Any ]) -> None : \"\"\" Restore the object's state from the provided state_dict. Args: state_dict: The state dict to restore from \"\"\"","title":"load_state_dict"},{"location":"core/types/#d9d.core.protocol.LRSchedulerProtocol.state_dict","text":"Objects should return their state_dict representation as a dictionary. The output of this function will be checkpointed, and later restored in load_state_dict() . Returns: Name Type Description Dict dict [ str , Any ] The objects state dict Source code in d9d/core/protocol/training.py 52 53 54 55 56 57 58 59 60 def state_dict ( self ) -> dict [ str , Any ]: \"\"\" Objects should return their state_dict representation as a dictionary. The output of this function will be checkpointed, and later restored in `load_state_dict()`. Returns: Dict: The objects state dict \"\"\"","title":"state_dict"},{"location":"core/types/#d9d.core.protocol.LRSchedulerProtocol.step","text":"Performs a single learning rate scheduling step. Source code in d9d/core/protocol/training.py 47 48 49 50 def step ( self ): \"\"\"Performs a single learning rate scheduling step.\"\"\" ...","title":"step"},{"location":"core/types/#d9d.core.protocol.OptimizerProtocol","text":"Bases: Protocol Protocol defining an interface for standard PyTorch Optimizer object. This protocol ensures that the wrapped optimizer supports standard API and state checkpointing via the Stateful interface. Source code in d9d/core/protocol/training.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 @runtime_checkable class OptimizerProtocol ( Protocol ): \"\"\" Protocol defining an interface for standard PyTorch Optimizer object. This protocol ensures that the wrapped optimizer supports standard API and state checkpointing via the Stateful interface. \"\"\" def step ( self ): \"\"\"Performs a single optimization step.\"\"\" def zero_grad ( self ): \"\"\"Sets the gradients of all optimized tensors to zero.\"\"\" def state_dict ( self ) -> dict [ str , Any ]: \"\"\" Objects should return their state_dict representation as a dictionary. The output of this function will be checkpointed, and later restored in `load_state_dict()`. Returns: Dict: The objects state dict \"\"\" def load_state_dict ( self , state_dict : dict [ str , Any ]) -> None : \"\"\" Restore the object's state from the provided state_dict. Args: state_dict: The state dict to restore from \"\"\"","title":"OptimizerProtocol"},{"location":"core/types/#d9d.core.protocol.OptimizerProtocol.load_state_dict","text":"Restore the object's state from the provided state_dict. Parameters: Name Type Description Default state_dict dict [ str , Any ] The state dict to restore from required Source code in d9d/core/protocol/training.py 29 30 31 32 33 34 35 def load_state_dict ( self , state_dict : dict [ str , Any ]) -> None : \"\"\" Restore the object's state from the provided state_dict. Args: state_dict: The state dict to restore from \"\"\"","title":"load_state_dict"},{"location":"core/types/#d9d.core.protocol.OptimizerProtocol.state_dict","text":"Objects should return their state_dict representation as a dictionary. The output of this function will be checkpointed, and later restored in load_state_dict() . Returns: Name Type Description Dict dict [ str , Any ] The objects state dict Source code in d9d/core/protocol/training.py 19 20 21 22 23 24 25 26 27 def state_dict ( self ) -> dict [ str , Any ]: \"\"\" Objects should return their state_dict representation as a dictionary. The output of this function will be checkpointed, and later restored in `load_state_dict()`. Returns: Dict: The objects state dict \"\"\"","title":"state_dict"},{"location":"core/types/#d9d.core.protocol.OptimizerProtocol.step","text":"Performs a single optimization step. Source code in d9d/core/protocol/training.py 13 14 def step ( self ): \"\"\"Performs a single optimization step.\"\"\"","title":"step"},{"location":"core/types/#d9d.core.protocol.OptimizerProtocol.zero_grad","text":"Sets the gradients of all optimized tensors to zero. Source code in d9d/core/protocol/training.py 16 17 def zero_grad ( self ): \"\"\"Sets the gradients of all optimized tensors to zero.\"\"\"","title":"zero_grad"},{"location":"dataset/","text":"About The d9d.dataset package provides specialized PyTorch Dataset wrappers designed for distributed training scenarios. Core Concepts Why Not Auto-Wrap Datasets Automatically? d9d provides explicit composable wrappers rather than relying on implicit \"magic\" or automatic Sampler injection often found in other frameworks. Flexible Composition and Order-of-Operations : The behavior of a data pipeline changes significantly depending on the order of composition. By stacking wrappers manually, you control the data flow logic: Granular Configuration : Different datasets have different physical constraints that require specific configurations. A dataset loaded from network storage might require contiguous reads to be performant ( ShardIndexingMode.chunked ), while an in-memory dataset might prefer round-robin access ( ShardIndexingMode.sequential ). Explicit wrappers ensure that these configuration options are exposed to the user rather than buried in global trainer arguments. Features Smart Bucketing In NLP and Sequence processing, batches often contain items of varying lengths. Standard random sampling forces the batch to be padded to the length of the longest sequence, wasting computational resources on padding tokens. BufferSortedDataset implements a \"Smart Bucketing\" strategy to balance efficiency and statistical variance. It ensures that items within a specific micro-batch have similar lengths (minimizing padding), while preventing the data stream from becoming strictly deterministic or sorted. Usage Example To use BufferSortedDataset , your underlying dataset must implement the DatasetImplementingSortKeyProtocol (i.e., it must have a sort_key(index) method). from torch.utils.data import DataLoader from torch.utils.data import Dataset from d9d.dataset import BufferSortedDataset class MyTextDataset ( Dataset ): def __init__ ( self , data : list [ str ]): self . data = data # list of strings def __len__ ( self ): return len ( self . data ) def __getitem__ ( self , index ): return self . data [ index ] # ! You need to implement this one \\/ ! def sort_key ( self , index ): return len ( self . data [ index ]) # Create Base Dataset (Ideally globally shuffled beforehand) raw_data = [ \"short\" , \"very very very long phrase\" , \"tiny\" , \"medium size\" ] * 100 base_ds = MyTextDataset ( raw_data ) # Wrap with Smart Bucketing # - buffer_size=100: Look at 100 items at a time to find similar lengths # - pack_size=4: Group them into batches of 4 sorted_ds = BufferSortedDataset ( base_dataset = base_ds , buffer_size = 100 , pack_size = 4 , init_seed = 42 ) Sharding When using Data Parallelism, each GPU processes a subset of the data. ShardedDataset provides a deterministic view of a specific shard of the data based on the rank (shard ID). It supports: Sequential Sharding : Round-robin distribution ( 0, 4, 8... for rank 0). Chunked Sharding : Contiguous blocks ( 0, 1, 2... for rank 0). Optional Padding : Ensuring all shards have exactly the same length. This is critical for distributed training loops where uneven dataset sizes can cause process hangs. Usage Example (for Data Parallel) import torch from torch.utils.data import TensorDataset from d9d.core.dist_context import DistributedContext , BATCH_DOMAIN from d9d.dataset import shard_dataset_data_parallel , ShardIndexingMode # You can infer your Data Parallel size and rank from DistributedContext object context : DistributedContext # Create Full Dataset base_ds = TensorDataset ( torch . randn ( 100 , 10 )) # Shard it sharded_ds = shard_dataset_data_parallel ( dataset = base_ds , dist_context = context , # Optional Parameters: indexing_mode = ShardIndexingMode . chunked , pad_to_equal_size_across_shards = True ) print ( f \"I am rank { dp_rank } , I see { len ( sharded_ds ) } items.\" ) Usage Example (Manual) import torch from torch.utils.data import TensorDataset from d9d.core.dist_context import DistributedContext , BATCH_DOMAIN from d9d.dataset import ShardedDataset , ShardIndexingMode # You can infer your Data Parallel size and rank from DistributedContext object context : DistributedContext batch_mesh = context . mesh_for ( BATCH_DOMAIN ) dp_size = batch_mesh . size ( 'dp' ) dp_rank = batch_mesh . get_local_rank ( 'dp' ) # Create Full Dataset base_ds = TensorDataset ( torch . randn ( 100 , 10 )) # Shard it sharded_ds = ShardedDataset ( dataset = base_ds , total_shards = dp_size , current_shard = dp_rank , indexing_mode = ShardIndexingMode . chunked , # Crucial for preventing distributed hangs pad_to_equal_size_across_shards = True ) print ( f \"I am rank { dp_rank } , I see { len ( sharded_ds ) } items.\" ) Padding Utilities When creating batches from variable-length sequences, tensors must be padded to the same length to form a valid tensor stack. pad_stack_1d provides a robust utility for this, specifically designed to help writing collate_fn . Usage Example import torch from d9d.dataset import pad_stack_1d , PaddingSide1D # Variable length sequences items = [ torch . tensor ([ 1 , 2 , 3 ]), torch . tensor ([ 4 ]), torch . tensor ([ 5 , 6 ]) ] # 1. Standard Right Padding batch = pad_stack_1d ( items , pad_value = 0 , padding_side = PaddingSide1D . right ) # 2. Left Padding batch_gen = pad_stack_1d ( items , pad_value = 0 , padding_side = PaddingSide1D . left ) # 3. Aligned Padding # Ensures the dimensions are friendly to GPU kernels or for Context Parallel sharding batch_aligned = pad_stack_1d ( items , pad_value = 0 , pad_to_multiple_of = 8 ) d9d.dataset This package provides utilities and torch.utils.data.Dataset implementations. BufferSortedDataset Bases: Dataset [ _T_co ] , Stateful A dataset wrapper that groups items into buffers, sorts them, and yields them with local shuffling. This prevents extreme padding in variable-length training (by grouping similar lengths) while maintaining enough randomness to ensure statistical variance in updates. Algorithm: Select a range of indices (size buffer_size ). Generate sort keys: (base_dataset.sort_key(), random_tie_breaker). Sort indices by this tuple. Group sorted list into packs of size pack_size . Shuffle the order of these packs (inter-pack shuffle). Shuffle the items within these packs (intra-pack shuffle). Flatten and serve. Source code in d9d/dataset/buffer_sorted.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 class BufferSortedDataset ( Dataset [ _T_co ], Stateful ): \"\"\" A dataset wrapper that groups items into buffers, sorts them, and yields them with local shuffling. This prevents extreme padding in variable-length training (by grouping similar lengths) while maintaining enough randomness to ensure statistical variance in updates. Algorithm: 1. Select a range of indices (size `buffer_size`). 2. Generate sort keys: (base_dataset.sort_key(), random_tie_breaker). 3. Sort indices by this tuple. 4. Group sorted list into packs of size `pack_size`. 5. Shuffle the order of these packs (inter-pack shuffle). 6. Shuffle the items within these packs (intra-pack shuffle). 7. Flatten and serve. \"\"\" def __init__ ( self , base_dataset : DatasetImplementingSortKeyProtocol [ _T_co ], buffer_size : int , pack_size : int , init_seed : int | None = None , ): \"\"\" Args: base_dataset: The underlying dataset. buffer_size: The number of items to load into the buffer for sorting. pack_size: The size of local groups (batches/micro-batches). init_seed: Seed for the random number generator. \"\"\" self . _base_dataset = base_dataset self . _buffer_size = buffer_size self . _pack_size = pack_size self . _rng = random . Random ( init_seed ^ 0x105E7 if init_seed is not None else None ) self . _buffer_indices : list [ int ] = [] self . _buffer_idx : int = - 1 def _update_buffer_idx ( self , buffer_idx : int ): select_start = buffer_idx * self . _buffer_size select_end = ( buffer_idx + 1 ) * self . _buffer_size select_end = min ( select_end , len ( self . _base_dataset )) base_idx = list ( range ( select_start , select_end )) sort_keys = [ ( self . _base_dataset . sort_key ( idx ), self . _rng . random ()) # use random tiebreaker for idx in base_idx ] local_idx = list ( range ( len ( base_idx ))) local_idx . sort ( key = lambda i : sort_keys [ i ]) local_idx_packs = [ local_idx [ i : i + self . _pack_size ] for i in range ( 0 , len ( local_idx ), self . _pack_size )] self . _rng . shuffle ( local_idx_packs ) for pack in local_idx_packs : self . _rng . shuffle ( pack ) flat_local_idx = [ y for x in local_idx_packs for y in x ] self . _buffer_indices = [ base_idx [ local_id ] for local_id in flat_local_idx ] self . _buffer_idx = buffer_idx def __getitem__ ( self , index : int ) -> _T_co : needs_buffer_idx = index // self . _buffer_size if self . _buffer_idx != needs_buffer_idx : self . _update_buffer_idx ( needs_buffer_idx ) take_id = self . _buffer_indices [ index % self . _buffer_size ] return self . _base_dataset [ take_id ] def __len__ ( self ) -> int : return len ( self . _base_dataset ) def state_dict ( self ) -> dict [ str , Any ]: ret = { \"seed\" : pickle . dumps ( self . _rng . getstate ()), \"buffer_idx\" : self . _buffer_idx , \"buffer_indices\" : self . _buffer_indices , } if isinstance ( self . _base_dataset , Stateful ): ret [ \"base_dataset\" ] = self . _base_dataset . state_dict () return ret def load_state_dict ( self , state_dict : dict [ str , Any ]) -> None : self . _rng . setstate ( pickle . loads ( state_dict [ \"seed\" ])) # noqa: S301 self . _buffer_idx = state_dict [ \"buffer_idx\" ] self . _buffer_indices = state_dict [ \"buffer_indices\" ] if isinstance ( self . _base_dataset , Stateful ): self . _base_dataset . load_state_dict ( state_dict [ \"base_dataset\" ]) __init__ ( base_dataset , buffer_size , pack_size , init_seed = None ) Parameters: Name Type Description Default base_dataset DatasetImplementingSortKeyProtocol [ _T_co ] The underlying dataset. required buffer_size int The number of items to load into the buffer for sorting. required pack_size int The size of local groups (batches/micro-batches). required init_seed int | None Seed for the random number generator. None Source code in d9d/dataset/buffer_sorted.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def __init__ ( self , base_dataset : DatasetImplementingSortKeyProtocol [ _T_co ], buffer_size : int , pack_size : int , init_seed : int | None = None , ): \"\"\" Args: base_dataset: The underlying dataset. buffer_size: The number of items to load into the buffer for sorting. pack_size: The size of local groups (batches/micro-batches). init_seed: Seed for the random number generator. \"\"\" self . _base_dataset = base_dataset self . _buffer_size = buffer_size self . _pack_size = pack_size self . _rng = random . Random ( init_seed ^ 0x105E7 if init_seed is not None else None ) self . _buffer_indices : list [ int ] = [] self . _buffer_idx : int = - 1 DatasetImplementingSortKeyProtocol Bases: Protocol [ _T_co ] Protocol for datasets that support retrieval of a specific key for sorting purposes. This is typically used for length-based bucketing/sorting where the dataset needs to expose the length of an item without loading the full item. Source code in d9d/dataset/buffer_sorted.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class DatasetImplementingSortKeyProtocol ( Protocol [ _T_co ]): \"\"\" Protocol for datasets that support retrieval of a specific key for sorting purposes. This is typically used for length-based bucketing/sorting where the dataset needs to expose the length of an item without loading the full item. \"\"\" def __len__ ( self ) -> int : \"\"\"Returns the total number of items in the dataset.\"\"\" ... def sort_key ( self , index : int ) -> Any : \"\"\" Returns a value used for sorting the dataset at the given index. Args: index: The index of the item. Returns: A comparable value (e.g., int length) used for sorting. \"\"\" ... def __getitem__ ( self , item : int ) -> _T_co : \"\"\"Retrieves the item at the specific index.\"\"\" ... __getitem__ ( item ) Retrieves the item at the specific index. Source code in d9d/dataset/buffer_sorted.py 35 36 37 def __getitem__ ( self , item : int ) -> _T_co : \"\"\"Retrieves the item at the specific index.\"\"\" ... __len__ () Returns the total number of items in the dataset. Source code in d9d/dataset/buffer_sorted.py 19 20 21 def __len__ ( self ) -> int : \"\"\"Returns the total number of items in the dataset.\"\"\" ... sort_key ( index ) Returns a value used for sorting the dataset at the given index. Parameters: Name Type Description Default index int The index of the item. required Returns: Type Description Any A comparable value (e.g., int length) used for sorting. Source code in d9d/dataset/buffer_sorted.py 23 24 25 26 27 28 29 30 31 32 33 def sort_key ( self , index : int ) -> Any : \"\"\" Returns a value used for sorting the dataset at the given index. Args: index: The index of the item. Returns: A comparable value (e.g., int length) used for sorting. \"\"\" ... PaddingSide1D Bases: StrEnum Enum specifying the side for padding 1D sequences. Attributes: Name Type Description left Pad on the left side. right Pad on the right side. Source code in d9d/dataset/padding.py 8 9 10 11 12 13 14 15 16 17 18 class PaddingSide1D ( StrEnum ): \"\"\" Enum specifying the side for padding 1D sequences. Attributes: left: Pad on the left side. right: Pad on the right side. \"\"\" left = \"left\" right = \"right\" ShardIndexingMode Bases: StrEnum Defines how the dataset is split across shards. Modes sequential: Round-robin distribution. shard0 : 0 , 4 , 8 , 12 shard1 : 1 , 5 , 9 , 13 shard2 : 2 , 6 , 10 shard3 : 3 , 7 , 11 chunked: Contiguous blocks. shard0 : 0 , 1 , 2 , 3 shard1 : 4 , 5 , 6 , 7 shard2 : 8 , 9 , 10 , 11 shard3 : 12 , 13 Source code in d9d/dataset/sharded.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 class ShardIndexingMode ( StrEnum ): \"\"\" Defines how the dataset is split across shards. Modes: sequential: Round-robin distribution. shard0: 0, 4, 8, 12 shard1: 1, 5, 9, 13 shard2: 2, 6, 10 shard3: 3, 7, 11 chunked: Contiguous blocks. shard0: 0, 1, 2, 3 shard1: 4, 5, 6, 7 shard2: 8, 9, 10, 11 shard3: 12, 13 \"\"\" sequential = \"sequential\" chunked = \"chunked\" ShardedDataset Bases: Dataset [ _T_co ] , Stateful A dataset wrapper that acts as a view onto a specific shard of the underlying dataset. This is useful for Data Parallel training where each process should only see a subset of the data. It supports different indexing modes and optional padding to ensure all shards have equal length (preventing hangs in distributed collectives). Source code in d9d/dataset/sharded.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 class ShardedDataset ( Dataset [ _T_co ], Stateful ): \"\"\" A dataset wrapper that acts as a view onto a specific shard of the underlying dataset. This is useful for Data Parallel training where each process should only see a subset of the data. It supports different indexing modes and optional padding to ensure all shards have equal length (preventing hangs in distributed collectives). \"\"\" def __init__ ( self , dataset : Dataset [ _T_co ], total_shards : int , current_shard : int , indexing_mode : ShardIndexingMode , pad_to_equal_size_across_shards : bool , ): \"\"\" Constructs a ShardedDataset object. Args: dataset: The underlying dataset to shard. total_shards: The total number of shards (e.g., number of DP ranks). current_shard: The index of the current shard (e.g., current DP rank). indexing_mode: How indices are assigned to shards (sequential/round-robin or chunked). pad_to_equal_size_across_shards: If True, the length of the dataset will be padded so that all shards report the same length. The last standard element is repeated. \"\"\" if not isinstance ( dataset , Sized ): raise ValueError ( \"Dataset should implement __len__ method\" ) self . _dataset = dataset self . _total_shards = total_shards self . _current_shard = current_shard self . _indexing_mode = indexing_mode self . _pad_to_equal_size_across_shards = pad_to_equal_size_across_shards def _compute_real_index_sequential ( self , index : int ) -> int : return index * self . _total_shards + self . _current_shard def _get_base_index_unsafe ( self , index : int ) -> int : \"\"\" Calculates the underlying dataset index for a given shard index, without boundary checking. \"\"\" match self . _indexing_mode : case ShardIndexingMode . sequential : base_index = index * self . _total_shards + self . _current_shard return base_index case ShardIndexingMode . chunked : ceil_len = math . ceil ( len ( self . _dataset ) / self . _total_shards ) shard_start_offset = ceil_len * self . _current_shard return shard_start_offset + index case _ : raise ValueError ( f \"Unknown shard indexing mode: { self . _indexing_mode } \" ) def __getitem__ ( self , index : int ) -> _T_co : \"\"\" Retrieves an item from the underlying dataset mapping logic shard index to physical index. If padding is enabled and the index exceeds the valid data for this shard, the last item in the dataset is returned. Args: index: The index relative to this shard. Returns: The data item. \"\"\" base_index = self . _get_base_index_unsafe ( index ) if base_index >= len ( self . _dataset ): base_index = len ( self . _dataset ) - 1 return self . _dataset [ base_index ] def __len__ ( self ) -> int : \"\"\" Returns the number of items in this specific shard. If `pad_to_equal_size_across_shards` is True, this returns the ceiling length (max length across all shards). \"\"\" ceil_len = math . ceil ( len ( self . _dataset ) / self . _total_shards ) if self . _pad_to_equal_size_across_shards : return ceil_len shards_remainder = len ( self . _dataset ) % self . _total_shards match self . _indexing_mode : case ShardIndexingMode . sequential : shards_full = len ( self . _dataset ) // self . _total_shards return shards_full + 1 if self . _current_shard < shards_remainder else shards_full case ShardIndexingMode . chunked : is_shard_last = self . _current_shard == self . _total_shards - 1 if not is_shard_last or shards_remainder == 0 : return ceil_len else : return ceil_len - ( self . _total_shards - shards_remainder ) case _ : raise ValueError ( f \"Unknown ShardIndexingMode: { self . _indexing_mode } \" ) def load_state_dict ( self , state_dict : dict [ str , Any ]) -> None : if isinstance ( self . _dataset , Stateful ): self . _dataset . load_state_dict ( state_dict [ \"dataset\" ]) # check whether env mismatched if state_dict [ \"total_shards\" ] != self . _total_shards : raise ValueError ( \"Shard count mismatch\" ) self . _total_shards = state_dict [ \"total_shards\" ] self . _current_shard = state_dict [ \"current_shard\" ] def state_dict ( self ) -> dict [ str , Any ]: dct : dict [ str , Any ] = { \"total_shards\" : self . _total_shards , \"current_shard\" : self . _current_shard } if isinstance ( self . _dataset , Stateful ): dct [ \"dataset\" ] = self . _dataset . state_dict () return dct __getitem__ ( index ) Retrieves an item from the underlying dataset mapping logic shard index to physical index. If padding is enabled and the index exceeds the valid data for this shard, the last item in the dataset is returned. Parameters: Name Type Description Default index int The index relative to this shard. required Returns: Type Description _T_co The data item. Source code in d9d/dataset/sharded.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def __getitem__ ( self , index : int ) -> _T_co : \"\"\" Retrieves an item from the underlying dataset mapping logic shard index to physical index. If padding is enabled and the index exceeds the valid data for this shard, the last item in the dataset is returned. Args: index: The index relative to this shard. Returns: The data item. \"\"\" base_index = self . _get_base_index_unsafe ( index ) if base_index >= len ( self . _dataset ): base_index = len ( self . _dataset ) - 1 return self . _dataset [ base_index ] __init__ ( dataset , total_shards , current_shard , indexing_mode , pad_to_equal_size_across_shards ) Constructs a ShardedDataset object. Parameters: Name Type Description Default dataset Dataset [ _T_co ] The underlying dataset to shard. required total_shards int The total number of shards (e.g., number of DP ranks). required current_shard int The index of the current shard (e.g., current DP rank). required indexing_mode ShardIndexingMode How indices are assigned to shards (sequential/round-robin or chunked). required pad_to_equal_size_across_shards bool If True, the length of the dataset will be padded so that all shards report the same length. The last standard element is repeated. required Source code in d9d/dataset/sharded.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def __init__ ( self , dataset : Dataset [ _T_co ], total_shards : int , current_shard : int , indexing_mode : ShardIndexingMode , pad_to_equal_size_across_shards : bool , ): \"\"\" Constructs a ShardedDataset object. Args: dataset: The underlying dataset to shard. total_shards: The total number of shards (e.g., number of DP ranks). current_shard: The index of the current shard (e.g., current DP rank). indexing_mode: How indices are assigned to shards (sequential/round-robin or chunked). pad_to_equal_size_across_shards: If True, the length of the dataset will be padded so that all shards report the same length. The last standard element is repeated. \"\"\" if not isinstance ( dataset , Sized ): raise ValueError ( \"Dataset should implement __len__ method\" ) self . _dataset = dataset self . _total_shards = total_shards self . _current_shard = current_shard self . _indexing_mode = indexing_mode self . _pad_to_equal_size_across_shards = pad_to_equal_size_across_shards __len__ () Returns the number of items in this specific shard. If pad_to_equal_size_across_shards is True, this returns the ceiling length (max length across all shards). Source code in d9d/dataset/sharded.py 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def __len__ ( self ) -> int : \"\"\" Returns the number of items in this specific shard. If `pad_to_equal_size_across_shards` is True, this returns the ceiling length (max length across all shards). \"\"\" ceil_len = math . ceil ( len ( self . _dataset ) / self . _total_shards ) if self . _pad_to_equal_size_across_shards : return ceil_len shards_remainder = len ( self . _dataset ) % self . _total_shards match self . _indexing_mode : case ShardIndexingMode . sequential : shards_full = len ( self . _dataset ) // self . _total_shards return shards_full + 1 if self . _current_shard < shards_remainder else shards_full case ShardIndexingMode . chunked : is_shard_last = self . _current_shard == self . _total_shards - 1 if not is_shard_last or shards_remainder == 0 : return ceil_len else : return ceil_len - ( self . _total_shards - shards_remainder ) case _ : raise ValueError ( f \"Unknown ShardIndexingMode: { self . _indexing_mode } \" ) TokenPoolingType Bases: StrEnum Enumeration of supported token pooling strategies. Attributes: Name Type Description first Selects the first token of the sequence (e.g., [CLS] token). last Selects the last non-padding token of the sequence (e.g., for Transformer Decoder). all Selects all non-padding tokens (e.g., for mean pooling). Source code in d9d/dataset/pooling.py 6 7 8 9 10 11 12 13 14 15 16 17 class TokenPoolingType ( StrEnum ): \"\"\"Enumeration of supported token pooling strategies. Attributes: first: Selects the first token of the sequence (e.g., [CLS] token). last: Selects the last non-padding token of the sequence (e.g., for Transformer Decoder). all: Selects all non-padding tokens (e.g., for mean pooling). \"\"\" first = \"first\" last = \"last\" all = \"all\" pad_stack_1d ( items , pad_value , padding_side = PaddingSide1D . right , pad_to_multiple_of = None ) Stacks 1D tensors into a batch, applying padding. Calculates the maximum length among the input tensors (optionally aligning to a multiple), pads elements to match this length on the specified side, and stacks them. Parameters: Name Type Description Default items Sequence [ Tensor ] A sequence of 1D tensors to be stacked. required pad_value int The value used for padding. required padding_side PaddingSide1D The side on which to apply padding (left or right). right pad_to_multiple_of int | None Optional integer. If provided, ensures the target length is a multiple of this value. None Returns: Type Description Tensor A single stacked tensor of shape (batch, max_length). Raises: Type Description ValueError If no items are provided or if pad_to_multiple_of is <= 0. Source code in d9d/dataset/padding.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def pad_stack_1d ( items : Sequence [ torch . Tensor ], pad_value : int , padding_side : PaddingSide1D = PaddingSide1D . right , pad_to_multiple_of : int | None = None , ) -> torch . Tensor : \"\"\" Stacks 1D tensors into a batch, applying padding. Calculates the maximum length among the input tensors (optionally aligning to a multiple), pads elements to match this length on the specified side, and stacks them. Args: items: A sequence of 1D tensors to be stacked. pad_value: The value used for padding. padding_side: The side on which to apply padding (left or right). pad_to_multiple_of: Optional integer. If provided, ensures the target length is a multiple of this value. Returns: A single stacked tensor of shape (batch, max_length). Raises: ValueError: If no items are provided or if `pad_to_multiple_of` is <= 0. \"\"\" if not items : raise ValueError ( \"Cannot stack 0 items\" ) if pad_to_multiple_of is not None and pad_to_multiple_of <= 0 : raise ValueError ( \"pad_to_multiple_of should be > 0\" ) max_len = max ( x . shape [ 0 ] for x in items ) if pad_to_multiple_of is not None and ( remainder := max_len % pad_to_multiple_of ) != 0 : max_len = max_len + ( pad_to_multiple_of - remainder ) padded_items = [] for x in items : difference = max_len - x . shape [ 0 ] if difference == 0 : padded_items . append ( x ) else : padded_items . append ( F . pad ( x , _padding_side_1d_to_config ( padding_side , difference ), value = pad_value )) return torch . stack ( padded_items , dim = 0 ) shard_dataset_data_parallel ( dataset , dist_context , indexing_mode = ShardIndexingMode . sequential , pad_to_equal_size_across_shards = True ) Wraps a dataset into a ShardedDataset based on the Data Parallel dimension of the distributed context. This is a helper function to automatically determine the correct rank and world size from the 'dp' (Data Parallel) mesh dimension within the batch domain DeviceMesh. Parameters: Name Type Description Default dataset Dataset [ _T_co ] The source dataset to shard. required dist_context DistributedContext The distributed context. required indexing_mode ShardIndexingMode The strategy for splitting data indices (sequential/round-robin or chunked). sequential pad_to_equal_size_across_shards bool If True, ensures all shards have the same length by padding. True Returns: Type Description Dataset [ _T_co ] A dataset instance representing the local shard. Source code in d9d/dataset/sharded.py 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 def shard_dataset_data_parallel ( dataset : Dataset [ _T_co ], dist_context : DistributedContext , indexing_mode : ShardIndexingMode = ShardIndexingMode . sequential , pad_to_equal_size_across_shards : bool = True , ) -> Dataset [ _T_co ]: \"\"\" Wraps a dataset into a ShardedDataset based on the Data Parallel dimension of the distributed context. This is a helper function to automatically determine the correct rank and world size from the 'dp' (Data Parallel) mesh dimension within the batch domain DeviceMesh. Args: dataset: The source dataset to shard. dist_context: The distributed context. indexing_mode: The strategy for splitting data indices (sequential/round-robin or chunked). pad_to_equal_size_across_shards: If True, ensures all shards have the same length by padding. Returns: A dataset instance representing the local shard. \"\"\" if dist_context . mesh_params . is_distributed : dp_mesh = dist_context . mesh_for ( BATCH_DOMAIN )[ \"dp\" ] n_shards = dp_mesh . size () current_shard = dp_mesh . get_local_rank () else : n_shards = 1 current_shard = 0 return ShardedDataset ( dataset = dataset , total_shards = n_shards , current_shard = current_shard , indexing_mode = indexing_mode , pad_to_equal_size_across_shards = pad_to_equal_size_across_shards , ) token_pooling_mask_from_attention_mask ( attention_mask , pooling_type ) Generates a binary mask for token pooling based on the specified strategy. Parameters: Name Type Description Default attention_mask Tensor A binary mask indicating valid tokens (1) and padding (0). Expected shape is (batch_size, sequence_length). required pooling_type TokenPoolingType The strategy to use for selecting tokens. required Returns: Type Description Tensor A LongTensor of the same shape as input containing 1s at positions Tensor to be included in pooling and 0s elsewhere. Raises: Type Description ValueError If the provided pooling type is not supported. Source code in d9d/dataset/pooling.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def token_pooling_mask_from_attention_mask ( attention_mask : torch . Tensor , pooling_type : TokenPoolingType ) -> torch . Tensor : \"\"\"Generates a binary mask for token pooling based on the specified strategy. Args: attention_mask: A binary mask indicating valid tokens (1) and padding (0). Expected shape is (batch_size, sequence_length). pooling_type: The strategy to use for selecting tokens. Returns: A LongTensor of the same shape as input containing 1s at positions to be included in pooling and 0s elsewhere. Raises: ValueError: If the provided pooling type is not supported. \"\"\" match pooling_type : case TokenPoolingType . first : mask = torch . zeros_like ( attention_mask , dtype = torch . long ) mask [:, 0 ] = 1 return mask case TokenPoolingType . last : batch_indices = torch . arange ( attention_mask . size ( 0 ), device = attention_mask . device ) last_token_indices = attention_mask . sum ( dim = 1 ) - 1 mask = torch . zeros_like ( attention_mask , dtype = torch . long ) mask [ batch_indices , last_token_indices ] = 1 return mask case TokenPoolingType . all : return attention_mask case _ : raise ValueError ( f \"Unknown pooling type: { pooling_type } \" )","title":"Datasets"},{"location":"dataset/#about","text":"The d9d.dataset package provides specialized PyTorch Dataset wrappers designed for distributed training scenarios.","title":"About"},{"location":"dataset/#core-concepts","text":"","title":"Core Concepts"},{"location":"dataset/#why-not-auto-wrap-datasets-automatically","text":"d9d provides explicit composable wrappers rather than relying on implicit \"magic\" or automatic Sampler injection often found in other frameworks. Flexible Composition and Order-of-Operations : The behavior of a data pipeline changes significantly depending on the order of composition. By stacking wrappers manually, you control the data flow logic: Granular Configuration : Different datasets have different physical constraints that require specific configurations. A dataset loaded from network storage might require contiguous reads to be performant ( ShardIndexingMode.chunked ), while an in-memory dataset might prefer round-robin access ( ShardIndexingMode.sequential ). Explicit wrappers ensure that these configuration options are exposed to the user rather than buried in global trainer arguments.","title":"Why Not Auto-Wrap Datasets Automatically?"},{"location":"dataset/#features","text":"","title":"Features"},{"location":"dataset/#smart-bucketing","text":"In NLP and Sequence processing, batches often contain items of varying lengths. Standard random sampling forces the batch to be padded to the length of the longest sequence, wasting computational resources on padding tokens. BufferSortedDataset implements a \"Smart Bucketing\" strategy to balance efficiency and statistical variance. It ensures that items within a specific micro-batch have similar lengths (minimizing padding), while preventing the data stream from becoming strictly deterministic or sorted.","title":"Smart Bucketing"},{"location":"dataset/#usage-example","text":"To use BufferSortedDataset , your underlying dataset must implement the DatasetImplementingSortKeyProtocol (i.e., it must have a sort_key(index) method). from torch.utils.data import DataLoader from torch.utils.data import Dataset from d9d.dataset import BufferSortedDataset class MyTextDataset ( Dataset ): def __init__ ( self , data : list [ str ]): self . data = data # list of strings def __len__ ( self ): return len ( self . data ) def __getitem__ ( self , index ): return self . data [ index ] # ! You need to implement this one \\/ ! def sort_key ( self , index ): return len ( self . data [ index ]) # Create Base Dataset (Ideally globally shuffled beforehand) raw_data = [ \"short\" , \"very very very long phrase\" , \"tiny\" , \"medium size\" ] * 100 base_ds = MyTextDataset ( raw_data ) # Wrap with Smart Bucketing # - buffer_size=100: Look at 100 items at a time to find similar lengths # - pack_size=4: Group them into batches of 4 sorted_ds = BufferSortedDataset ( base_dataset = base_ds , buffer_size = 100 , pack_size = 4 , init_seed = 42 )","title":"Usage Example"},{"location":"dataset/#sharding","text":"When using Data Parallelism, each GPU processes a subset of the data. ShardedDataset provides a deterministic view of a specific shard of the data based on the rank (shard ID). It supports: Sequential Sharding : Round-robin distribution ( 0, 4, 8... for rank 0). Chunked Sharding : Contiguous blocks ( 0, 1, 2... for rank 0). Optional Padding : Ensuring all shards have exactly the same length. This is critical for distributed training loops where uneven dataset sizes can cause process hangs.","title":"Sharding"},{"location":"dataset/#usage-example-for-data-parallel","text":"import torch from torch.utils.data import TensorDataset from d9d.core.dist_context import DistributedContext , BATCH_DOMAIN from d9d.dataset import shard_dataset_data_parallel , ShardIndexingMode # You can infer your Data Parallel size and rank from DistributedContext object context : DistributedContext # Create Full Dataset base_ds = TensorDataset ( torch . randn ( 100 , 10 )) # Shard it sharded_ds = shard_dataset_data_parallel ( dataset = base_ds , dist_context = context , # Optional Parameters: indexing_mode = ShardIndexingMode . chunked , pad_to_equal_size_across_shards = True ) print ( f \"I am rank { dp_rank } , I see { len ( sharded_ds ) } items.\" )","title":"Usage Example (for Data Parallel)"},{"location":"dataset/#usage-example-manual","text":"import torch from torch.utils.data import TensorDataset from d9d.core.dist_context import DistributedContext , BATCH_DOMAIN from d9d.dataset import ShardedDataset , ShardIndexingMode # You can infer your Data Parallel size and rank from DistributedContext object context : DistributedContext batch_mesh = context . mesh_for ( BATCH_DOMAIN ) dp_size = batch_mesh . size ( 'dp' ) dp_rank = batch_mesh . get_local_rank ( 'dp' ) # Create Full Dataset base_ds = TensorDataset ( torch . randn ( 100 , 10 )) # Shard it sharded_ds = ShardedDataset ( dataset = base_ds , total_shards = dp_size , current_shard = dp_rank , indexing_mode = ShardIndexingMode . chunked , # Crucial for preventing distributed hangs pad_to_equal_size_across_shards = True ) print ( f \"I am rank { dp_rank } , I see { len ( sharded_ds ) } items.\" )","title":"Usage Example (Manual)"},{"location":"dataset/#padding-utilities","text":"When creating batches from variable-length sequences, tensors must be padded to the same length to form a valid tensor stack. pad_stack_1d provides a robust utility for this, specifically designed to help writing collate_fn .","title":"Padding Utilities"},{"location":"dataset/#usage-example_1","text":"import torch from d9d.dataset import pad_stack_1d , PaddingSide1D # Variable length sequences items = [ torch . tensor ([ 1 , 2 , 3 ]), torch . tensor ([ 4 ]), torch . tensor ([ 5 , 6 ]) ] # 1. Standard Right Padding batch = pad_stack_1d ( items , pad_value = 0 , padding_side = PaddingSide1D . right ) # 2. Left Padding batch_gen = pad_stack_1d ( items , pad_value = 0 , padding_side = PaddingSide1D . left ) # 3. Aligned Padding # Ensures the dimensions are friendly to GPU kernels or for Context Parallel sharding batch_aligned = pad_stack_1d ( items , pad_value = 0 , pad_to_multiple_of = 8 )","title":"Usage Example"},{"location":"dataset/#d9d.dataset","text":"This package provides utilities and torch.utils.data.Dataset implementations.","title":"dataset"},{"location":"dataset/#d9d.dataset.BufferSortedDataset","text":"Bases: Dataset [ _T_co ] , Stateful A dataset wrapper that groups items into buffers, sorts them, and yields them with local shuffling. This prevents extreme padding in variable-length training (by grouping similar lengths) while maintaining enough randomness to ensure statistical variance in updates. Algorithm: Select a range of indices (size buffer_size ). Generate sort keys: (base_dataset.sort_key(), random_tie_breaker). Sort indices by this tuple. Group sorted list into packs of size pack_size . Shuffle the order of these packs (inter-pack shuffle). Shuffle the items within these packs (intra-pack shuffle). Flatten and serve. Source code in d9d/dataset/buffer_sorted.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 class BufferSortedDataset ( Dataset [ _T_co ], Stateful ): \"\"\" A dataset wrapper that groups items into buffers, sorts them, and yields them with local shuffling. This prevents extreme padding in variable-length training (by grouping similar lengths) while maintaining enough randomness to ensure statistical variance in updates. Algorithm: 1. Select a range of indices (size `buffer_size`). 2. Generate sort keys: (base_dataset.sort_key(), random_tie_breaker). 3. Sort indices by this tuple. 4. Group sorted list into packs of size `pack_size`. 5. Shuffle the order of these packs (inter-pack shuffle). 6. Shuffle the items within these packs (intra-pack shuffle). 7. Flatten and serve. \"\"\" def __init__ ( self , base_dataset : DatasetImplementingSortKeyProtocol [ _T_co ], buffer_size : int , pack_size : int , init_seed : int | None = None , ): \"\"\" Args: base_dataset: The underlying dataset. buffer_size: The number of items to load into the buffer for sorting. pack_size: The size of local groups (batches/micro-batches). init_seed: Seed for the random number generator. \"\"\" self . _base_dataset = base_dataset self . _buffer_size = buffer_size self . _pack_size = pack_size self . _rng = random . Random ( init_seed ^ 0x105E7 if init_seed is not None else None ) self . _buffer_indices : list [ int ] = [] self . _buffer_idx : int = - 1 def _update_buffer_idx ( self , buffer_idx : int ): select_start = buffer_idx * self . _buffer_size select_end = ( buffer_idx + 1 ) * self . _buffer_size select_end = min ( select_end , len ( self . _base_dataset )) base_idx = list ( range ( select_start , select_end )) sort_keys = [ ( self . _base_dataset . sort_key ( idx ), self . _rng . random ()) # use random tiebreaker for idx in base_idx ] local_idx = list ( range ( len ( base_idx ))) local_idx . sort ( key = lambda i : sort_keys [ i ]) local_idx_packs = [ local_idx [ i : i + self . _pack_size ] for i in range ( 0 , len ( local_idx ), self . _pack_size )] self . _rng . shuffle ( local_idx_packs ) for pack in local_idx_packs : self . _rng . shuffle ( pack ) flat_local_idx = [ y for x in local_idx_packs for y in x ] self . _buffer_indices = [ base_idx [ local_id ] for local_id in flat_local_idx ] self . _buffer_idx = buffer_idx def __getitem__ ( self , index : int ) -> _T_co : needs_buffer_idx = index // self . _buffer_size if self . _buffer_idx != needs_buffer_idx : self . _update_buffer_idx ( needs_buffer_idx ) take_id = self . _buffer_indices [ index % self . _buffer_size ] return self . _base_dataset [ take_id ] def __len__ ( self ) -> int : return len ( self . _base_dataset ) def state_dict ( self ) -> dict [ str , Any ]: ret = { \"seed\" : pickle . dumps ( self . _rng . getstate ()), \"buffer_idx\" : self . _buffer_idx , \"buffer_indices\" : self . _buffer_indices , } if isinstance ( self . _base_dataset , Stateful ): ret [ \"base_dataset\" ] = self . _base_dataset . state_dict () return ret def load_state_dict ( self , state_dict : dict [ str , Any ]) -> None : self . _rng . setstate ( pickle . loads ( state_dict [ \"seed\" ])) # noqa: S301 self . _buffer_idx = state_dict [ \"buffer_idx\" ] self . _buffer_indices = state_dict [ \"buffer_indices\" ] if isinstance ( self . _base_dataset , Stateful ): self . _base_dataset . load_state_dict ( state_dict [ \"base_dataset\" ])","title":"BufferSortedDataset"},{"location":"dataset/#d9d.dataset.BufferSortedDataset.__init__","text":"Parameters: Name Type Description Default base_dataset DatasetImplementingSortKeyProtocol [ _T_co ] The underlying dataset. required buffer_size int The number of items to load into the buffer for sorting. required pack_size int The size of local groups (batches/micro-batches). required init_seed int | None Seed for the random number generator. None Source code in d9d/dataset/buffer_sorted.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def __init__ ( self , base_dataset : DatasetImplementingSortKeyProtocol [ _T_co ], buffer_size : int , pack_size : int , init_seed : int | None = None , ): \"\"\" Args: base_dataset: The underlying dataset. buffer_size: The number of items to load into the buffer for sorting. pack_size: The size of local groups (batches/micro-batches). init_seed: Seed for the random number generator. \"\"\" self . _base_dataset = base_dataset self . _buffer_size = buffer_size self . _pack_size = pack_size self . _rng = random . Random ( init_seed ^ 0x105E7 if init_seed is not None else None ) self . _buffer_indices : list [ int ] = [] self . _buffer_idx : int = - 1","title":"__init__"},{"location":"dataset/#d9d.dataset.DatasetImplementingSortKeyProtocol","text":"Bases: Protocol [ _T_co ] Protocol for datasets that support retrieval of a specific key for sorting purposes. This is typically used for length-based bucketing/sorting where the dataset needs to expose the length of an item without loading the full item. Source code in d9d/dataset/buffer_sorted.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class DatasetImplementingSortKeyProtocol ( Protocol [ _T_co ]): \"\"\" Protocol for datasets that support retrieval of a specific key for sorting purposes. This is typically used for length-based bucketing/sorting where the dataset needs to expose the length of an item without loading the full item. \"\"\" def __len__ ( self ) -> int : \"\"\"Returns the total number of items in the dataset.\"\"\" ... def sort_key ( self , index : int ) -> Any : \"\"\" Returns a value used for sorting the dataset at the given index. Args: index: The index of the item. Returns: A comparable value (e.g., int length) used for sorting. \"\"\" ... def __getitem__ ( self , item : int ) -> _T_co : \"\"\"Retrieves the item at the specific index.\"\"\" ...","title":"DatasetImplementingSortKeyProtocol"},{"location":"dataset/#d9d.dataset.DatasetImplementingSortKeyProtocol.__getitem__","text":"Retrieves the item at the specific index. Source code in d9d/dataset/buffer_sorted.py 35 36 37 def __getitem__ ( self , item : int ) -> _T_co : \"\"\"Retrieves the item at the specific index.\"\"\" ...","title":"__getitem__"},{"location":"dataset/#d9d.dataset.DatasetImplementingSortKeyProtocol.__len__","text":"Returns the total number of items in the dataset. Source code in d9d/dataset/buffer_sorted.py 19 20 21 def __len__ ( self ) -> int : \"\"\"Returns the total number of items in the dataset.\"\"\" ...","title":"__len__"},{"location":"dataset/#d9d.dataset.DatasetImplementingSortKeyProtocol.sort_key","text":"Returns a value used for sorting the dataset at the given index. Parameters: Name Type Description Default index int The index of the item. required Returns: Type Description Any A comparable value (e.g., int length) used for sorting. Source code in d9d/dataset/buffer_sorted.py 23 24 25 26 27 28 29 30 31 32 33 def sort_key ( self , index : int ) -> Any : \"\"\" Returns a value used for sorting the dataset at the given index. Args: index: The index of the item. Returns: A comparable value (e.g., int length) used for sorting. \"\"\" ...","title":"sort_key"},{"location":"dataset/#d9d.dataset.PaddingSide1D","text":"Bases: StrEnum Enum specifying the side for padding 1D sequences. Attributes: Name Type Description left Pad on the left side. right Pad on the right side. Source code in d9d/dataset/padding.py 8 9 10 11 12 13 14 15 16 17 18 class PaddingSide1D ( StrEnum ): \"\"\" Enum specifying the side for padding 1D sequences. Attributes: left: Pad on the left side. right: Pad on the right side. \"\"\" left = \"left\" right = \"right\"","title":"PaddingSide1D"},{"location":"dataset/#d9d.dataset.ShardIndexingMode","text":"Bases: StrEnum Defines how the dataset is split across shards. Modes sequential: Round-robin distribution. shard0 : 0 , 4 , 8 , 12 shard1 : 1 , 5 , 9 , 13 shard2 : 2 , 6 , 10 shard3 : 3 , 7 , 11 chunked: Contiguous blocks. shard0 : 0 , 1 , 2 , 3 shard1 : 4 , 5 , 6 , 7 shard2 : 8 , 9 , 10 , 11 shard3 : 12 , 13 Source code in d9d/dataset/sharded.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 class ShardIndexingMode ( StrEnum ): \"\"\" Defines how the dataset is split across shards. Modes: sequential: Round-robin distribution. shard0: 0, 4, 8, 12 shard1: 1, 5, 9, 13 shard2: 2, 6, 10 shard3: 3, 7, 11 chunked: Contiguous blocks. shard0: 0, 1, 2, 3 shard1: 4, 5, 6, 7 shard2: 8, 9, 10, 11 shard3: 12, 13 \"\"\" sequential = \"sequential\" chunked = \"chunked\"","title":"ShardIndexingMode"},{"location":"dataset/#d9d.dataset.ShardedDataset","text":"Bases: Dataset [ _T_co ] , Stateful A dataset wrapper that acts as a view onto a specific shard of the underlying dataset. This is useful for Data Parallel training where each process should only see a subset of the data. It supports different indexing modes and optional padding to ensure all shards have equal length (preventing hangs in distributed collectives). Source code in d9d/dataset/sharded.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 class ShardedDataset ( Dataset [ _T_co ], Stateful ): \"\"\" A dataset wrapper that acts as a view onto a specific shard of the underlying dataset. This is useful for Data Parallel training where each process should only see a subset of the data. It supports different indexing modes and optional padding to ensure all shards have equal length (preventing hangs in distributed collectives). \"\"\" def __init__ ( self , dataset : Dataset [ _T_co ], total_shards : int , current_shard : int , indexing_mode : ShardIndexingMode , pad_to_equal_size_across_shards : bool , ): \"\"\" Constructs a ShardedDataset object. Args: dataset: The underlying dataset to shard. total_shards: The total number of shards (e.g., number of DP ranks). current_shard: The index of the current shard (e.g., current DP rank). indexing_mode: How indices are assigned to shards (sequential/round-robin or chunked). pad_to_equal_size_across_shards: If True, the length of the dataset will be padded so that all shards report the same length. The last standard element is repeated. \"\"\" if not isinstance ( dataset , Sized ): raise ValueError ( \"Dataset should implement __len__ method\" ) self . _dataset = dataset self . _total_shards = total_shards self . _current_shard = current_shard self . _indexing_mode = indexing_mode self . _pad_to_equal_size_across_shards = pad_to_equal_size_across_shards def _compute_real_index_sequential ( self , index : int ) -> int : return index * self . _total_shards + self . _current_shard def _get_base_index_unsafe ( self , index : int ) -> int : \"\"\" Calculates the underlying dataset index for a given shard index, without boundary checking. \"\"\" match self . _indexing_mode : case ShardIndexingMode . sequential : base_index = index * self . _total_shards + self . _current_shard return base_index case ShardIndexingMode . chunked : ceil_len = math . ceil ( len ( self . _dataset ) / self . _total_shards ) shard_start_offset = ceil_len * self . _current_shard return shard_start_offset + index case _ : raise ValueError ( f \"Unknown shard indexing mode: { self . _indexing_mode } \" ) def __getitem__ ( self , index : int ) -> _T_co : \"\"\" Retrieves an item from the underlying dataset mapping logic shard index to physical index. If padding is enabled and the index exceeds the valid data for this shard, the last item in the dataset is returned. Args: index: The index relative to this shard. Returns: The data item. \"\"\" base_index = self . _get_base_index_unsafe ( index ) if base_index >= len ( self . _dataset ): base_index = len ( self . _dataset ) - 1 return self . _dataset [ base_index ] def __len__ ( self ) -> int : \"\"\" Returns the number of items in this specific shard. If `pad_to_equal_size_across_shards` is True, this returns the ceiling length (max length across all shards). \"\"\" ceil_len = math . ceil ( len ( self . _dataset ) / self . _total_shards ) if self . _pad_to_equal_size_across_shards : return ceil_len shards_remainder = len ( self . _dataset ) % self . _total_shards match self . _indexing_mode : case ShardIndexingMode . sequential : shards_full = len ( self . _dataset ) // self . _total_shards return shards_full + 1 if self . _current_shard < shards_remainder else shards_full case ShardIndexingMode . chunked : is_shard_last = self . _current_shard == self . _total_shards - 1 if not is_shard_last or shards_remainder == 0 : return ceil_len else : return ceil_len - ( self . _total_shards - shards_remainder ) case _ : raise ValueError ( f \"Unknown ShardIndexingMode: { self . _indexing_mode } \" ) def load_state_dict ( self , state_dict : dict [ str , Any ]) -> None : if isinstance ( self . _dataset , Stateful ): self . _dataset . load_state_dict ( state_dict [ \"dataset\" ]) # check whether env mismatched if state_dict [ \"total_shards\" ] != self . _total_shards : raise ValueError ( \"Shard count mismatch\" ) self . _total_shards = state_dict [ \"total_shards\" ] self . _current_shard = state_dict [ \"current_shard\" ] def state_dict ( self ) -> dict [ str , Any ]: dct : dict [ str , Any ] = { \"total_shards\" : self . _total_shards , \"current_shard\" : self . _current_shard } if isinstance ( self . _dataset , Stateful ): dct [ \"dataset\" ] = self . _dataset . state_dict () return dct","title":"ShardedDataset"},{"location":"dataset/#d9d.dataset.ShardedDataset.__getitem__","text":"Retrieves an item from the underlying dataset mapping logic shard index to physical index. If padding is enabled and the index exceeds the valid data for this shard, the last item in the dataset is returned. Parameters: Name Type Description Default index int The index relative to this shard. required Returns: Type Description _T_co The data item. Source code in d9d/dataset/sharded.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def __getitem__ ( self , index : int ) -> _T_co : \"\"\" Retrieves an item from the underlying dataset mapping logic shard index to physical index. If padding is enabled and the index exceeds the valid data for this shard, the last item in the dataset is returned. Args: index: The index relative to this shard. Returns: The data item. \"\"\" base_index = self . _get_base_index_unsafe ( index ) if base_index >= len ( self . _dataset ): base_index = len ( self . _dataset ) - 1 return self . _dataset [ base_index ]","title":"__getitem__"},{"location":"dataset/#d9d.dataset.ShardedDataset.__init__","text":"Constructs a ShardedDataset object. Parameters: Name Type Description Default dataset Dataset [ _T_co ] The underlying dataset to shard. required total_shards int The total number of shards (e.g., number of DP ranks). required current_shard int The index of the current shard (e.g., current DP rank). required indexing_mode ShardIndexingMode How indices are assigned to shards (sequential/round-robin or chunked). required pad_to_equal_size_across_shards bool If True, the length of the dataset will be padded so that all shards report the same length. The last standard element is repeated. required Source code in d9d/dataset/sharded.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def __init__ ( self , dataset : Dataset [ _T_co ], total_shards : int , current_shard : int , indexing_mode : ShardIndexingMode , pad_to_equal_size_across_shards : bool , ): \"\"\" Constructs a ShardedDataset object. Args: dataset: The underlying dataset to shard. total_shards: The total number of shards (e.g., number of DP ranks). current_shard: The index of the current shard (e.g., current DP rank). indexing_mode: How indices are assigned to shards (sequential/round-robin or chunked). pad_to_equal_size_across_shards: If True, the length of the dataset will be padded so that all shards report the same length. The last standard element is repeated. \"\"\" if not isinstance ( dataset , Sized ): raise ValueError ( \"Dataset should implement __len__ method\" ) self . _dataset = dataset self . _total_shards = total_shards self . _current_shard = current_shard self . _indexing_mode = indexing_mode self . _pad_to_equal_size_across_shards = pad_to_equal_size_across_shards","title":"__init__"},{"location":"dataset/#d9d.dataset.ShardedDataset.__len__","text":"Returns the number of items in this specific shard. If pad_to_equal_size_across_shards is True, this returns the ceiling length (max length across all shards). Source code in d9d/dataset/sharded.py 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def __len__ ( self ) -> int : \"\"\" Returns the number of items in this specific shard. If `pad_to_equal_size_across_shards` is True, this returns the ceiling length (max length across all shards). \"\"\" ceil_len = math . ceil ( len ( self . _dataset ) / self . _total_shards ) if self . _pad_to_equal_size_across_shards : return ceil_len shards_remainder = len ( self . _dataset ) % self . _total_shards match self . _indexing_mode : case ShardIndexingMode . sequential : shards_full = len ( self . _dataset ) // self . _total_shards return shards_full + 1 if self . _current_shard < shards_remainder else shards_full case ShardIndexingMode . chunked : is_shard_last = self . _current_shard == self . _total_shards - 1 if not is_shard_last or shards_remainder == 0 : return ceil_len else : return ceil_len - ( self . _total_shards - shards_remainder ) case _ : raise ValueError ( f \"Unknown ShardIndexingMode: { self . _indexing_mode } \" )","title":"__len__"},{"location":"dataset/#d9d.dataset.TokenPoolingType","text":"Bases: StrEnum Enumeration of supported token pooling strategies. Attributes: Name Type Description first Selects the first token of the sequence (e.g., [CLS] token). last Selects the last non-padding token of the sequence (e.g., for Transformer Decoder). all Selects all non-padding tokens (e.g., for mean pooling). Source code in d9d/dataset/pooling.py 6 7 8 9 10 11 12 13 14 15 16 17 class TokenPoolingType ( StrEnum ): \"\"\"Enumeration of supported token pooling strategies. Attributes: first: Selects the first token of the sequence (e.g., [CLS] token). last: Selects the last non-padding token of the sequence (e.g., for Transformer Decoder). all: Selects all non-padding tokens (e.g., for mean pooling). \"\"\" first = \"first\" last = \"last\" all = \"all\"","title":"TokenPoolingType"},{"location":"dataset/#d9d.dataset.pad_stack_1d","text":"Stacks 1D tensors into a batch, applying padding. Calculates the maximum length among the input tensors (optionally aligning to a multiple), pads elements to match this length on the specified side, and stacks them. Parameters: Name Type Description Default items Sequence [ Tensor ] A sequence of 1D tensors to be stacked. required pad_value int The value used for padding. required padding_side PaddingSide1D The side on which to apply padding (left or right). right pad_to_multiple_of int | None Optional integer. If provided, ensures the target length is a multiple of this value. None Returns: Type Description Tensor A single stacked tensor of shape (batch, max_length). Raises: Type Description ValueError If no items are provided or if pad_to_multiple_of is <= 0. Source code in d9d/dataset/padding.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def pad_stack_1d ( items : Sequence [ torch . Tensor ], pad_value : int , padding_side : PaddingSide1D = PaddingSide1D . right , pad_to_multiple_of : int | None = None , ) -> torch . Tensor : \"\"\" Stacks 1D tensors into a batch, applying padding. Calculates the maximum length among the input tensors (optionally aligning to a multiple), pads elements to match this length on the specified side, and stacks them. Args: items: A sequence of 1D tensors to be stacked. pad_value: The value used for padding. padding_side: The side on which to apply padding (left or right). pad_to_multiple_of: Optional integer. If provided, ensures the target length is a multiple of this value. Returns: A single stacked tensor of shape (batch, max_length). Raises: ValueError: If no items are provided or if `pad_to_multiple_of` is <= 0. \"\"\" if not items : raise ValueError ( \"Cannot stack 0 items\" ) if pad_to_multiple_of is not None and pad_to_multiple_of <= 0 : raise ValueError ( \"pad_to_multiple_of should be > 0\" ) max_len = max ( x . shape [ 0 ] for x in items ) if pad_to_multiple_of is not None and ( remainder := max_len % pad_to_multiple_of ) != 0 : max_len = max_len + ( pad_to_multiple_of - remainder ) padded_items = [] for x in items : difference = max_len - x . shape [ 0 ] if difference == 0 : padded_items . append ( x ) else : padded_items . append ( F . pad ( x , _padding_side_1d_to_config ( padding_side , difference ), value = pad_value )) return torch . stack ( padded_items , dim = 0 )","title":"pad_stack_1d"},{"location":"dataset/#d9d.dataset.shard_dataset_data_parallel","text":"Wraps a dataset into a ShardedDataset based on the Data Parallel dimension of the distributed context. This is a helper function to automatically determine the correct rank and world size from the 'dp' (Data Parallel) mesh dimension within the batch domain DeviceMesh. Parameters: Name Type Description Default dataset Dataset [ _T_co ] The source dataset to shard. required dist_context DistributedContext The distributed context. required indexing_mode ShardIndexingMode The strategy for splitting data indices (sequential/round-robin or chunked). sequential pad_to_equal_size_across_shards bool If True, ensures all shards have the same length by padding. True Returns: Type Description Dataset [ _T_co ] A dataset instance representing the local shard. Source code in d9d/dataset/sharded.py 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 def shard_dataset_data_parallel ( dataset : Dataset [ _T_co ], dist_context : DistributedContext , indexing_mode : ShardIndexingMode = ShardIndexingMode . sequential , pad_to_equal_size_across_shards : bool = True , ) -> Dataset [ _T_co ]: \"\"\" Wraps a dataset into a ShardedDataset based on the Data Parallel dimension of the distributed context. This is a helper function to automatically determine the correct rank and world size from the 'dp' (Data Parallel) mesh dimension within the batch domain DeviceMesh. Args: dataset: The source dataset to shard. dist_context: The distributed context. indexing_mode: The strategy for splitting data indices (sequential/round-robin or chunked). pad_to_equal_size_across_shards: If True, ensures all shards have the same length by padding. Returns: A dataset instance representing the local shard. \"\"\" if dist_context . mesh_params . is_distributed : dp_mesh = dist_context . mesh_for ( BATCH_DOMAIN )[ \"dp\" ] n_shards = dp_mesh . size () current_shard = dp_mesh . get_local_rank () else : n_shards = 1 current_shard = 0 return ShardedDataset ( dataset = dataset , total_shards = n_shards , current_shard = current_shard , indexing_mode = indexing_mode , pad_to_equal_size_across_shards = pad_to_equal_size_across_shards , )","title":"shard_dataset_data_parallel"},{"location":"dataset/#d9d.dataset.token_pooling_mask_from_attention_mask","text":"Generates a binary mask for token pooling based on the specified strategy. Parameters: Name Type Description Default attention_mask Tensor A binary mask indicating valid tokens (1) and padding (0). Expected shape is (batch_size, sequence_length). required pooling_type TokenPoolingType The strategy to use for selecting tokens. required Returns: Type Description Tensor A LongTensor of the same shape as input containing 1s at positions Tensor to be included in pooling and 0s elsewhere. Raises: Type Description ValueError If the provided pooling type is not supported. Source code in d9d/dataset/pooling.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def token_pooling_mask_from_attention_mask ( attention_mask : torch . Tensor , pooling_type : TokenPoolingType ) -> torch . Tensor : \"\"\"Generates a binary mask for token pooling based on the specified strategy. Args: attention_mask: A binary mask indicating valid tokens (1) and padding (0). Expected shape is (batch_size, sequence_length). pooling_type: The strategy to use for selecting tokens. Returns: A LongTensor of the same shape as input containing 1s at positions to be included in pooling and 0s elsewhere. Raises: ValueError: If the provided pooling type is not supported. \"\"\" match pooling_type : case TokenPoolingType . first : mask = torch . zeros_like ( attention_mask , dtype = torch . long ) mask [:, 0 ] = 1 return mask case TokenPoolingType . last : batch_indices = torch . arange ( attention_mask . size ( 0 ), device = attention_mask . device ) last_token_indices = attention_mask . sum ( dim = 1 ) - 1 mask = torch . zeros_like ( attention_mask , dtype = torch . long ) mask [ batch_indices , last_token_indices ] = 1 return mask case TokenPoolingType . all : return attention_mask case _ : raise ValueError ( f \"Unknown pooling type: { pooling_type } \" )","title":"token_pooling_mask_from_attention_mask"},{"location":"internals/determinism/","text":"About Warning: If you are utilizing the standard d9d training or inference infrastructure, you do not need to call these functions manually. The framework automatically handles seed initialization during startup. This package is primarily intended for users extending d9d . The d9d.internals.determinism package handles the initialization of Random Number Generators (RNG) across distributed processes. d9d.internals.determinism This package provides utilities for making your distributed setup deterministic. set_seeds ( dist_context , seed , distinct_seed_mesh_dim = 'pp' ) Sets random seeds for Python, NumPy, and PyTorch. This function sets seeds deterministically based on the provided base seed and the process's rank within a specific mesh dimension. The seed is shifted by the rank in the distinct_seed_mesh_dim (e.g., Pipeline Parallel rank). This ensures that processes in different pipeline stages operate with different random states, while processes that should share randomness (like Expert Parallel peers) can be synchronized. Parameters: Name Type Description Default dist_context DistributedContext The distributed context. required seed int The base random seed. required distinct_seed_mesh_dim str The name of the mesh dimension along which seeds should be distinct (e.g., 'pp' for pipeline parallelism). Ranks along other dimensions will share the seed. 'pp' Source code in d9d/internals/determinism/seed.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def set_seeds ( dist_context : DistributedContext , seed : int , distinct_seed_mesh_dim : str = \"pp\" , ) -> None : \"\"\" Sets random seeds for Python, NumPy, and PyTorch. This function sets seeds deterministically based on the provided base seed and the process's rank within a specific mesh dimension. The seed is shifted by the rank in the `distinct_seed_mesh_dim` (e.g., Pipeline Parallel rank). This ensures that processes in different pipeline stages operate with different random states, while processes that should share randomness (like Expert Parallel peers) can be synchronized. Args: dist_context: The distributed context. seed: The base random seed. distinct_seed_mesh_dim: The name of the mesh dimension along which seeds should be distinct (e.g., 'pp' for pipeline parallelism). Ranks along other dimensions will share the seed. \"\"\" # Mutate seed based on PP rank if distributed if dist_context . mesh_params . is_distributed : distinct_mesh = dist_context . mesh_for ( REGULAR_DOMAIN )[ distinct_seed_mesh_dim ] seed = ( seed + distinct_mesh . get_local_rank ()) % 2 ** 64 dist_context . logger . info ( f \"Set seed { seed } \" ) torch . manual_seed ( seed ) os . environ [ \"PYTHONHASHSEED\" ] = str ( seed % 2 ** 32 ) random . seed ( seed ) try : import numpy as np # noqa: PLC0415 np . random . seed ( seed ) except ImportError : pass # Set DTensor seeding if distributed if dist_context . mesh_params . is_distributed : mesh_regular = dist_context . mesh_for ( REGULAR_DOMAIN ) duplicate_seed_mesh_dim = tuple ( name for name in cast ( list [ str ], mesh_regular . mesh_dim_names ) if name != distinct_seed_mesh_dim ) duplicate_seed_mesh = mesh_regular [ duplicate_seed_mesh_dim ] if len ( duplicate_seed_mesh_dim ) != 0 else None if duplicate_seed_mesh and duplicate_seed_mesh . get_coordinate () is not None : torch . distributed . tensor . _random . manual_seed ( seed , duplicate_seed_mesh ) # noqa: SLF001","title":"Determinism"},{"location":"internals/determinism/#about","text":"Warning: If you are utilizing the standard d9d training or inference infrastructure, you do not need to call these functions manually. The framework automatically handles seed initialization during startup. This package is primarily intended for users extending d9d . The d9d.internals.determinism package handles the initialization of Random Number Generators (RNG) across distributed processes.","title":"About"},{"location":"internals/determinism/#d9d.internals.determinism","text":"This package provides utilities for making your distributed setup deterministic.","title":"determinism"},{"location":"internals/determinism/#d9d.internals.determinism.set_seeds","text":"Sets random seeds for Python, NumPy, and PyTorch. This function sets seeds deterministically based on the provided base seed and the process's rank within a specific mesh dimension. The seed is shifted by the rank in the distinct_seed_mesh_dim (e.g., Pipeline Parallel rank). This ensures that processes in different pipeline stages operate with different random states, while processes that should share randomness (like Expert Parallel peers) can be synchronized. Parameters: Name Type Description Default dist_context DistributedContext The distributed context. required seed int The base random seed. required distinct_seed_mesh_dim str The name of the mesh dimension along which seeds should be distinct (e.g., 'pp' for pipeline parallelism). Ranks along other dimensions will share the seed. 'pp' Source code in d9d/internals/determinism/seed.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def set_seeds ( dist_context : DistributedContext , seed : int , distinct_seed_mesh_dim : str = \"pp\" , ) -> None : \"\"\" Sets random seeds for Python, NumPy, and PyTorch. This function sets seeds deterministically based on the provided base seed and the process's rank within a specific mesh dimension. The seed is shifted by the rank in the `distinct_seed_mesh_dim` (e.g., Pipeline Parallel rank). This ensures that processes in different pipeline stages operate with different random states, while processes that should share randomness (like Expert Parallel peers) can be synchronized. Args: dist_context: The distributed context. seed: The base random seed. distinct_seed_mesh_dim: The name of the mesh dimension along which seeds should be distinct (e.g., 'pp' for pipeline parallelism). Ranks along other dimensions will share the seed. \"\"\" # Mutate seed based on PP rank if distributed if dist_context . mesh_params . is_distributed : distinct_mesh = dist_context . mesh_for ( REGULAR_DOMAIN )[ distinct_seed_mesh_dim ] seed = ( seed + distinct_mesh . get_local_rank ()) % 2 ** 64 dist_context . logger . info ( f \"Set seed { seed } \" ) torch . manual_seed ( seed ) os . environ [ \"PYTHONHASHSEED\" ] = str ( seed % 2 ** 32 ) random . seed ( seed ) try : import numpy as np # noqa: PLC0415 np . random . seed ( seed ) except ImportError : pass # Set DTensor seeding if distributed if dist_context . mesh_params . is_distributed : mesh_regular = dist_context . mesh_for ( REGULAR_DOMAIN ) duplicate_seed_mesh_dim = tuple ( name for name in cast ( list [ str ], mesh_regular . mesh_dim_names ) if name != distinct_seed_mesh_dim ) duplicate_seed_mesh = mesh_regular [ duplicate_seed_mesh_dim ] if len ( duplicate_seed_mesh_dim ) != 0 else None if duplicate_seed_mesh and duplicate_seed_mesh . get_coordinate () is not None : torch . distributed . tensor . _random . manual_seed ( seed , duplicate_seed_mesh ) # noqa: SLF001","title":"set_seeds"},{"location":"internals/grad_norm/","text":"About Warning: If you are utilizing the standard d9d training infrastructure, you do not need to call these functions manually. The framework automatically handles gradient clipping. This package is primarily intended for users extending the internals of d9d . The d9d.internals.grad_norm package handles the calculation and clipping of gradient norms in complex distributed environments. Standard PyTorch clip_grad_norm_ functions are not fully aware of heterogeneous ND-Parallelism strategies (mixing Pipeline, Data, Tensor, and Context Parallelism). This package ensures that the global norm is correctly calculated across all parallel dimensions and that DTensor sharding is handled without unnecessary full-tensor materialization. Concepts Distributed Heterogeneity Some parameters might be Shard ed across a TP/FSDP mesh, while others are Replicate d. Also model may be pipelined. To handle this, we decompose the problem: Local Norm : Calculate the norm of the tensor shards actually present in GPU memory (using to_local() ). Horizontal Reduction : Perform all_reduce strictly on the meshes where parameters are sharded. This ensures that sharded parameters contribute correctly to the global norm, while replicated parameters do not trigger double-counting or unnecessary communication for norm calculation. Pipeline Reduction : Finally, norms are summed across the Pipeline Parallel mesh, as different stages hold completely different parameters. Grouping & Overlap To optimize performance, group_parameters_for_norm groups parameters into GradNormGroup buckets. This grouping is based on: Sharding Strategy : Parameters sharded on the same mesh are grouped together so their norms can be reduced in a single collective operation. Device & DType : Ensures compatibility for local math operations. The system attempts to overlap communication with computation. Groups containing sharded tensors are prioritized so their all_reduce operations can run asynchronously while local norms for other groups are being computed. Mathematical Correctness The goal of distributed gradient clipping is to calculate the Global Norm ( \\(\\|\\mathbf{g}\\|\\) ) of a single model instance , regardless of how that model is physically fragmented across GPUs. Let the total set of model parameters \\(\\mathcal{P}\\) be divided into disjoint subsets based on parallelism strategy: 1. \\(\\mathcal{P}_{pp}\\) : Sets of parameters residing on different Pipeline stages. 2. \\(\\mathcal{P}_{sharded}\\) : Parameters split across a TP/EP/FSDP group. 3. \\(\\mathcal{P}_{repl}\\) : Parameters replicated across other groups. The definition of the global \\(L_2\\) norm is: \\[ \\|\\mathbf{g}\\|_2 = \\sqrt{ \\sum_{p \\in \\mathcal{P}} \\|g_p\\|^2 } \\] We prove that our strategy of separating aggregation logic based on placement prevents double-counting. Proof for Sharded Parameters (TP/EP/FSDP) For a parameter \\(w \\in \\mathcal{P}_{sharded}\\) , the logical gradient tensor \\(G\\) is split into physical shards \\(G_1, G_2, \\dots, G_k\\) across \\(k\\) devices. By the definition of the Frobenius norm: \\[ \\|G\\|^2 = \\sum_{rank=1}^{k} \\|G_{rank}\\|^2 \\] Strategy: We calculate local norms and apply all_reduce(op=SUM) . Proof for Replicated Parameters (DP) For a parameter \\(w \\in \\mathcal{P}_{repl}\\) , the logical gradient tensor \\(G\\) is identical on all \\(k\\) devices (assuming DP synchronization has occurred). $$ G_{rank_1} = G_{rank_2} = \\dots = G $$ If we were to sum these (as we did for TP), we would obtain: $$ \\sum_{rank=1}^{k} |G_{rank}|^2 = k \\cdot |G|^2 \\quad (\\text{Incorrect: Double Counting}) $$ Strategy: We group these parameters separately and do not communicate. Proof for Pipeline Parallelism (PP) Pipeline stages hold disjoint sets of parameters. The total norm is simply the sum of the norms of the stages. \\[ \\|\\mathbf{g}\\|^2 = \\|\\mathbf{g}_{stage_1}\\|^2 + \\|\\mathbf{g}_{stage_2}\\|^2 + \\dots \\] Strategy: We apply all_reduce(op=SUM) across the PP mesh. Result The final formula utilized by d9d ensures \\(1:1\\) correspondence with a single-device baseline: \\[ \\|\\mathbf{g}\\|_{global} = \\sqrt{ \\underbrace{\\sum_{pp} \\left( \\underbrace{\\sum_{tp} \\|g_{sharded}\\|^2}_{\\text{Sum Unique Shards}} + \\underbrace{\\|g_{replicated}\\|^2}_{\\text{Do Not Duplicate}} \\right)}_{\\text{Sum Disjoint Layers}} } \\] d9d.internals.grad_norm clip_grad_norm_distributed_ ( parameter_groups , max_norm , norm_type , pp_mesh ) Clips gradient norms in a fully distributed environment. This function calculates the global gradient norm across all dimensions of parallelism (Horizontal - DP/CP/TP/EP/..., and Pipeline) and scales the gradients in-place to ensure the norm does not exceed max_norm. It accurately handles DTensors by identifying their sharding placements and performing reductions only on the necessary process groups. Overlaps communication and computation if possible. Parameters: Name Type Description Default parameter_groups ParametersForNorm Dictionary grouping parameters by synchronization requirements, typically created by group_parameters_for_norm . required max_norm float | None The maximum allowed norm of the gradients. If None, the function calculates and returns the global norm without modifying the gradients. required norm_type float The type of the norm to calculate (e.g., 2.0 for L2 norm, inf for max norm). required pp_mesh DeviceMesh | None The device mesh representing the pipeline parallel dimension, needed to reduce norms across pipeline stages. required Returns: Type Description Tensor The calculated global gradient norm. Source code in d9d/internals/grad_norm/norm.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 def clip_grad_norm_distributed_ ( parameter_groups : ParametersForNorm , max_norm : float | None , norm_type : float , pp_mesh : DeviceMesh | None ) -> torch . Tensor : \"\"\" Clips gradient norms in a fully distributed environment. This function calculates the global gradient norm across all dimensions of parallelism (Horizontal - DP/CP/TP/EP/..., and Pipeline) and scales the gradients in-place to ensure the norm does not exceed max_norm. It accurately handles DTensors by identifying their sharding placements and performing reductions only on the necessary process groups. Overlaps communication and computation if possible. Args: parameter_groups: Dictionary grouping parameters by synchronization requirements, typically created by `group_parameters_for_norm`. max_norm: The maximum allowed norm of the gradients. If None, the function calculates and returns the global norm without modifying the gradients. norm_type: The type of the norm to calculate (e.g., 2.0 for L2 norm, inf for max norm). pp_mesh: The device mesh representing the pipeline parallel dimension, needed to reduce norms across pipeline stages. Returns: The calculated global gradient norm. \"\"\" with record_function ( \"Gradient Clipping\" ): global_norm_pow = _get_global_norm_pow_pp ( parameter_groups = parameter_groups , norm_type = norm_type , pp_mesh = pp_mesh ) if math . isinf ( norm_type ): global_norm = global_norm_pow else : global_norm = global_norm_pow ** ( 1.0 / norm_type ) if max_norm : _clip_grad_with_norm_ ( parameter_groups , max_norm = max_norm , total_norm = global_norm ) return global_norm group_parameters_for_norm ( parameters ) Groups parameters based on their distributed tensor characteristics. Groups parameters by their sharding meshes, device, and gradient data type. Parameters: Name Type Description Default parameters Iterable [ Parameter ] The iterable of parameters to group. required Returns: Type Description ParametersForNorm A dictionary mapping synchronization groups to lists of parameters. Source code in d9d/internals/grad_norm/group.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def group_parameters_for_norm ( parameters : Iterable [ nn . Parameter ]) -> ParametersForNorm : \"\"\" Groups parameters based on their distributed tensor characteristics. Groups parameters by their sharding meshes, device, and gradient data type. Args: parameters: The iterable of parameters to group. Returns: A dictionary mapping synchronization groups to lists of parameters. \"\"\" grouped_params : ParametersForNorm = defaultdict ( list ) for param in parameters : if not param . requires_grad : continue group = GradNormGroup ( shard_meshes = _extract_shard_meshes ( param ), grad_dtype = param . grad_dtype , device = param . device ) grouped_params [ group ] . append ( param ) # we are sure dict is ordered in python 3.11 so we can sort it... return dict ( sorted ( grouped_params . items (), key = _group_sort_key ))","title":"Gradient Norm & Clipping"},{"location":"internals/grad_norm/#about","text":"Warning: If you are utilizing the standard d9d training infrastructure, you do not need to call these functions manually. The framework automatically handles gradient clipping. This package is primarily intended for users extending the internals of d9d . The d9d.internals.grad_norm package handles the calculation and clipping of gradient norms in complex distributed environments. Standard PyTorch clip_grad_norm_ functions are not fully aware of heterogeneous ND-Parallelism strategies (mixing Pipeline, Data, Tensor, and Context Parallelism). This package ensures that the global norm is correctly calculated across all parallel dimensions and that DTensor sharding is handled without unnecessary full-tensor materialization.","title":"About"},{"location":"internals/grad_norm/#concepts","text":"","title":"Concepts"},{"location":"internals/grad_norm/#distributed-heterogeneity","text":"Some parameters might be Shard ed across a TP/FSDP mesh, while others are Replicate d. Also model may be pipelined. To handle this, we decompose the problem: Local Norm : Calculate the norm of the tensor shards actually present in GPU memory (using to_local() ). Horizontal Reduction : Perform all_reduce strictly on the meshes where parameters are sharded. This ensures that sharded parameters contribute correctly to the global norm, while replicated parameters do not trigger double-counting or unnecessary communication for norm calculation. Pipeline Reduction : Finally, norms are summed across the Pipeline Parallel mesh, as different stages hold completely different parameters.","title":"Distributed Heterogeneity"},{"location":"internals/grad_norm/#grouping-overlap","text":"To optimize performance, group_parameters_for_norm groups parameters into GradNormGroup buckets. This grouping is based on: Sharding Strategy : Parameters sharded on the same mesh are grouped together so their norms can be reduced in a single collective operation. Device & DType : Ensures compatibility for local math operations. The system attempts to overlap communication with computation. Groups containing sharded tensors are prioritized so their all_reduce operations can run asynchronously while local norms for other groups are being computed.","title":"Grouping &amp; Overlap"},{"location":"internals/grad_norm/#mathematical-correctness","text":"The goal of distributed gradient clipping is to calculate the Global Norm ( \\(\\|\\mathbf{g}\\|\\) ) of a single model instance , regardless of how that model is physically fragmented across GPUs. Let the total set of model parameters \\(\\mathcal{P}\\) be divided into disjoint subsets based on parallelism strategy: 1. \\(\\mathcal{P}_{pp}\\) : Sets of parameters residing on different Pipeline stages. 2. \\(\\mathcal{P}_{sharded}\\) : Parameters split across a TP/EP/FSDP group. 3. \\(\\mathcal{P}_{repl}\\) : Parameters replicated across other groups. The definition of the global \\(L_2\\) norm is: \\[ \\|\\mathbf{g}\\|_2 = \\sqrt{ \\sum_{p \\in \\mathcal{P}} \\|g_p\\|^2 } \\] We prove that our strategy of separating aggregation logic based on placement prevents double-counting.","title":"Mathematical Correctness"},{"location":"internals/grad_norm/#proof-for-sharded-parameters-tpepfsdp","text":"For a parameter \\(w \\in \\mathcal{P}_{sharded}\\) , the logical gradient tensor \\(G\\) is split into physical shards \\(G_1, G_2, \\dots, G_k\\) across \\(k\\) devices. By the definition of the Frobenius norm: \\[ \\|G\\|^2 = \\sum_{rank=1}^{k} \\|G_{rank}\\|^2 \\] Strategy: We calculate local norms and apply all_reduce(op=SUM) .","title":"Proof for Sharded Parameters (TP/EP/FSDP)"},{"location":"internals/grad_norm/#proof-for-replicated-parameters-dp","text":"For a parameter \\(w \\in \\mathcal{P}_{repl}\\) , the logical gradient tensor \\(G\\) is identical on all \\(k\\) devices (assuming DP synchronization has occurred). $$ G_{rank_1} = G_{rank_2} = \\dots = G $$ If we were to sum these (as we did for TP), we would obtain: $$ \\sum_{rank=1}^{k} |G_{rank}|^2 = k \\cdot |G|^2 \\quad (\\text{Incorrect: Double Counting}) $$ Strategy: We group these parameters separately and do not communicate.","title":"Proof for Replicated Parameters (DP)"},{"location":"internals/grad_norm/#proof-for-pipeline-parallelism-pp","text":"Pipeline stages hold disjoint sets of parameters. The total norm is simply the sum of the norms of the stages. \\[ \\|\\mathbf{g}\\|^2 = \\|\\mathbf{g}_{stage_1}\\|^2 + \\|\\mathbf{g}_{stage_2}\\|^2 + \\dots \\] Strategy: We apply all_reduce(op=SUM) across the PP mesh.","title":"Proof for Pipeline Parallelism (PP)"},{"location":"internals/grad_norm/#result","text":"The final formula utilized by d9d ensures \\(1:1\\) correspondence with a single-device baseline: \\[ \\|\\mathbf{g}\\|_{global} = \\sqrt{ \\underbrace{\\sum_{pp} \\left( \\underbrace{\\sum_{tp} \\|g_{sharded}\\|^2}_{\\text{Sum Unique Shards}} + \\underbrace{\\|g_{replicated}\\|^2}_{\\text{Do Not Duplicate}} \\right)}_{\\text{Sum Disjoint Layers}} } \\]","title":"Result"},{"location":"internals/grad_norm/#d9d.internals.grad_norm","text":"","title":"grad_norm"},{"location":"internals/grad_norm/#d9d.internals.grad_norm.clip_grad_norm_distributed_","text":"Clips gradient norms in a fully distributed environment. This function calculates the global gradient norm across all dimensions of parallelism (Horizontal - DP/CP/TP/EP/..., and Pipeline) and scales the gradients in-place to ensure the norm does not exceed max_norm. It accurately handles DTensors by identifying their sharding placements and performing reductions only on the necessary process groups. Overlaps communication and computation if possible. Parameters: Name Type Description Default parameter_groups ParametersForNorm Dictionary grouping parameters by synchronization requirements, typically created by group_parameters_for_norm . required max_norm float | None The maximum allowed norm of the gradients. If None, the function calculates and returns the global norm without modifying the gradients. required norm_type float The type of the norm to calculate (e.g., 2.0 for L2 norm, inf for max norm). required pp_mesh DeviceMesh | None The device mesh representing the pipeline parallel dimension, needed to reduce norms across pipeline stages. required Returns: Type Description Tensor The calculated global gradient norm. Source code in d9d/internals/grad_norm/norm.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 def clip_grad_norm_distributed_ ( parameter_groups : ParametersForNorm , max_norm : float | None , norm_type : float , pp_mesh : DeviceMesh | None ) -> torch . Tensor : \"\"\" Clips gradient norms in a fully distributed environment. This function calculates the global gradient norm across all dimensions of parallelism (Horizontal - DP/CP/TP/EP/..., and Pipeline) and scales the gradients in-place to ensure the norm does not exceed max_norm. It accurately handles DTensors by identifying their sharding placements and performing reductions only on the necessary process groups. Overlaps communication and computation if possible. Args: parameter_groups: Dictionary grouping parameters by synchronization requirements, typically created by `group_parameters_for_norm`. max_norm: The maximum allowed norm of the gradients. If None, the function calculates and returns the global norm without modifying the gradients. norm_type: The type of the norm to calculate (e.g., 2.0 for L2 norm, inf for max norm). pp_mesh: The device mesh representing the pipeline parallel dimension, needed to reduce norms across pipeline stages. Returns: The calculated global gradient norm. \"\"\" with record_function ( \"Gradient Clipping\" ): global_norm_pow = _get_global_norm_pow_pp ( parameter_groups = parameter_groups , norm_type = norm_type , pp_mesh = pp_mesh ) if math . isinf ( norm_type ): global_norm = global_norm_pow else : global_norm = global_norm_pow ** ( 1.0 / norm_type ) if max_norm : _clip_grad_with_norm_ ( parameter_groups , max_norm = max_norm , total_norm = global_norm ) return global_norm","title":"clip_grad_norm_distributed_"},{"location":"internals/grad_norm/#d9d.internals.grad_norm.group_parameters_for_norm","text":"Groups parameters based on their distributed tensor characteristics. Groups parameters by their sharding meshes, device, and gradient data type. Parameters: Name Type Description Default parameters Iterable [ Parameter ] The iterable of parameters to group. required Returns: Type Description ParametersForNorm A dictionary mapping synchronization groups to lists of parameters. Source code in d9d/internals/grad_norm/group.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def group_parameters_for_norm ( parameters : Iterable [ nn . Parameter ]) -> ParametersForNorm : \"\"\" Groups parameters based on their distributed tensor characteristics. Groups parameters by their sharding meshes, device, and gradient data type. Args: parameters: The iterable of parameters to group. Returns: A dictionary mapping synchronization groups to lists of parameters. \"\"\" grouped_params : ParametersForNorm = defaultdict ( list ) for param in parameters : if not param . requires_grad : continue group = GradNormGroup ( shard_meshes = _extract_shard_meshes ( param ), grad_dtype = param . grad_dtype , device = param . device ) grouped_params [ group ] . append ( param ) # we are sure dict is ordered in python 3.11 so we can sort it... return dict ( sorted ( grouped_params . items (), key = _group_sort_key ))","title":"group_parameters_for_norm"},{"location":"internals/grad_sync/","text":"About Warning: If you are utilizing the standard d9d training infrastructure, you do not need to call these functions manually. The framework automatically handles gradient synchronization. This package is primarily intended for users extending d9d . The d9d.internals.grad_sync package provides low-level primitives for synchronizing gradients in distributed training setups utilizing DTensor . Unlike standard PyTorch DistributedDataParallel which assumes a uniform communication strategy for the entire model, this package is designed to work with heterogeneous distributions often found in ND-parallelism (e.g., mixtures of Data, Tensor, Sequence, and Pipeline parallelism). It inspects DTensor placements to automatically determine which dimensions require reduction (all-reduce) and groups parameters into efficient communication buckets. Core Concepts Bucketing & Flattening Communication overhead is dominated by latency when reducing many small tensors. To mitigate this, GradientSynchronizer groups parameters into Buckets . Inside a SyncGradientBucket , gradients for multiple parameters are flattened into a single contiguous block of memory. When a reduction is triggered, the system performs a single all_reduce operation on this large buffer instead of hundreds of small operations. Parameters are grouped automatically based on: Device DType Associated DeviceMesh Asynchronous Reduction In large-scale training, effective batch size is often increased by accumulating gradients over multiple micro-batches before performing an optimizer step. This package manages the lifecycle of distributed DTensor gradients during this accumulation phase without simple no_sync context managers. Local Accumulation : During the backward pass of the first \\(N-1\\) micro-batches, local gradients are accumulated into the bucket's buffer. Conceptually, while the parameter DTensor is Replicate d, these intermediate local gradients also represent a Replicate (although contain different data) state across the Data Parallel mesh. Automatic Triggering : Each bucket maintains an internal counter. The all_reduce communication is only triggered when the specific parameter group has reached the require_accumulations count. This trigger happens automatically inside the backward hook of the last micro-batch, allowing communication to immediately overlap with the computation of remaining layers higher up in the model. This communication is made in a separate CUDA stream that should be awaited before using the gradients in your default stream. Synchronization : Once the asynchronous reduction completes, the flat buffer contains the globally summed gradient. Metadata of the contained parameter gradients is marked as Replicate , making them safe for the Optimizer to consume without involving synchronization later. d9d.internals.grad_sync Gradient synchronization utilities. This package provides the infrastructure for manual gradient bucketing and asynchronous reduction, similar to DistributedDataParallel but exposed for internal framework usage with DTensors. GradientSynchronizer Manages gradient synchronization for distributed training. This class handles the bucketing of parameters, memory allocation for flat gradient buffers, and the orchestration of asynchronous all-reduce operations during the backward pass. Source code in d9d/internals/grad_sync/synchronizer.py 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 class GradientSynchronizer : \"\"\" Manages gradient synchronization for distributed training. This class handles the bucketing of parameters, memory allocation for flat gradient buffers, and the orchestration of asynchronous all-reduce operations during the backward pass. \"\"\" def __init__ ( self , param_groups : list [ list [ nn . Parameter ]], bucket_size_mb : int , require_accumulations : int ): \"\"\" Constructs a GradientSynchronizer. Args: param_groups: List of parameter groups. bucket_size_mb: Maximal size of a single gradient bucket in MB. require_accumulations: Number of micro-batches to accumulate before reducing. \"\"\" self . _param_groups = param_groups self . _bucket_size_mb = bucket_size_mb self . _require_accumulations = require_accumulations self . _communicate_stream : torch . cuda . Stream | None = None self . _can_sync : bool self . _buckets : list [ AbstractGradientBucket ] = [] def bind ( self ): \"\"\" Initializes the synchronizer for training. Groups parameters, creates buckets, allocates memory, and registers hooks. Must be called before the backward pass. \"\"\" stream = torch . cuda . Stream () self . _communicate_stream = stream self . _buckets = _fill_buckets ( _group_params_for_buckets ( self . _param_groups ), bucket_size_mb = self . _bucket_size_mb , require_accumulations = self . _require_accumulations , communicate_stream = stream , ) for bucket in self . _buckets : bucket . bind () def unbind ( self ): \"\"\" Releases resources. Destroys buckets, frees memory buffers, and removes hooks. \"\"\" for bucket in self . _buckets : bucket . unbind () self . _buckets = [] self . _communicate_stream = None def wait ( self ): \"\"\" Waits for all bucket operations (async reductions) to complete. \"\"\" torch . cuda . current_stream () . wait_stream ( self . _communicate_stream ) for bucket in self . _buckets : bucket . mark_sync () def zero_grad ( self ): \"\"\" Resets gradients and accumulation counters for all managed parameters. \"\"\" for bucket in self . _buckets : bucket . zero_grad () __init__ ( param_groups , bucket_size_mb , require_accumulations ) Constructs a GradientSynchronizer. Parameters: Name Type Description Default param_groups list [ list [ Parameter ]] List of parameter groups. required bucket_size_mb int Maximal size of a single gradient bucket in MB. required require_accumulations int Number of micro-batches to accumulate before reducing. required Source code in d9d/internals/grad_sync/synchronizer.py 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 def __init__ ( self , param_groups : list [ list [ nn . Parameter ]], bucket_size_mb : int , require_accumulations : int ): \"\"\" Constructs a GradientSynchronizer. Args: param_groups: List of parameter groups. bucket_size_mb: Maximal size of a single gradient bucket in MB. require_accumulations: Number of micro-batches to accumulate before reducing. \"\"\" self . _param_groups = param_groups self . _bucket_size_mb = bucket_size_mb self . _require_accumulations = require_accumulations self . _communicate_stream : torch . cuda . Stream | None = None self . _can_sync : bool self . _buckets : list [ AbstractGradientBucket ] = [] bind () Initializes the synchronizer for training. Groups parameters, creates buckets, allocates memory, and registers hooks. Must be called before the backward pass. Source code in d9d/internals/grad_sync/synchronizer.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 def bind ( self ): \"\"\" Initializes the synchronizer for training. Groups parameters, creates buckets, allocates memory, and registers hooks. Must be called before the backward pass. \"\"\" stream = torch . cuda . Stream () self . _communicate_stream = stream self . _buckets = _fill_buckets ( _group_params_for_buckets ( self . _param_groups ), bucket_size_mb = self . _bucket_size_mb , require_accumulations = self . _require_accumulations , communicate_stream = stream , ) for bucket in self . _buckets : bucket . bind () unbind () Releases resources. Destroys buckets, frees memory buffers, and removes hooks. Source code in d9d/internals/grad_sync/synchronizer.py 222 223 224 225 226 227 228 229 230 231 232 233 def unbind ( self ): \"\"\" Releases resources. Destroys buckets, frees memory buffers, and removes hooks. \"\"\" for bucket in self . _buckets : bucket . unbind () self . _buckets = [] self . _communicate_stream = None wait () Waits for all bucket operations (async reductions) to complete. Source code in d9d/internals/grad_sync/synchronizer.py 235 236 237 238 239 240 241 242 243 def wait ( self ): \"\"\" Waits for all bucket operations (async reductions) to complete. \"\"\" torch . cuda . current_stream () . wait_stream ( self . _communicate_stream ) for bucket in self . _buckets : bucket . mark_sync () zero_grad () Resets gradients and accumulation counters for all managed parameters. Source code in d9d/internals/grad_sync/synchronizer.py 245 246 247 248 249 250 251 def zero_grad ( self ): \"\"\" Resets gradients and accumulation counters for all managed parameters. \"\"\" for bucket in self . _buckets : bucket . zero_grad ()","title":"Gradient Synchronization"},{"location":"internals/grad_sync/#about","text":"Warning: If you are utilizing the standard d9d training infrastructure, you do not need to call these functions manually. The framework automatically handles gradient synchronization. This package is primarily intended for users extending d9d . The d9d.internals.grad_sync package provides low-level primitives for synchronizing gradients in distributed training setups utilizing DTensor . Unlike standard PyTorch DistributedDataParallel which assumes a uniform communication strategy for the entire model, this package is designed to work with heterogeneous distributions often found in ND-parallelism (e.g., mixtures of Data, Tensor, Sequence, and Pipeline parallelism). It inspects DTensor placements to automatically determine which dimensions require reduction (all-reduce) and groups parameters into efficient communication buckets.","title":"About"},{"location":"internals/grad_sync/#core-concepts","text":"","title":"Core Concepts"},{"location":"internals/grad_sync/#bucketing-flattening","text":"Communication overhead is dominated by latency when reducing many small tensors. To mitigate this, GradientSynchronizer groups parameters into Buckets . Inside a SyncGradientBucket , gradients for multiple parameters are flattened into a single contiguous block of memory. When a reduction is triggered, the system performs a single all_reduce operation on this large buffer instead of hundreds of small operations. Parameters are grouped automatically based on: Device DType Associated DeviceMesh","title":"Bucketing &amp; Flattening"},{"location":"internals/grad_sync/#asynchronous-reduction","text":"In large-scale training, effective batch size is often increased by accumulating gradients over multiple micro-batches before performing an optimizer step. This package manages the lifecycle of distributed DTensor gradients during this accumulation phase without simple no_sync context managers. Local Accumulation : During the backward pass of the first \\(N-1\\) micro-batches, local gradients are accumulated into the bucket's buffer. Conceptually, while the parameter DTensor is Replicate d, these intermediate local gradients also represent a Replicate (although contain different data) state across the Data Parallel mesh. Automatic Triggering : Each bucket maintains an internal counter. The all_reduce communication is only triggered when the specific parameter group has reached the require_accumulations count. This trigger happens automatically inside the backward hook of the last micro-batch, allowing communication to immediately overlap with the computation of remaining layers higher up in the model. This communication is made in a separate CUDA stream that should be awaited before using the gradients in your default stream. Synchronization : Once the asynchronous reduction completes, the flat buffer contains the globally summed gradient. Metadata of the contained parameter gradients is marked as Replicate , making them safe for the Optimizer to consume without involving synchronization later.","title":"Asynchronous Reduction"},{"location":"internals/grad_sync/#d9d.internals.grad_sync","text":"Gradient synchronization utilities. This package provides the infrastructure for manual gradient bucketing and asynchronous reduction, similar to DistributedDataParallel but exposed for internal framework usage with DTensors.","title":"grad_sync"},{"location":"internals/grad_sync/#d9d.internals.grad_sync.GradientSynchronizer","text":"Manages gradient synchronization for distributed training. This class handles the bucketing of parameters, memory allocation for flat gradient buffers, and the orchestration of asynchronous all-reduce operations during the backward pass. Source code in d9d/internals/grad_sync/synchronizer.py 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 class GradientSynchronizer : \"\"\" Manages gradient synchronization for distributed training. This class handles the bucketing of parameters, memory allocation for flat gradient buffers, and the orchestration of asynchronous all-reduce operations during the backward pass. \"\"\" def __init__ ( self , param_groups : list [ list [ nn . Parameter ]], bucket_size_mb : int , require_accumulations : int ): \"\"\" Constructs a GradientSynchronizer. Args: param_groups: List of parameter groups. bucket_size_mb: Maximal size of a single gradient bucket in MB. require_accumulations: Number of micro-batches to accumulate before reducing. \"\"\" self . _param_groups = param_groups self . _bucket_size_mb = bucket_size_mb self . _require_accumulations = require_accumulations self . _communicate_stream : torch . cuda . Stream | None = None self . _can_sync : bool self . _buckets : list [ AbstractGradientBucket ] = [] def bind ( self ): \"\"\" Initializes the synchronizer for training. Groups parameters, creates buckets, allocates memory, and registers hooks. Must be called before the backward pass. \"\"\" stream = torch . cuda . Stream () self . _communicate_stream = stream self . _buckets = _fill_buckets ( _group_params_for_buckets ( self . _param_groups ), bucket_size_mb = self . _bucket_size_mb , require_accumulations = self . _require_accumulations , communicate_stream = stream , ) for bucket in self . _buckets : bucket . bind () def unbind ( self ): \"\"\" Releases resources. Destroys buckets, frees memory buffers, and removes hooks. \"\"\" for bucket in self . _buckets : bucket . unbind () self . _buckets = [] self . _communicate_stream = None def wait ( self ): \"\"\" Waits for all bucket operations (async reductions) to complete. \"\"\" torch . cuda . current_stream () . wait_stream ( self . _communicate_stream ) for bucket in self . _buckets : bucket . mark_sync () def zero_grad ( self ): \"\"\" Resets gradients and accumulation counters for all managed parameters. \"\"\" for bucket in self . _buckets : bucket . zero_grad ()","title":"GradientSynchronizer"},{"location":"internals/grad_sync/#d9d.internals.grad_sync.GradientSynchronizer.__init__","text":"Constructs a GradientSynchronizer. Parameters: Name Type Description Default param_groups list [ list [ Parameter ]] List of parameter groups. required bucket_size_mb int Maximal size of a single gradient bucket in MB. required require_accumulations int Number of micro-batches to accumulate before reducing. required Source code in d9d/internals/grad_sync/synchronizer.py 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 def __init__ ( self , param_groups : list [ list [ nn . Parameter ]], bucket_size_mb : int , require_accumulations : int ): \"\"\" Constructs a GradientSynchronizer. Args: param_groups: List of parameter groups. bucket_size_mb: Maximal size of a single gradient bucket in MB. require_accumulations: Number of micro-batches to accumulate before reducing. \"\"\" self . _param_groups = param_groups self . _bucket_size_mb = bucket_size_mb self . _require_accumulations = require_accumulations self . _communicate_stream : torch . cuda . Stream | None = None self . _can_sync : bool self . _buckets : list [ AbstractGradientBucket ] = []","title":"__init__"},{"location":"internals/grad_sync/#d9d.internals.grad_sync.GradientSynchronizer.bind","text":"Initializes the synchronizer for training. Groups parameters, creates buckets, allocates memory, and registers hooks. Must be called before the backward pass. Source code in d9d/internals/grad_sync/synchronizer.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 def bind ( self ): \"\"\" Initializes the synchronizer for training. Groups parameters, creates buckets, allocates memory, and registers hooks. Must be called before the backward pass. \"\"\" stream = torch . cuda . Stream () self . _communicate_stream = stream self . _buckets = _fill_buckets ( _group_params_for_buckets ( self . _param_groups ), bucket_size_mb = self . _bucket_size_mb , require_accumulations = self . _require_accumulations , communicate_stream = stream , ) for bucket in self . _buckets : bucket . bind ()","title":"bind"},{"location":"internals/grad_sync/#d9d.internals.grad_sync.GradientSynchronizer.unbind","text":"Releases resources. Destroys buckets, frees memory buffers, and removes hooks. Source code in d9d/internals/grad_sync/synchronizer.py 222 223 224 225 226 227 228 229 230 231 232 233 def unbind ( self ): \"\"\" Releases resources. Destroys buckets, frees memory buffers, and removes hooks. \"\"\" for bucket in self . _buckets : bucket . unbind () self . _buckets = [] self . _communicate_stream = None","title":"unbind"},{"location":"internals/grad_sync/#d9d.internals.grad_sync.GradientSynchronizer.wait","text":"Waits for all bucket operations (async reductions) to complete. Source code in d9d/internals/grad_sync/synchronizer.py 235 236 237 238 239 240 241 242 243 def wait ( self ): \"\"\" Waits for all bucket operations (async reductions) to complete. \"\"\" torch . cuda . current_stream () . wait_stream ( self . _communicate_stream ) for bucket in self . _buckets : bucket . mark_sync ()","title":"wait"},{"location":"internals/grad_sync/#d9d.internals.grad_sync.GradientSynchronizer.zero_grad","text":"Resets gradients and accumulation counters for all managed parameters. Source code in d9d/internals/grad_sync/synchronizer.py 245 246 247 248 249 250 251 def zero_grad ( self ): \"\"\" Resets gradients and accumulation counters for all managed parameters. \"\"\" for bucket in self . _buckets : bucket . zero_grad ()","title":"zero_grad"},{"location":"internals/metric_collector/","text":"About Warning: If you are using the standard d9d Trainer , you do not need to interact with this package directly. It is handled automatically. This documentation is intended for users implementing custom training loops or logging infrastructure. The d9d.internals.metric_collector package provides the infrastructure for non-blocking metric processing. While the Metric interface is synchronous by design, the AsyncMetricCollector wraps a metric instance and schedules its synchronization and computation on a secondary CUDA stream. This allows the main training loop to proceed immediately without waiting for metric reductions (all-reduce) to complete. d9d.internals.metric_collector AsyncMetricCollector Helper class to synchronize and compute metrics asynchronously on a separate CUDA stream. This class decouples metric synchronization and computation from the main training loop. It schedules the heavy lifting (distributed reduction and tensor operations) on a secondary stream. Source code in d9d/internals/metric_collector/collector.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 class AsyncMetricCollector : \"\"\"Helper class to synchronize and compute metrics asynchronously on a separate CUDA stream. This class decouples metric synchronization and computation from the main training loop. It schedules the heavy lifting (distributed reduction and tensor operations) on a secondary stream. \"\"\" def __init__ ( self , metric : Metric ): \"\"\"Constructs AsyncMetricCollector object. Args: metric: The metric instance to collect and compute asynchronously. \"\"\" self . _metric = metric self . _stream : torch . cuda . Stream | None = None self . _compute_buffer : PyTree [ torch . Tensor ] | None = None def bind ( self ): \"\"\"Moves the underlying metric to CUDA and initializes the side stream.\"\"\" self . _metric . to ( \"cuda\" ) self . _stream = torch . cuda . Stream () def unbind ( self ): \"\"\"Releases the reference to the side stream.\"\"\" self . _stream = None def schedule_collection ( self , dist_context : DistributedContext ): \"\"\"Schedules metric synchronization and computation on the side stream. This method records a dependency on the current stream to ensure all data required for the metric is available, then launches the synchronization (if distributed) and computation tasks on the dedicated side stream. Args: dist_context: Distributed context used for metric synchronization across ranks. Raises: RuntimeError: If the collector has not been bound via .bind(). \"\"\" if self . _stream is None : raise RuntimeError ( \"AsyncMetricSynchronizer is not bound. Call .bind() first.\" ) # depend on main stream self . _stream . wait_stream ( torch . cuda . current_stream ()) with torch . cuda . stream ( self . _stream ), record_function ( \"Async Metric Sync & Compute\" ): if dist_context . mesh_params . is_distributed : self . _metric . sync ( dist_context ) self . _compute_buffer = self . _metric . compute () def collect_results ( self ) -> PyTree [ float | int | bool ]: \"\"\"Waits for the async computation to finish and retrieves results. This method synchronizes the current stream with the side stream, moves results to CPU, converts them to Python scalars, and resets the underlying metric. Returns: A PyTree structure matching the metric's output containing python scalars (float, int, or bool) located on the CPU. Raises: RuntimeError: If the collector is not bound or if schedule_collection was not called prior to this method. \"\"\" if self . _stream is None : raise RuntimeError ( \"AsyncMetricSynchronizer is not bound. Call .bind() first.\" ) if self . _compute_buffer is None : raise RuntimeError ( \"sync_and_compute() was not called.\" ) # wait for synchronization and computation to finish torch . cuda . current_stream () . wait_stream ( self . _stream ) results = self . _compute_buffer self . _compute_buffer = None # sync to CPU results = pytree . tree_map ( lambda x : x . cpu (), results ) results = pytree . tree_map ( lambda x : x . item (), results ) # reset on GPU safely self . _metric . reset () return results __init__ ( metric ) Constructs AsyncMetricCollector object. Parameters: Name Type Description Default metric Metric The metric instance to collect and compute asynchronously. required Source code in d9d/internals/metric_collector/collector.py 18 19 20 21 22 23 24 25 26 def __init__ ( self , metric : Metric ): \"\"\"Constructs AsyncMetricCollector object. Args: metric: The metric instance to collect and compute asynchronously. \"\"\" self . _metric = metric self . _stream : torch . cuda . Stream | None = None self . _compute_buffer : PyTree [ torch . Tensor ] | None = None bind () Moves the underlying metric to CUDA and initializes the side stream. Source code in d9d/internals/metric_collector/collector.py 28 29 30 31 def bind ( self ): \"\"\"Moves the underlying metric to CUDA and initializes the side stream.\"\"\" self . _metric . to ( \"cuda\" ) self . _stream = torch . cuda . Stream () collect_results () Waits for the async computation to finish and retrieves results. This method synchronizes the current stream with the side stream, moves results to CPU, converts them to Python scalars, and resets the underlying metric. Returns: Type Description PyTree [ float | int | bool ] A PyTree structure matching the metric's output containing python scalars PyTree [ float | int | bool ] (float, int, or bool) located on the CPU. Raises: Type Description RuntimeError If the collector is not bound or if schedule_collection was not called prior to this method. Source code in d9d/internals/metric_collector/collector.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def collect_results ( self ) -> PyTree [ float | int | bool ]: \"\"\"Waits for the async computation to finish and retrieves results. This method synchronizes the current stream with the side stream, moves results to CPU, converts them to Python scalars, and resets the underlying metric. Returns: A PyTree structure matching the metric's output containing python scalars (float, int, or bool) located on the CPU. Raises: RuntimeError: If the collector is not bound or if schedule_collection was not called prior to this method. \"\"\" if self . _stream is None : raise RuntimeError ( \"AsyncMetricSynchronizer is not bound. Call .bind() first.\" ) if self . _compute_buffer is None : raise RuntimeError ( \"sync_and_compute() was not called.\" ) # wait for synchronization and computation to finish torch . cuda . current_stream () . wait_stream ( self . _stream ) results = self . _compute_buffer self . _compute_buffer = None # sync to CPU results = pytree . tree_map ( lambda x : x . cpu (), results ) results = pytree . tree_map ( lambda x : x . item (), results ) # reset on GPU safely self . _metric . reset () return results schedule_collection ( dist_context ) Schedules metric synchronization and computation on the side stream. This method records a dependency on the current stream to ensure all data required for the metric is available, then launches the synchronization (if distributed) and computation tasks on the dedicated side stream. Parameters: Name Type Description Default dist_context DistributedContext Distributed context used for metric synchronization across ranks. required Raises: Type Description RuntimeError If the collector has not been bound via .bind(). Source code in d9d/internals/metric_collector/collector.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def schedule_collection ( self , dist_context : DistributedContext ): \"\"\"Schedules metric synchronization and computation on the side stream. This method records a dependency on the current stream to ensure all data required for the metric is available, then launches the synchronization (if distributed) and computation tasks on the dedicated side stream. Args: dist_context: Distributed context used for metric synchronization across ranks. Raises: RuntimeError: If the collector has not been bound via .bind(). \"\"\" if self . _stream is None : raise RuntimeError ( \"AsyncMetricSynchronizer is not bound. Call .bind() first.\" ) # depend on main stream self . _stream . wait_stream ( torch . cuda . current_stream ()) with torch . cuda . stream ( self . _stream ), record_function ( \"Async Metric Sync & Compute\" ): if dist_context . mesh_params . is_distributed : self . _metric . sync ( dist_context ) self . _compute_buffer = self . _metric . compute () unbind () Releases the reference to the side stream. Source code in d9d/internals/metric_collector/collector.py 33 34 35 def unbind ( self ): \"\"\"Releases the reference to the side stream.\"\"\" self . _stream = None","title":"Metric Collection"},{"location":"internals/metric_collector/#about","text":"Warning: If you are using the standard d9d Trainer , you do not need to interact with this package directly. It is handled automatically. This documentation is intended for users implementing custom training loops or logging infrastructure. The d9d.internals.metric_collector package provides the infrastructure for non-blocking metric processing. While the Metric interface is synchronous by design, the AsyncMetricCollector wraps a metric instance and schedules its synchronization and computation on a secondary CUDA stream. This allows the main training loop to proceed immediately without waiting for metric reductions (all-reduce) to complete.","title":"About"},{"location":"internals/metric_collector/#d9d.internals.metric_collector","text":"","title":"metric_collector"},{"location":"internals/metric_collector/#d9d.internals.metric_collector.AsyncMetricCollector","text":"Helper class to synchronize and compute metrics asynchronously on a separate CUDA stream. This class decouples metric synchronization and computation from the main training loop. It schedules the heavy lifting (distributed reduction and tensor operations) on a secondary stream. Source code in d9d/internals/metric_collector/collector.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 class AsyncMetricCollector : \"\"\"Helper class to synchronize and compute metrics asynchronously on a separate CUDA stream. This class decouples metric synchronization and computation from the main training loop. It schedules the heavy lifting (distributed reduction and tensor operations) on a secondary stream. \"\"\" def __init__ ( self , metric : Metric ): \"\"\"Constructs AsyncMetricCollector object. Args: metric: The metric instance to collect and compute asynchronously. \"\"\" self . _metric = metric self . _stream : torch . cuda . Stream | None = None self . _compute_buffer : PyTree [ torch . Tensor ] | None = None def bind ( self ): \"\"\"Moves the underlying metric to CUDA and initializes the side stream.\"\"\" self . _metric . to ( \"cuda\" ) self . _stream = torch . cuda . Stream () def unbind ( self ): \"\"\"Releases the reference to the side stream.\"\"\" self . _stream = None def schedule_collection ( self , dist_context : DistributedContext ): \"\"\"Schedules metric synchronization and computation on the side stream. This method records a dependency on the current stream to ensure all data required for the metric is available, then launches the synchronization (if distributed) and computation tasks on the dedicated side stream. Args: dist_context: Distributed context used for metric synchronization across ranks. Raises: RuntimeError: If the collector has not been bound via .bind(). \"\"\" if self . _stream is None : raise RuntimeError ( \"AsyncMetricSynchronizer is not bound. Call .bind() first.\" ) # depend on main stream self . _stream . wait_stream ( torch . cuda . current_stream ()) with torch . cuda . stream ( self . _stream ), record_function ( \"Async Metric Sync & Compute\" ): if dist_context . mesh_params . is_distributed : self . _metric . sync ( dist_context ) self . _compute_buffer = self . _metric . compute () def collect_results ( self ) -> PyTree [ float | int | bool ]: \"\"\"Waits for the async computation to finish and retrieves results. This method synchronizes the current stream with the side stream, moves results to CPU, converts them to Python scalars, and resets the underlying metric. Returns: A PyTree structure matching the metric's output containing python scalars (float, int, or bool) located on the CPU. Raises: RuntimeError: If the collector is not bound or if schedule_collection was not called prior to this method. \"\"\" if self . _stream is None : raise RuntimeError ( \"AsyncMetricSynchronizer is not bound. Call .bind() first.\" ) if self . _compute_buffer is None : raise RuntimeError ( \"sync_and_compute() was not called.\" ) # wait for synchronization and computation to finish torch . cuda . current_stream () . wait_stream ( self . _stream ) results = self . _compute_buffer self . _compute_buffer = None # sync to CPU results = pytree . tree_map ( lambda x : x . cpu (), results ) results = pytree . tree_map ( lambda x : x . item (), results ) # reset on GPU safely self . _metric . reset () return results","title":"AsyncMetricCollector"},{"location":"internals/metric_collector/#d9d.internals.metric_collector.AsyncMetricCollector.__init__","text":"Constructs AsyncMetricCollector object. Parameters: Name Type Description Default metric Metric The metric instance to collect and compute asynchronously. required Source code in d9d/internals/metric_collector/collector.py 18 19 20 21 22 23 24 25 26 def __init__ ( self , metric : Metric ): \"\"\"Constructs AsyncMetricCollector object. Args: metric: The metric instance to collect and compute asynchronously. \"\"\" self . _metric = metric self . _stream : torch . cuda . Stream | None = None self . _compute_buffer : PyTree [ torch . Tensor ] | None = None","title":"__init__"},{"location":"internals/metric_collector/#d9d.internals.metric_collector.AsyncMetricCollector.bind","text":"Moves the underlying metric to CUDA and initializes the side stream. Source code in d9d/internals/metric_collector/collector.py 28 29 30 31 def bind ( self ): \"\"\"Moves the underlying metric to CUDA and initializes the side stream.\"\"\" self . _metric . to ( \"cuda\" ) self . _stream = torch . cuda . Stream ()","title":"bind"},{"location":"internals/metric_collector/#d9d.internals.metric_collector.AsyncMetricCollector.collect_results","text":"Waits for the async computation to finish and retrieves results. This method synchronizes the current stream with the side stream, moves results to CPU, converts them to Python scalars, and resets the underlying metric. Returns: Type Description PyTree [ float | int | bool ] A PyTree structure matching the metric's output containing python scalars PyTree [ float | int | bool ] (float, int, or bool) located on the CPU. Raises: Type Description RuntimeError If the collector is not bound or if schedule_collection was not called prior to this method. Source code in d9d/internals/metric_collector/collector.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def collect_results ( self ) -> PyTree [ float | int | bool ]: \"\"\"Waits for the async computation to finish and retrieves results. This method synchronizes the current stream with the side stream, moves results to CPU, converts them to Python scalars, and resets the underlying metric. Returns: A PyTree structure matching the metric's output containing python scalars (float, int, or bool) located on the CPU. Raises: RuntimeError: If the collector is not bound or if schedule_collection was not called prior to this method. \"\"\" if self . _stream is None : raise RuntimeError ( \"AsyncMetricSynchronizer is not bound. Call .bind() first.\" ) if self . _compute_buffer is None : raise RuntimeError ( \"sync_and_compute() was not called.\" ) # wait for synchronization and computation to finish torch . cuda . current_stream () . wait_stream ( self . _stream ) results = self . _compute_buffer self . _compute_buffer = None # sync to CPU results = pytree . tree_map ( lambda x : x . cpu (), results ) results = pytree . tree_map ( lambda x : x . item (), results ) # reset on GPU safely self . _metric . reset () return results","title":"collect_results"},{"location":"internals/metric_collector/#d9d.internals.metric_collector.AsyncMetricCollector.schedule_collection","text":"Schedules metric synchronization and computation on the side stream. This method records a dependency on the current stream to ensure all data required for the metric is available, then launches the synchronization (if distributed) and computation tasks on the dedicated side stream. Parameters: Name Type Description Default dist_context DistributedContext Distributed context used for metric synchronization across ranks. required Raises: Type Description RuntimeError If the collector has not been bound via .bind(). Source code in d9d/internals/metric_collector/collector.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def schedule_collection ( self , dist_context : DistributedContext ): \"\"\"Schedules metric synchronization and computation on the side stream. This method records a dependency on the current stream to ensure all data required for the metric is available, then launches the synchronization (if distributed) and computation tasks on the dedicated side stream. Args: dist_context: Distributed context used for metric synchronization across ranks. Raises: RuntimeError: If the collector has not been bound via .bind(). \"\"\" if self . _stream is None : raise RuntimeError ( \"AsyncMetricSynchronizer is not bound. Call .bind() first.\" ) # depend on main stream self . _stream . wait_stream ( torch . cuda . current_stream ()) with torch . cuda . stream ( self . _stream ), record_function ( \"Async Metric Sync & Compute\" ): if dist_context . mesh_params . is_distributed : self . _metric . sync ( dist_context ) self . _compute_buffer = self . _metric . compute ()","title":"schedule_collection"},{"location":"internals/metric_collector/#d9d.internals.metric_collector.AsyncMetricCollector.unbind","text":"Releases the reference to the side stream. Source code in d9d/internals/metric_collector/collector.py 33 34 35 def unbind ( self ): \"\"\"Releases the reference to the side stream.\"\"\" self . _stream = None","title":"unbind"},{"location":"internals/pipeline_state/","text":"Warning: If you are utilizing the standard d9d training infrastructure, you do not need to manage pipeline states manually. The framework automatically handles this. This package is primarily intended for users extending d9d . About The d9d.internals.pipeline_state package provides a unified mechanism to manage data lifecycle within a training step. It specifically addresses the complexity of transitioning between the Global Context (an entire training step/batch) and the Sharded Context (partial execution, i.e. within pipeline parallel loss computation). For instance, a typical data flow in a pipelined step is: Prepare the data using a global view. Compute loss value for a microbatch, it now requires to create a sharded view of the data. Log metrics, using a global view again. PipelineState abstracts the slicing (Global -> Sharded) and aggregation (Sharded -> Global) operations behind a simple dictionary-like interface, allowing the training loop to act as a seamless bridge between these two contexts. d9d.internals.pipeline_state Pipeline State management package. This package provides mechanisms to store, retrieve, and synchronize state across different stages of a distributed pipeline, providing global and sharded view for these states. PipelineState Bases: ABC Object representing the state of a pipeline. This class defines the interface for accessing state variables like a dictionary, abstracting away whether the underlying storage is local, sharded, or global. Source code in d9d/internals/pipeline_state/api.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 class PipelineState ( abc . ABC ): \"\"\" Object representing the state of a pipeline. This class defines the interface for accessing state variables like a dictionary, abstracting away whether the underlying storage is local, sharded, or global. \"\"\" @abc . abstractmethod def __setitem__ ( self , key : str , value : Any ): \"\"\" Sets a state value for a given key. Args: key: The identifier for the state variable. value: The value to store. \"\"\" @abc . abstractmethod def __getitem__ ( self , item : str ) -> Any : \"\"\" Retrieves a state value for a given key. Args: item: The identifier for the state variable. Returns: The value associated with the key. \"\"\" @abc . abstractmethod def __contains__ ( self , item : str ) -> bool : \"\"\" Checks if a key exists in the state. Args: item: The identifier to check. Returns: True if the key exists, False otherwise. \"\"\" __contains__ ( item ) abstractmethod Checks if a key exists in the state. Parameters: Name Type Description Default item str The identifier to check. required Returns: Type Description bool True if the key exists, False otherwise. Source code in d9d/internals/pipeline_state/api.py 35 36 37 38 39 40 41 42 43 44 45 @abc . abstractmethod def __contains__ ( self , item : str ) -> bool : \"\"\" Checks if a key exists in the state. Args: item: The identifier to check. Returns: True if the key exists, False otherwise. \"\"\" __getitem__ ( item ) abstractmethod Retrieves a state value for a given key. Parameters: Name Type Description Default item str The identifier for the state variable. required Returns: Type Description Any The value associated with the key. Source code in d9d/internals/pipeline_state/api.py 23 24 25 26 27 28 29 30 31 32 33 @abc . abstractmethod def __getitem__ ( self , item : str ) -> Any : \"\"\" Retrieves a state value for a given key. Args: item: The identifier for the state variable. Returns: The value associated with the key. \"\"\" __setitem__ ( key , value ) abstractmethod Sets a state value for a given key. Parameters: Name Type Description Default key str The identifier for the state variable. required value Any The value to store. required Source code in d9d/internals/pipeline_state/api.py 13 14 15 16 17 18 19 20 21 @abc . abstractmethod def __setitem__ ( self , key : str , value : Any ): \"\"\" Sets a state value for a given key. Args: key: The identifier for the state variable. value: The value to store. \"\"\" PipelineStateHandler Manages the lifecycle and access patterns of pipeline states. This handler initializes the underlying storage and provides specific views (global or sharded) into that storage. Source code in d9d/internals/pipeline_state/handler.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 class PipelineStateHandler : \"\"\" Manages the lifecycle and access patterns of pipeline states. This handler initializes the underlying storage and provides specific views (global or sharded) into that storage. \"\"\" def __init__ ( self , sharding_spec : dict [ str , ShardingSpecLeaf ], num_shards : int ): \"\"\" Constructs a PipelineStateHandler object. Args: sharding_spec: A definition of how specific keys should be sharded. num_shards: The total number of shards in the pipeline. \"\"\" self . _storage = PipelineStateStorage ( sharding_spec = {( k ,): v for k , v in sharding_spec . items ()}, num_shards = num_shards ) def global_state ( self ) -> PipelineState : \"\"\" Returns a view interface for accessing global state. Returns: A PipelineState interface that accesses the full, aggregated data. \"\"\" return PipelineStateGlobal ( self . _storage ) def sharded_state ( self , shard_id : int ) -> PipelineState : \"\"\" Returns a view interface for accessing state specific to a shard ID. Args: shard_id: The index of the shard to access. Returns: A PipelineState interface that accesses partial data for the given shard. \"\"\" return PipelineStateShard ( self . _storage , shard_id ) def reset ( self ): \"\"\" Resets the underlying storage, clearing all state. \"\"\" self . _storage . reset () __init__ ( sharding_spec , num_shards ) Constructs a PipelineStateHandler object. Parameters: Name Type Description Default sharding_spec dict [ str , ShardingSpecLeaf ] A definition of how specific keys should be sharded. required num_shards int The total number of shards in the pipeline. required Source code in d9d/internals/pipeline_state/handler.py 69 70 71 72 73 74 75 76 77 78 79 80 def __init__ ( self , sharding_spec : dict [ str , ShardingSpecLeaf ], num_shards : int ): \"\"\" Constructs a PipelineStateHandler object. Args: sharding_spec: A definition of how specific keys should be sharded. num_shards: The total number of shards in the pipeline. \"\"\" self . _storage = PipelineStateStorage ( sharding_spec = {( k ,): v for k , v in sharding_spec . items ()}, num_shards = num_shards ) global_state () Returns a view interface for accessing global state. Returns: Type Description PipelineState A PipelineState interface that accesses the full, aggregated data. Source code in d9d/internals/pipeline_state/handler.py 82 83 84 85 86 87 88 89 90 def global_state ( self ) -> PipelineState : \"\"\" Returns a view interface for accessing global state. Returns: A PipelineState interface that accesses the full, aggregated data. \"\"\" return PipelineStateGlobal ( self . _storage ) reset () Resets the underlying storage, clearing all state. Source code in d9d/internals/pipeline_state/handler.py 105 106 107 108 109 110 def reset ( self ): \"\"\" Resets the underlying storage, clearing all state. \"\"\" self . _storage . reset () sharded_state ( shard_id ) Returns a view interface for accessing state specific to a shard ID. Parameters: Name Type Description Default shard_id int The index of the shard to access. required Returns: Type Description PipelineState A PipelineState interface that accesses partial data for the given shard. Source code in d9d/internals/pipeline_state/handler.py 92 93 94 95 96 97 98 99 100 101 102 103 def sharded_state ( self , shard_id : int ) -> PipelineState : \"\"\" Returns a view interface for accessing state specific to a shard ID. Args: shard_id: The index of the shard to access. Returns: A PipelineState interface that accesses partial data for the given shard. \"\"\" return PipelineStateShard ( self . _storage , shard_id )","title":"Pipeline State Management"},{"location":"internals/pipeline_state/#about","text":"The d9d.internals.pipeline_state package provides a unified mechanism to manage data lifecycle within a training step. It specifically addresses the complexity of transitioning between the Global Context (an entire training step/batch) and the Sharded Context (partial execution, i.e. within pipeline parallel loss computation). For instance, a typical data flow in a pipelined step is: Prepare the data using a global view. Compute loss value for a microbatch, it now requires to create a sharded view of the data. Log metrics, using a global view again. PipelineState abstracts the slicing (Global -> Sharded) and aggregation (Sharded -> Global) operations behind a simple dictionary-like interface, allowing the training loop to act as a seamless bridge between these two contexts.","title":"About"},{"location":"internals/pipeline_state/#d9d.internals.pipeline_state","text":"Pipeline State management package. This package provides mechanisms to store, retrieve, and synchronize state across different stages of a distributed pipeline, providing global and sharded view for these states.","title":"pipeline_state"},{"location":"internals/pipeline_state/#d9d.internals.pipeline_state.PipelineState","text":"Bases: ABC Object representing the state of a pipeline. This class defines the interface for accessing state variables like a dictionary, abstracting away whether the underlying storage is local, sharded, or global. Source code in d9d/internals/pipeline_state/api.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 class PipelineState ( abc . ABC ): \"\"\" Object representing the state of a pipeline. This class defines the interface for accessing state variables like a dictionary, abstracting away whether the underlying storage is local, sharded, or global. \"\"\" @abc . abstractmethod def __setitem__ ( self , key : str , value : Any ): \"\"\" Sets a state value for a given key. Args: key: The identifier for the state variable. value: The value to store. \"\"\" @abc . abstractmethod def __getitem__ ( self , item : str ) -> Any : \"\"\" Retrieves a state value for a given key. Args: item: The identifier for the state variable. Returns: The value associated with the key. \"\"\" @abc . abstractmethod def __contains__ ( self , item : str ) -> bool : \"\"\" Checks if a key exists in the state. Args: item: The identifier to check. Returns: True if the key exists, False otherwise. \"\"\"","title":"PipelineState"},{"location":"internals/pipeline_state/#d9d.internals.pipeline_state.PipelineState.__contains__","text":"Checks if a key exists in the state. Parameters: Name Type Description Default item str The identifier to check. required Returns: Type Description bool True if the key exists, False otherwise. Source code in d9d/internals/pipeline_state/api.py 35 36 37 38 39 40 41 42 43 44 45 @abc . abstractmethod def __contains__ ( self , item : str ) -> bool : \"\"\" Checks if a key exists in the state. Args: item: The identifier to check. Returns: True if the key exists, False otherwise. \"\"\"","title":"__contains__"},{"location":"internals/pipeline_state/#d9d.internals.pipeline_state.PipelineState.__getitem__","text":"Retrieves a state value for a given key. Parameters: Name Type Description Default item str The identifier for the state variable. required Returns: Type Description Any The value associated with the key. Source code in d9d/internals/pipeline_state/api.py 23 24 25 26 27 28 29 30 31 32 33 @abc . abstractmethod def __getitem__ ( self , item : str ) -> Any : \"\"\" Retrieves a state value for a given key. Args: item: The identifier for the state variable. Returns: The value associated with the key. \"\"\"","title":"__getitem__"},{"location":"internals/pipeline_state/#d9d.internals.pipeline_state.PipelineState.__setitem__","text":"Sets a state value for a given key. Parameters: Name Type Description Default key str The identifier for the state variable. required value Any The value to store. required Source code in d9d/internals/pipeline_state/api.py 13 14 15 16 17 18 19 20 21 @abc . abstractmethod def __setitem__ ( self , key : str , value : Any ): \"\"\" Sets a state value for a given key. Args: key: The identifier for the state variable. value: The value to store. \"\"\"","title":"__setitem__"},{"location":"internals/pipeline_state/#d9d.internals.pipeline_state.PipelineStateHandler","text":"Manages the lifecycle and access patterns of pipeline states. This handler initializes the underlying storage and provides specific views (global or sharded) into that storage. Source code in d9d/internals/pipeline_state/handler.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 class PipelineStateHandler : \"\"\" Manages the lifecycle and access patterns of pipeline states. This handler initializes the underlying storage and provides specific views (global or sharded) into that storage. \"\"\" def __init__ ( self , sharding_spec : dict [ str , ShardingSpecLeaf ], num_shards : int ): \"\"\" Constructs a PipelineStateHandler object. Args: sharding_spec: A definition of how specific keys should be sharded. num_shards: The total number of shards in the pipeline. \"\"\" self . _storage = PipelineStateStorage ( sharding_spec = {( k ,): v for k , v in sharding_spec . items ()}, num_shards = num_shards ) def global_state ( self ) -> PipelineState : \"\"\" Returns a view interface for accessing global state. Returns: A PipelineState interface that accesses the full, aggregated data. \"\"\" return PipelineStateGlobal ( self . _storage ) def sharded_state ( self , shard_id : int ) -> PipelineState : \"\"\" Returns a view interface for accessing state specific to a shard ID. Args: shard_id: The index of the shard to access. Returns: A PipelineState interface that accesses partial data for the given shard. \"\"\" return PipelineStateShard ( self . _storage , shard_id ) def reset ( self ): \"\"\" Resets the underlying storage, clearing all state. \"\"\" self . _storage . reset ()","title":"PipelineStateHandler"},{"location":"internals/pipeline_state/#d9d.internals.pipeline_state.PipelineStateHandler.__init__","text":"Constructs a PipelineStateHandler object. Parameters: Name Type Description Default sharding_spec dict [ str , ShardingSpecLeaf ] A definition of how specific keys should be sharded. required num_shards int The total number of shards in the pipeline. required Source code in d9d/internals/pipeline_state/handler.py 69 70 71 72 73 74 75 76 77 78 79 80 def __init__ ( self , sharding_spec : dict [ str , ShardingSpecLeaf ], num_shards : int ): \"\"\" Constructs a PipelineStateHandler object. Args: sharding_spec: A definition of how specific keys should be sharded. num_shards: The total number of shards in the pipeline. \"\"\" self . _storage = PipelineStateStorage ( sharding_spec = {( k ,): v for k , v in sharding_spec . items ()}, num_shards = num_shards )","title":"__init__"},{"location":"internals/pipeline_state/#d9d.internals.pipeline_state.PipelineStateHandler.global_state","text":"Returns a view interface for accessing global state. Returns: Type Description PipelineState A PipelineState interface that accesses the full, aggregated data. Source code in d9d/internals/pipeline_state/handler.py 82 83 84 85 86 87 88 89 90 def global_state ( self ) -> PipelineState : \"\"\" Returns a view interface for accessing global state. Returns: A PipelineState interface that accesses the full, aggregated data. \"\"\" return PipelineStateGlobal ( self . _storage )","title":"global_state"},{"location":"internals/pipeline_state/#d9d.internals.pipeline_state.PipelineStateHandler.reset","text":"Resets the underlying storage, clearing all state. Source code in d9d/internals/pipeline_state/handler.py 105 106 107 108 109 110 def reset ( self ): \"\"\" Resets the underlying storage, clearing all state. \"\"\" self . _storage . reset ()","title":"reset"},{"location":"internals/pipeline_state/#d9d.internals.pipeline_state.PipelineStateHandler.sharded_state","text":"Returns a view interface for accessing state specific to a shard ID. Parameters: Name Type Description Default shard_id int The index of the shard to access. required Returns: Type Description PipelineState A PipelineState interface that accesses partial data for the given shard. Source code in d9d/internals/pipeline_state/handler.py 92 93 94 95 96 97 98 99 100 101 102 103 def sharded_state ( self , shard_id : int ) -> PipelineState : \"\"\" Returns a view interface for accessing state specific to a shard ID. Args: shard_id: The index of the shard to access. Returns: A PipelineState interface that accesses partial data for the given shard. \"\"\" return PipelineStateShard ( self . _storage , shard_id )","title":"sharded_state"},{"location":"internals/pipelining/","text":"This section details the internals of the d9d.pipelining module. It is intended for those who wish to implement new layouts, schedules, or modify the execution engine. Architecture The Idea d9d decouples the Schedule Structure from the Runtime Execution . You write a builder (e.g., 1F1B , DualPipe ) that generates a linear list of logical Actions (e.g., Forward(Stage=0, MB=1) , Backward(Stage=0, MB=0) ). If you want, d9d may automatically inject Send / Recv actions into your compute-only schedule based on data dependencies, preventing deadlocks. You run a dumb virtual machine simply iterates the action list and executes them. This makes implementing complex research schedules (like Zero Bubble or DualPipeV) significantly easier than managing state machines or recursive calls. Core Components PipelineStage ( infra/stage/stage.py ) Encapsulates a user nn.Module . It is not responsible for deciding when to run. Instead, it provides atomic pipeline stage capabilities (such as forward and backward passes) to the actions and the executor. Consists of: Computation Handlers : ForwardComputeHandler : Performs forward pass, caches inputs/outputs for backward passes. BackwardComputeHandler : Performs backward pass, capable of splitting backward passes into backward_input (dI) and backward_weight (dW) for advanced schedules. Communication Handlers : Contain and manage the P2P buffers for both forward and backward passes. Actions ( infra/schedule/component/runtime/action.py ) The atomic instructions for the pipeline virtual machine. ForwardComputeAction : Run forward on specific microbatch. BackwardFullInputComputeAction : Run backward. Can be configured to compute gradients for inputs-only or inputs+weights. BackwardWeightComputeAction : Compute gradients for weights (used in Zero Bubble schedules). ForwardSendAction / ForwardReceiveAction / BackwardSendAction / BackwardReceiveAction : Network IO. ComposeAction : Composes multiple actions into a single one. Used for Forward/Backward overlap in schedules such as DualPipeV. Actions are designed to be declarative and immutable. Programs A Program is simply dict[int, list[ActionBase]] \u2014 a mapping of Rank ID to a sequential list of Actions. Executor ( infra/schedule/component/runtime/executor.py ) The PipelineScheduleExecutor is the runtime engine. It: Shards global inputs into microbatches. Iterates through the Program action list. Dispatches calls to Action s that perform computation or communication workload. Comparison with PyTorch The d9d pipelining implementation is heavily inspired by and borrows concepts from the torch.distributed.pipelining API (e.g., ZeroBubble implementation), but refactors the codebase significantly to improve clarity, type safety, and modularity. The main architectural differences lie in the strict separation of concerns and composition over inheritance : Decomposed Stage Logic : PyTorch : Uses a monolithic _PipelineStageBase class that simultaneously manages P2P buffer allocation, gradient accumulation state, and forward/backward execution logic. d9d : Adopts a compositional approach. The PipelineStage class is a thin orchestrator that delegates responsibilities to dedicated handlers. Polymorphic Actions vs Enumeration : PyTorch : Represents schedule instructions using a single generic _Action NamedTuple combined with an Enum ( _ComputationType.FORWARD , _ComputationType.SEND_F , etc.). d9d : Uses a class hierarchy for actions ( ForwardComputeAction , ForwardSendAction , ComposeAction ). This allows the runtime executor to use structural pattern matching ( match/case ) rather than large if/elif blocks checking enums, allows different actions to carry different metadata (e.g. full_backward flag), and improves static type checking. Builder Pattern vs Schedule Classes : PyTorch : Often couples the schedule definition with the runtime object (e.g., Schedule1F1B class contains both the logic to generate the ordering and the logic to execute it). d9d : Strictly separates the Program Builder (which generates the list of actions) from the Executor (which runs the actions). This makes it easier to inspect a schedule plan before execution or swap scheduling algorithms without changing the runtime driver. Building Custom Schedules To build a new schedule, you create a PipelineProgramBuilder . Implement the Builder You must implement the pipeline program builder. from collections import defaultdict from d9d.pipelining.infra.schedule.component.program import PipelineProgramBuilder , build_stage_to_host_rank_topology , ScheduleStyle , add_communication_ops from d9d.pipelining.infra.schedule.component.runtime import ActionBase , ForwardComputeAction class MyFancyScheduleBuilder ( PipelineProgramBuilder ): def __init__ ( self , stages_per_rank : int ): self . _stages_per_rank = stages_per_rank @property def num_stages_per_rank ( self ) -> int : return self . _stages_per_rank @property def topology_style ( self ) -> ScheduleStyle : return ScheduleStyle . loop def compose ( self , num_microbatches : int , pp_size : int ) -> dict [ int , list [ ActionBase ]]: # Map logical stages to ranks stage_to_rank = build_stage_to_host_rank_topology ( num_stages = self . _stages_per_rank * pp_size , style = ScheduleStyle . loop , pp_size = pp_size ) actions = defaultdict ( list ) # 1. Generate Compute Schedule for rank in range ( pp_size ): # ... custom logic to decide order of Fwd/Bwd ... actions [ rank ] . append ( ForwardComputeAction ( stage_idx =... , microbatch_idx =... )) # 2. Inject Communications (Magic Pass) # This analyzes data dependencies between stages and inserts Send/Recvs return add_communication_ops ( actions , stage_to_rank , num_stages = self . _stages_per_rank * pp_size ) Registering Add your configuration to factory/config.py and register the builder in factory/factory.py . d9d.pipelining.infra.stage PipelineStage Represents a single structural stage in a Pipelined Model. This class acts as an orchestrator that combines StageCommunicationHandler (for I/O) and Forward/BackwardComputeHandler (for execution). It abstracts away the complexity of buffer management, distributed communication, and gradient calculation from the scheduler. Source code in d9d/pipelining/infra/stage/stage.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 class PipelineStage : \"\"\" Represents a single structural stage in a Pipelined Model. This class acts as an orchestrator that combines `StageCommunicationHandler` (for I/O) and `Forward/BackwardComputeHandler` (for execution). It abstracts away the complexity of buffer management, distributed communication, and gradient calculation from the scheduler. \"\"\" def __init__ ( self , info : PipelineStageInfo , module : nn . Module , group : dist . ProcessGroup , stage_to_host_topology : dict [ int , int ], ): \"\"\" Constructs a PipelineStage object. Args: info: Metadata about the stage (index, total stages). module: The PyTorch module executed by this stage. group: The distributed process group for pipeline communications. stage_to_host_topology: Dict mapping stage ID to PP rank hosting it. \"\"\" self . _info = info self . _module = module self . _group = group self . _stage_to_host_topology = stage_to_host_topology self . _has_backward = False self . _forward_comm : StageCommunicationHandler | None = None self . _backward_comm : StageCommunicationHandler | None = None self . _forward_comp = ForwardComputeHandler ( stage_index = info . current_stage , module = module ) self . _backward_comp = BackwardComputeHandler ( stage_index = info . current_stage , module = module ) @property def info ( self ) -> PipelineStageInfo : return self . _info def configure_buffers ( self , num_microbatches : int , has_backward : bool , pipeline_inputs : dict [ str , torch . Tensor ]): \"\"\" Initializes the communication handlers and buffers for the stage. This must be called before execution to establish P2P buffer sizes and directions. Args: num_microbatches: Total number of microbatches to process. has_backward: Does this pipeline stage should store info for a backward pass pipeline_inputs: Pipeline input data. \"\"\" self . _has_backward = has_backward prev_stage_idx = None if self . _info . is_current_stage_first else self . _info . current_stage - 1 next_stage_idx = None if self . _info . is_current_stage_last else self . _info . current_stage + 1 with torch . device ( \"meta\" ): if not isinstance ( self . _module , ModuleSupportsPipelining ): raise TypeError ( \"Module does not implement ModuleSupportsPipelining protocol\" ) inputs_meta = self . _module . infer_stage_inputs_from_pipeline_inputs ( inputs = pipeline_inputs , n_microbatches = num_microbatches ) outputs_meta = self . _module . infer_stage_outputs_from_pipeline_inputs ( inputs = pipeline_inputs , n_microbatches = num_microbatches ) self . _forward_comm = StageCommunicationHandler ( name = \"fwd\" , stage_index = self . _info . current_stage , num_microbatches = num_microbatches , input_stage_index = prev_stage_idx , input_args = inputs_meta , output_stage_index = next_stage_idx , output_args = outputs_meta , group = self . _group , stage_idx_to_host_rank = self . _stage_to_host_topology , ) self . _forward_comm . set_input_requires_grad_ ( requires_grad = has_backward ) if has_backward : # for grad - current stage receives OUTPUTS as inputs and sends INPUTS as outputs # because it is reversed forward self . _backward_comm = StageCommunicationHandler ( name = \"bwd\" , stage_index = self . _info . current_stage , num_microbatches = num_microbatches , input_stage_index = next_stage_idx , input_args = outputs_meta , output_stage_index = prev_stage_idx , output_args = inputs_meta , group = self . _group , stage_idx_to_host_rank = self . _stage_to_host_topology , ) else : self . _backward_comm = None def set_local_fwd_input ( self , inputs : dict [ str , torch . Tensor ], microbatch_index : int ): \"\"\" Sets local forward inputs manually. Used for the V-shape schedulers. \"\"\" if self . _forward_comm is None : raise ValueError ( \"You must configure stage buffers first\" ) self . _forward_comm . set_inputs_local ( inputs , microbatch_index ) def get_local_fwd_output ( self , microbatch_index : int ) -> dict [ str , torch . Tensor ]: return self . _forward_comp . get_outputs ( microbatch_index ) def pop_local_bwd_output ( self , microbatch_index : int ) -> dict [ str , torch . Tensor ]: \"\"\" Retrieves local backward outputs (gradients). \"\"\" if not self . _has_backward : raise ValueError () return self . _backward_comp . pop_for_sending ( microbatch_index ) def set_local_bwd_input ( self , inputs : dict [ str , torch . Tensor ], microbatch_index : int ): \"\"\" Sets local backward inputs (output gradients) manually. \"\"\" if not self . _has_backward : raise ValueError () if self . _backward_comm is None : raise ValueError ( \"You must configure stage buffers first\" ) self . _backward_comm . set_inputs_local ( inputs , microbatch_index ) def get_fwd_recv_ops ( self , microbatch_index : int ) -> list [ dist . P2POp ]: \"\"\"Returns P2P ops to receive forward inputs for the given microbatch.\"\"\" if self . _forward_comm is None : raise ValueError ( \"You must configure stage buffers first\" ) return self . _forward_comm . create_receive_ops ( microbatch_index ) def get_fwd_send_ops ( self , microbatch_index : int ) -> list [ dist . P2POp ]: \"\"\"Returns P2P ops to send forward outputs for the given microbatch.\"\"\" if self . _forward_comm is None : raise ValueError ( \"You must configure stage buffers first\" ) fwd_result = self . _forward_comp . get_outputs ( microbatch_index ) return self . _forward_comm . create_send_ops ( fwd_result ) def get_bwd_recv_ops ( self , microbatch_index : int ) -> list [ dist . P2POp ]: \"\"\"Returns P2P ops to receive backward gradients for the given microbatch.\"\"\" if not self . _has_backward : return [] if self . _backward_comm is None : raise ValueError ( \"You must configure stage buffers first\" ) return self . _backward_comm . create_receive_ops ( microbatch_index ) def get_bwd_send_ops ( self , microbatch_index : int ) -> list [ dist . P2POp ]: \"\"\"Returns P2P ops to send backward gradients for the given microbatch.\"\"\" if not self . _has_backward : return [] if self . _backward_comm is None : raise ValueError ( \"You must configure stage buffers first\" ) bwd_result = self . _backward_comp . pop_for_sending ( microbatch_index ) return self . _backward_comm . create_send_ops ( bwd_result ) def forward_one_chunk ( self , microbatch_index : int , pipeline_inputs : dict [ str , torch . Tensor ], pipeline_kwargs : dict [ str , Any ] | None = None , ): \"\"\" Executes a forward pass for a single microbatch chunk. Fetches inputs from the communication buffer (or `pipeline_inputs` if first stage), runs the computation, and caches the result. Args: microbatch_index: The microbatch index. pipeline_inputs: Inputs provided locally (only used if this is the first stage). pipeline_kwargs: Additional arguments for the module. Returns: The output tensors of the forward pass. \"\"\" if self . _forward_comm is None : raise ValueError ( \"You must configure stage buffers first\" ) if self . _info . is_current_stage_first : inputs = pipeline_inputs else : inputs = self . _forward_comm . get_inputs ( microbatch_index ) kwargs = pipeline_kwargs or {} self . _forward_comp . run ( microbatch_index = microbatch_index , inputs = inputs , kwargs = kwargs ) def backward_one_chunk ( self , microbatch_index : int , loss : torch . Tensor | None = None , full_backward : bool = True ): \"\"\" Executes a backward pass for a single microbatch chunk. Can perform either a full backward or just the input gradients (if `full_backward=False`). It fetches required data from forward cache and communication buffers. Args: microbatch_index: The microbatch index. loss: The loss tensor (only used if this is the last stage). full_backward: If True, computes grads for inputs and weights. If False, only for inputs. \"\"\" if not self . _has_backward : raise ValueError () if self . _backward_comm is None : raise ValueError ( \"You must configure stage buffers first\" ) inputs , fwd_outputs = self . _forward_comp . pop_inputs_outputs ( microbatch_index ) outputs : dict [ str , torch . Tensor ] outputs_grad : dict [ str , torch . Tensor ] | None if self . _info . is_current_stage_last : if loss is None : raise ValueError ( \"Cannot perform backward on last stage without loss specified\" ) outputs = { \"loss\" : loss } outputs_grad = None else : outputs = fwd_outputs outputs_grad = self . _backward_comm . get_inputs ( microbatch_index ) if full_backward : self . _backward_comp . backward_full ( microbatch_index = microbatch_index , inputs = inputs , outputs = outputs , outputs_grad = outputs_grad ) else : self . _backward_comp . backward_input ( microbatch_index = microbatch_index , inputs = inputs , outputs = outputs , outputs_grad = outputs_grad ) if self . _info . is_current_stage_last and not self . _info . is_current_stage_first : for t in fwd_outputs . values (): if not t . _is_view (): # noqa: SLF001 t . detach_ () def backward_weight_one_chunk ( self , microbatch_index : int ): \"\"\" Executes the weight gradient accumulation part of the backward pass. This assumes `backward_one_chunk(..., full_backward=False)` was already called for this microbatch. Args: microbatch_index: The microbatch index. \"\"\" if not self . _has_backward : raise ValueError () self . _backward_comp . backward_weight ( microbatch_index = microbatch_index ) def reset ( self ): \"\"\"Resets the internal state of communication handlers, clearing gradients on buffers.\"\"\" if self . _forward_comm is not None : self . _forward_comm . reset () if self . _backward_comm is not None : self . _backward_comm . reset () __init__ ( info , module , group , stage_to_host_topology ) Constructs a PipelineStage object. Parameters: Name Type Description Default info PipelineStageInfo Metadata about the stage (index, total stages). required module Module The PyTorch module executed by this stage. required group ProcessGroup The distributed process group for pipeline communications. required stage_to_host_topology dict [ int , int ] Dict mapping stage ID to PP rank hosting it. required Source code in d9d/pipelining/infra/stage/stage.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def __init__ ( self , info : PipelineStageInfo , module : nn . Module , group : dist . ProcessGroup , stage_to_host_topology : dict [ int , int ], ): \"\"\" Constructs a PipelineStage object. Args: info: Metadata about the stage (index, total stages). module: The PyTorch module executed by this stage. group: The distributed process group for pipeline communications. stage_to_host_topology: Dict mapping stage ID to PP rank hosting it. \"\"\" self . _info = info self . _module = module self . _group = group self . _stage_to_host_topology = stage_to_host_topology self . _has_backward = False self . _forward_comm : StageCommunicationHandler | None = None self . _backward_comm : StageCommunicationHandler | None = None self . _forward_comp = ForwardComputeHandler ( stage_index = info . current_stage , module = module ) self . _backward_comp = BackwardComputeHandler ( stage_index = info . current_stage , module = module ) backward_one_chunk ( microbatch_index , loss = None , full_backward = True ) Executes a backward pass for a single microbatch chunk. Can perform either a full backward or just the input gradients (if full_backward=False ). It fetches required data from forward cache and communication buffers. Parameters: Name Type Description Default microbatch_index int The microbatch index. required loss Tensor | None The loss tensor (only used if this is the last stage). None full_backward bool If True, computes grads for inputs and weights. If False, only for inputs. True Source code in d9d/pipelining/infra/stage/stage.py 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 def backward_one_chunk ( self , microbatch_index : int , loss : torch . Tensor | None = None , full_backward : bool = True ): \"\"\" Executes a backward pass for a single microbatch chunk. Can perform either a full backward or just the input gradients (if `full_backward=False`). It fetches required data from forward cache and communication buffers. Args: microbatch_index: The microbatch index. loss: The loss tensor (only used if this is the last stage). full_backward: If True, computes grads for inputs and weights. If False, only for inputs. \"\"\" if not self . _has_backward : raise ValueError () if self . _backward_comm is None : raise ValueError ( \"You must configure stage buffers first\" ) inputs , fwd_outputs = self . _forward_comp . pop_inputs_outputs ( microbatch_index ) outputs : dict [ str , torch . Tensor ] outputs_grad : dict [ str , torch . Tensor ] | None if self . _info . is_current_stage_last : if loss is None : raise ValueError ( \"Cannot perform backward on last stage without loss specified\" ) outputs = { \"loss\" : loss } outputs_grad = None else : outputs = fwd_outputs outputs_grad = self . _backward_comm . get_inputs ( microbatch_index ) if full_backward : self . _backward_comp . backward_full ( microbatch_index = microbatch_index , inputs = inputs , outputs = outputs , outputs_grad = outputs_grad ) else : self . _backward_comp . backward_input ( microbatch_index = microbatch_index , inputs = inputs , outputs = outputs , outputs_grad = outputs_grad ) if self . _info . is_current_stage_last and not self . _info . is_current_stage_first : for t in fwd_outputs . values (): if not t . _is_view (): # noqa: SLF001 t . detach_ () backward_weight_one_chunk ( microbatch_index ) Executes the weight gradient accumulation part of the backward pass. This assumes backward_one_chunk(..., full_backward=False) was already called for this microbatch. Parameters: Name Type Description Default microbatch_index int The microbatch index. required Source code in d9d/pipelining/infra/stage/stage.py 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 def backward_weight_one_chunk ( self , microbatch_index : int ): \"\"\" Executes the weight gradient accumulation part of the backward pass. This assumes `backward_one_chunk(..., full_backward=False)` was already called for this microbatch. Args: microbatch_index: The microbatch index. \"\"\" if not self . _has_backward : raise ValueError () self . _backward_comp . backward_weight ( microbatch_index = microbatch_index ) configure_buffers ( num_microbatches , has_backward , pipeline_inputs ) Initializes the communication handlers and buffers for the stage. This must be called before execution to establish P2P buffer sizes and directions. Parameters: Name Type Description Default num_microbatches int Total number of microbatches to process. required has_backward bool Does this pipeline stage should store info for a backward pass required pipeline_inputs dict [ str , Tensor ] Pipeline input data. required Source code in d9d/pipelining/infra/stage/stage.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def configure_buffers ( self , num_microbatches : int , has_backward : bool , pipeline_inputs : dict [ str , torch . Tensor ]): \"\"\" Initializes the communication handlers and buffers for the stage. This must be called before execution to establish P2P buffer sizes and directions. Args: num_microbatches: Total number of microbatches to process. has_backward: Does this pipeline stage should store info for a backward pass pipeline_inputs: Pipeline input data. \"\"\" self . _has_backward = has_backward prev_stage_idx = None if self . _info . is_current_stage_first else self . _info . current_stage - 1 next_stage_idx = None if self . _info . is_current_stage_last else self . _info . current_stage + 1 with torch . device ( \"meta\" ): if not isinstance ( self . _module , ModuleSupportsPipelining ): raise TypeError ( \"Module does not implement ModuleSupportsPipelining protocol\" ) inputs_meta = self . _module . infer_stage_inputs_from_pipeline_inputs ( inputs = pipeline_inputs , n_microbatches = num_microbatches ) outputs_meta = self . _module . infer_stage_outputs_from_pipeline_inputs ( inputs = pipeline_inputs , n_microbatches = num_microbatches ) self . _forward_comm = StageCommunicationHandler ( name = \"fwd\" , stage_index = self . _info . current_stage , num_microbatches = num_microbatches , input_stage_index = prev_stage_idx , input_args = inputs_meta , output_stage_index = next_stage_idx , output_args = outputs_meta , group = self . _group , stage_idx_to_host_rank = self . _stage_to_host_topology , ) self . _forward_comm . set_input_requires_grad_ ( requires_grad = has_backward ) if has_backward : # for grad - current stage receives OUTPUTS as inputs and sends INPUTS as outputs # because it is reversed forward self . _backward_comm = StageCommunicationHandler ( name = \"bwd\" , stage_index = self . _info . current_stage , num_microbatches = num_microbatches , input_stage_index = next_stage_idx , input_args = outputs_meta , output_stage_index = prev_stage_idx , output_args = inputs_meta , group = self . _group , stage_idx_to_host_rank = self . _stage_to_host_topology , ) else : self . _backward_comm = None forward_one_chunk ( microbatch_index , pipeline_inputs , pipeline_kwargs = None ) Executes a forward pass for a single microbatch chunk. Fetches inputs from the communication buffer (or pipeline_inputs if first stage), runs the computation, and caches the result. Parameters: Name Type Description Default microbatch_index int The microbatch index. required pipeline_inputs dict [ str , Tensor ] Inputs provided locally (only used if this is the first stage). required pipeline_kwargs dict [ str , Any ] | None Additional arguments for the module. None Returns: Type Description The output tensors of the forward pass. Source code in d9d/pipelining/infra/stage/stage.py 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 def forward_one_chunk ( self , microbatch_index : int , pipeline_inputs : dict [ str , torch . Tensor ], pipeline_kwargs : dict [ str , Any ] | None = None , ): \"\"\" Executes a forward pass for a single microbatch chunk. Fetches inputs from the communication buffer (or `pipeline_inputs` if first stage), runs the computation, and caches the result. Args: microbatch_index: The microbatch index. pipeline_inputs: Inputs provided locally (only used if this is the first stage). pipeline_kwargs: Additional arguments for the module. Returns: The output tensors of the forward pass. \"\"\" if self . _forward_comm is None : raise ValueError ( \"You must configure stage buffers first\" ) if self . _info . is_current_stage_first : inputs = pipeline_inputs else : inputs = self . _forward_comm . get_inputs ( microbatch_index ) kwargs = pipeline_kwargs or {} self . _forward_comp . run ( microbatch_index = microbatch_index , inputs = inputs , kwargs = kwargs ) get_bwd_recv_ops ( microbatch_index ) Returns P2P ops to receive backward gradients for the given microbatch. Source code in d9d/pipelining/infra/stage/stage.py 168 169 170 171 172 173 174 175 176 177 def get_bwd_recv_ops ( self , microbatch_index : int ) -> list [ dist . P2POp ]: \"\"\"Returns P2P ops to receive backward gradients for the given microbatch.\"\"\" if not self . _has_backward : return [] if self . _backward_comm is None : raise ValueError ( \"You must configure stage buffers first\" ) return self . _backward_comm . create_receive_ops ( microbatch_index ) get_bwd_send_ops ( microbatch_index ) Returns P2P ops to send backward gradients for the given microbatch. Source code in d9d/pipelining/infra/stage/stage.py 179 180 181 182 183 184 185 186 187 188 189 def get_bwd_send_ops ( self , microbatch_index : int ) -> list [ dist . P2POp ]: \"\"\"Returns P2P ops to send backward gradients for the given microbatch.\"\"\" if not self . _has_backward : return [] if self . _backward_comm is None : raise ValueError ( \"You must configure stage buffers first\" ) bwd_result = self . _backward_comp . pop_for_sending ( microbatch_index ) return self . _backward_comm . create_send_ops ( bwd_result ) get_fwd_recv_ops ( microbatch_index ) Returns P2P ops to receive forward inputs for the given microbatch. Source code in d9d/pipelining/infra/stage/stage.py 151 152 153 154 155 156 157 def get_fwd_recv_ops ( self , microbatch_index : int ) -> list [ dist . P2POp ]: \"\"\"Returns P2P ops to receive forward inputs for the given microbatch.\"\"\" if self . _forward_comm is None : raise ValueError ( \"You must configure stage buffers first\" ) return self . _forward_comm . create_receive_ops ( microbatch_index ) get_fwd_send_ops ( microbatch_index ) Returns P2P ops to send forward outputs for the given microbatch. Source code in d9d/pipelining/infra/stage/stage.py 159 160 161 162 163 164 165 166 def get_fwd_send_ops ( self , microbatch_index : int ) -> list [ dist . P2POp ]: \"\"\"Returns P2P ops to send forward outputs for the given microbatch.\"\"\" if self . _forward_comm is None : raise ValueError ( \"You must configure stage buffers first\" ) fwd_result = self . _forward_comp . get_outputs ( microbatch_index ) return self . _forward_comm . create_send_ops ( fwd_result ) pop_local_bwd_output ( microbatch_index ) Retrieves local backward outputs (gradients). Source code in d9d/pipelining/infra/stage/stage.py 128 129 130 131 132 133 134 135 136 def pop_local_bwd_output ( self , microbatch_index : int ) -> dict [ str , torch . Tensor ]: \"\"\" Retrieves local backward outputs (gradients). \"\"\" if not self . _has_backward : raise ValueError () return self . _backward_comp . pop_for_sending ( microbatch_index ) reset () Resets the internal state of communication handlers, clearing gradients on buffers. Source code in d9d/pipelining/infra/stage/stage.py 287 288 289 290 291 292 293 def reset ( self ): \"\"\"Resets the internal state of communication handlers, clearing gradients on buffers.\"\"\" if self . _forward_comm is not None : self . _forward_comm . reset () if self . _backward_comm is not None : self . _backward_comm . reset () set_local_bwd_input ( inputs , microbatch_index ) Sets local backward inputs (output gradients) manually. Source code in d9d/pipelining/infra/stage/stage.py 138 139 140 141 142 143 144 145 146 147 148 149 def set_local_bwd_input ( self , inputs : dict [ str , torch . Tensor ], microbatch_index : int ): \"\"\" Sets local backward inputs (output gradients) manually. \"\"\" if not self . _has_backward : raise ValueError () if self . _backward_comm is None : raise ValueError ( \"You must configure stage buffers first\" ) self . _backward_comm . set_inputs_local ( inputs , microbatch_index ) set_local_fwd_input ( inputs , microbatch_index ) Sets local forward inputs manually. Used for the V-shape schedulers. Source code in d9d/pipelining/infra/stage/stage.py 113 114 115 116 117 118 119 120 121 122 123 def set_local_fwd_input ( self , inputs : dict [ str , torch . Tensor ], microbatch_index : int ): \"\"\" Sets local forward inputs manually. Used for the V-shape schedulers. \"\"\" if self . _forward_comm is None : raise ValueError ( \"You must configure stage buffers first\" ) self . _forward_comm . set_inputs_local ( inputs , microbatch_index ) d9d.pipelining.infra.schedule.component.runtime Pipelining Runtime Package. ActionBase Bases: ABC Abstract base class for all pipeline schedule actions. An action represents an atomic unit of work in a pipeline schedule, such as computing a microbatch or sending/receiving a tensor. Source code in d9d/pipelining/infra/schedule/component/runtime/action.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 class ActionBase ( abc . ABC ): \"\"\" Abstract base class for all pipeline schedule actions. An action represents an atomic unit of work in a pipeline schedule, such as computing a microbatch or sending/receiving a tensor. \"\"\" @abc . abstractmethod def apply ( self , ctx : ActionContext ): \"\"\" Executes the action logic using the provided context. Args: ctx: The runtime context containing stages, data, and communication handlers. \"\"\" ... @property @abc . abstractmethod def work_type ( self ) -> ActionWorkType : \"\"\"Returns the classification of work this action performs.\"\"\" ... @property @abc . abstractmethod def has_backward_work ( self ) -> bool : \"\"\"Returns True if this action involves backward pass computations.\"\"\" ... @abc . abstractmethod def __str__ ( self ) -> str : \"\"\"Returns a short string representation of the action for logging/visualization.\"\"\" ... has_backward_work abstractmethod property Returns True if this action involves backward pass computations. work_type abstractmethod property Returns the classification of work this action performs. __str__ () abstractmethod Returns a short string representation of the action for logging/visualization. Source code in d9d/pipelining/infra/schedule/component/runtime/action.py 79 80 81 82 @abc . abstractmethod def __str__ ( self ) -> str : \"\"\"Returns a short string representation of the action for logging/visualization.\"\"\" ... apply ( ctx ) abstractmethod Executes the action logic using the provided context. Parameters: Name Type Description Default ctx ActionContext The runtime context containing stages, data, and communication handlers. required Source code in d9d/pipelining/infra/schedule/component/runtime/action.py 56 57 58 59 60 61 62 63 64 65 @abc . abstractmethod def apply ( self , ctx : ActionContext ): \"\"\" Executes the action logic using the provided context. Args: ctx: The runtime context containing stages, data, and communication handlers. \"\"\" ... BackwardFullInputComputeAction dataclass Bases: ActionBase Action to perform backward computation with respect to inputs. Attributes: Name Type Description stage_idx int The integer index of the pipeline stage. microbatch_idx int The integer index of the microbatch to compute. full_backward bool If True, performs a full backward pass including inputs and weights. If False, may only compute gradients w.r.t inputs (depending on schedule implementation). Source code in d9d/pipelining/infra/schedule/component/runtime/action.py 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 @dataclasses . dataclass ( frozen = True , slots = True ) class BackwardFullInputComputeAction ( ActionBase ): \"\"\" Action to perform backward computation with respect to inputs. Attributes: stage_idx: The integer index of the pipeline stage. microbatch_idx: The integer index of the microbatch to compute. full_backward: If True, performs a full backward pass including inputs and weights. If False, may only compute gradients w.r.t inputs (depending on schedule implementation). \"\"\" stage_idx : int microbatch_idx : int full_backward : bool def apply ( self , ctx : ActionContext ): stage = ctx . stages [ self . stage_idx ] if not stage . info . is_current_stage_last and self . stage_idx + 1 not in ctx . stages : ctx . communications . wait_bwd_recv ( self . stage_idx , self . microbatch_idx ) if stage . info . is_current_stage_last and isinstance ( ctx . callback , PipelineLossHandler ): loss = ctx . callback . acquire_loss ( self . microbatch_idx ) else : loss = None stage . backward_one_chunk ( microbatch_index = self . microbatch_idx , full_backward = self . full_backward , loss = loss ) if not stage . info . is_current_stage_first and self . stage_idx - 1 in ctx . stages : ctx . stages [ self . stage_idx - 1 ] . set_local_bwd_input ( microbatch_index = self . microbatch_idx , inputs = stage . pop_local_bwd_output ( self . microbatch_idx ) ) @property def work_type ( self ) -> ActionWorkType : return ActionWorkType . compute @property def has_backward_work ( self ) -> bool : return True def __str__ ( self ) -> str : letter = \"B\" if self . full_backward else \"I\" return f \" { self . stage_idx }{ letter }{ self . microbatch_idx } \" BackwardReceiveAction dataclass Bases: ActionBase Action to schedule a backward pass gradient receive operation. Attributes: Name Type Description stage_idx int The integer index of the pipeline stage expecting the receive operation. microbatch_idx int The integer index of the microbatch being received. Source code in d9d/pipelining/infra/schedule/component/runtime/action.py 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 @dataclasses . dataclass ( frozen = True , slots = True ) class BackwardReceiveAction ( ActionBase ): \"\"\" Action to schedule a backward pass gradient receive operation. Attributes: stage_idx: The integer index of the pipeline stage expecting the receive operation. microbatch_idx: The integer index of the microbatch being received. \"\"\" stage_idx : int microbatch_idx : int def apply ( self , ctx : ActionContext ): ctx . communications . schedule_bwd_recv ( self . stage_idx , self . microbatch_idx ) @property def work_type ( self ) -> ActionWorkType : return ActionWorkType . communicate @property def has_backward_work ( self ) -> bool : return True def __str__ ( self ) -> str : return f \" { self . stage_idx } RECV_B { self . microbatch_idx } \" BackwardSendAction dataclass Bases: ActionBase Action to schedule a backward pass gradient send operation. Attributes: Name Type Description stage_idx int The integer index of the pipeline stage initiating the send operation. microbatch_idx int The integer index of the microbatch being sent. Source code in d9d/pipelining/infra/schedule/component/runtime/action.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 @dataclasses . dataclass ( frozen = True , slots = True ) class BackwardSendAction ( ActionBase ): \"\"\" Action to schedule a backward pass gradient send operation. Attributes: stage_idx: The integer index of the pipeline stage initiating the send operation. microbatch_idx: The integer index of the microbatch being sent. \"\"\" stage_idx : int microbatch_idx : int def apply ( self , ctx : ActionContext ): ctx . communications . schedule_bwd_send ( self . stage_idx , self . microbatch_idx ) @property def work_type ( self ) -> ActionWorkType : return ActionWorkType . communicate @property def has_backward_work ( self ) -> bool : return True def __str__ ( self ) -> str : return f \" { self . stage_idx } SEND_B { self . microbatch_idx } \" BackwardWeightComputeAction dataclass Bases: ActionBase Action to perform gradient accumulation on weights. Attributes: Name Type Description stage_idx int The integer index of the pipeline stage. microbatch_idx int The integer index of the microbatch to compute. Source code in d9d/pipelining/infra/schedule/component/runtime/action.py 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 @dataclasses . dataclass ( frozen = True , slots = True ) class BackwardWeightComputeAction ( ActionBase ): \"\"\" Action to perform gradient accumulation on weights. Attributes: stage_idx: The integer index of the pipeline stage. microbatch_idx: The integer index of the microbatch to compute. \"\"\" stage_idx : int microbatch_idx : int def apply ( self , ctx : ActionContext ): stage = ctx . stages [ self . stage_idx ] stage . backward_weight_one_chunk ( microbatch_index = self . microbatch_idx ) @property def work_type ( self ) -> ActionWorkType : return ActionWorkType . compute @property def has_backward_work ( self ) -> bool : return True def __str__ ( self ) -> str : return f \" { self . stage_idx } W { self . microbatch_idx } \" ComposeAction dataclass Bases: ActionBase Composite action scheduling multiple sub-actions sequentially. Used for forward/backward overlapping. Attributes: Name Type Description actions tuple [ ActionBase , ...] A tuple of sub-actions to be executed sequentially. Source code in d9d/pipelining/infra/schedule/component/runtime/action.py 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 @dataclasses . dataclass ( frozen = True , slots = True ) class ComposeAction ( ActionBase ): \"\"\" Composite action scheduling multiple sub-actions sequentially. Used for forward/backward overlapping. Attributes: actions: A tuple of sub-actions to be executed sequentially. \"\"\" actions : tuple [ ActionBase , ... ] def apply ( self , ctx : ActionContext ): for act in self . actions : act . apply ( ctx ) @property def work_type ( self ) -> ActionWorkType : sub_work_types = { x . work_type for x in self . actions } if len ( sub_work_types ) != 1 : raise ValueError ( \"\" ) return next ( iter ( sub_work_types )) @property def has_backward_work ( self ) -> bool : return any ( x . has_backward_work for x in self . actions ) def __str__ ( self ) -> str : return \"|\" . join ( map ( str , self . actions )) ForwardComputeAction dataclass Bases: ActionBase Action to perform forward computation for a specific microbatch. Attributes: Name Type Description stage_idx int The integer index of the pipeline stage. microbatch_idx int The integer index of the microbatch to compute. Source code in d9d/pipelining/infra/schedule/component/runtime/action.py 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 @dataclasses . dataclass ( frozen = True , slots = True ) class ForwardComputeAction ( ActionBase ): \"\"\" Action to perform forward computation for a specific microbatch. Attributes: stage_idx: The integer index of the pipeline stage. microbatch_idx: The integer index of the microbatch to compute. \"\"\" stage_idx : int microbatch_idx : int def apply ( self , ctx : ActionContext ): stage = ctx . stages [ self . stage_idx ] if not stage . info . is_current_stage_first and self . stage_idx - 1 not in ctx . stages : ctx . communications . wait_fwd_recv ( self . stage_idx , self . microbatch_idx ) stage . forward_one_chunk ( microbatch_index = self . microbatch_idx , pipeline_inputs = ctx . pipeline_inputs_microbatches [ self . microbatch_idx ], pipeline_kwargs = ctx . pipeline_kwargs_microbatches [ self . microbatch_idx ], ) result = stage . get_local_fwd_output ( self . microbatch_idx ) if stage . info . is_current_stage_last : ctx . callback . trigger ( result , self . microbatch_idx ) if not stage . info . is_current_stage_last and self . stage_idx + 1 in ctx . stages : ctx . stages [ self . stage_idx + 1 ] . set_local_fwd_input ( inputs = result , microbatch_index = self . microbatch_idx ) @property def work_type ( self ) -> ActionWorkType : return ActionWorkType . compute @property def has_backward_work ( self ) -> bool : return False def __str__ ( self ) -> str : return f \" { self . stage_idx } F { self . microbatch_idx } \" ForwardReceiveAction dataclass Bases: ActionBase Action to schedule a forward pass tensor receive operation. Attributes: Name Type Description stage_idx int The integer index of the pipeline stage expecting the receive operation. microbatch_idx int The integer index of the microbatch being received. Source code in d9d/pipelining/infra/schedule/component/runtime/action.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 @dataclasses . dataclass ( frozen = True , slots = True ) class ForwardReceiveAction ( ActionBase ): \"\"\" Action to schedule a forward pass tensor receive operation. Attributes: stage_idx: The integer index of the pipeline stage expecting the receive operation. microbatch_idx: The integer index of the microbatch being received. \"\"\" stage_idx : int microbatch_idx : int def apply ( self , ctx : ActionContext ): ctx . communications . schedule_fwd_recv ( self . stage_idx , self . microbatch_idx ) @property def work_type ( self ) -> ActionWorkType : return ActionWorkType . communicate @property def has_backward_work ( self ) -> bool : return True def __str__ ( self ) -> str : return f \" { self . stage_idx } RECV_F { self . microbatch_idx } \" ForwardSendAction dataclass Bases: ActionBase Action to schedule a forward pass tensor send operation. Attributes: Name Type Description stage_idx int The integer index of the pipeline stage initiating the send operation. microbatch_idx int The integer index of the microbatch being sent. Source code in d9d/pipelining/infra/schedule/component/runtime/action.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 @dataclasses . dataclass ( frozen = True , slots = True ) class ForwardSendAction ( ActionBase ): \"\"\" Action to schedule a forward pass tensor send operation. Attributes: stage_idx: The integer index of the pipeline stage initiating the send operation. microbatch_idx: The integer index of the microbatch being sent. \"\"\" stage_idx : int microbatch_idx : int def apply ( self , ctx : ActionContext ): ctx . communications . schedule_fwd_send ( self . stage_idx , self . microbatch_idx ) @property def work_type ( self ) -> ActionWorkType : return ActionWorkType . communicate @property def has_backward_work ( self ) -> bool : return False def __str__ ( self ) -> str : return f \" { self . stage_idx } SEND_F { self . microbatch_idx } \" OfflinePipelineExecutor Bases: PipelineSchedule Executes the model immediately without pipeline parallelism. This schedule treats the execution as a single stage with a single microbatch, running the forward and optionally backward pass directly. This is primarily used for single-device execution within the pipeline abstraction. Source code in d9d/pipelining/infra/schedule/component/runtime/offline.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 class OfflinePipelineExecutor ( PipelineSchedule ): \"\"\" Executes the model immediately without pipeline parallelism. This schedule treats the execution as a single stage with a single microbatch, running the forward and optionally backward pass directly. This is primarily used for single-device execution within the pipeline abstraction. \"\"\" def __init__ ( self , model : nn . Module , callback : PipelineLossFn | PipelineResultFn , do_backward : bool ): \"\"\" Constructs the offline pipeline executor. Args: model: The PyTorch module to execute. callback: Function to compute loss or process pipeline results. do_backward: Whether to execute the backward pass. \"\"\" self . _model = model self . _callback = callback self . _do_backward = do_backward def configure_buffers ( self , inputs : dict [ str , torch . Tensor ], kwargs : dict [ str , Any ], sharding_spec : PipelineShardingSpec | None ): pass def _forward_only ( self , inputs : dict [ str , torch . Tensor ], kwargs : dict [ str , Any ]): result = self . _model ( ** inputs , ** kwargs ) self . _callback ( result , 0 ) # microbatch=0 def _forward_backward ( self , inputs : dict [ str , torch . Tensor ], kwargs : dict [ str , Any ]): result = self . _model ( ** inputs , ** kwargs ) loss = self . _callback ( result , 0 ) # microbatch=0 del result # do not peak memory loss . backward () def step ( self , inputs : dict [ str , torch . Tensor ], kwargs : dict [ str , Any ]): result = self . _model ( ** inputs , ** kwargs ) processing_result = self . _callback ( result , 0 ) if self . _do_backward : if not isinstance ( processing_result , torch . Tensor ): raise ValueError ( \"Loss should be torch.Tensor\" ) del result # do not peak memory processing_result . backward () __init__ ( model , callback , do_backward ) Constructs the offline pipeline executor. Parameters: Name Type Description Default model Module The PyTorch module to execute. required callback PipelineLossFn | PipelineResultFn Function to compute loss or process pipeline results. required do_backward bool Whether to execute the backward pass. required Source code in d9d/pipelining/infra/schedule/component/runtime/offline.py 18 19 20 21 22 23 24 25 26 27 28 29 30 def __init__ ( self , model : nn . Module , callback : PipelineLossFn | PipelineResultFn , do_backward : bool ): \"\"\" Constructs the offline pipeline executor. Args: model: The PyTorch module to execute. callback: Function to compute loss or process pipeline results. do_backward: Whether to execute the backward pass. \"\"\" self . _model = model self . _callback = callback self . _do_backward = do_backward PipelineScheduleExecutor Bases: PipelineSchedule Executes a defined pipeline schedule by interpreting a sequence of actions. Source code in d9d/pipelining/infra/schedule/component/runtime/executor.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 class PipelineScheduleExecutor ( PipelineSchedule ): \"\"\"Executes a defined pipeline schedule by interpreting a sequence of actions.\"\"\" def __init__ ( self , dist_context : DistributedContext , stages : list [ PipelineStage ], num_microbatches : int , callback : PipelineLossFn | PipelineResultFn , program : dict [ int , list [ ActionBase ]], ): \"\"\" Constructs the schedule executor. Args: dist_context: The distributed context. stages: List of stages managed by this executor. num_microbatches: Number of microbatches the global batch is split. callback: Function to compute loss or process pipeline results. program: The execution plan mapping rank ID to a list of actions. \"\"\" self . _dist_ctx = dist_context self . _stages = { stage . info . current_stage : stage for stage in stages } self . _num_microbatches = num_microbatches self . _program = program self . _has_backward = any ( any ( action . has_backward_work for action in sub_program ) for sub_program in program . values () ) self . _comm_handler = PipelineCommunicationHandler ( self . _stages ) self . _callback : PipelineLossHandler | PipelineResultHandler if self . _has_backward : self . _callback = PipelineLossHandler ( callback ) else : self . _callback = PipelineResultHandler ( callback ) self . _input_data_sharding_spec : ShardingSpec | None = None self . _input_kwargs_sharding_spec : ShardingSpec | None = None def configure_buffers ( self , inputs : dict [ str , torch . Tensor ], kwargs : dict [ str , Any ], sharding_spec : PipelineShardingSpec | None ): if sharding_spec is None or sharding_spec . input_data is None : self . _input_data_sharding_spec = shard_spec_on_dim ( inputs , dim = 0 ) if sharding_spec is None or sharding_spec . input_kwargs is None : self . _input_kwargs_sharding_spec = shard_spec_on_dim ( kwargs , dim = 0 ) for stage in self . _stages . values (): stage . configure_buffers ( num_microbatches = self . _num_microbatches , pipeline_inputs = inputs , has_backward = self . _has_backward ) def step ( self , inputs : dict [ str , torch . Tensor ], kwargs : dict [ str , Any ]): if self . _input_data_sharding_spec is None or self . _input_kwargs_sharding_spec is None : raise ValueError ( \"Please configure sharding specs first\" ) self . _dist_ctx . logger . debug ( \"Begin pipeline step\" ) pp_group = self . _dist_ctx . mesh_for ( REGULAR_DOMAIN ) . get_group ( \"pp\" ) for stage in self . _stages . values (): stage . reset () # Shard inputs and kwargs to microbatches inputs_shard = shard_tree ( inputs , num_shards = self . _num_microbatches , sharding_spec = self . _input_data_sharding_spec , enforce_even_split = True , ) kwargs_shard = shard_tree ( kwargs , num_shards = self . _num_microbatches , sharding_spec = self . _input_kwargs_sharding_spec , enforce_even_split = True , ) my_program = self . _program [ pp_group . rank ()] for action in my_program : with record_function ( str ( action )): self . _dist_ctx . logger . debug ( f \"Running pipeline action { action } \" ) action . apply ( ActionContext ( callback = self . _callback , stages = self . _stages , communications = self . _comm_handler , pipeline_inputs_microbatches = inputs_shard , pipeline_kwargs_microbatches = kwargs_shard , ) ) self . _dist_ctx . logger . debug ( \"Waiting for potentially hanging PP send comms\" ) self . _comm_handler . wait_send_all () # finalize just in case self . _dist_ctx . logger . debug ( \"End pipeline step\" ) __init__ ( dist_context , stages , num_microbatches , callback , program ) Constructs the schedule executor. Parameters: Name Type Description Default dist_context DistributedContext The distributed context. required stages list [ PipelineStage ] List of stages managed by this executor. required num_microbatches int Number of microbatches the global batch is split. required callback PipelineLossFn | PipelineResultFn Function to compute loss or process pipeline results. required program dict [ int , list [ ActionBase ]] The execution plan mapping rank ID to a list of actions. required Source code in d9d/pipelining/infra/schedule/component/runtime/executor.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def __init__ ( self , dist_context : DistributedContext , stages : list [ PipelineStage ], num_microbatches : int , callback : PipelineLossFn | PipelineResultFn , program : dict [ int , list [ ActionBase ]], ): \"\"\" Constructs the schedule executor. Args: dist_context: The distributed context. stages: List of stages managed by this executor. num_microbatches: Number of microbatches the global batch is split. callback: Function to compute loss or process pipeline results. program: The execution plan mapping rank ID to a list of actions. \"\"\" self . _dist_ctx = dist_context self . _stages = { stage . info . current_stage : stage for stage in stages } self . _num_microbatches = num_microbatches self . _program = program self . _has_backward = any ( any ( action . has_backward_work for action in sub_program ) for sub_program in program . values () ) self . _comm_handler = PipelineCommunicationHandler ( self . _stages ) self . _callback : PipelineLossHandler | PipelineResultHandler if self . _has_backward : self . _callback = PipelineLossHandler ( callback ) else : self . _callback = PipelineResultHandler ( callback ) self . _input_data_sharding_spec : ShardingSpec | None = None self . _input_kwargs_sharding_spec : ShardingSpec | None = None d9d.pipelining.infra.schedule.component.program Pipeline Schedule Building Components. This package provides the core building blocks and compiler passes used to generate execution schedules for distributed pipelines. PipelineProgramBuilder Bases: ABC Abstract interface for building pipeline execution schedules. Source code in d9d/pipelining/infra/schedule/component/program/base.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class PipelineProgramBuilder ( abc . ABC ): \"\"\"Abstract interface for building pipeline execution schedules.\"\"\" @abc . abstractmethod def compose ( self , num_microbatches : int , pp_size : int ) -> dict [ int , list [ ActionBase ]]: \"\"\" Generates the execution program for all ranks in the pipeline. Args: num_microbatches: Number of microbatches per step. pp_size: Number of pipeline parallel ranks. Returns: A dictionary mapping rank indices to their list of sequential actions. \"\"\" ... @property @abc . abstractmethod def num_stages_per_rank ( self ) -> int : \"\"\"Returns the number of model stages designated for each rank.\"\"\" ... @property @abc . abstractmethod def topology_style ( self ) -> ScheduleStyle : \"\"\"Returns the topology style strategy used to assign stages to ranks.\"\"\" ... num_stages_per_rank abstractmethod property Returns the number of model stages designated for each rank. topology_style abstractmethod property Returns the topology style strategy used to assign stages to ranks. compose ( num_microbatches , pp_size ) abstractmethod Generates the execution program for all ranks in the pipeline. Parameters: Name Type Description Default num_microbatches int Number of microbatches per step. required pp_size int Number of pipeline parallel ranks. required Returns: Type Description dict [ int , list [ ActionBase ]] A dictionary mapping rank indices to their list of sequential actions. Source code in d9d/pipelining/infra/schedule/component/program/base.py 10 11 12 13 14 15 16 17 18 19 20 21 22 @abc . abstractmethod def compose ( self , num_microbatches : int , pp_size : int ) -> dict [ int , list [ ActionBase ]]: \"\"\" Generates the execution program for all ranks in the pipeline. Args: num_microbatches: Number of microbatches per step. pp_size: Number of pipeline parallel ranks. Returns: A dictionary mapping rank indices to their list of sequential actions. \"\"\" ... ScheduleStyle Bases: StrEnum Defines the strategy for mapping logical stages to physical ranks. Attributes: Name Type Description loop Assigns stages in a round-robin circular fashion (mod pp_size). v Assigns stages in a zig-zag V-shape pattern. Useful for interleaved 1F1B schedules. Source code in d9d/pipelining/infra/schedule/component/program/topology.py 5 6 7 8 9 10 11 12 13 14 15 class ScheduleStyle ( StrEnum ): \"\"\" Defines the strategy for mapping logical stages to physical ranks. Attributes: loop: Assigns stages in a round-robin circular fashion (mod pp_size). v: Assigns stages in a zig-zag V-shape pattern. Useful for interleaved 1F1B schedules. \"\"\" loop = \"loop\" v = \"v\" add_communication_ops ( compute_actions , stage_to_rank , num_stages ) Injects communication actions into a computation-only schedule. This function iterates through the provided compute schedule and simulates execution. When a compute action produces a result needed by a different rank, it injects Send/Receive pairs. It also reorders actions to ensure that Receive operations occur before the Computes that depend on them, preventing deadlocks. Parameters: Name Type Description Default compute_actions dict [ int , list [ ActionBase ]] Initial schedule containing only compute operations. required stage_to_rank dict [ int , int ] Mapping from stage index to rank index. required num_stages int Total number of pipeline stages. required Returns: Type Description dict [ int , list [ ActionBase ]] A new schedule dictionary including both compute and communication actions. Raises: Type Description RuntimeError If the schedule simulation enters a deadlock state. Source code in d9d/pipelining/infra/schedule/component/program/communications.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 def add_communication_ops ( compute_actions : dict [ int , list [ ActionBase ]], stage_to_rank : dict [ int , int ], num_stages : int , ) -> dict [ int , list [ ActionBase ]]: \"\"\" Injects communication actions into a computation-only schedule. This function iterates through the provided compute schedule and simulates execution. When a compute action produces a result needed by a different rank, it injects Send/Receive pairs. It also reorders actions to ensure that Receive operations occur before the Computes that depend on them, preventing deadlocks. Args: compute_actions: Initial schedule containing only compute operations. stage_to_rank: Mapping from stage index to rank index. num_stages: Total number of pipeline stages. Returns: A new schedule dictionary including both compute and communication actions. Raises: RuntimeError: If the schedule simulation enters a deadlock state. \"\"\" compute_actions = copy . deepcopy ( compute_actions ) full_actions : dict [ int , list [ ActionBase ]] = { rank : [] for rank in compute_actions } completed_events : dict [ int , set [ ActionBase ]] = { rank : set () for rank in compute_actions } while compute_actions : progress = False for rank in sorted ( compute_actions . keys ()): if not compute_actions [ rank ]: del compute_actions [ rank ] continue current_action = compute_actions [ rank ][ 0 ] sub_actions = _get_sub_actions ( current_action ) # Check readiness if not check_action_communication_dependencies_fulfilled ( current_action , completed_events [ rank ], num_stages ): continue # Execute full_actions [ rank ] . append ( current_action ) compute_actions [ rank ] . pop ( 0 ) progress = True for sub_action in sub_actions : completed_events [ rank ] . add ( sub_action ) comm_pkg = _create_communications_for_action ( sub_action , num_stages = num_stages , stage_to_rank = stage_to_rank ) if comm_pkg : # Add Send locally full_actions [ rank ] . append ( comm_pkg . send ) completed_events [ rank ] . add ( comm_pkg . send ) # Add Recv remotely and unblock target full_actions [ comm_pkg . sends_to_rank ] . append ( comm_pkg . recv ) completed_events [ comm_pkg . sends_to_rank ] . add ( comm_pkg . recv ) if not progress and compute_actions : raise RuntimeError ( \"Deadlock in schedule simulation\" ) return full_actions build_stage_to_host_rank_topology ( pp_size , num_stages , style ) Constructs the mapping from stage index to rank index. Parameters: Name Type Description Default pp_size int Number of pipeline parallel ranks. required num_stages int Total number of model stages. required style ScheduleStyle The topology style to use for assignment. required Returns: Type Description dict [ int , int ] A dictionary mapping stage IDs to Rank IDs. Raises: Type Description ValueError If the style is unknown or if V-style parameters are invalid (num_stages must be divisible by pp_size). Source code in d9d/pipelining/infra/schedule/component/program/topology.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def build_stage_to_host_rank_topology ( pp_size : int , num_stages : int , style : ScheduleStyle ) -> dict [ int , int ]: \"\"\" Constructs the mapping from stage index to rank index. Args: pp_size: Number of pipeline parallel ranks. num_stages: Total number of model stages. style: The topology style to use for assignment. Returns: A dictionary mapping stage IDs to Rank IDs. Raises: ValueError: If the style is unknown or if V-style parameters are invalid (num_stages must be divisible by pp_size). \"\"\" match style : case ScheduleStyle . loop : return { stage_index : stage_index % pp_size for stage_index in range ( num_stages )} case ScheduleStyle . v : if num_stages % pp_size != 0 : raise ValueError ( f \"num_stages { num_stages } must be evenly divisible by pp_size { pp_size } for V schedules\" ) result = {} rank_index = 0 for stage_index in range ( num_stages ): result [ stage_index ] = rank_index if ( stage_index + 1 ) % pp_size == 0 : continue if ( stage_index // pp_size ) % 2 == 0 : rank_index += 1 else : rank_index -= 1 return result case _ : raise ValueError () invert_stage_to_host_rank_topology ( stage_to_host ) Inverts the topology mapping to list execution stages per rank. Parameters: Name Type Description Default stage_to_host dict [ int , int ] Mapping from stage index to rank index. required Returns: Type Description dict [ int , list [ int ]] A dictionary where keys are Rank IDs and values are lists of Stage IDs dict [ int , list [ int ]] managed by that rank. Source code in d9d/pipelining/infra/schedule/component/program/topology.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def invert_stage_to_host_rank_topology ( stage_to_host : dict [ int , int ]) -> dict [ int , list [ int ]]: \"\"\" Inverts the topology mapping to list execution stages per rank. Args: stage_to_host: Mapping from stage index to rank index. Returns: A dictionary where keys are Rank IDs and values are lists of Stage IDs managed by that rank. \"\"\" host_to_stage = defaultdict ( list ) for stage_idx , host in stage_to_host . items (): host_to_stage [ host ] . append ( stage_idx ) return dict ( host_to_stage ) d9d.pipelining.infra.schedule.program Pipeline Schedule Implementations DualPipeVPipelineProgramBuilder Bases: PipelineProgramBuilder Builder for the DualPipeV Pipeline Parallelism schedule. DualPipeV is a specialized bi-directional pipeline schedule designed for high throughput training. It requires exactly 2 stages per pipeline rank (V-shape) and utilizes split backward passes (Input gradients vs Weight gradients) to fill pipeline bubbles. References https://github.com/deepseek-ai/DualPipe https://hackmd.io/@ufotalent/r1lVXsa9Jg Source code in d9d/pipelining/infra/schedule/program/dualpipev.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 class DualPipeVPipelineProgramBuilder ( PipelineProgramBuilder ): \"\"\" Builder for the DualPipeV Pipeline Parallelism schedule. DualPipeV is a specialized bi-directional pipeline schedule designed for high throughput training. It requires exactly 2 stages per pipeline rank (V-shape) and utilizes split backward passes (Input gradients vs Weight gradients) to fill pipeline bubbles. References: https://github.com/deepseek-ai/DualPipe https://hackmd.io/@ufotalent/r1lVXsa9Jg \"\"\" def __init__ ( self ): \"\"\" Constructs the DualPipeV builder. \"\"\" @staticmethod def _build_for_rank ( # noqa: C901 rank : int , stage_to_rank : dict [ int , int ], num_microbatches : int , pp_size : int ) -> list [ ActionBase ]: compute_actions : list [ ActionBase ] = [] # Identify local stages: s0 is Phase 0, s1 is Phase 1 my_stages = sorted ([ s for s , r in stage_to_rank . items () if r == rank ]) s0 , s1 = my_stages [ 0 ], my_stages [ 1 ] # Track microbatch indices for each stage and operation type # f_idx: Next Forward microbatch # b_idx: Next Backward microbatch (Input or Full) f_idx = { s0 : 0 , s1 : 0 } b_idx = { s0 : 0 , s1 : 0 } # Queue for Zero Bubble optimization: stores (stage, mb_idx) for deferred weight grads weight_queue : deque [ tuple [ int , int ]] = deque () # --- Helper Functions for Action Emission --- def _add_f ( stage : int ): compute_actions . append ( ForwardComputeAction ( stage_idx = stage , microbatch_idx = f_idx [ stage ])) f_idx [ stage ] += 1 def _add_b_full ( stage : int ): compute_actions . append ( BackwardFullInputComputeAction ( stage_idx = stage , microbatch_idx = b_idx [ stage ], full_backward = True , ) ) b_idx [ stage ] += 1 def _add_b_input ( stage : int ): mb = b_idx [ stage ] compute_actions . append ( BackwardFullInputComputeAction ( stage_idx = stage , microbatch_idx = mb , full_backward = False , ) ) weight_queue . append (( stage , mb )) b_idx [ stage ] += 1 def _pop_w (): if not weight_queue : return s , mb = weight_queue . popleft () compute_actions . append ( BackwardWeightComputeAction ( stage_idx = s , microbatch_idx = mb )) def _add_overlap_f_b ( stage_f : int , stage_b : int , b_is_full : bool ): \"\"\"Emit overlapped Forward and Backward actions.\"\"\" mb_f = f_idx [ stage_f ] mb_b = b_idx [ stage_b ] act_f = ForwardComputeAction ( stage_idx = stage_f , microbatch_idx = mb_f ) act_b = BackwardFullInputComputeAction ( stage_idx = stage_b , microbatch_idx = mb_b , full_backward = b_is_full ) if not b_is_full : weight_queue . append (( stage_b , mb_b )) f_idx [ stage_f ] += 1 b_idx [ stage_b ] += 1 # Note: d9d infra treats ComposeAction sequentially in simulation, # but runtime may overlap them. compute_actions . append ( ComposeAction ( actions = ( act_f , act_b ))) # Step 1: nF0 (Startup Phase 0) step_1 = ( pp_size - rank - 1 ) * 2 for _ in range ( step_1 ): _add_f ( s0 ) # Step 2: nF0F1 (Forward fill) step_2 = rank + 1 for _ in range ( step_2 ): _add_f ( s0 ) _add_f ( s1 ) # Step 3: nI1W1F1 (Mixed Phase with Zero Bubble) step_3 = pp_size - rank - 1 for _ in range ( step_3 ): _add_b_input ( s1 ) # Backward Input Phase 1 _pop_w () # Weight Phase (accumulated from prev) _add_f ( s1 ) # Forward Phase 1 # Step 4: The Main Loop (Interleaved Forward/Backward) step_4 = num_microbatches - 2 * pp_size + rank + 1 for i in range ( step_4 ): # Sub-step A: F0 & B1 if i == 0 and rank == pp_size - 1 : # Specific case for last rank on first iter: do not overlap _add_f ( s0 ) _add_b_full ( s1 ) else : # Overlap F0 and B1 (usually full backward unless we were in ZB mode, # but DualPipeV main loop defaults to full for simplicity unless tuned) # DeepSeek impl uses standard backward here (zb=False). _add_overlap_f_b ( stage_f = s0 , stage_b = s1 , b_is_full = True ) # Sub-step B: F1 & B0 # Overlap F1 and B0 (Full) _add_overlap_f_b ( stage_f = s1 , stage_b = s0 , b_is_full = True ) # Step 5: Cooldown F1/B0 step_5 = pp_size - rank - 1 for _ in range ( step_5 ): _add_b_full ( s1 ) _add_overlap_f_b ( stage_f = s1 , stage_b = s0 , b_is_full = True ) # Step 6: Cooldown B1/B0 with Zero Bubble ramp-up step_6 = rank + 1 enable_zb = False for i in range ( step_6 ): # Phase 1 Backward if i == step_6 // 2 and rank % 2 == 1 : enable_zb = True if enable_zb : _add_b_input ( s1 ) else : _add_b_full ( s1 ) # Phase 0 Backward if i == step_6 // 2 and rank % 2 == 0 : enable_zb = True if enable_zb : _add_b_input ( s0 ) else : _add_b_full ( s0 ) # Step 7: Zero Bubble Weights + B0 step_7 = pp_size - rank - 1 for _ in range ( step_7 ): _pop_w () # DeepSeek source explicitly uses enable_zb=True here for chunk 0 _add_b_input ( s0 ) # Step 8: Flush Weights step_8 = rank + 1 for _ in range ( step_8 ): _pop_w () return compute_actions def compose ( self , num_microbatches : int , pp_size : int ) -> dict [ int , list [ ActionBase ]]: num_stages = self . num_stages_per_rank * pp_size if num_microbatches < num_stages : raise ValueError ( f \"DualPipeV requires num_microbatches ( { num_microbatches } ) >= num_stages ( { num_stages } ).\" ) # Ranks hold stages in a V pattern (e.g., Rank 0 holds Stage 0 and Stage N-1). # We rely on the sorted order of local steps to determine Phase 0 (Forward-going) # and Phase 1 (Backward-coming). stage_to_rank = build_stage_to_host_rank_topology ( pp_size = pp_size , num_stages = num_stages , style = ScheduleStyle . v ) compute_actions : dict [ int , list [ ActionBase ]] = { r : [] for r in range ( pp_size )} for rank in range ( pp_size ): compute_actions [ rank ] = self . _build_for_rank ( rank = rank , pp_size = pp_size , num_microbatches = num_microbatches , stage_to_rank = stage_to_rank ) # 4. Inject Communication Operations # This wrapper handles dependency analysis and inserts Send/Recv/Wait ops. return add_communication_ops ( compute_actions = compute_actions , stage_to_rank = stage_to_rank , num_stages = num_stages ) @property def num_stages_per_rank ( self ) -> int : return 2 @property def topology_style ( self ) -> ScheduleStyle : return ScheduleStyle . v __init__ () Constructs the DualPipeV builder. Source code in d9d/pipelining/infra/schedule/program/dualpipev.py 32 33 34 35 def __init__ ( self ): \"\"\" Constructs the DualPipeV builder. \"\"\" Interleaved1F1BPipelineProgramBuilder Bases: PipelineProgramBuilder Builder for Interleaved Pipeline Parallelism schedules. This builder supports: Standard Interleaved 1F1B : Assigns multiple stages per rank and prioritizes depth-first execution. (See https://arxiv.org/pdf/2104.04473) Interleaved Zero Bubble (ZB1P) : Extends 1F1B by splitting backward passes into Input Gradients and Weight Gradients. Weight gradients are delayed to fill pipeline bubbles. (See https://arxiv.org/pdf/2401.10241) Source code in d9d/pipelining/infra/schedule/program/interleaved.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 class Interleaved1F1BPipelineProgramBuilder ( PipelineProgramBuilder ): \"\"\" Builder for Interleaved Pipeline Parallelism schedules. This builder supports: 1. **Standard Interleaved 1F1B**: Assigns multiple stages per rank and prioritizes depth-first execution. (See https://arxiv.org/pdf/2104.04473) 2. **Interleaved Zero Bubble (ZB1P)**: Extends 1F1B by splitting backward passes into Input Gradients and Weight Gradients. Weight gradients are delayed to fill pipeline bubbles. (See https://arxiv.org/pdf/2401.10241) \"\"\" def __init__ ( self , num_stages_per_rank : int , enable_zero_bubble : bool = False ): \"\"\" Constructs the Interleaved 1F1B builder. Args: num_stages_per_rank: Number of stages per rank. enable_zero_bubble: If True, uses the ZB1P schedule variant which splits backward passes to reduce bubble size. \"\"\" self . _num_stages_per_rank = num_stages_per_rank self . _enable_zero_bubble = enable_zero_bubble def _get_warmup_ops ( self , rank : int , microbatches_per_round : int , pp_size : int , n_microbatches : int , multiply_factor : int , ) -> int : \"\"\" Calculates the number of warmup steps required before entering steady state. \"\"\" warmups_ops_last_stage = ( self . _num_stages_per_rank - 1 ) * microbatches_per_round warmup_ops = warmups_ops_last_stage + multiply_factor * (( pp_size - 1 ) - rank ) return min ( warmup_ops , n_microbatches * self . _num_stages_per_rank ) def compose ( self , num_microbatches : int , pp_size : int ) -> dict [ int , list [ ActionBase ]]: \"\"\" Generates the execution program for all ranks. Args: num_microbatches: Total microbatches. Must be divisible by the derived number of rounds. pp_size: Number of pipeline ranks. Returns: A dictionary mapping rank indices to their list of sequential actions. \"\"\" num_stages = self . num_stages_per_rank * pp_size if num_stages % pp_size != 0 : raise ValueError ( f \"num_stages ( { num_stages } ) must be divisible by pp_size ( { pp_size } ) for interleaved schedules.\" ) # 1. Topology Setup # Use Loop/Round-Robin assignment: Rank 0 gets Stage 0, PP, 2*PP... stage_to_rank = build_stage_to_host_rank_topology ( pp_size = pp_size , num_stages = num_stages , style = ScheduleStyle . loop ) num_rounds = max ( 1 , num_microbatches // pp_size ) if num_microbatches % num_rounds != 0 : raise ValueError ( f \"microbatches ( { num_microbatches } ) must be divisible by rounds ( { num_rounds } ).\" ) microbatches_per_round = num_microbatches // num_rounds # 2. Schedule Generation actions : dict [ int , list [ ActionBase ]] = {} # Zero Bubble 1f1b uses a shorter warmup heuristic (factor 1) than Standard (factor 2) warmup_multiplier = 1 if self . _enable_zero_bubble else 2 for rank in range ( pp_size ): actions [ rank ] = self . _generate_rank_schedule ( rank = rank , pp_size = pp_size , n_microbatches = num_microbatches , microbatches_per_round = microbatches_per_round , multiply_factor = warmup_multiplier , ) # 3. Communication Injection return add_communication_ops ( compute_actions = actions , stage_to_rank = stage_to_rank , num_stages = num_stages , ) def _generate_rank_schedule ( # noqa: C901 self , rank : int , pp_size : int , n_microbatches : int , microbatches_per_round : int , multiply_factor : int , ) -> list [ ActionBase ]: \"\"\" Generates the sequential list of compute actions for a specific rank. \"\"\" rank_actions : list [ ActionBase ] = [] # -- State Tracking -- # Map: stage_idx -> next_microbatch_idx fwd_counters : dict [ int , int ] = defaultdict ( int ) bwd_counters : dict [ int , int ] = defaultdict ( int ) # FIFO Queue for deferred weight gradients in Zero Bubble # Stores: (stage_idx, microbatch_idx) pending_weights : deque [ tuple [ int , int ]] = deque () # -- Helpers -- def get_global_stage ( local_idx : int ) -> int : \"\"\"Converts a local virtual stage index (0..N) to global stage ID.\"\"\" return ( local_idx * pp_size ) + rank def get_fwd_local_idx ( op_idx : int ) -> int : return ( op_idx // microbatches_per_round ) % self . _num_stages_per_rank def get_bwd_local_idx ( op_idx : int , warmup_offset : int ) -> int : return ( self . _num_stages_per_rank - 1 - (( op_idx - warmup_offset ) // microbatches_per_round ) % self . _num_stages_per_rank ) def emit_forward ( op_idx : int ): local_idx = get_fwd_local_idx ( op_idx ) stage = get_global_stage ( local_idx ) mb = fwd_counters [ stage ] rank_actions . append ( ForwardComputeAction ( stage_idx = stage , microbatch_idx = mb )) fwd_counters [ stage ] += 1 def emit_backward ( op_idx : int , warmup_offset : int ): local_idx = get_bwd_local_idx ( op_idx , warmup_offset ) stage = get_global_stage ( local_idx ) mb = bwd_counters [ stage ] # In Zero Bubble, we split: Backward Input (Now) + Backward Weight (Later) # In Standard 1F1B, we do full backward now. is_full = not self . _enable_zero_bubble rank_actions . append ( BackwardFullInputComputeAction ( stage_idx = stage , microbatch_idx = mb , full_backward = is_full ) ) if self . _enable_zero_bubble : pending_weights . append (( stage , mb )) bwd_counters [ stage ] += 1 def try_emit_weight_zb ( op_idx : int , warmup_offset : int ): if not self . _enable_zero_bubble or not pending_weights : return steps_into_1f1b = op_idx - warmup_offset # The earliest reasonable time to start weaving in weights is proportional to rank depth if steps_into_1f1b >= rank : w_stage , w_mb = pending_weights . popleft () rank_actions . append ( BackwardWeightComputeAction ( stage_idx = w_stage , microbatch_idx = w_mb )) # -- Execution Phase Math -- warmup_ops = self . _get_warmup_ops ( rank , microbatches_per_round , pp_size , n_microbatches , multiply_factor ) total_microbatch_ops = self . _num_stages_per_rank * n_microbatches fwd_bwd_ops = total_microbatch_ops - warmup_ops cooldown_ops = total_microbatch_ops - fwd_bwd_ops # Combine into one sequence for iteration, but handle logic per phase total_ops = warmup_ops + fwd_bwd_ops + cooldown_ops # -- Main Schedule Loop -- for op in range ( total_ops ): # Phase 1: Warmup (Forward Only) if op < warmup_ops : emit_forward ( op ) # Phase 2: Steady State (1F1B) elif op < warmup_ops + fwd_bwd_ops : emit_forward ( op ) emit_backward ( op , warmup_offset = warmup_ops ) try_emit_weight_zb ( op , warmup_offset = warmup_ops ) # Phase 3: Cooldown (Backward Only) else : emit_backward ( op , warmup_offset = warmup_ops ) try_emit_weight_zb ( op , warmup_offset = warmup_ops ) # -- Post-Loop: Flush Remaining Weights (ZB only) -- while pending_weights : w_stage , w_mb = pending_weights . popleft () rank_actions . append ( BackwardWeightComputeAction ( stage_idx = w_stage , microbatch_idx = w_mb )) return rank_actions @property def num_stages_per_rank ( self ) -> int : return self . _num_stages_per_rank @property def topology_style ( self ) -> ScheduleStyle : return ScheduleStyle . loop __init__ ( num_stages_per_rank , enable_zero_bubble = False ) Constructs the Interleaved 1F1B builder. Parameters: Name Type Description Default num_stages_per_rank int Number of stages per rank. required enable_zero_bubble bool If True, uses the ZB1P schedule variant which splits backward passes to reduce bubble size. False Source code in d9d/pipelining/infra/schedule/program/interleaved.py 30 31 32 33 34 35 36 37 38 39 40 def __init__ ( self , num_stages_per_rank : int , enable_zero_bubble : bool = False ): \"\"\" Constructs the Interleaved 1F1B builder. Args: num_stages_per_rank: Number of stages per rank. enable_zero_bubble: If True, uses the ZB1P schedule variant which splits backward passes to reduce bubble size. \"\"\" self . _num_stages_per_rank = num_stages_per_rank self . _enable_zero_bubble = enable_zero_bubble compose ( num_microbatches , pp_size ) Generates the execution program for all ranks. Parameters: Name Type Description Default num_microbatches int Total microbatches. Must be divisible by the derived number of rounds. required pp_size int Number of pipeline ranks. required Returns: Type Description dict [ int , list [ ActionBase ]] A dictionary mapping rank indices to their list of sequential actions. Source code in d9d/pipelining/infra/schedule/program/interleaved.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def compose ( self , num_microbatches : int , pp_size : int ) -> dict [ int , list [ ActionBase ]]: \"\"\" Generates the execution program for all ranks. Args: num_microbatches: Total microbatches. Must be divisible by the derived number of rounds. pp_size: Number of pipeline ranks. Returns: A dictionary mapping rank indices to their list of sequential actions. \"\"\" num_stages = self . num_stages_per_rank * pp_size if num_stages % pp_size != 0 : raise ValueError ( f \"num_stages ( { num_stages } ) must be divisible by pp_size ( { pp_size } ) for interleaved schedules.\" ) # 1. Topology Setup # Use Loop/Round-Robin assignment: Rank 0 gets Stage 0, PP, 2*PP... stage_to_rank = build_stage_to_host_rank_topology ( pp_size = pp_size , num_stages = num_stages , style = ScheduleStyle . loop ) num_rounds = max ( 1 , num_microbatches // pp_size ) if num_microbatches % num_rounds != 0 : raise ValueError ( f \"microbatches ( { num_microbatches } ) must be divisible by rounds ( { num_rounds } ).\" ) microbatches_per_round = num_microbatches // num_rounds # 2. Schedule Generation actions : dict [ int , list [ ActionBase ]] = {} # Zero Bubble 1f1b uses a shorter warmup heuristic (factor 1) than Standard (factor 2) warmup_multiplier = 1 if self . _enable_zero_bubble else 2 for rank in range ( pp_size ): actions [ rank ] = self . _generate_rank_schedule ( rank = rank , pp_size = pp_size , n_microbatches = num_microbatches , microbatches_per_round = microbatches_per_round , multiply_factor = warmup_multiplier , ) # 3. Communication Injection return add_communication_ops ( compute_actions = actions , stage_to_rank = stage_to_rank , num_stages = num_stages , ) LoopedBFSPipelineProgramBuilder Bases: PipelineProgramBuilder Builder for the Breadth-First Pipeline Parallelism schedule. This schedule runs all available forward microbatches for local stages first. If configured for training, it then runs backwards in reverse topological order. References https://arxiv.org/pdf/2211.05953 Source code in d9d/pipelining/infra/schedule/program/bfs.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 class LoopedBFSPipelineProgramBuilder ( PipelineProgramBuilder ): \"\"\" Builder for the Breadth-First Pipeline Parallelism schedule. This schedule runs all available forward microbatches for local stages first. If configured for training, it then runs backwards in reverse topological order. References: https://arxiv.org/pdf/2211.05953 \"\"\" def __init__ ( self , num_stages_per_rank : int , inference_mode : bool = False ): \"\"\" Constructs the LoopedBFS builder. Args: num_stages_per_rank: Number of stages per rank. inference_mode: If True, only forward passes are scheduled. If False, both forward and backward passes are scheduled. \"\"\" self . _num_stages_per_rank = num_stages_per_rank self . _inference_mode = inference_mode def compose ( self , num_microbatches : int , pp_size : int ) -> dict [ int , list [ ActionBase ]]: num_stages = self . _num_stages_per_rank * pp_size stage_to_rank = build_stage_to_host_rank_topology ( pp_size = pp_size , num_stages = num_stages , style = ScheduleStyle . loop ) compute_actions : dict [ int , list [ ActionBase ]] = { r : [] for r in range ( pp_size )} for rank in range ( pp_size ): my_stages = [ s for s in range ( num_stages ) if stage_to_rank [ s ] == rank ] # Schedule all Forwards # In Breadth-First loops, we finish all microbatches for the current stage # before moving to the next stage assigned to this rank. for stage_idx in my_stages : for mb_idx in range ( num_microbatches ): compute_actions [ rank ] . append ( ForwardComputeAction ( stage_idx = stage_idx , microbatch_idx = mb_idx )) # Schedule all Backwards (Reverse order) - Only if training if not self . _inference_mode : for stage_idx in reversed ( my_stages ): for mb_idx in reversed ( range ( num_microbatches )): compute_actions [ rank ] . append ( BackwardFullInputComputeAction ( stage_idx = stage_idx , microbatch_idx = mb_idx , full_backward = True ) ) return add_communication_ops ( compute_actions = compute_actions , stage_to_rank = stage_to_rank , num_stages = num_stages ) @property def num_stages_per_rank ( self ) -> int : return self . _num_stages_per_rank @property def topology_style ( self ) -> ScheduleStyle : return ScheduleStyle . loop __init__ ( num_stages_per_rank , inference_mode = False ) Constructs the LoopedBFS builder. Parameters: Name Type Description Default num_stages_per_rank int Number of stages per rank. required inference_mode bool If True, only forward passes are scheduled. If False, both forward and backward passes are scheduled. False Source code in d9d/pipelining/infra/schedule/program/bfs.py 25 26 27 28 29 30 31 32 33 34 35 def __init__ ( self , num_stages_per_rank : int , inference_mode : bool = False ): \"\"\" Constructs the LoopedBFS builder. Args: num_stages_per_rank: Number of stages per rank. inference_mode: If True, only forward passes are scheduled. If False, both forward and backward passes are scheduled. \"\"\" self . _num_stages_per_rank = num_stages_per_rank self . _inference_mode = inference_mode ZeroBubbleVPipelineProgramBuilder Bases: PipelineProgramBuilder Builder for the Zero Bubble V (ZBV) Pipeline Schedule. This schedule is designed for V-shape topologies (2 stages per rank) and utilizes the Zero Bubble optimizations by splitting backward passes. It requires exactly two stages per rank organized in a V-shape topology and splits backward passes into Input and Weight gradients to optimize pipeline throughput. References https://arxiv.org/pdf/2401.10241, Section 6 Source code in d9d/pipelining/infra/schedule/program/zerobubblev.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 class ZeroBubbleVPipelineProgramBuilder ( PipelineProgramBuilder ): \"\"\" Builder for the Zero Bubble V (ZBV) Pipeline Schedule. This schedule is designed for V-shape topologies (2 stages per rank) and utilizes the Zero Bubble optimizations by splitting backward passes. It requires exactly two stages per rank organized in a V-shape topology and splits backward passes into Input and Weight gradients to optimize pipeline throughput. References: https://arxiv.org/pdf/2401.10241, Section 6 \"\"\" def __init__ ( self ): \"\"\"Constructs the ZBV builder.\"\"\" def compose ( self , num_microbatches : int , pp_size : int ) -> dict [ int , list [ ActionBase ]]: num_stages = self . num_stages_per_rank * pp_size # 1. Topology # V-style: Rank 0 gets Stage 0 & Stage N-1. Rank 1 gets Stage 1 & Stage N-2... stage_to_rank = build_stage_to_host_rank_topology ( pp_size = pp_size , num_stages = num_stages , style = ScheduleStyle . v ) actions : dict [ int , list [ ActionBase ]] = {} for rank in range ( pp_size ): actions [ rank ] = self . _generate_rank_schedule ( rank = rank , pp_size = pp_size , num_stages = num_stages , target_microbatches = num_microbatches , ) # 2. Inject Communications return add_communication_ops ( compute_actions = actions , stage_to_rank = stage_to_rank , num_stages = num_stages ) def _generate_rank_schedule ( # noqa: C901 self , rank : int , pp_size : int , num_stages : int , target_microbatches : int , ) -> list [ ActionBase ]: # ZBV logic assumes the pipeline is fully saturated to define the loop bounds. # We simulate enough steps to cover the topology startup, then filter # down to the user's requested microbatches at the end. simulated_n_micro = max ( 2 * pp_size - 1 , target_microbatches ) rank_ops : list [ ActionBase ] = [] # -- Stage Identification (V-Shape) -- # s0: The \"Forward-going\" chunk (e.g., Stage 0 for Rank 0) # s1: The \"Backward-coming\" chunk (e.g., Stage N-1 for Rank 0) s0 = rank s1 = num_stages - 1 - rank # -- Counters -- # Track next microbatch index for each operation type on each chunk. # F: Forward, I: Backward Input, W: Backward Weight f0_cnt = 0 b0_cnt = 0 # Input Grad Counter (Chunk 0) w0_cnt = 0 # Weight Grad Counter (Chunk 0) f1_cnt = 0 b1_cnt = 0 # Input Grad Counter (Chunk 1) w1_cnt = 0 # Weight Grad Counter (Chunk 1) # -- Helpers -- def emit_f ( stage : int , idx : int ): rank_ops . append ( ForwardComputeAction ( stage_idx = stage , microbatch_idx = idx )) def emit_i_and_w ( stage : int , idx : int ): rank_ops . append ( BackwardFullInputComputeAction ( stage_idx = stage , microbatch_idx = idx , full_backward = False )) rank_ops . append ( BackwardWeightComputeAction ( stage_idx = stage , microbatch_idx = idx )) def emit_i ( stage : int , idx : int ): rank_ops . append ( BackwardFullInputComputeAction ( stage_idx = stage , microbatch_idx = idx , full_backward = False )) def emit_w ( stage : int , idx : int ): rank_ops . append ( BackwardWeightComputeAction ( stage_idx = stage , microbatch_idx = idx )) # -- Phase 1: Warmup 1 (Chunk 0 Forwards) -- warmup_n1 = 2 * ( pp_size - rank ) - 1 for _ in range ( warmup_n1 ): emit_f ( s0 , f0_cnt ) f0_cnt += 1 # -- Phase 2: Warmup 2 (Interleave F1, F0) -- warmup_n2 = rank for _ in range ( warmup_n2 ): emit_f ( s1 , f1_cnt ) f1_cnt += 1 emit_f ( s0 , f0_cnt ) f0_cnt += 1 # -- Phase 3: Warmup 3 (F1, then B1 I+W) -- warmup_n3 = pp_size - rank for _ in range ( warmup_n3 ): emit_f ( s1 , f1_cnt ) f1_cnt += 1 emit_i_and_w ( s1 , b1_cnt ) b1_cnt += 1 w1_cnt += 1 # -- Phase 4: Stable State -- while f1_cnt < f0_cnt or f0_cnt < simulated_n_micro : # Emit F0 if within bounds if f0_cnt < simulated_n_micro : emit_f ( s0 , f0_cnt ) f0_cnt += 1 # Emit B0 (I+W) emit_i_and_w ( s0 , b0_cnt ) b0_cnt += 1 w0_cnt += 1 # Emit F1 emit_f ( s1 , f1_cnt ) f1_cnt += 1 # Emit B1 (I+W) emit_i_and_w ( s1 , b1_cnt ) b1_cnt += 1 w1_cnt += 1 # -- Phase 5: Cooldown 1 (Splitting I and W) -- # In cooldown, the I and W streams diverge to fill bubbles. cooldown_n1 = rank for _ in range ( cooldown_n1 ): emit_i ( s0 , b0_cnt ) b0_cnt += 1 emit_i ( s1 , b1_cnt ) b1_cnt += 1 # -- Phase 6: Cooldown 2 (I0, then W0) -- cooldown_n2 = pp_size - rank for _ in range ( cooldown_n2 ): # Input Grad Chunk 0 emit_i ( s0 , b0_cnt ) b0_cnt += 1 # Weight Grad Chunk 0 (delayed from previous steps) emit_w ( s0 , w0_cnt ) w0_cnt += 1 # -- Phase 7: Flush Remaining Weights -- # Flush W1 while w1_cnt < b1_cnt : emit_w ( s1 , w1_cnt ) w1_cnt += 1 # Flush W0 while w0_cnt < b0_cnt : emit_w ( s0 , w0_cnt ) w0_cnt += 1 # -- Integrity Check -- if not ( w0_cnt == b0_cnt == f0_cnt ): raise RuntimeError ( f \"ZBV Schedule Failed (Chunk 0): F= { f0_cnt } , I= { b0_cnt } , W= { w0_cnt } \" ) if not ( w1_cnt == b1_cnt == f1_cnt ): raise RuntimeError ( f \"ZBV Schedule Failed (Chunk 1): F= { f1_cnt } , I= { b1_cnt } , W= { w1_cnt } \" ) # -- Post-Process: Filter to Target Microbatches -- # Remove any actions involving simulated microbatches beyond the user's request. final_ops : list [ ActionBase ] = [] for action in rank_ops : if isinstance ( action , ( ForwardComputeAction , BackwardFullInputComputeAction , BackwardWeightComputeAction )): if action . microbatch_idx < target_microbatches : final_ops . append ( action ) else : final_ops . append ( action ) return final_ops @property def num_stages_per_rank ( self ) -> int : return 2 @property def topology_style ( self ) -> ScheduleStyle : return ScheduleStyle . v __init__ () Constructs the ZBV builder. Source code in d9d/pipelining/infra/schedule/program/zerobubblev.py 30 31 def __init__ ( self ): \"\"\"Constructs the ZBV builder.\"\"\" d9d.pipelining.training PipelinedLRScheduler Bases: LRSchedulerProtocol Wrapper that manages multiple LR schedulers for a pipeline parallel rank. Similar to PipelinedOptimizer , this aggregates schedulers corresponding to multiple model stages hosted on the current rank. Source code in d9d/pipelining/training/scheduler.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class PipelinedLRScheduler ( LRSchedulerProtocol ): \"\"\" Wrapper that manages multiple LR schedulers for a pipeline parallel rank. Similar to `PipelinedOptimizer`, this aggregates schedulers corresponding to multiple model stages hosted on the current rank. \"\"\" def __init__ ( self , mesh_pp : DeviceMesh | None , schedulers : list [ LRSchedulerProtocol ]): self . _pp_rank = mesh_pp . get_local_rank () if mesh_pp is not None else 0 self . _schedulers = schedulers def state_dict ( self ) -> dict [ str , Any ]: return { f \"pp_ { self . _pp_rank } _stage_ { i } \" : scheduler . state_dict () for i , scheduler in enumerate ( self . _schedulers )} def load_state_dict ( self , state_dict : dict [ str , Any ]) -> None : for i , scheduler in enumerate ( self . _schedulers ): scheduler . load_state_dict ( state_dict [ f \"pp_ { self . _pp_rank } _stage_ { i } \" ]) def step ( self ) -> None : for scheduler in self . _schedulers : scheduler . step () PipelinedOptimizer Bases: OptimizerProtocol Wrapper that manages multiple optimizers for a pipeline parallel rank. In a pipeline parallel setup, a single rank might host multiple stages, each having its own parameters and optimizer. This class aggregates them into a single interface. Source code in d9d/pipelining/training/optimizer.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 class PipelinedOptimizer ( OptimizerProtocol ): \"\"\" Wrapper that manages multiple optimizers for a pipeline parallel rank. In a pipeline parallel setup, a single rank might host multiple stages, each having its own parameters and optimizer. This class aggregates them into a single interface. \"\"\" def __init__ ( self , mesh_pp : DeviceMesh | None , optimizers : list [ OptimizerProtocol ]): super () . __init__ () self . _pp_rank = mesh_pp . get_local_rank () if mesh_pp is not None else 0 self . _optimizers = optimizers def state_dict ( self ) -> dict [ str , Any ]: pp_rank = self . _pp_rank return { f \"pp_ { pp_rank } _stage_ { i } \" : optimizer . state_dict () for i , optimizer in enumerate ( self . _optimizers )} def load_state_dict ( self , state_dict : dict [ str , Any ]) -> None : pp_rank = self . _pp_rank for i , optimizer in enumerate ( self . _optimizers ): optimizer . load_state_dict ( state_dict [ f \"pp_ { pp_rank } _stage_ { i } \" ]) def step ( self ) -> None : for optimizer in self . _optimizers : optimizer . step () def zero_grad ( self ) -> None : for optimizer in self . _optimizers : optimizer . zero_grad ()","title":"Pipelining Internals"},{"location":"internals/pipelining/#architecture","text":"","title":"Architecture"},{"location":"internals/pipelining/#the-idea","text":"d9d decouples the Schedule Structure from the Runtime Execution . You write a builder (e.g., 1F1B , DualPipe ) that generates a linear list of logical Actions (e.g., Forward(Stage=0, MB=1) , Backward(Stage=0, MB=0) ). If you want, d9d may automatically inject Send / Recv actions into your compute-only schedule based on data dependencies, preventing deadlocks. You run a dumb virtual machine simply iterates the action list and executes them. This makes implementing complex research schedules (like Zero Bubble or DualPipeV) significantly easier than managing state machines or recursive calls.","title":"The Idea"},{"location":"internals/pipelining/#core-components","text":"","title":"Core Components"},{"location":"internals/pipelining/#pipelinestage-infrastagestagepy","text":"Encapsulates a user nn.Module . It is not responsible for deciding when to run. Instead, it provides atomic pipeline stage capabilities (such as forward and backward passes) to the actions and the executor. Consists of: Computation Handlers : ForwardComputeHandler : Performs forward pass, caches inputs/outputs for backward passes. BackwardComputeHandler : Performs backward pass, capable of splitting backward passes into backward_input (dI) and backward_weight (dW) for advanced schedules. Communication Handlers : Contain and manage the P2P buffers for both forward and backward passes.","title":"PipelineStage (infra/stage/stage.py)"},{"location":"internals/pipelining/#actions-infraschedulecomponentruntimeactionpy","text":"The atomic instructions for the pipeline virtual machine. ForwardComputeAction : Run forward on specific microbatch. BackwardFullInputComputeAction : Run backward. Can be configured to compute gradients for inputs-only or inputs+weights. BackwardWeightComputeAction : Compute gradients for weights (used in Zero Bubble schedules). ForwardSendAction / ForwardReceiveAction / BackwardSendAction / BackwardReceiveAction : Network IO. ComposeAction : Composes multiple actions into a single one. Used for Forward/Backward overlap in schedules such as DualPipeV. Actions are designed to be declarative and immutable.","title":"Actions (infra/schedule/component/runtime/action.py)"},{"location":"internals/pipelining/#programs","text":"A Program is simply dict[int, list[ActionBase]] \u2014 a mapping of Rank ID to a sequential list of Actions.","title":"Programs"},{"location":"internals/pipelining/#executor-infraschedulecomponentruntimeexecutorpy","text":"The PipelineScheduleExecutor is the runtime engine. It: Shards global inputs into microbatches. Iterates through the Program action list. Dispatches calls to Action s that perform computation or communication workload.","title":"Executor (infra/schedule/component/runtime/executor.py)"},{"location":"internals/pipelining/#comparison-with-pytorch","text":"The d9d pipelining implementation is heavily inspired by and borrows concepts from the torch.distributed.pipelining API (e.g., ZeroBubble implementation), but refactors the codebase significantly to improve clarity, type safety, and modularity. The main architectural differences lie in the strict separation of concerns and composition over inheritance : Decomposed Stage Logic : PyTorch : Uses a monolithic _PipelineStageBase class that simultaneously manages P2P buffer allocation, gradient accumulation state, and forward/backward execution logic. d9d : Adopts a compositional approach. The PipelineStage class is a thin orchestrator that delegates responsibilities to dedicated handlers. Polymorphic Actions vs Enumeration : PyTorch : Represents schedule instructions using a single generic _Action NamedTuple combined with an Enum ( _ComputationType.FORWARD , _ComputationType.SEND_F , etc.). d9d : Uses a class hierarchy for actions ( ForwardComputeAction , ForwardSendAction , ComposeAction ). This allows the runtime executor to use structural pattern matching ( match/case ) rather than large if/elif blocks checking enums, allows different actions to carry different metadata (e.g. full_backward flag), and improves static type checking. Builder Pattern vs Schedule Classes : PyTorch : Often couples the schedule definition with the runtime object (e.g., Schedule1F1B class contains both the logic to generate the ordering and the logic to execute it). d9d : Strictly separates the Program Builder (which generates the list of actions) from the Executor (which runs the actions). This makes it easier to inspect a schedule plan before execution or swap scheduling algorithms without changing the runtime driver.","title":"Comparison with PyTorch"},{"location":"internals/pipelining/#building-custom-schedules","text":"To build a new schedule, you create a PipelineProgramBuilder .","title":"Building Custom Schedules"},{"location":"internals/pipelining/#implement-the-builder","text":"You must implement the pipeline program builder. from collections import defaultdict from d9d.pipelining.infra.schedule.component.program import PipelineProgramBuilder , build_stage_to_host_rank_topology , ScheduleStyle , add_communication_ops from d9d.pipelining.infra.schedule.component.runtime import ActionBase , ForwardComputeAction class MyFancyScheduleBuilder ( PipelineProgramBuilder ): def __init__ ( self , stages_per_rank : int ): self . _stages_per_rank = stages_per_rank @property def num_stages_per_rank ( self ) -> int : return self . _stages_per_rank @property def topology_style ( self ) -> ScheduleStyle : return ScheduleStyle . loop def compose ( self , num_microbatches : int , pp_size : int ) -> dict [ int , list [ ActionBase ]]: # Map logical stages to ranks stage_to_rank = build_stage_to_host_rank_topology ( num_stages = self . _stages_per_rank * pp_size , style = ScheduleStyle . loop , pp_size = pp_size ) actions = defaultdict ( list ) # 1. Generate Compute Schedule for rank in range ( pp_size ): # ... custom logic to decide order of Fwd/Bwd ... actions [ rank ] . append ( ForwardComputeAction ( stage_idx =... , microbatch_idx =... )) # 2. Inject Communications (Magic Pass) # This analyzes data dependencies between stages and inserts Send/Recvs return add_communication_ops ( actions , stage_to_rank , num_stages = self . _stages_per_rank * pp_size )","title":"Implement the Builder"},{"location":"internals/pipelining/#registering","text":"Add your configuration to factory/config.py and register the builder in factory/factory.py .","title":"Registering"},{"location":"internals/pipelining/#d9d.pipelining.infra.stage","text":"","title":"stage"},{"location":"internals/pipelining/#d9d.pipelining.infra.stage.PipelineStage","text":"Represents a single structural stage in a Pipelined Model. This class acts as an orchestrator that combines StageCommunicationHandler (for I/O) and Forward/BackwardComputeHandler (for execution). It abstracts away the complexity of buffer management, distributed communication, and gradient calculation from the scheduler. Source code in d9d/pipelining/infra/stage/stage.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 class PipelineStage : \"\"\" Represents a single structural stage in a Pipelined Model. This class acts as an orchestrator that combines `StageCommunicationHandler` (for I/O) and `Forward/BackwardComputeHandler` (for execution). It abstracts away the complexity of buffer management, distributed communication, and gradient calculation from the scheduler. \"\"\" def __init__ ( self , info : PipelineStageInfo , module : nn . Module , group : dist . ProcessGroup , stage_to_host_topology : dict [ int , int ], ): \"\"\" Constructs a PipelineStage object. Args: info: Metadata about the stage (index, total stages). module: The PyTorch module executed by this stage. group: The distributed process group for pipeline communications. stage_to_host_topology: Dict mapping stage ID to PP rank hosting it. \"\"\" self . _info = info self . _module = module self . _group = group self . _stage_to_host_topology = stage_to_host_topology self . _has_backward = False self . _forward_comm : StageCommunicationHandler | None = None self . _backward_comm : StageCommunicationHandler | None = None self . _forward_comp = ForwardComputeHandler ( stage_index = info . current_stage , module = module ) self . _backward_comp = BackwardComputeHandler ( stage_index = info . current_stage , module = module ) @property def info ( self ) -> PipelineStageInfo : return self . _info def configure_buffers ( self , num_microbatches : int , has_backward : bool , pipeline_inputs : dict [ str , torch . Tensor ]): \"\"\" Initializes the communication handlers and buffers for the stage. This must be called before execution to establish P2P buffer sizes and directions. Args: num_microbatches: Total number of microbatches to process. has_backward: Does this pipeline stage should store info for a backward pass pipeline_inputs: Pipeline input data. \"\"\" self . _has_backward = has_backward prev_stage_idx = None if self . _info . is_current_stage_first else self . _info . current_stage - 1 next_stage_idx = None if self . _info . is_current_stage_last else self . _info . current_stage + 1 with torch . device ( \"meta\" ): if not isinstance ( self . _module , ModuleSupportsPipelining ): raise TypeError ( \"Module does not implement ModuleSupportsPipelining protocol\" ) inputs_meta = self . _module . infer_stage_inputs_from_pipeline_inputs ( inputs = pipeline_inputs , n_microbatches = num_microbatches ) outputs_meta = self . _module . infer_stage_outputs_from_pipeline_inputs ( inputs = pipeline_inputs , n_microbatches = num_microbatches ) self . _forward_comm = StageCommunicationHandler ( name = \"fwd\" , stage_index = self . _info . current_stage , num_microbatches = num_microbatches , input_stage_index = prev_stage_idx , input_args = inputs_meta , output_stage_index = next_stage_idx , output_args = outputs_meta , group = self . _group , stage_idx_to_host_rank = self . _stage_to_host_topology , ) self . _forward_comm . set_input_requires_grad_ ( requires_grad = has_backward ) if has_backward : # for grad - current stage receives OUTPUTS as inputs and sends INPUTS as outputs # because it is reversed forward self . _backward_comm = StageCommunicationHandler ( name = \"bwd\" , stage_index = self . _info . current_stage , num_microbatches = num_microbatches , input_stage_index = next_stage_idx , input_args = outputs_meta , output_stage_index = prev_stage_idx , output_args = inputs_meta , group = self . _group , stage_idx_to_host_rank = self . _stage_to_host_topology , ) else : self . _backward_comm = None def set_local_fwd_input ( self , inputs : dict [ str , torch . Tensor ], microbatch_index : int ): \"\"\" Sets local forward inputs manually. Used for the V-shape schedulers. \"\"\" if self . _forward_comm is None : raise ValueError ( \"You must configure stage buffers first\" ) self . _forward_comm . set_inputs_local ( inputs , microbatch_index ) def get_local_fwd_output ( self , microbatch_index : int ) -> dict [ str , torch . Tensor ]: return self . _forward_comp . get_outputs ( microbatch_index ) def pop_local_bwd_output ( self , microbatch_index : int ) -> dict [ str , torch . Tensor ]: \"\"\" Retrieves local backward outputs (gradients). \"\"\" if not self . _has_backward : raise ValueError () return self . _backward_comp . pop_for_sending ( microbatch_index ) def set_local_bwd_input ( self , inputs : dict [ str , torch . Tensor ], microbatch_index : int ): \"\"\" Sets local backward inputs (output gradients) manually. \"\"\" if not self . _has_backward : raise ValueError () if self . _backward_comm is None : raise ValueError ( \"You must configure stage buffers first\" ) self . _backward_comm . set_inputs_local ( inputs , microbatch_index ) def get_fwd_recv_ops ( self , microbatch_index : int ) -> list [ dist . P2POp ]: \"\"\"Returns P2P ops to receive forward inputs for the given microbatch.\"\"\" if self . _forward_comm is None : raise ValueError ( \"You must configure stage buffers first\" ) return self . _forward_comm . create_receive_ops ( microbatch_index ) def get_fwd_send_ops ( self , microbatch_index : int ) -> list [ dist . P2POp ]: \"\"\"Returns P2P ops to send forward outputs for the given microbatch.\"\"\" if self . _forward_comm is None : raise ValueError ( \"You must configure stage buffers first\" ) fwd_result = self . _forward_comp . get_outputs ( microbatch_index ) return self . _forward_comm . create_send_ops ( fwd_result ) def get_bwd_recv_ops ( self , microbatch_index : int ) -> list [ dist . P2POp ]: \"\"\"Returns P2P ops to receive backward gradients for the given microbatch.\"\"\" if not self . _has_backward : return [] if self . _backward_comm is None : raise ValueError ( \"You must configure stage buffers first\" ) return self . _backward_comm . create_receive_ops ( microbatch_index ) def get_bwd_send_ops ( self , microbatch_index : int ) -> list [ dist . P2POp ]: \"\"\"Returns P2P ops to send backward gradients for the given microbatch.\"\"\" if not self . _has_backward : return [] if self . _backward_comm is None : raise ValueError ( \"You must configure stage buffers first\" ) bwd_result = self . _backward_comp . pop_for_sending ( microbatch_index ) return self . _backward_comm . create_send_ops ( bwd_result ) def forward_one_chunk ( self , microbatch_index : int , pipeline_inputs : dict [ str , torch . Tensor ], pipeline_kwargs : dict [ str , Any ] | None = None , ): \"\"\" Executes a forward pass for a single microbatch chunk. Fetches inputs from the communication buffer (or `pipeline_inputs` if first stage), runs the computation, and caches the result. Args: microbatch_index: The microbatch index. pipeline_inputs: Inputs provided locally (only used if this is the first stage). pipeline_kwargs: Additional arguments for the module. Returns: The output tensors of the forward pass. \"\"\" if self . _forward_comm is None : raise ValueError ( \"You must configure stage buffers first\" ) if self . _info . is_current_stage_first : inputs = pipeline_inputs else : inputs = self . _forward_comm . get_inputs ( microbatch_index ) kwargs = pipeline_kwargs or {} self . _forward_comp . run ( microbatch_index = microbatch_index , inputs = inputs , kwargs = kwargs ) def backward_one_chunk ( self , microbatch_index : int , loss : torch . Tensor | None = None , full_backward : bool = True ): \"\"\" Executes a backward pass for a single microbatch chunk. Can perform either a full backward or just the input gradients (if `full_backward=False`). It fetches required data from forward cache and communication buffers. Args: microbatch_index: The microbatch index. loss: The loss tensor (only used if this is the last stage). full_backward: If True, computes grads for inputs and weights. If False, only for inputs. \"\"\" if not self . _has_backward : raise ValueError () if self . _backward_comm is None : raise ValueError ( \"You must configure stage buffers first\" ) inputs , fwd_outputs = self . _forward_comp . pop_inputs_outputs ( microbatch_index ) outputs : dict [ str , torch . Tensor ] outputs_grad : dict [ str , torch . Tensor ] | None if self . _info . is_current_stage_last : if loss is None : raise ValueError ( \"Cannot perform backward on last stage without loss specified\" ) outputs = { \"loss\" : loss } outputs_grad = None else : outputs = fwd_outputs outputs_grad = self . _backward_comm . get_inputs ( microbatch_index ) if full_backward : self . _backward_comp . backward_full ( microbatch_index = microbatch_index , inputs = inputs , outputs = outputs , outputs_grad = outputs_grad ) else : self . _backward_comp . backward_input ( microbatch_index = microbatch_index , inputs = inputs , outputs = outputs , outputs_grad = outputs_grad ) if self . _info . is_current_stage_last and not self . _info . is_current_stage_first : for t in fwd_outputs . values (): if not t . _is_view (): # noqa: SLF001 t . detach_ () def backward_weight_one_chunk ( self , microbatch_index : int ): \"\"\" Executes the weight gradient accumulation part of the backward pass. This assumes `backward_one_chunk(..., full_backward=False)` was already called for this microbatch. Args: microbatch_index: The microbatch index. \"\"\" if not self . _has_backward : raise ValueError () self . _backward_comp . backward_weight ( microbatch_index = microbatch_index ) def reset ( self ): \"\"\"Resets the internal state of communication handlers, clearing gradients on buffers.\"\"\" if self . _forward_comm is not None : self . _forward_comm . reset () if self . _backward_comm is not None : self . _backward_comm . reset ()","title":"PipelineStage"},{"location":"internals/pipelining/#d9d.pipelining.infra.stage.PipelineStage.__init__","text":"Constructs a PipelineStage object. Parameters: Name Type Description Default info PipelineStageInfo Metadata about the stage (index, total stages). required module Module The PyTorch module executed by this stage. required group ProcessGroup The distributed process group for pipeline communications. required stage_to_host_topology dict [ int , int ] Dict mapping stage ID to PP rank hosting it. required Source code in d9d/pipelining/infra/stage/stage.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def __init__ ( self , info : PipelineStageInfo , module : nn . Module , group : dist . ProcessGroup , stage_to_host_topology : dict [ int , int ], ): \"\"\" Constructs a PipelineStage object. Args: info: Metadata about the stage (index, total stages). module: The PyTorch module executed by this stage. group: The distributed process group for pipeline communications. stage_to_host_topology: Dict mapping stage ID to PP rank hosting it. \"\"\" self . _info = info self . _module = module self . _group = group self . _stage_to_host_topology = stage_to_host_topology self . _has_backward = False self . _forward_comm : StageCommunicationHandler | None = None self . _backward_comm : StageCommunicationHandler | None = None self . _forward_comp = ForwardComputeHandler ( stage_index = info . current_stage , module = module ) self . _backward_comp = BackwardComputeHandler ( stage_index = info . current_stage , module = module )","title":"__init__"},{"location":"internals/pipelining/#d9d.pipelining.infra.stage.PipelineStage.backward_one_chunk","text":"Executes a backward pass for a single microbatch chunk. Can perform either a full backward or just the input gradients (if full_backward=False ). It fetches required data from forward cache and communication buffers. Parameters: Name Type Description Default microbatch_index int The microbatch index. required loss Tensor | None The loss tensor (only used if this is the last stage). None full_backward bool If True, computes grads for inputs and weights. If False, only for inputs. True Source code in d9d/pipelining/infra/stage/stage.py 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 def backward_one_chunk ( self , microbatch_index : int , loss : torch . Tensor | None = None , full_backward : bool = True ): \"\"\" Executes a backward pass for a single microbatch chunk. Can perform either a full backward or just the input gradients (if `full_backward=False`). It fetches required data from forward cache and communication buffers. Args: microbatch_index: The microbatch index. loss: The loss tensor (only used if this is the last stage). full_backward: If True, computes grads for inputs and weights. If False, only for inputs. \"\"\" if not self . _has_backward : raise ValueError () if self . _backward_comm is None : raise ValueError ( \"You must configure stage buffers first\" ) inputs , fwd_outputs = self . _forward_comp . pop_inputs_outputs ( microbatch_index ) outputs : dict [ str , torch . Tensor ] outputs_grad : dict [ str , torch . Tensor ] | None if self . _info . is_current_stage_last : if loss is None : raise ValueError ( \"Cannot perform backward on last stage without loss specified\" ) outputs = { \"loss\" : loss } outputs_grad = None else : outputs = fwd_outputs outputs_grad = self . _backward_comm . get_inputs ( microbatch_index ) if full_backward : self . _backward_comp . backward_full ( microbatch_index = microbatch_index , inputs = inputs , outputs = outputs , outputs_grad = outputs_grad ) else : self . _backward_comp . backward_input ( microbatch_index = microbatch_index , inputs = inputs , outputs = outputs , outputs_grad = outputs_grad ) if self . _info . is_current_stage_last and not self . _info . is_current_stage_first : for t in fwd_outputs . values (): if not t . _is_view (): # noqa: SLF001 t . detach_ ()","title":"backward_one_chunk"},{"location":"internals/pipelining/#d9d.pipelining.infra.stage.PipelineStage.backward_weight_one_chunk","text":"Executes the weight gradient accumulation part of the backward pass. This assumes backward_one_chunk(..., full_backward=False) was already called for this microbatch. Parameters: Name Type Description Default microbatch_index int The microbatch index. required Source code in d9d/pipelining/infra/stage/stage.py 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 def backward_weight_one_chunk ( self , microbatch_index : int ): \"\"\" Executes the weight gradient accumulation part of the backward pass. This assumes `backward_one_chunk(..., full_backward=False)` was already called for this microbatch. Args: microbatch_index: The microbatch index. \"\"\" if not self . _has_backward : raise ValueError () self . _backward_comp . backward_weight ( microbatch_index = microbatch_index )","title":"backward_weight_one_chunk"},{"location":"internals/pipelining/#d9d.pipelining.infra.stage.PipelineStage.configure_buffers","text":"Initializes the communication handlers and buffers for the stage. This must be called before execution to establish P2P buffer sizes and directions. Parameters: Name Type Description Default num_microbatches int Total number of microbatches to process. required has_backward bool Does this pipeline stage should store info for a backward pass required pipeline_inputs dict [ str , Tensor ] Pipeline input data. required Source code in d9d/pipelining/infra/stage/stage.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def configure_buffers ( self , num_microbatches : int , has_backward : bool , pipeline_inputs : dict [ str , torch . Tensor ]): \"\"\" Initializes the communication handlers and buffers for the stage. This must be called before execution to establish P2P buffer sizes and directions. Args: num_microbatches: Total number of microbatches to process. has_backward: Does this pipeline stage should store info for a backward pass pipeline_inputs: Pipeline input data. \"\"\" self . _has_backward = has_backward prev_stage_idx = None if self . _info . is_current_stage_first else self . _info . current_stage - 1 next_stage_idx = None if self . _info . is_current_stage_last else self . _info . current_stage + 1 with torch . device ( \"meta\" ): if not isinstance ( self . _module , ModuleSupportsPipelining ): raise TypeError ( \"Module does not implement ModuleSupportsPipelining protocol\" ) inputs_meta = self . _module . infer_stage_inputs_from_pipeline_inputs ( inputs = pipeline_inputs , n_microbatches = num_microbatches ) outputs_meta = self . _module . infer_stage_outputs_from_pipeline_inputs ( inputs = pipeline_inputs , n_microbatches = num_microbatches ) self . _forward_comm = StageCommunicationHandler ( name = \"fwd\" , stage_index = self . _info . current_stage , num_microbatches = num_microbatches , input_stage_index = prev_stage_idx , input_args = inputs_meta , output_stage_index = next_stage_idx , output_args = outputs_meta , group = self . _group , stage_idx_to_host_rank = self . _stage_to_host_topology , ) self . _forward_comm . set_input_requires_grad_ ( requires_grad = has_backward ) if has_backward : # for grad - current stage receives OUTPUTS as inputs and sends INPUTS as outputs # because it is reversed forward self . _backward_comm = StageCommunicationHandler ( name = \"bwd\" , stage_index = self . _info . current_stage , num_microbatches = num_microbatches , input_stage_index = next_stage_idx , input_args = outputs_meta , output_stage_index = prev_stage_idx , output_args = inputs_meta , group = self . _group , stage_idx_to_host_rank = self . _stage_to_host_topology , ) else : self . _backward_comm = None","title":"configure_buffers"},{"location":"internals/pipelining/#d9d.pipelining.infra.stage.PipelineStage.forward_one_chunk","text":"Executes a forward pass for a single microbatch chunk. Fetches inputs from the communication buffer (or pipeline_inputs if first stage), runs the computation, and caches the result. Parameters: Name Type Description Default microbatch_index int The microbatch index. required pipeline_inputs dict [ str , Tensor ] Inputs provided locally (only used if this is the first stage). required pipeline_kwargs dict [ str , Any ] | None Additional arguments for the module. None Returns: Type Description The output tensors of the forward pass. Source code in d9d/pipelining/infra/stage/stage.py 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 def forward_one_chunk ( self , microbatch_index : int , pipeline_inputs : dict [ str , torch . Tensor ], pipeline_kwargs : dict [ str , Any ] | None = None , ): \"\"\" Executes a forward pass for a single microbatch chunk. Fetches inputs from the communication buffer (or `pipeline_inputs` if first stage), runs the computation, and caches the result. Args: microbatch_index: The microbatch index. pipeline_inputs: Inputs provided locally (only used if this is the first stage). pipeline_kwargs: Additional arguments for the module. Returns: The output tensors of the forward pass. \"\"\" if self . _forward_comm is None : raise ValueError ( \"You must configure stage buffers first\" ) if self . _info . is_current_stage_first : inputs = pipeline_inputs else : inputs = self . _forward_comm . get_inputs ( microbatch_index ) kwargs = pipeline_kwargs or {} self . _forward_comp . run ( microbatch_index = microbatch_index , inputs = inputs , kwargs = kwargs )","title":"forward_one_chunk"},{"location":"internals/pipelining/#d9d.pipelining.infra.stage.PipelineStage.get_bwd_recv_ops","text":"Returns P2P ops to receive backward gradients for the given microbatch. Source code in d9d/pipelining/infra/stage/stage.py 168 169 170 171 172 173 174 175 176 177 def get_bwd_recv_ops ( self , microbatch_index : int ) -> list [ dist . P2POp ]: \"\"\"Returns P2P ops to receive backward gradients for the given microbatch.\"\"\" if not self . _has_backward : return [] if self . _backward_comm is None : raise ValueError ( \"You must configure stage buffers first\" ) return self . _backward_comm . create_receive_ops ( microbatch_index )","title":"get_bwd_recv_ops"},{"location":"internals/pipelining/#d9d.pipelining.infra.stage.PipelineStage.get_bwd_send_ops","text":"Returns P2P ops to send backward gradients for the given microbatch. Source code in d9d/pipelining/infra/stage/stage.py 179 180 181 182 183 184 185 186 187 188 189 def get_bwd_send_ops ( self , microbatch_index : int ) -> list [ dist . P2POp ]: \"\"\"Returns P2P ops to send backward gradients for the given microbatch.\"\"\" if not self . _has_backward : return [] if self . _backward_comm is None : raise ValueError ( \"You must configure stage buffers first\" ) bwd_result = self . _backward_comp . pop_for_sending ( microbatch_index ) return self . _backward_comm . create_send_ops ( bwd_result )","title":"get_bwd_send_ops"},{"location":"internals/pipelining/#d9d.pipelining.infra.stage.PipelineStage.get_fwd_recv_ops","text":"Returns P2P ops to receive forward inputs for the given microbatch. Source code in d9d/pipelining/infra/stage/stage.py 151 152 153 154 155 156 157 def get_fwd_recv_ops ( self , microbatch_index : int ) -> list [ dist . P2POp ]: \"\"\"Returns P2P ops to receive forward inputs for the given microbatch.\"\"\" if self . _forward_comm is None : raise ValueError ( \"You must configure stage buffers first\" ) return self . _forward_comm . create_receive_ops ( microbatch_index )","title":"get_fwd_recv_ops"},{"location":"internals/pipelining/#d9d.pipelining.infra.stage.PipelineStage.get_fwd_send_ops","text":"Returns P2P ops to send forward outputs for the given microbatch. Source code in d9d/pipelining/infra/stage/stage.py 159 160 161 162 163 164 165 166 def get_fwd_send_ops ( self , microbatch_index : int ) -> list [ dist . P2POp ]: \"\"\"Returns P2P ops to send forward outputs for the given microbatch.\"\"\" if self . _forward_comm is None : raise ValueError ( \"You must configure stage buffers first\" ) fwd_result = self . _forward_comp . get_outputs ( microbatch_index ) return self . _forward_comm . create_send_ops ( fwd_result )","title":"get_fwd_send_ops"},{"location":"internals/pipelining/#d9d.pipelining.infra.stage.PipelineStage.pop_local_bwd_output","text":"Retrieves local backward outputs (gradients). Source code in d9d/pipelining/infra/stage/stage.py 128 129 130 131 132 133 134 135 136 def pop_local_bwd_output ( self , microbatch_index : int ) -> dict [ str , torch . Tensor ]: \"\"\" Retrieves local backward outputs (gradients). \"\"\" if not self . _has_backward : raise ValueError () return self . _backward_comp . pop_for_sending ( microbatch_index )","title":"pop_local_bwd_output"},{"location":"internals/pipelining/#d9d.pipelining.infra.stage.PipelineStage.reset","text":"Resets the internal state of communication handlers, clearing gradients on buffers. Source code in d9d/pipelining/infra/stage/stage.py 287 288 289 290 291 292 293 def reset ( self ): \"\"\"Resets the internal state of communication handlers, clearing gradients on buffers.\"\"\" if self . _forward_comm is not None : self . _forward_comm . reset () if self . _backward_comm is not None : self . _backward_comm . reset ()","title":"reset"},{"location":"internals/pipelining/#d9d.pipelining.infra.stage.PipelineStage.set_local_bwd_input","text":"Sets local backward inputs (output gradients) manually. Source code in d9d/pipelining/infra/stage/stage.py 138 139 140 141 142 143 144 145 146 147 148 149 def set_local_bwd_input ( self , inputs : dict [ str , torch . Tensor ], microbatch_index : int ): \"\"\" Sets local backward inputs (output gradients) manually. \"\"\" if not self . _has_backward : raise ValueError () if self . _backward_comm is None : raise ValueError ( \"You must configure stage buffers first\" ) self . _backward_comm . set_inputs_local ( inputs , microbatch_index )","title":"set_local_bwd_input"},{"location":"internals/pipelining/#d9d.pipelining.infra.stage.PipelineStage.set_local_fwd_input","text":"Sets local forward inputs manually. Used for the V-shape schedulers. Source code in d9d/pipelining/infra/stage/stage.py 113 114 115 116 117 118 119 120 121 122 123 def set_local_fwd_input ( self , inputs : dict [ str , torch . Tensor ], microbatch_index : int ): \"\"\" Sets local forward inputs manually. Used for the V-shape schedulers. \"\"\" if self . _forward_comm is None : raise ValueError ( \"You must configure stage buffers first\" ) self . _forward_comm . set_inputs_local ( inputs , microbatch_index )","title":"set_local_fwd_input"},{"location":"internals/pipelining/#d9d.pipelining.infra.schedule.component.runtime","text":"Pipelining Runtime Package.","title":"runtime"},{"location":"internals/pipelining/#d9d.pipelining.infra.schedule.component.runtime.ActionBase","text":"Bases: ABC Abstract base class for all pipeline schedule actions. An action represents an atomic unit of work in a pipeline schedule, such as computing a microbatch or sending/receiving a tensor. Source code in d9d/pipelining/infra/schedule/component/runtime/action.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 class ActionBase ( abc . ABC ): \"\"\" Abstract base class for all pipeline schedule actions. An action represents an atomic unit of work in a pipeline schedule, such as computing a microbatch or sending/receiving a tensor. \"\"\" @abc . abstractmethod def apply ( self , ctx : ActionContext ): \"\"\" Executes the action logic using the provided context. Args: ctx: The runtime context containing stages, data, and communication handlers. \"\"\" ... @property @abc . abstractmethod def work_type ( self ) -> ActionWorkType : \"\"\"Returns the classification of work this action performs.\"\"\" ... @property @abc . abstractmethod def has_backward_work ( self ) -> bool : \"\"\"Returns True if this action involves backward pass computations.\"\"\" ... @abc . abstractmethod def __str__ ( self ) -> str : \"\"\"Returns a short string representation of the action for logging/visualization.\"\"\" ...","title":"ActionBase"},{"location":"internals/pipelining/#d9d.pipelining.infra.schedule.component.runtime.ActionBase.has_backward_work","text":"Returns True if this action involves backward pass computations.","title":"has_backward_work"},{"location":"internals/pipelining/#d9d.pipelining.infra.schedule.component.runtime.ActionBase.work_type","text":"Returns the classification of work this action performs.","title":"work_type"},{"location":"internals/pipelining/#d9d.pipelining.infra.schedule.component.runtime.ActionBase.__str__","text":"Returns a short string representation of the action for logging/visualization. Source code in d9d/pipelining/infra/schedule/component/runtime/action.py 79 80 81 82 @abc . abstractmethod def __str__ ( self ) -> str : \"\"\"Returns a short string representation of the action for logging/visualization.\"\"\" ...","title":"__str__"},{"location":"internals/pipelining/#d9d.pipelining.infra.schedule.component.runtime.ActionBase.apply","text":"Executes the action logic using the provided context. Parameters: Name Type Description Default ctx ActionContext The runtime context containing stages, data, and communication handlers. required Source code in d9d/pipelining/infra/schedule/component/runtime/action.py 56 57 58 59 60 61 62 63 64 65 @abc . abstractmethod def apply ( self , ctx : ActionContext ): \"\"\" Executes the action logic using the provided context. Args: ctx: The runtime context containing stages, data, and communication handlers. \"\"\" ...","title":"apply"},{"location":"internals/pipelining/#d9d.pipelining.infra.schedule.component.runtime.BackwardFullInputComputeAction","text":"Bases: ActionBase Action to perform backward computation with respect to inputs. Attributes: Name Type Description stage_idx int The integer index of the pipeline stage. microbatch_idx int The integer index of the microbatch to compute. full_backward bool If True, performs a full backward pass including inputs and weights. If False, may only compute gradients w.r.t inputs (depending on schedule implementation). Source code in d9d/pipelining/infra/schedule/component/runtime/action.py 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 @dataclasses . dataclass ( frozen = True , slots = True ) class BackwardFullInputComputeAction ( ActionBase ): \"\"\" Action to perform backward computation with respect to inputs. Attributes: stage_idx: The integer index of the pipeline stage. microbatch_idx: The integer index of the microbatch to compute. full_backward: If True, performs a full backward pass including inputs and weights. If False, may only compute gradients w.r.t inputs (depending on schedule implementation). \"\"\" stage_idx : int microbatch_idx : int full_backward : bool def apply ( self , ctx : ActionContext ): stage = ctx . stages [ self . stage_idx ] if not stage . info . is_current_stage_last and self . stage_idx + 1 not in ctx . stages : ctx . communications . wait_bwd_recv ( self . stage_idx , self . microbatch_idx ) if stage . info . is_current_stage_last and isinstance ( ctx . callback , PipelineLossHandler ): loss = ctx . callback . acquire_loss ( self . microbatch_idx ) else : loss = None stage . backward_one_chunk ( microbatch_index = self . microbatch_idx , full_backward = self . full_backward , loss = loss ) if not stage . info . is_current_stage_first and self . stage_idx - 1 in ctx . stages : ctx . stages [ self . stage_idx - 1 ] . set_local_bwd_input ( microbatch_index = self . microbatch_idx , inputs = stage . pop_local_bwd_output ( self . microbatch_idx ) ) @property def work_type ( self ) -> ActionWorkType : return ActionWorkType . compute @property def has_backward_work ( self ) -> bool : return True def __str__ ( self ) -> str : letter = \"B\" if self . full_backward else \"I\" return f \" { self . stage_idx }{ letter }{ self . microbatch_idx } \"","title":"BackwardFullInputComputeAction"},{"location":"internals/pipelining/#d9d.pipelining.infra.schedule.component.runtime.BackwardReceiveAction","text":"Bases: ActionBase Action to schedule a backward pass gradient receive operation. Attributes: Name Type Description stage_idx int The integer index of the pipeline stage expecting the receive operation. microbatch_idx int The integer index of the microbatch being received. Source code in d9d/pipelining/infra/schedule/component/runtime/action.py 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 @dataclasses . dataclass ( frozen = True , slots = True ) class BackwardReceiveAction ( ActionBase ): \"\"\" Action to schedule a backward pass gradient receive operation. Attributes: stage_idx: The integer index of the pipeline stage expecting the receive operation. microbatch_idx: The integer index of the microbatch being received. \"\"\" stage_idx : int microbatch_idx : int def apply ( self , ctx : ActionContext ): ctx . communications . schedule_bwd_recv ( self . stage_idx , self . microbatch_idx ) @property def work_type ( self ) -> ActionWorkType : return ActionWorkType . communicate @property def has_backward_work ( self ) -> bool : return True def __str__ ( self ) -> str : return f \" { self . stage_idx } RECV_B { self . microbatch_idx } \"","title":"BackwardReceiveAction"},{"location":"internals/pipelining/#d9d.pipelining.infra.schedule.component.runtime.BackwardSendAction","text":"Bases: ActionBase Action to schedule a backward pass gradient send operation. Attributes: Name Type Description stage_idx int The integer index of the pipeline stage initiating the send operation. microbatch_idx int The integer index of the microbatch being sent. Source code in d9d/pipelining/infra/schedule/component/runtime/action.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 @dataclasses . dataclass ( frozen = True , slots = True ) class BackwardSendAction ( ActionBase ): \"\"\" Action to schedule a backward pass gradient send operation. Attributes: stage_idx: The integer index of the pipeline stage initiating the send operation. microbatch_idx: The integer index of the microbatch being sent. \"\"\" stage_idx : int microbatch_idx : int def apply ( self , ctx : ActionContext ): ctx . communications . schedule_bwd_send ( self . stage_idx , self . microbatch_idx ) @property def work_type ( self ) -> ActionWorkType : return ActionWorkType . communicate @property def has_backward_work ( self ) -> bool : return True def __str__ ( self ) -> str : return f \" { self . stage_idx } SEND_B { self . microbatch_idx } \"","title":"BackwardSendAction"},{"location":"internals/pipelining/#d9d.pipelining.infra.schedule.component.runtime.BackwardWeightComputeAction","text":"Bases: ActionBase Action to perform gradient accumulation on weights. Attributes: Name Type Description stage_idx int The integer index of the pipeline stage. microbatch_idx int The integer index of the microbatch to compute. Source code in d9d/pipelining/infra/schedule/component/runtime/action.py 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 @dataclasses . dataclass ( frozen = True , slots = True ) class BackwardWeightComputeAction ( ActionBase ): \"\"\" Action to perform gradient accumulation on weights. Attributes: stage_idx: The integer index of the pipeline stage. microbatch_idx: The integer index of the microbatch to compute. \"\"\" stage_idx : int microbatch_idx : int def apply ( self , ctx : ActionContext ): stage = ctx . stages [ self . stage_idx ] stage . backward_weight_one_chunk ( microbatch_index = self . microbatch_idx ) @property def work_type ( self ) -> ActionWorkType : return ActionWorkType . compute @property def has_backward_work ( self ) -> bool : return True def __str__ ( self ) -> str : return f \" { self . stage_idx } W { self . microbatch_idx } \"","title":"BackwardWeightComputeAction"},{"location":"internals/pipelining/#d9d.pipelining.infra.schedule.component.runtime.ComposeAction","text":"Bases: ActionBase Composite action scheduling multiple sub-actions sequentially. Used for forward/backward overlapping. Attributes: Name Type Description actions tuple [ ActionBase , ...] A tuple of sub-actions to be executed sequentially. Source code in d9d/pipelining/infra/schedule/component/runtime/action.py 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 @dataclasses . dataclass ( frozen = True , slots = True ) class ComposeAction ( ActionBase ): \"\"\" Composite action scheduling multiple sub-actions sequentially. Used for forward/backward overlapping. Attributes: actions: A tuple of sub-actions to be executed sequentially. \"\"\" actions : tuple [ ActionBase , ... ] def apply ( self , ctx : ActionContext ): for act in self . actions : act . apply ( ctx ) @property def work_type ( self ) -> ActionWorkType : sub_work_types = { x . work_type for x in self . actions } if len ( sub_work_types ) != 1 : raise ValueError ( \"\" ) return next ( iter ( sub_work_types )) @property def has_backward_work ( self ) -> bool : return any ( x . has_backward_work for x in self . actions ) def __str__ ( self ) -> str : return \"|\" . join ( map ( str , self . actions ))","title":"ComposeAction"},{"location":"internals/pipelining/#d9d.pipelining.infra.schedule.component.runtime.ForwardComputeAction","text":"Bases: ActionBase Action to perform forward computation for a specific microbatch. Attributes: Name Type Description stage_idx int The integer index of the pipeline stage. microbatch_idx int The integer index of the microbatch to compute. Source code in d9d/pipelining/infra/schedule/component/runtime/action.py 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 @dataclasses . dataclass ( frozen = True , slots = True ) class ForwardComputeAction ( ActionBase ): \"\"\" Action to perform forward computation for a specific microbatch. Attributes: stage_idx: The integer index of the pipeline stage. microbatch_idx: The integer index of the microbatch to compute. \"\"\" stage_idx : int microbatch_idx : int def apply ( self , ctx : ActionContext ): stage = ctx . stages [ self . stage_idx ] if not stage . info . is_current_stage_first and self . stage_idx - 1 not in ctx . stages : ctx . communications . wait_fwd_recv ( self . stage_idx , self . microbatch_idx ) stage . forward_one_chunk ( microbatch_index = self . microbatch_idx , pipeline_inputs = ctx . pipeline_inputs_microbatches [ self . microbatch_idx ], pipeline_kwargs = ctx . pipeline_kwargs_microbatches [ self . microbatch_idx ], ) result = stage . get_local_fwd_output ( self . microbatch_idx ) if stage . info . is_current_stage_last : ctx . callback . trigger ( result , self . microbatch_idx ) if not stage . info . is_current_stage_last and self . stage_idx + 1 in ctx . stages : ctx . stages [ self . stage_idx + 1 ] . set_local_fwd_input ( inputs = result , microbatch_index = self . microbatch_idx ) @property def work_type ( self ) -> ActionWorkType : return ActionWorkType . compute @property def has_backward_work ( self ) -> bool : return False def __str__ ( self ) -> str : return f \" { self . stage_idx } F { self . microbatch_idx } \"","title":"ForwardComputeAction"},{"location":"internals/pipelining/#d9d.pipelining.infra.schedule.component.runtime.ForwardReceiveAction","text":"Bases: ActionBase Action to schedule a forward pass tensor receive operation. Attributes: Name Type Description stage_idx int The integer index of the pipeline stage expecting the receive operation. microbatch_idx int The integer index of the microbatch being received. Source code in d9d/pipelining/infra/schedule/component/runtime/action.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 @dataclasses . dataclass ( frozen = True , slots = True ) class ForwardReceiveAction ( ActionBase ): \"\"\" Action to schedule a forward pass tensor receive operation. Attributes: stage_idx: The integer index of the pipeline stage expecting the receive operation. microbatch_idx: The integer index of the microbatch being received. \"\"\" stage_idx : int microbatch_idx : int def apply ( self , ctx : ActionContext ): ctx . communications . schedule_fwd_recv ( self . stage_idx , self . microbatch_idx ) @property def work_type ( self ) -> ActionWorkType : return ActionWorkType . communicate @property def has_backward_work ( self ) -> bool : return True def __str__ ( self ) -> str : return f \" { self . stage_idx } RECV_F { self . microbatch_idx } \"","title":"ForwardReceiveAction"},{"location":"internals/pipelining/#d9d.pipelining.infra.schedule.component.runtime.ForwardSendAction","text":"Bases: ActionBase Action to schedule a forward pass tensor send operation. Attributes: Name Type Description stage_idx int The integer index of the pipeline stage initiating the send operation. microbatch_idx int The integer index of the microbatch being sent. Source code in d9d/pipelining/infra/schedule/component/runtime/action.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 @dataclasses . dataclass ( frozen = True , slots = True ) class ForwardSendAction ( ActionBase ): \"\"\" Action to schedule a forward pass tensor send operation. Attributes: stage_idx: The integer index of the pipeline stage initiating the send operation. microbatch_idx: The integer index of the microbatch being sent. \"\"\" stage_idx : int microbatch_idx : int def apply ( self , ctx : ActionContext ): ctx . communications . schedule_fwd_send ( self . stage_idx , self . microbatch_idx ) @property def work_type ( self ) -> ActionWorkType : return ActionWorkType . communicate @property def has_backward_work ( self ) -> bool : return False def __str__ ( self ) -> str : return f \" { self . stage_idx } SEND_F { self . microbatch_idx } \"","title":"ForwardSendAction"},{"location":"internals/pipelining/#d9d.pipelining.infra.schedule.component.runtime.OfflinePipelineExecutor","text":"Bases: PipelineSchedule Executes the model immediately without pipeline parallelism. This schedule treats the execution as a single stage with a single microbatch, running the forward and optionally backward pass directly. This is primarily used for single-device execution within the pipeline abstraction. Source code in d9d/pipelining/infra/schedule/component/runtime/offline.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 class OfflinePipelineExecutor ( PipelineSchedule ): \"\"\" Executes the model immediately without pipeline parallelism. This schedule treats the execution as a single stage with a single microbatch, running the forward and optionally backward pass directly. This is primarily used for single-device execution within the pipeline abstraction. \"\"\" def __init__ ( self , model : nn . Module , callback : PipelineLossFn | PipelineResultFn , do_backward : bool ): \"\"\" Constructs the offline pipeline executor. Args: model: The PyTorch module to execute. callback: Function to compute loss or process pipeline results. do_backward: Whether to execute the backward pass. \"\"\" self . _model = model self . _callback = callback self . _do_backward = do_backward def configure_buffers ( self , inputs : dict [ str , torch . Tensor ], kwargs : dict [ str , Any ], sharding_spec : PipelineShardingSpec | None ): pass def _forward_only ( self , inputs : dict [ str , torch . Tensor ], kwargs : dict [ str , Any ]): result = self . _model ( ** inputs , ** kwargs ) self . _callback ( result , 0 ) # microbatch=0 def _forward_backward ( self , inputs : dict [ str , torch . Tensor ], kwargs : dict [ str , Any ]): result = self . _model ( ** inputs , ** kwargs ) loss = self . _callback ( result , 0 ) # microbatch=0 del result # do not peak memory loss . backward () def step ( self , inputs : dict [ str , torch . Tensor ], kwargs : dict [ str , Any ]): result = self . _model ( ** inputs , ** kwargs ) processing_result = self . _callback ( result , 0 ) if self . _do_backward : if not isinstance ( processing_result , torch . Tensor ): raise ValueError ( \"Loss should be torch.Tensor\" ) del result # do not peak memory processing_result . backward ()","title":"OfflinePipelineExecutor"},{"location":"internals/pipelining/#d9d.pipelining.infra.schedule.component.runtime.OfflinePipelineExecutor.__init__","text":"Constructs the offline pipeline executor. Parameters: Name Type Description Default model Module The PyTorch module to execute. required callback PipelineLossFn | PipelineResultFn Function to compute loss or process pipeline results. required do_backward bool Whether to execute the backward pass. required Source code in d9d/pipelining/infra/schedule/component/runtime/offline.py 18 19 20 21 22 23 24 25 26 27 28 29 30 def __init__ ( self , model : nn . Module , callback : PipelineLossFn | PipelineResultFn , do_backward : bool ): \"\"\" Constructs the offline pipeline executor. Args: model: The PyTorch module to execute. callback: Function to compute loss or process pipeline results. do_backward: Whether to execute the backward pass. \"\"\" self . _model = model self . _callback = callback self . _do_backward = do_backward","title":"__init__"},{"location":"internals/pipelining/#d9d.pipelining.infra.schedule.component.runtime.PipelineScheduleExecutor","text":"Bases: PipelineSchedule Executes a defined pipeline schedule by interpreting a sequence of actions. Source code in d9d/pipelining/infra/schedule/component/runtime/executor.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 class PipelineScheduleExecutor ( PipelineSchedule ): \"\"\"Executes a defined pipeline schedule by interpreting a sequence of actions.\"\"\" def __init__ ( self , dist_context : DistributedContext , stages : list [ PipelineStage ], num_microbatches : int , callback : PipelineLossFn | PipelineResultFn , program : dict [ int , list [ ActionBase ]], ): \"\"\" Constructs the schedule executor. Args: dist_context: The distributed context. stages: List of stages managed by this executor. num_microbatches: Number of microbatches the global batch is split. callback: Function to compute loss or process pipeline results. program: The execution plan mapping rank ID to a list of actions. \"\"\" self . _dist_ctx = dist_context self . _stages = { stage . info . current_stage : stage for stage in stages } self . _num_microbatches = num_microbatches self . _program = program self . _has_backward = any ( any ( action . has_backward_work for action in sub_program ) for sub_program in program . values () ) self . _comm_handler = PipelineCommunicationHandler ( self . _stages ) self . _callback : PipelineLossHandler | PipelineResultHandler if self . _has_backward : self . _callback = PipelineLossHandler ( callback ) else : self . _callback = PipelineResultHandler ( callback ) self . _input_data_sharding_spec : ShardingSpec | None = None self . _input_kwargs_sharding_spec : ShardingSpec | None = None def configure_buffers ( self , inputs : dict [ str , torch . Tensor ], kwargs : dict [ str , Any ], sharding_spec : PipelineShardingSpec | None ): if sharding_spec is None or sharding_spec . input_data is None : self . _input_data_sharding_spec = shard_spec_on_dim ( inputs , dim = 0 ) if sharding_spec is None or sharding_spec . input_kwargs is None : self . _input_kwargs_sharding_spec = shard_spec_on_dim ( kwargs , dim = 0 ) for stage in self . _stages . values (): stage . configure_buffers ( num_microbatches = self . _num_microbatches , pipeline_inputs = inputs , has_backward = self . _has_backward ) def step ( self , inputs : dict [ str , torch . Tensor ], kwargs : dict [ str , Any ]): if self . _input_data_sharding_spec is None or self . _input_kwargs_sharding_spec is None : raise ValueError ( \"Please configure sharding specs first\" ) self . _dist_ctx . logger . debug ( \"Begin pipeline step\" ) pp_group = self . _dist_ctx . mesh_for ( REGULAR_DOMAIN ) . get_group ( \"pp\" ) for stage in self . _stages . values (): stage . reset () # Shard inputs and kwargs to microbatches inputs_shard = shard_tree ( inputs , num_shards = self . _num_microbatches , sharding_spec = self . _input_data_sharding_spec , enforce_even_split = True , ) kwargs_shard = shard_tree ( kwargs , num_shards = self . _num_microbatches , sharding_spec = self . _input_kwargs_sharding_spec , enforce_even_split = True , ) my_program = self . _program [ pp_group . rank ()] for action in my_program : with record_function ( str ( action )): self . _dist_ctx . logger . debug ( f \"Running pipeline action { action } \" ) action . apply ( ActionContext ( callback = self . _callback , stages = self . _stages , communications = self . _comm_handler , pipeline_inputs_microbatches = inputs_shard , pipeline_kwargs_microbatches = kwargs_shard , ) ) self . _dist_ctx . logger . debug ( \"Waiting for potentially hanging PP send comms\" ) self . _comm_handler . wait_send_all () # finalize just in case self . _dist_ctx . logger . debug ( \"End pipeline step\" )","title":"PipelineScheduleExecutor"},{"location":"internals/pipelining/#d9d.pipelining.infra.schedule.component.runtime.PipelineScheduleExecutor.__init__","text":"Constructs the schedule executor. Parameters: Name Type Description Default dist_context DistributedContext The distributed context. required stages list [ PipelineStage ] List of stages managed by this executor. required num_microbatches int Number of microbatches the global batch is split. required callback PipelineLossFn | PipelineResultFn Function to compute loss or process pipeline results. required program dict [ int , list [ ActionBase ]] The execution plan mapping rank ID to a list of actions. required Source code in d9d/pipelining/infra/schedule/component/runtime/executor.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def __init__ ( self , dist_context : DistributedContext , stages : list [ PipelineStage ], num_microbatches : int , callback : PipelineLossFn | PipelineResultFn , program : dict [ int , list [ ActionBase ]], ): \"\"\" Constructs the schedule executor. Args: dist_context: The distributed context. stages: List of stages managed by this executor. num_microbatches: Number of microbatches the global batch is split. callback: Function to compute loss or process pipeline results. program: The execution plan mapping rank ID to a list of actions. \"\"\" self . _dist_ctx = dist_context self . _stages = { stage . info . current_stage : stage for stage in stages } self . _num_microbatches = num_microbatches self . _program = program self . _has_backward = any ( any ( action . has_backward_work for action in sub_program ) for sub_program in program . values () ) self . _comm_handler = PipelineCommunicationHandler ( self . _stages ) self . _callback : PipelineLossHandler | PipelineResultHandler if self . _has_backward : self . _callback = PipelineLossHandler ( callback ) else : self . _callback = PipelineResultHandler ( callback ) self . _input_data_sharding_spec : ShardingSpec | None = None self . _input_kwargs_sharding_spec : ShardingSpec | None = None","title":"__init__"},{"location":"internals/pipelining/#d9d.pipelining.infra.schedule.component.program","text":"Pipeline Schedule Building Components. This package provides the core building blocks and compiler passes used to generate execution schedules for distributed pipelines.","title":"program"},{"location":"internals/pipelining/#d9d.pipelining.infra.schedule.component.program.PipelineProgramBuilder","text":"Bases: ABC Abstract interface for building pipeline execution schedules. Source code in d9d/pipelining/infra/schedule/component/program/base.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class PipelineProgramBuilder ( abc . ABC ): \"\"\"Abstract interface for building pipeline execution schedules.\"\"\" @abc . abstractmethod def compose ( self , num_microbatches : int , pp_size : int ) -> dict [ int , list [ ActionBase ]]: \"\"\" Generates the execution program for all ranks in the pipeline. Args: num_microbatches: Number of microbatches per step. pp_size: Number of pipeline parallel ranks. Returns: A dictionary mapping rank indices to their list of sequential actions. \"\"\" ... @property @abc . abstractmethod def num_stages_per_rank ( self ) -> int : \"\"\"Returns the number of model stages designated for each rank.\"\"\" ... @property @abc . abstractmethod def topology_style ( self ) -> ScheduleStyle : \"\"\"Returns the topology style strategy used to assign stages to ranks.\"\"\" ...","title":"PipelineProgramBuilder"},{"location":"internals/pipelining/#d9d.pipelining.infra.schedule.component.program.PipelineProgramBuilder.num_stages_per_rank","text":"Returns the number of model stages designated for each rank.","title":"num_stages_per_rank"},{"location":"internals/pipelining/#d9d.pipelining.infra.schedule.component.program.PipelineProgramBuilder.topology_style","text":"Returns the topology style strategy used to assign stages to ranks.","title":"topology_style"},{"location":"internals/pipelining/#d9d.pipelining.infra.schedule.component.program.PipelineProgramBuilder.compose","text":"Generates the execution program for all ranks in the pipeline. Parameters: Name Type Description Default num_microbatches int Number of microbatches per step. required pp_size int Number of pipeline parallel ranks. required Returns: Type Description dict [ int , list [ ActionBase ]] A dictionary mapping rank indices to their list of sequential actions. Source code in d9d/pipelining/infra/schedule/component/program/base.py 10 11 12 13 14 15 16 17 18 19 20 21 22 @abc . abstractmethod def compose ( self , num_microbatches : int , pp_size : int ) -> dict [ int , list [ ActionBase ]]: \"\"\" Generates the execution program for all ranks in the pipeline. Args: num_microbatches: Number of microbatches per step. pp_size: Number of pipeline parallel ranks. Returns: A dictionary mapping rank indices to their list of sequential actions. \"\"\" ...","title":"compose"},{"location":"internals/pipelining/#d9d.pipelining.infra.schedule.component.program.ScheduleStyle","text":"Bases: StrEnum Defines the strategy for mapping logical stages to physical ranks. Attributes: Name Type Description loop Assigns stages in a round-robin circular fashion (mod pp_size). v Assigns stages in a zig-zag V-shape pattern. Useful for interleaved 1F1B schedules. Source code in d9d/pipelining/infra/schedule/component/program/topology.py 5 6 7 8 9 10 11 12 13 14 15 class ScheduleStyle ( StrEnum ): \"\"\" Defines the strategy for mapping logical stages to physical ranks. Attributes: loop: Assigns stages in a round-robin circular fashion (mod pp_size). v: Assigns stages in a zig-zag V-shape pattern. Useful for interleaved 1F1B schedules. \"\"\" loop = \"loop\" v = \"v\"","title":"ScheduleStyle"},{"location":"internals/pipelining/#d9d.pipelining.infra.schedule.component.program.add_communication_ops","text":"Injects communication actions into a computation-only schedule. This function iterates through the provided compute schedule and simulates execution. When a compute action produces a result needed by a different rank, it injects Send/Receive pairs. It also reorders actions to ensure that Receive operations occur before the Computes that depend on them, preventing deadlocks. Parameters: Name Type Description Default compute_actions dict [ int , list [ ActionBase ]] Initial schedule containing only compute operations. required stage_to_rank dict [ int , int ] Mapping from stage index to rank index. required num_stages int Total number of pipeline stages. required Returns: Type Description dict [ int , list [ ActionBase ]] A new schedule dictionary including both compute and communication actions. Raises: Type Description RuntimeError If the schedule simulation enters a deadlock state. Source code in d9d/pipelining/infra/schedule/component/program/communications.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 def add_communication_ops ( compute_actions : dict [ int , list [ ActionBase ]], stage_to_rank : dict [ int , int ], num_stages : int , ) -> dict [ int , list [ ActionBase ]]: \"\"\" Injects communication actions into a computation-only schedule. This function iterates through the provided compute schedule and simulates execution. When a compute action produces a result needed by a different rank, it injects Send/Receive pairs. It also reorders actions to ensure that Receive operations occur before the Computes that depend on them, preventing deadlocks. Args: compute_actions: Initial schedule containing only compute operations. stage_to_rank: Mapping from stage index to rank index. num_stages: Total number of pipeline stages. Returns: A new schedule dictionary including both compute and communication actions. Raises: RuntimeError: If the schedule simulation enters a deadlock state. \"\"\" compute_actions = copy . deepcopy ( compute_actions ) full_actions : dict [ int , list [ ActionBase ]] = { rank : [] for rank in compute_actions } completed_events : dict [ int , set [ ActionBase ]] = { rank : set () for rank in compute_actions } while compute_actions : progress = False for rank in sorted ( compute_actions . keys ()): if not compute_actions [ rank ]: del compute_actions [ rank ] continue current_action = compute_actions [ rank ][ 0 ] sub_actions = _get_sub_actions ( current_action ) # Check readiness if not check_action_communication_dependencies_fulfilled ( current_action , completed_events [ rank ], num_stages ): continue # Execute full_actions [ rank ] . append ( current_action ) compute_actions [ rank ] . pop ( 0 ) progress = True for sub_action in sub_actions : completed_events [ rank ] . add ( sub_action ) comm_pkg = _create_communications_for_action ( sub_action , num_stages = num_stages , stage_to_rank = stage_to_rank ) if comm_pkg : # Add Send locally full_actions [ rank ] . append ( comm_pkg . send ) completed_events [ rank ] . add ( comm_pkg . send ) # Add Recv remotely and unblock target full_actions [ comm_pkg . sends_to_rank ] . append ( comm_pkg . recv ) completed_events [ comm_pkg . sends_to_rank ] . add ( comm_pkg . recv ) if not progress and compute_actions : raise RuntimeError ( \"Deadlock in schedule simulation\" ) return full_actions","title":"add_communication_ops"},{"location":"internals/pipelining/#d9d.pipelining.infra.schedule.component.program.build_stage_to_host_rank_topology","text":"Constructs the mapping from stage index to rank index. Parameters: Name Type Description Default pp_size int Number of pipeline parallel ranks. required num_stages int Total number of model stages. required style ScheduleStyle The topology style to use for assignment. required Returns: Type Description dict [ int , int ] A dictionary mapping stage IDs to Rank IDs. Raises: Type Description ValueError If the style is unknown or if V-style parameters are invalid (num_stages must be divisible by pp_size). Source code in d9d/pipelining/infra/schedule/component/program/topology.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def build_stage_to_host_rank_topology ( pp_size : int , num_stages : int , style : ScheduleStyle ) -> dict [ int , int ]: \"\"\" Constructs the mapping from stage index to rank index. Args: pp_size: Number of pipeline parallel ranks. num_stages: Total number of model stages. style: The topology style to use for assignment. Returns: A dictionary mapping stage IDs to Rank IDs. Raises: ValueError: If the style is unknown or if V-style parameters are invalid (num_stages must be divisible by pp_size). \"\"\" match style : case ScheduleStyle . loop : return { stage_index : stage_index % pp_size for stage_index in range ( num_stages )} case ScheduleStyle . v : if num_stages % pp_size != 0 : raise ValueError ( f \"num_stages { num_stages } must be evenly divisible by pp_size { pp_size } for V schedules\" ) result = {} rank_index = 0 for stage_index in range ( num_stages ): result [ stage_index ] = rank_index if ( stage_index + 1 ) % pp_size == 0 : continue if ( stage_index // pp_size ) % 2 == 0 : rank_index += 1 else : rank_index -= 1 return result case _ : raise ValueError ()","title":"build_stage_to_host_rank_topology"},{"location":"internals/pipelining/#d9d.pipelining.infra.schedule.component.program.invert_stage_to_host_rank_topology","text":"Inverts the topology mapping to list execution stages per rank. Parameters: Name Type Description Default stage_to_host dict [ int , int ] Mapping from stage index to rank index. required Returns: Type Description dict [ int , list [ int ]] A dictionary where keys are Rank IDs and values are lists of Stage IDs dict [ int , list [ int ]] managed by that rank. Source code in d9d/pipelining/infra/schedule/component/program/topology.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def invert_stage_to_host_rank_topology ( stage_to_host : dict [ int , int ]) -> dict [ int , list [ int ]]: \"\"\" Inverts the topology mapping to list execution stages per rank. Args: stage_to_host: Mapping from stage index to rank index. Returns: A dictionary where keys are Rank IDs and values are lists of Stage IDs managed by that rank. \"\"\" host_to_stage = defaultdict ( list ) for stage_idx , host in stage_to_host . items (): host_to_stage [ host ] . append ( stage_idx ) return dict ( host_to_stage )","title":"invert_stage_to_host_rank_topology"},{"location":"internals/pipelining/#d9d.pipelining.infra.schedule.program","text":"Pipeline Schedule Implementations","title":"program"},{"location":"internals/pipelining/#d9d.pipelining.infra.schedule.program.DualPipeVPipelineProgramBuilder","text":"Bases: PipelineProgramBuilder Builder for the DualPipeV Pipeline Parallelism schedule. DualPipeV is a specialized bi-directional pipeline schedule designed for high throughput training. It requires exactly 2 stages per pipeline rank (V-shape) and utilizes split backward passes (Input gradients vs Weight gradients) to fill pipeline bubbles. References https://github.com/deepseek-ai/DualPipe https://hackmd.io/@ufotalent/r1lVXsa9Jg Source code in d9d/pipelining/infra/schedule/program/dualpipev.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 class DualPipeVPipelineProgramBuilder ( PipelineProgramBuilder ): \"\"\" Builder for the DualPipeV Pipeline Parallelism schedule. DualPipeV is a specialized bi-directional pipeline schedule designed for high throughput training. It requires exactly 2 stages per pipeline rank (V-shape) and utilizes split backward passes (Input gradients vs Weight gradients) to fill pipeline bubbles. References: https://github.com/deepseek-ai/DualPipe https://hackmd.io/@ufotalent/r1lVXsa9Jg \"\"\" def __init__ ( self ): \"\"\" Constructs the DualPipeV builder. \"\"\" @staticmethod def _build_for_rank ( # noqa: C901 rank : int , stage_to_rank : dict [ int , int ], num_microbatches : int , pp_size : int ) -> list [ ActionBase ]: compute_actions : list [ ActionBase ] = [] # Identify local stages: s0 is Phase 0, s1 is Phase 1 my_stages = sorted ([ s for s , r in stage_to_rank . items () if r == rank ]) s0 , s1 = my_stages [ 0 ], my_stages [ 1 ] # Track microbatch indices for each stage and operation type # f_idx: Next Forward microbatch # b_idx: Next Backward microbatch (Input or Full) f_idx = { s0 : 0 , s1 : 0 } b_idx = { s0 : 0 , s1 : 0 } # Queue for Zero Bubble optimization: stores (stage, mb_idx) for deferred weight grads weight_queue : deque [ tuple [ int , int ]] = deque () # --- Helper Functions for Action Emission --- def _add_f ( stage : int ): compute_actions . append ( ForwardComputeAction ( stage_idx = stage , microbatch_idx = f_idx [ stage ])) f_idx [ stage ] += 1 def _add_b_full ( stage : int ): compute_actions . append ( BackwardFullInputComputeAction ( stage_idx = stage , microbatch_idx = b_idx [ stage ], full_backward = True , ) ) b_idx [ stage ] += 1 def _add_b_input ( stage : int ): mb = b_idx [ stage ] compute_actions . append ( BackwardFullInputComputeAction ( stage_idx = stage , microbatch_idx = mb , full_backward = False , ) ) weight_queue . append (( stage , mb )) b_idx [ stage ] += 1 def _pop_w (): if not weight_queue : return s , mb = weight_queue . popleft () compute_actions . append ( BackwardWeightComputeAction ( stage_idx = s , microbatch_idx = mb )) def _add_overlap_f_b ( stage_f : int , stage_b : int , b_is_full : bool ): \"\"\"Emit overlapped Forward and Backward actions.\"\"\" mb_f = f_idx [ stage_f ] mb_b = b_idx [ stage_b ] act_f = ForwardComputeAction ( stage_idx = stage_f , microbatch_idx = mb_f ) act_b = BackwardFullInputComputeAction ( stage_idx = stage_b , microbatch_idx = mb_b , full_backward = b_is_full ) if not b_is_full : weight_queue . append (( stage_b , mb_b )) f_idx [ stage_f ] += 1 b_idx [ stage_b ] += 1 # Note: d9d infra treats ComposeAction sequentially in simulation, # but runtime may overlap them. compute_actions . append ( ComposeAction ( actions = ( act_f , act_b ))) # Step 1: nF0 (Startup Phase 0) step_1 = ( pp_size - rank - 1 ) * 2 for _ in range ( step_1 ): _add_f ( s0 ) # Step 2: nF0F1 (Forward fill) step_2 = rank + 1 for _ in range ( step_2 ): _add_f ( s0 ) _add_f ( s1 ) # Step 3: nI1W1F1 (Mixed Phase with Zero Bubble) step_3 = pp_size - rank - 1 for _ in range ( step_3 ): _add_b_input ( s1 ) # Backward Input Phase 1 _pop_w () # Weight Phase (accumulated from prev) _add_f ( s1 ) # Forward Phase 1 # Step 4: The Main Loop (Interleaved Forward/Backward) step_4 = num_microbatches - 2 * pp_size + rank + 1 for i in range ( step_4 ): # Sub-step A: F0 & B1 if i == 0 and rank == pp_size - 1 : # Specific case for last rank on first iter: do not overlap _add_f ( s0 ) _add_b_full ( s1 ) else : # Overlap F0 and B1 (usually full backward unless we were in ZB mode, # but DualPipeV main loop defaults to full for simplicity unless tuned) # DeepSeek impl uses standard backward here (zb=False). _add_overlap_f_b ( stage_f = s0 , stage_b = s1 , b_is_full = True ) # Sub-step B: F1 & B0 # Overlap F1 and B0 (Full) _add_overlap_f_b ( stage_f = s1 , stage_b = s0 , b_is_full = True ) # Step 5: Cooldown F1/B0 step_5 = pp_size - rank - 1 for _ in range ( step_5 ): _add_b_full ( s1 ) _add_overlap_f_b ( stage_f = s1 , stage_b = s0 , b_is_full = True ) # Step 6: Cooldown B1/B0 with Zero Bubble ramp-up step_6 = rank + 1 enable_zb = False for i in range ( step_6 ): # Phase 1 Backward if i == step_6 // 2 and rank % 2 == 1 : enable_zb = True if enable_zb : _add_b_input ( s1 ) else : _add_b_full ( s1 ) # Phase 0 Backward if i == step_6 // 2 and rank % 2 == 0 : enable_zb = True if enable_zb : _add_b_input ( s0 ) else : _add_b_full ( s0 ) # Step 7: Zero Bubble Weights + B0 step_7 = pp_size - rank - 1 for _ in range ( step_7 ): _pop_w () # DeepSeek source explicitly uses enable_zb=True here for chunk 0 _add_b_input ( s0 ) # Step 8: Flush Weights step_8 = rank + 1 for _ in range ( step_8 ): _pop_w () return compute_actions def compose ( self , num_microbatches : int , pp_size : int ) -> dict [ int , list [ ActionBase ]]: num_stages = self . num_stages_per_rank * pp_size if num_microbatches < num_stages : raise ValueError ( f \"DualPipeV requires num_microbatches ( { num_microbatches } ) >= num_stages ( { num_stages } ).\" ) # Ranks hold stages in a V pattern (e.g., Rank 0 holds Stage 0 and Stage N-1). # We rely on the sorted order of local steps to determine Phase 0 (Forward-going) # and Phase 1 (Backward-coming). stage_to_rank = build_stage_to_host_rank_topology ( pp_size = pp_size , num_stages = num_stages , style = ScheduleStyle . v ) compute_actions : dict [ int , list [ ActionBase ]] = { r : [] for r in range ( pp_size )} for rank in range ( pp_size ): compute_actions [ rank ] = self . _build_for_rank ( rank = rank , pp_size = pp_size , num_microbatches = num_microbatches , stage_to_rank = stage_to_rank ) # 4. Inject Communication Operations # This wrapper handles dependency analysis and inserts Send/Recv/Wait ops. return add_communication_ops ( compute_actions = compute_actions , stage_to_rank = stage_to_rank , num_stages = num_stages ) @property def num_stages_per_rank ( self ) -> int : return 2 @property def topology_style ( self ) -> ScheduleStyle : return ScheduleStyle . v","title":"DualPipeVPipelineProgramBuilder"},{"location":"internals/pipelining/#d9d.pipelining.infra.schedule.program.DualPipeVPipelineProgramBuilder.__init__","text":"Constructs the DualPipeV builder. Source code in d9d/pipelining/infra/schedule/program/dualpipev.py 32 33 34 35 def __init__ ( self ): \"\"\" Constructs the DualPipeV builder. \"\"\"","title":"__init__"},{"location":"internals/pipelining/#d9d.pipelining.infra.schedule.program.Interleaved1F1BPipelineProgramBuilder","text":"Bases: PipelineProgramBuilder Builder for Interleaved Pipeline Parallelism schedules. This builder supports: Standard Interleaved 1F1B : Assigns multiple stages per rank and prioritizes depth-first execution. (See https://arxiv.org/pdf/2104.04473) Interleaved Zero Bubble (ZB1P) : Extends 1F1B by splitting backward passes into Input Gradients and Weight Gradients. Weight gradients are delayed to fill pipeline bubbles. (See https://arxiv.org/pdf/2401.10241) Source code in d9d/pipelining/infra/schedule/program/interleaved.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 class Interleaved1F1BPipelineProgramBuilder ( PipelineProgramBuilder ): \"\"\" Builder for Interleaved Pipeline Parallelism schedules. This builder supports: 1. **Standard Interleaved 1F1B**: Assigns multiple stages per rank and prioritizes depth-first execution. (See https://arxiv.org/pdf/2104.04473) 2. **Interleaved Zero Bubble (ZB1P)**: Extends 1F1B by splitting backward passes into Input Gradients and Weight Gradients. Weight gradients are delayed to fill pipeline bubbles. (See https://arxiv.org/pdf/2401.10241) \"\"\" def __init__ ( self , num_stages_per_rank : int , enable_zero_bubble : bool = False ): \"\"\" Constructs the Interleaved 1F1B builder. Args: num_stages_per_rank: Number of stages per rank. enable_zero_bubble: If True, uses the ZB1P schedule variant which splits backward passes to reduce bubble size. \"\"\" self . _num_stages_per_rank = num_stages_per_rank self . _enable_zero_bubble = enable_zero_bubble def _get_warmup_ops ( self , rank : int , microbatches_per_round : int , pp_size : int , n_microbatches : int , multiply_factor : int , ) -> int : \"\"\" Calculates the number of warmup steps required before entering steady state. \"\"\" warmups_ops_last_stage = ( self . _num_stages_per_rank - 1 ) * microbatches_per_round warmup_ops = warmups_ops_last_stage + multiply_factor * (( pp_size - 1 ) - rank ) return min ( warmup_ops , n_microbatches * self . _num_stages_per_rank ) def compose ( self , num_microbatches : int , pp_size : int ) -> dict [ int , list [ ActionBase ]]: \"\"\" Generates the execution program for all ranks. Args: num_microbatches: Total microbatches. Must be divisible by the derived number of rounds. pp_size: Number of pipeline ranks. Returns: A dictionary mapping rank indices to their list of sequential actions. \"\"\" num_stages = self . num_stages_per_rank * pp_size if num_stages % pp_size != 0 : raise ValueError ( f \"num_stages ( { num_stages } ) must be divisible by pp_size ( { pp_size } ) for interleaved schedules.\" ) # 1. Topology Setup # Use Loop/Round-Robin assignment: Rank 0 gets Stage 0, PP, 2*PP... stage_to_rank = build_stage_to_host_rank_topology ( pp_size = pp_size , num_stages = num_stages , style = ScheduleStyle . loop ) num_rounds = max ( 1 , num_microbatches // pp_size ) if num_microbatches % num_rounds != 0 : raise ValueError ( f \"microbatches ( { num_microbatches } ) must be divisible by rounds ( { num_rounds } ).\" ) microbatches_per_round = num_microbatches // num_rounds # 2. Schedule Generation actions : dict [ int , list [ ActionBase ]] = {} # Zero Bubble 1f1b uses a shorter warmup heuristic (factor 1) than Standard (factor 2) warmup_multiplier = 1 if self . _enable_zero_bubble else 2 for rank in range ( pp_size ): actions [ rank ] = self . _generate_rank_schedule ( rank = rank , pp_size = pp_size , n_microbatches = num_microbatches , microbatches_per_round = microbatches_per_round , multiply_factor = warmup_multiplier , ) # 3. Communication Injection return add_communication_ops ( compute_actions = actions , stage_to_rank = stage_to_rank , num_stages = num_stages , ) def _generate_rank_schedule ( # noqa: C901 self , rank : int , pp_size : int , n_microbatches : int , microbatches_per_round : int , multiply_factor : int , ) -> list [ ActionBase ]: \"\"\" Generates the sequential list of compute actions for a specific rank. \"\"\" rank_actions : list [ ActionBase ] = [] # -- State Tracking -- # Map: stage_idx -> next_microbatch_idx fwd_counters : dict [ int , int ] = defaultdict ( int ) bwd_counters : dict [ int , int ] = defaultdict ( int ) # FIFO Queue for deferred weight gradients in Zero Bubble # Stores: (stage_idx, microbatch_idx) pending_weights : deque [ tuple [ int , int ]] = deque () # -- Helpers -- def get_global_stage ( local_idx : int ) -> int : \"\"\"Converts a local virtual stage index (0..N) to global stage ID.\"\"\" return ( local_idx * pp_size ) + rank def get_fwd_local_idx ( op_idx : int ) -> int : return ( op_idx // microbatches_per_round ) % self . _num_stages_per_rank def get_bwd_local_idx ( op_idx : int , warmup_offset : int ) -> int : return ( self . _num_stages_per_rank - 1 - (( op_idx - warmup_offset ) // microbatches_per_round ) % self . _num_stages_per_rank ) def emit_forward ( op_idx : int ): local_idx = get_fwd_local_idx ( op_idx ) stage = get_global_stage ( local_idx ) mb = fwd_counters [ stage ] rank_actions . append ( ForwardComputeAction ( stage_idx = stage , microbatch_idx = mb )) fwd_counters [ stage ] += 1 def emit_backward ( op_idx : int , warmup_offset : int ): local_idx = get_bwd_local_idx ( op_idx , warmup_offset ) stage = get_global_stage ( local_idx ) mb = bwd_counters [ stage ] # In Zero Bubble, we split: Backward Input (Now) + Backward Weight (Later) # In Standard 1F1B, we do full backward now. is_full = not self . _enable_zero_bubble rank_actions . append ( BackwardFullInputComputeAction ( stage_idx = stage , microbatch_idx = mb , full_backward = is_full ) ) if self . _enable_zero_bubble : pending_weights . append (( stage , mb )) bwd_counters [ stage ] += 1 def try_emit_weight_zb ( op_idx : int , warmup_offset : int ): if not self . _enable_zero_bubble or not pending_weights : return steps_into_1f1b = op_idx - warmup_offset # The earliest reasonable time to start weaving in weights is proportional to rank depth if steps_into_1f1b >= rank : w_stage , w_mb = pending_weights . popleft () rank_actions . append ( BackwardWeightComputeAction ( stage_idx = w_stage , microbatch_idx = w_mb )) # -- Execution Phase Math -- warmup_ops = self . _get_warmup_ops ( rank , microbatches_per_round , pp_size , n_microbatches , multiply_factor ) total_microbatch_ops = self . _num_stages_per_rank * n_microbatches fwd_bwd_ops = total_microbatch_ops - warmup_ops cooldown_ops = total_microbatch_ops - fwd_bwd_ops # Combine into one sequence for iteration, but handle logic per phase total_ops = warmup_ops + fwd_bwd_ops + cooldown_ops # -- Main Schedule Loop -- for op in range ( total_ops ): # Phase 1: Warmup (Forward Only) if op < warmup_ops : emit_forward ( op ) # Phase 2: Steady State (1F1B) elif op < warmup_ops + fwd_bwd_ops : emit_forward ( op ) emit_backward ( op , warmup_offset = warmup_ops ) try_emit_weight_zb ( op , warmup_offset = warmup_ops ) # Phase 3: Cooldown (Backward Only) else : emit_backward ( op , warmup_offset = warmup_ops ) try_emit_weight_zb ( op , warmup_offset = warmup_ops ) # -- Post-Loop: Flush Remaining Weights (ZB only) -- while pending_weights : w_stage , w_mb = pending_weights . popleft () rank_actions . append ( BackwardWeightComputeAction ( stage_idx = w_stage , microbatch_idx = w_mb )) return rank_actions @property def num_stages_per_rank ( self ) -> int : return self . _num_stages_per_rank @property def topology_style ( self ) -> ScheduleStyle : return ScheduleStyle . loop","title":"Interleaved1F1BPipelineProgramBuilder"},{"location":"internals/pipelining/#d9d.pipelining.infra.schedule.program.Interleaved1F1BPipelineProgramBuilder.__init__","text":"Constructs the Interleaved 1F1B builder. Parameters: Name Type Description Default num_stages_per_rank int Number of stages per rank. required enable_zero_bubble bool If True, uses the ZB1P schedule variant which splits backward passes to reduce bubble size. False Source code in d9d/pipelining/infra/schedule/program/interleaved.py 30 31 32 33 34 35 36 37 38 39 40 def __init__ ( self , num_stages_per_rank : int , enable_zero_bubble : bool = False ): \"\"\" Constructs the Interleaved 1F1B builder. Args: num_stages_per_rank: Number of stages per rank. enable_zero_bubble: If True, uses the ZB1P schedule variant which splits backward passes to reduce bubble size. \"\"\" self . _num_stages_per_rank = num_stages_per_rank self . _enable_zero_bubble = enable_zero_bubble","title":"__init__"},{"location":"internals/pipelining/#d9d.pipelining.infra.schedule.program.Interleaved1F1BPipelineProgramBuilder.compose","text":"Generates the execution program for all ranks. Parameters: Name Type Description Default num_microbatches int Total microbatches. Must be divisible by the derived number of rounds. required pp_size int Number of pipeline ranks. required Returns: Type Description dict [ int , list [ ActionBase ]] A dictionary mapping rank indices to their list of sequential actions. Source code in d9d/pipelining/infra/schedule/program/interleaved.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def compose ( self , num_microbatches : int , pp_size : int ) -> dict [ int , list [ ActionBase ]]: \"\"\" Generates the execution program for all ranks. Args: num_microbatches: Total microbatches. Must be divisible by the derived number of rounds. pp_size: Number of pipeline ranks. Returns: A dictionary mapping rank indices to their list of sequential actions. \"\"\" num_stages = self . num_stages_per_rank * pp_size if num_stages % pp_size != 0 : raise ValueError ( f \"num_stages ( { num_stages } ) must be divisible by pp_size ( { pp_size } ) for interleaved schedules.\" ) # 1. Topology Setup # Use Loop/Round-Robin assignment: Rank 0 gets Stage 0, PP, 2*PP... stage_to_rank = build_stage_to_host_rank_topology ( pp_size = pp_size , num_stages = num_stages , style = ScheduleStyle . loop ) num_rounds = max ( 1 , num_microbatches // pp_size ) if num_microbatches % num_rounds != 0 : raise ValueError ( f \"microbatches ( { num_microbatches } ) must be divisible by rounds ( { num_rounds } ).\" ) microbatches_per_round = num_microbatches // num_rounds # 2. Schedule Generation actions : dict [ int , list [ ActionBase ]] = {} # Zero Bubble 1f1b uses a shorter warmup heuristic (factor 1) than Standard (factor 2) warmup_multiplier = 1 if self . _enable_zero_bubble else 2 for rank in range ( pp_size ): actions [ rank ] = self . _generate_rank_schedule ( rank = rank , pp_size = pp_size , n_microbatches = num_microbatches , microbatches_per_round = microbatches_per_round , multiply_factor = warmup_multiplier , ) # 3. Communication Injection return add_communication_ops ( compute_actions = actions , stage_to_rank = stage_to_rank , num_stages = num_stages , )","title":"compose"},{"location":"internals/pipelining/#d9d.pipelining.infra.schedule.program.LoopedBFSPipelineProgramBuilder","text":"Bases: PipelineProgramBuilder Builder for the Breadth-First Pipeline Parallelism schedule. This schedule runs all available forward microbatches for local stages first. If configured for training, it then runs backwards in reverse topological order. References https://arxiv.org/pdf/2211.05953 Source code in d9d/pipelining/infra/schedule/program/bfs.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 class LoopedBFSPipelineProgramBuilder ( PipelineProgramBuilder ): \"\"\" Builder for the Breadth-First Pipeline Parallelism schedule. This schedule runs all available forward microbatches for local stages first. If configured for training, it then runs backwards in reverse topological order. References: https://arxiv.org/pdf/2211.05953 \"\"\" def __init__ ( self , num_stages_per_rank : int , inference_mode : bool = False ): \"\"\" Constructs the LoopedBFS builder. Args: num_stages_per_rank: Number of stages per rank. inference_mode: If True, only forward passes are scheduled. If False, both forward and backward passes are scheduled. \"\"\" self . _num_stages_per_rank = num_stages_per_rank self . _inference_mode = inference_mode def compose ( self , num_microbatches : int , pp_size : int ) -> dict [ int , list [ ActionBase ]]: num_stages = self . _num_stages_per_rank * pp_size stage_to_rank = build_stage_to_host_rank_topology ( pp_size = pp_size , num_stages = num_stages , style = ScheduleStyle . loop ) compute_actions : dict [ int , list [ ActionBase ]] = { r : [] for r in range ( pp_size )} for rank in range ( pp_size ): my_stages = [ s for s in range ( num_stages ) if stage_to_rank [ s ] == rank ] # Schedule all Forwards # In Breadth-First loops, we finish all microbatches for the current stage # before moving to the next stage assigned to this rank. for stage_idx in my_stages : for mb_idx in range ( num_microbatches ): compute_actions [ rank ] . append ( ForwardComputeAction ( stage_idx = stage_idx , microbatch_idx = mb_idx )) # Schedule all Backwards (Reverse order) - Only if training if not self . _inference_mode : for stage_idx in reversed ( my_stages ): for mb_idx in reversed ( range ( num_microbatches )): compute_actions [ rank ] . append ( BackwardFullInputComputeAction ( stage_idx = stage_idx , microbatch_idx = mb_idx , full_backward = True ) ) return add_communication_ops ( compute_actions = compute_actions , stage_to_rank = stage_to_rank , num_stages = num_stages ) @property def num_stages_per_rank ( self ) -> int : return self . _num_stages_per_rank @property def topology_style ( self ) -> ScheduleStyle : return ScheduleStyle . loop","title":"LoopedBFSPipelineProgramBuilder"},{"location":"internals/pipelining/#d9d.pipelining.infra.schedule.program.LoopedBFSPipelineProgramBuilder.__init__","text":"Constructs the LoopedBFS builder. Parameters: Name Type Description Default num_stages_per_rank int Number of stages per rank. required inference_mode bool If True, only forward passes are scheduled. If False, both forward and backward passes are scheduled. False Source code in d9d/pipelining/infra/schedule/program/bfs.py 25 26 27 28 29 30 31 32 33 34 35 def __init__ ( self , num_stages_per_rank : int , inference_mode : bool = False ): \"\"\" Constructs the LoopedBFS builder. Args: num_stages_per_rank: Number of stages per rank. inference_mode: If True, only forward passes are scheduled. If False, both forward and backward passes are scheduled. \"\"\" self . _num_stages_per_rank = num_stages_per_rank self . _inference_mode = inference_mode","title":"__init__"},{"location":"internals/pipelining/#d9d.pipelining.infra.schedule.program.ZeroBubbleVPipelineProgramBuilder","text":"Bases: PipelineProgramBuilder Builder for the Zero Bubble V (ZBV) Pipeline Schedule. This schedule is designed for V-shape topologies (2 stages per rank) and utilizes the Zero Bubble optimizations by splitting backward passes. It requires exactly two stages per rank organized in a V-shape topology and splits backward passes into Input and Weight gradients to optimize pipeline throughput. References https://arxiv.org/pdf/2401.10241, Section 6 Source code in d9d/pipelining/infra/schedule/program/zerobubblev.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 class ZeroBubbleVPipelineProgramBuilder ( PipelineProgramBuilder ): \"\"\" Builder for the Zero Bubble V (ZBV) Pipeline Schedule. This schedule is designed for V-shape topologies (2 stages per rank) and utilizes the Zero Bubble optimizations by splitting backward passes. It requires exactly two stages per rank organized in a V-shape topology and splits backward passes into Input and Weight gradients to optimize pipeline throughput. References: https://arxiv.org/pdf/2401.10241, Section 6 \"\"\" def __init__ ( self ): \"\"\"Constructs the ZBV builder.\"\"\" def compose ( self , num_microbatches : int , pp_size : int ) -> dict [ int , list [ ActionBase ]]: num_stages = self . num_stages_per_rank * pp_size # 1. Topology # V-style: Rank 0 gets Stage 0 & Stage N-1. Rank 1 gets Stage 1 & Stage N-2... stage_to_rank = build_stage_to_host_rank_topology ( pp_size = pp_size , num_stages = num_stages , style = ScheduleStyle . v ) actions : dict [ int , list [ ActionBase ]] = {} for rank in range ( pp_size ): actions [ rank ] = self . _generate_rank_schedule ( rank = rank , pp_size = pp_size , num_stages = num_stages , target_microbatches = num_microbatches , ) # 2. Inject Communications return add_communication_ops ( compute_actions = actions , stage_to_rank = stage_to_rank , num_stages = num_stages ) def _generate_rank_schedule ( # noqa: C901 self , rank : int , pp_size : int , num_stages : int , target_microbatches : int , ) -> list [ ActionBase ]: # ZBV logic assumes the pipeline is fully saturated to define the loop bounds. # We simulate enough steps to cover the topology startup, then filter # down to the user's requested microbatches at the end. simulated_n_micro = max ( 2 * pp_size - 1 , target_microbatches ) rank_ops : list [ ActionBase ] = [] # -- Stage Identification (V-Shape) -- # s0: The \"Forward-going\" chunk (e.g., Stage 0 for Rank 0) # s1: The \"Backward-coming\" chunk (e.g., Stage N-1 for Rank 0) s0 = rank s1 = num_stages - 1 - rank # -- Counters -- # Track next microbatch index for each operation type on each chunk. # F: Forward, I: Backward Input, W: Backward Weight f0_cnt = 0 b0_cnt = 0 # Input Grad Counter (Chunk 0) w0_cnt = 0 # Weight Grad Counter (Chunk 0) f1_cnt = 0 b1_cnt = 0 # Input Grad Counter (Chunk 1) w1_cnt = 0 # Weight Grad Counter (Chunk 1) # -- Helpers -- def emit_f ( stage : int , idx : int ): rank_ops . append ( ForwardComputeAction ( stage_idx = stage , microbatch_idx = idx )) def emit_i_and_w ( stage : int , idx : int ): rank_ops . append ( BackwardFullInputComputeAction ( stage_idx = stage , microbatch_idx = idx , full_backward = False )) rank_ops . append ( BackwardWeightComputeAction ( stage_idx = stage , microbatch_idx = idx )) def emit_i ( stage : int , idx : int ): rank_ops . append ( BackwardFullInputComputeAction ( stage_idx = stage , microbatch_idx = idx , full_backward = False )) def emit_w ( stage : int , idx : int ): rank_ops . append ( BackwardWeightComputeAction ( stage_idx = stage , microbatch_idx = idx )) # -- Phase 1: Warmup 1 (Chunk 0 Forwards) -- warmup_n1 = 2 * ( pp_size - rank ) - 1 for _ in range ( warmup_n1 ): emit_f ( s0 , f0_cnt ) f0_cnt += 1 # -- Phase 2: Warmup 2 (Interleave F1, F0) -- warmup_n2 = rank for _ in range ( warmup_n2 ): emit_f ( s1 , f1_cnt ) f1_cnt += 1 emit_f ( s0 , f0_cnt ) f0_cnt += 1 # -- Phase 3: Warmup 3 (F1, then B1 I+W) -- warmup_n3 = pp_size - rank for _ in range ( warmup_n3 ): emit_f ( s1 , f1_cnt ) f1_cnt += 1 emit_i_and_w ( s1 , b1_cnt ) b1_cnt += 1 w1_cnt += 1 # -- Phase 4: Stable State -- while f1_cnt < f0_cnt or f0_cnt < simulated_n_micro : # Emit F0 if within bounds if f0_cnt < simulated_n_micro : emit_f ( s0 , f0_cnt ) f0_cnt += 1 # Emit B0 (I+W) emit_i_and_w ( s0 , b0_cnt ) b0_cnt += 1 w0_cnt += 1 # Emit F1 emit_f ( s1 , f1_cnt ) f1_cnt += 1 # Emit B1 (I+W) emit_i_and_w ( s1 , b1_cnt ) b1_cnt += 1 w1_cnt += 1 # -- Phase 5: Cooldown 1 (Splitting I and W) -- # In cooldown, the I and W streams diverge to fill bubbles. cooldown_n1 = rank for _ in range ( cooldown_n1 ): emit_i ( s0 , b0_cnt ) b0_cnt += 1 emit_i ( s1 , b1_cnt ) b1_cnt += 1 # -- Phase 6: Cooldown 2 (I0, then W0) -- cooldown_n2 = pp_size - rank for _ in range ( cooldown_n2 ): # Input Grad Chunk 0 emit_i ( s0 , b0_cnt ) b0_cnt += 1 # Weight Grad Chunk 0 (delayed from previous steps) emit_w ( s0 , w0_cnt ) w0_cnt += 1 # -- Phase 7: Flush Remaining Weights -- # Flush W1 while w1_cnt < b1_cnt : emit_w ( s1 , w1_cnt ) w1_cnt += 1 # Flush W0 while w0_cnt < b0_cnt : emit_w ( s0 , w0_cnt ) w0_cnt += 1 # -- Integrity Check -- if not ( w0_cnt == b0_cnt == f0_cnt ): raise RuntimeError ( f \"ZBV Schedule Failed (Chunk 0): F= { f0_cnt } , I= { b0_cnt } , W= { w0_cnt } \" ) if not ( w1_cnt == b1_cnt == f1_cnt ): raise RuntimeError ( f \"ZBV Schedule Failed (Chunk 1): F= { f1_cnt } , I= { b1_cnt } , W= { w1_cnt } \" ) # -- Post-Process: Filter to Target Microbatches -- # Remove any actions involving simulated microbatches beyond the user's request. final_ops : list [ ActionBase ] = [] for action in rank_ops : if isinstance ( action , ( ForwardComputeAction , BackwardFullInputComputeAction , BackwardWeightComputeAction )): if action . microbatch_idx < target_microbatches : final_ops . append ( action ) else : final_ops . append ( action ) return final_ops @property def num_stages_per_rank ( self ) -> int : return 2 @property def topology_style ( self ) -> ScheduleStyle : return ScheduleStyle . v","title":"ZeroBubbleVPipelineProgramBuilder"},{"location":"internals/pipelining/#d9d.pipelining.infra.schedule.program.ZeroBubbleVPipelineProgramBuilder.__init__","text":"Constructs the ZBV builder. Source code in d9d/pipelining/infra/schedule/program/zerobubblev.py 30 31 def __init__ ( self ): \"\"\"Constructs the ZBV builder.\"\"\"","title":"__init__"},{"location":"internals/pipelining/#d9d.pipelining.training","text":"","title":"training"},{"location":"internals/pipelining/#d9d.pipelining.training.PipelinedLRScheduler","text":"Bases: LRSchedulerProtocol Wrapper that manages multiple LR schedulers for a pipeline parallel rank. Similar to PipelinedOptimizer , this aggregates schedulers corresponding to multiple model stages hosted on the current rank. Source code in d9d/pipelining/training/scheduler.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class PipelinedLRScheduler ( LRSchedulerProtocol ): \"\"\" Wrapper that manages multiple LR schedulers for a pipeline parallel rank. Similar to `PipelinedOptimizer`, this aggregates schedulers corresponding to multiple model stages hosted on the current rank. \"\"\" def __init__ ( self , mesh_pp : DeviceMesh | None , schedulers : list [ LRSchedulerProtocol ]): self . _pp_rank = mesh_pp . get_local_rank () if mesh_pp is not None else 0 self . _schedulers = schedulers def state_dict ( self ) -> dict [ str , Any ]: return { f \"pp_ { self . _pp_rank } _stage_ { i } \" : scheduler . state_dict () for i , scheduler in enumerate ( self . _schedulers )} def load_state_dict ( self , state_dict : dict [ str , Any ]) -> None : for i , scheduler in enumerate ( self . _schedulers ): scheduler . load_state_dict ( state_dict [ f \"pp_ { self . _pp_rank } _stage_ { i } \" ]) def step ( self ) -> None : for scheduler in self . _schedulers : scheduler . step ()","title":"PipelinedLRScheduler"},{"location":"internals/pipelining/#d9d.pipelining.training.PipelinedOptimizer","text":"Bases: OptimizerProtocol Wrapper that manages multiple optimizers for a pipeline parallel rank. In a pipeline parallel setup, a single rank might host multiple stages, each having its own parameters and optimizer. This class aggregates them into a single interface. Source code in d9d/pipelining/training/optimizer.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 class PipelinedOptimizer ( OptimizerProtocol ): \"\"\" Wrapper that manages multiple optimizers for a pipeline parallel rank. In a pipeline parallel setup, a single rank might host multiple stages, each having its own parameters and optimizer. This class aggregates them into a single interface. \"\"\" def __init__ ( self , mesh_pp : DeviceMesh | None , optimizers : list [ OptimizerProtocol ]): super () . __init__ () self . _pp_rank = mesh_pp . get_local_rank () if mesh_pp is not None else 0 self . _optimizers = optimizers def state_dict ( self ) -> dict [ str , Any ]: pp_rank = self . _pp_rank return { f \"pp_ { pp_rank } _stage_ { i } \" : optimizer . state_dict () for i , optimizer in enumerate ( self . _optimizers )} def load_state_dict ( self , state_dict : dict [ str , Any ]) -> None : pp_rank = self . _pp_rank for i , optimizer in enumerate ( self . _optimizers ): optimizer . load_state_dict ( state_dict [ f \"pp_ { pp_rank } _stage_ { i } \" ]) def step ( self ) -> None : for optimizer in self . _optimizers : optimizer . step () def zero_grad ( self ) -> None : for optimizer in self . _optimizers : optimizer . zero_grad ()","title":"PipelinedOptimizer"},{"location":"internals/profiling/","text":"About Warning: If you are utilizing the standard d9d training infrastructure, you do not need to call these functions manually. The framework automatically handles profiling based on configuration. This package is primarily intended for users extending d9d . The d9d.internals.profiling package provides a distributed-aware wrapper around the standard PyTorch Profiler. In large-scale distributed training, profiling often becomes difficult due to: File Naming : Thousands of ranks writing to the same filename causes race conditions. Storage Space : Raw Chrome tracing JSON files can grow to gigabytes very quickly. Synchronization : Ensuring all ranks profile the same specific step without manual intervention. The Profiler class solves these issues by automatically handling file naming based on the DeviceMesh coordinates, compressing traces into .tar.gz archives on the fly, and managing the profiling schedule (wait/warmup/active). d9d.internals.profiling Exposes the internal distributed profiler. Profiler Manages distributed performance profiling using PyTorch Profiler. This class wraps torch.profiler to provide automatic trace exporting, compression, and file naming consistent with the distributed DeviceMesh topology. It configures the schedule to repeat periodically based on the provided step counts. Source code in d9d/internals/profiling/profile.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 class Profiler : \"\"\" Manages distributed performance profiling using PyTorch Profiler. This class wraps `torch.profiler` to provide automatic trace exporting, compression, and file naming consistent with the distributed DeviceMesh topology. It configures the schedule to repeat periodically based on the provided step counts. \"\"\" def __init__ ( self , save_dir : Path , period_steps : int , warmup_steps : int , active_steps : int , dist_context : DistributedContext ): \"\"\" Constructs a Profiler object. Args: save_dir: Directory where trace files will be saved. period_steps: Total length of a profiling cycle (wait + warmup + active). warmup_steps: Number of steps to ignore before recording to allow for warming-up. active_steps: Number of steps to actively record traces. dist_context: The distributed context object. \"\"\" self . _save_dir = save_dir self . _period = period_steps self . _warmup = warmup_steps self . _active = active_steps self . _dist_context = dist_context def _get_save_file_name ( self ) -> str : if self . _dist_context . mesh_params . is_distributed : mesh_regular = self . _dist_context . mesh_for ( REGULAR_DOMAIN ) coord = mesh_regular . get_coordinate () if coord is None : raise RuntimeError ( \"Invalid mesh\" ) coord_str = \"-\" . join ( str ( x ) for x in coord ) rank = mesh_regular . get_rank () return f \"rank- { rank } -coord- { coord_str } -trace.json\" else : return \"trace.json\" def _dump_trace ( self , prof : tprof . profile ): save_dir = self . _save_dir / f \"step_ { prof . step_num } \" save_dir . mkdir ( parents = True , exist_ok = True ) save_file = save_dir / self . _get_save_file_name () begin = time . monotonic () prof . export_chrome_trace ( str ( save_file )) with tarfile . open ( save_file . with_suffix ( \".tar.gz\" ), \"w:gz\" ) as tar : tar . add ( save_file , arcname = save_file . name ) save_file . unlink () end = time . monotonic () self . _dist_context . logger . info ( f \"Finished dumping profiler traces in { end - begin : .2f } seconds\" ) @contextmanager def open ( self , start_step : int ): \"\"\" Opens a context manager for profiling execution. This sets up the `torch.profiler.profile` with a schedule derived from the initialization parameters. It captures both CPU and CUDA activities, records shapes, and tracks stack traces. When the schedule triggers `on_trace_ready`, the trace is automatically exported to the `save_dir`, compressed into a `.tar.gz` file, and the raw JSON is removed to save space. Args: start_step: The current global step number to initialize the profiler state. Yields: The configured torch profiler instance. \"\"\" wait = self . _period - ( self . _active + self . _warmup ) warmup = self . _warmup active = self . _active with tprof . profile ( activities = [ tprof . ProfilerActivity . CPU , tprof . ProfilerActivity . CUDA ], schedule = tprof . schedule ( wait = wait , warmup = warmup , active = active ), on_trace_ready = self . _dump_trace , record_shapes = True , with_stack = True , ) as profiler : profiler . step_num = start_step yield profiler __init__ ( save_dir , period_steps , warmup_steps , active_steps , dist_context ) Constructs a Profiler object. Parameters: Name Type Description Default save_dir Path Directory where trace files will be saved. required period_steps int Total length of a profiling cycle (wait + warmup + active). required warmup_steps int Number of steps to ignore before recording to allow for warming-up. required active_steps int Number of steps to actively record traces. required dist_context DistributedContext The distributed context object. required Source code in d9d/internals/profiling/profile.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def __init__ ( self , save_dir : Path , period_steps : int , warmup_steps : int , active_steps : int , dist_context : DistributedContext ): \"\"\" Constructs a Profiler object. Args: save_dir: Directory where trace files will be saved. period_steps: Total length of a profiling cycle (wait + warmup + active). warmup_steps: Number of steps to ignore before recording to allow for warming-up. active_steps: Number of steps to actively record traces. dist_context: The distributed context object. \"\"\" self . _save_dir = save_dir self . _period = period_steps self . _warmup = warmup_steps self . _active = active_steps self . _dist_context = dist_context open ( start_step ) Opens a context manager for profiling execution. This sets up the torch.profiler.profile with a schedule derived from the initialization parameters. It captures both CPU and CUDA activities, records shapes, and tracks stack traces. When the schedule triggers on_trace_ready , the trace is automatically exported to the save_dir , compressed into a .tar.gz file, and the raw JSON is removed to save space. Parameters: Name Type Description Default start_step int The current global step number to initialize the profiler state. required Yields: Type Description The configured torch profiler instance. Source code in d9d/internals/profiling/profile.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 @contextmanager def open ( self , start_step : int ): \"\"\" Opens a context manager for profiling execution. This sets up the `torch.profiler.profile` with a schedule derived from the initialization parameters. It captures both CPU and CUDA activities, records shapes, and tracks stack traces. When the schedule triggers `on_trace_ready`, the trace is automatically exported to the `save_dir`, compressed into a `.tar.gz` file, and the raw JSON is removed to save space. Args: start_step: The current global step number to initialize the profiler state. Yields: The configured torch profiler instance. \"\"\" wait = self . _period - ( self . _active + self . _warmup ) warmup = self . _warmup active = self . _active with tprof . profile ( activities = [ tprof . ProfilerActivity . CPU , tprof . ProfilerActivity . CUDA ], schedule = tprof . schedule ( wait = wait , warmup = warmup , active = active ), on_trace_ready = self . _dump_trace , record_shapes = True , with_stack = True , ) as profiler : profiler . step_num = start_step yield profiler","title":"Distributed Profiling"},{"location":"internals/profiling/#about","text":"Warning: If you are utilizing the standard d9d training infrastructure, you do not need to call these functions manually. The framework automatically handles profiling based on configuration. This package is primarily intended for users extending d9d . The d9d.internals.profiling package provides a distributed-aware wrapper around the standard PyTorch Profiler. In large-scale distributed training, profiling often becomes difficult due to: File Naming : Thousands of ranks writing to the same filename causes race conditions. Storage Space : Raw Chrome tracing JSON files can grow to gigabytes very quickly. Synchronization : Ensuring all ranks profile the same specific step without manual intervention. The Profiler class solves these issues by automatically handling file naming based on the DeviceMesh coordinates, compressing traces into .tar.gz archives on the fly, and managing the profiling schedule (wait/warmup/active).","title":"About"},{"location":"internals/profiling/#d9d.internals.profiling","text":"Exposes the internal distributed profiler.","title":"profiling"},{"location":"internals/profiling/#d9d.internals.profiling.Profiler","text":"Manages distributed performance profiling using PyTorch Profiler. This class wraps torch.profiler to provide automatic trace exporting, compression, and file naming consistent with the distributed DeviceMesh topology. It configures the schedule to repeat periodically based on the provided step counts. Source code in d9d/internals/profiling/profile.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 class Profiler : \"\"\" Manages distributed performance profiling using PyTorch Profiler. This class wraps `torch.profiler` to provide automatic trace exporting, compression, and file naming consistent with the distributed DeviceMesh topology. It configures the schedule to repeat periodically based on the provided step counts. \"\"\" def __init__ ( self , save_dir : Path , period_steps : int , warmup_steps : int , active_steps : int , dist_context : DistributedContext ): \"\"\" Constructs a Profiler object. Args: save_dir: Directory where trace files will be saved. period_steps: Total length of a profiling cycle (wait + warmup + active). warmup_steps: Number of steps to ignore before recording to allow for warming-up. active_steps: Number of steps to actively record traces. dist_context: The distributed context object. \"\"\" self . _save_dir = save_dir self . _period = period_steps self . _warmup = warmup_steps self . _active = active_steps self . _dist_context = dist_context def _get_save_file_name ( self ) -> str : if self . _dist_context . mesh_params . is_distributed : mesh_regular = self . _dist_context . mesh_for ( REGULAR_DOMAIN ) coord = mesh_regular . get_coordinate () if coord is None : raise RuntimeError ( \"Invalid mesh\" ) coord_str = \"-\" . join ( str ( x ) for x in coord ) rank = mesh_regular . get_rank () return f \"rank- { rank } -coord- { coord_str } -trace.json\" else : return \"trace.json\" def _dump_trace ( self , prof : tprof . profile ): save_dir = self . _save_dir / f \"step_ { prof . step_num } \" save_dir . mkdir ( parents = True , exist_ok = True ) save_file = save_dir / self . _get_save_file_name () begin = time . monotonic () prof . export_chrome_trace ( str ( save_file )) with tarfile . open ( save_file . with_suffix ( \".tar.gz\" ), \"w:gz\" ) as tar : tar . add ( save_file , arcname = save_file . name ) save_file . unlink () end = time . monotonic () self . _dist_context . logger . info ( f \"Finished dumping profiler traces in { end - begin : .2f } seconds\" ) @contextmanager def open ( self , start_step : int ): \"\"\" Opens a context manager for profiling execution. This sets up the `torch.profiler.profile` with a schedule derived from the initialization parameters. It captures both CPU and CUDA activities, records shapes, and tracks stack traces. When the schedule triggers `on_trace_ready`, the trace is automatically exported to the `save_dir`, compressed into a `.tar.gz` file, and the raw JSON is removed to save space. Args: start_step: The current global step number to initialize the profiler state. Yields: The configured torch profiler instance. \"\"\" wait = self . _period - ( self . _active + self . _warmup ) warmup = self . _warmup active = self . _active with tprof . profile ( activities = [ tprof . ProfilerActivity . CPU , tprof . ProfilerActivity . CUDA ], schedule = tprof . schedule ( wait = wait , warmup = warmup , active = active ), on_trace_ready = self . _dump_trace , record_shapes = True , with_stack = True , ) as profiler : profiler . step_num = start_step yield profiler","title":"Profiler"},{"location":"internals/profiling/#d9d.internals.profiling.Profiler.__init__","text":"Constructs a Profiler object. Parameters: Name Type Description Default save_dir Path Directory where trace files will be saved. required period_steps int Total length of a profiling cycle (wait + warmup + active). required warmup_steps int Number of steps to ignore before recording to allow for warming-up. required active_steps int Number of steps to actively record traces. required dist_context DistributedContext The distributed context object. required Source code in d9d/internals/profiling/profile.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def __init__ ( self , save_dir : Path , period_steps : int , warmup_steps : int , active_steps : int , dist_context : DistributedContext ): \"\"\" Constructs a Profiler object. Args: save_dir: Directory where trace files will be saved. period_steps: Total length of a profiling cycle (wait + warmup + active). warmup_steps: Number of steps to ignore before recording to allow for warming-up. active_steps: Number of steps to actively record traces. dist_context: The distributed context object. \"\"\" self . _save_dir = save_dir self . _period = period_steps self . _warmup = warmup_steps self . _active = active_steps self . _dist_context = dist_context","title":"__init__"},{"location":"internals/profiling/#d9d.internals.profiling.Profiler.open","text":"Opens a context manager for profiling execution. This sets up the torch.profiler.profile with a schedule derived from the initialization parameters. It captures both CPU and CUDA activities, records shapes, and tracks stack traces. When the schedule triggers on_trace_ready , the trace is automatically exported to the save_dir , compressed into a .tar.gz file, and the raw JSON is removed to save space. Parameters: Name Type Description Default start_step int The current global step number to initialize the profiler state. required Yields: Type Description The configured torch profiler instance. Source code in d9d/internals/profiling/profile.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 @contextmanager def open ( self , start_step : int ): \"\"\" Opens a context manager for profiling execution. This sets up the `torch.profiler.profile` with a schedule derived from the initialization parameters. It captures both CPU and CUDA activities, records shapes, and tracks stack traces. When the schedule triggers `on_trace_ready`, the trace is automatically exported to the `save_dir`, compressed into a `.tar.gz` file, and the raw JSON is removed to save space. Args: start_step: The current global step number to initialize the profiler state. Yields: The configured torch profiler instance. \"\"\" wait = self . _period - ( self . _active + self . _warmup ) warmup = self . _warmup active = self . _active with tprof . profile ( activities = [ tprof . ProfilerActivity . CPU , tprof . ProfilerActivity . CUDA ], schedule = tprof . schedule ( wait = wait , warmup = warmup , active = active ), on_trace_ready = self . _dump_trace , record_shapes = True , with_stack = True , ) as profiler : profiler . step_num = start_step yield profiler","title":"open"},{"location":"internals/tracker_integration/","text":"About Warning: If you are utilizing the standard d9d training infrastructure, you do not need to call these functions manually. The framework automatically handles tracking based on configuration. This package is primarily intended for users extending d9d . The d9d.tracker package provides a unified, configuration-driven interface for logging metrics, hyperparameters, and distributions during training. It abstracts the specific backend (such as Aim or simple console logging) behind a common API. This coupled with a pydantic configuration system allows users to switch logging backends via configuration files without changing a single line of training loop code. Crucially, the tracker is State Aware . It implements the PyTorch Stateful protocol, ensuring that if a training job is interrupted and resumed, the tracker automatically re-attaches to the existing experiment run rather than creating a fragmented new one. Architecture Separation of Concerns The module splits tracking logic into two distinct phases: The Tracker (Factory/Manager) : Represented by BaseTracker . This object persists throughout the lifecycle of the application. It holds configuration (where to save logs) and state (the ID of the current run). It is responsible for creating \"Runs\". The Run (Session) : Represented by BaseTrackerRun . This is a context-managed object active only during the actual training loop. It handles the set_step , scalar , and bins operations. There is also factory method called tracker_from_config that can create a BaseTracker object based on Pydantic configuration. Adding a New Tracker To support a new logging backend (e.g., Weights & Biases, MLFlow), you need to implement three components and register them in the factory. The Configuration Create a Pydantic model for your tracker's settings. Functionally, it must contain a provider literal field which acts as the discriminator for the polymorphic deserialization. from typing import Literal from pydantic import BaseModel class WandbConfig ( BaseModel ): provider : Literal [ 'wandb' ] = 'wandb' project : str entity : str | None = None The Run Handler Implement BaseTrackerRun . This class maps d9d calls ( scalar , bins ) to the specific calls of your backend SDK. from d9d.tracker import BaseTrackerRun class WandbRun ( BaseTrackerRun ): def __init__ ( self , run_obj ): self . _run = run_obj self . _step = 0 def set_step ( self , step : int ): self . _step = step # ... implement scalar(), bins(), etc. to call self._run.log() The Tracker Factory Implement BaseTracker . This handles initialization and state persistence (resuming). from contextlib import contextmanager from d9d.tracker import BaseTracker , RunConfig class WandbTracker ( BaseTracker [ WandbConfig ]): def __init__ ( self , config : WandbConfig ): self . config = config self . run_id = None # State to persist def state_dict ( self ): # This is saved to the checkpoint return { \"run_id\" : self . run_id } def load_state_dict ( self , state_dict ): # This is restored from the checkpoint self . run_id = state_dict . get ( \"run_id\" ) @contextmanager def open ( self , props : RunConfig ): # Logic to init e.g. wandb.init(id=self.run_id, resume=\"allow\", ...) # self.run_id = ... # yield WandbRun(...) # cleanup if necessary Registration To make tracker_from_config recognize your new tracker, you must modify d9d/tracker/factory.py . Add your config to AnyTrackerConfig type alias: AnyTrackerConfig = Annotated [ AimConfig | NullTrackerConfig | WandbConfig , # <--- Add here Field ( discriminator = 'provider' ) ] Register the mapping in _MAP (wrapping imports in try/except is recommended if the SDK is an optional dependency): try : from .provider.wandb.tracker import WandbTracker _MAP [ WandbConfig ] = WandbTracker except ImportError as e : _MAP [ WandbConfig ] = _TrackerImportFailed ( 'wandb' , e ) d9d.tracker Package providing a unified interface for experiment tracking and logging. BaseTracker Bases: ABC , Stateful , Generic [ TConfig ] Abstract base class for a tracker backend factory. This class manages the lifecycle of runs and integration with the distributed checkpointing system to ensure experiment continuity (e.g., resuming the same run hash after a restart). Source code in d9d/tracker/base.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 class BaseTracker ( abc . ABC , Stateful , Generic [ TConfig ]): \"\"\" Abstract base class for a tracker backend factory. This class manages the lifecycle of runs and integration with the distributed checkpointing system to ensure experiment continuity (e.g., resuming the same run hash after a restart). \"\"\" @contextmanager @abc . abstractmethod def open ( self , properties : RunConfig ) -> Generator [ BaseTrackerRun , None , None ]: \"\"\" Context manager that initiates and manages an experiment run. Args: properties: Configuration metadata for the run. Yields: An active BaseTrackerRun instance for logging metrics. \"\"\" ... @classmethod @abc . abstractmethod def from_config ( cls , config : TConfig ) -> Self : \"\"\" Factory method to create a tracker instance from a configuration object. Args: config: The backend-specific configuration object. Returns: An initialized instance of the tracker. \"\"\" ... from_config ( config ) abstractmethod classmethod Factory method to create a tracker instance from a configuration object. Parameters: Name Type Description Default config TConfig The backend-specific configuration object. required Returns: Type Description Self An initialized instance of the tracker. Source code in d9d/tracker/base.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 @classmethod @abc . abstractmethod def from_config ( cls , config : TConfig ) -> Self : \"\"\" Factory method to create a tracker instance from a configuration object. Args: config: The backend-specific configuration object. Returns: An initialized instance of the tracker. \"\"\" ... open ( properties ) abstractmethod Context manager that initiates and manages an experiment run. Parameters: Name Type Description Default properties RunConfig Configuration metadata for the run. required Yields: Type Description BaseTrackerRun An active BaseTrackerRun instance for logging metrics. Source code in d9d/tracker/base.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 @contextmanager @abc . abstractmethod def open ( self , properties : RunConfig ) -> Generator [ BaseTrackerRun , None , None ]: \"\"\" Context manager that initiates and manages an experiment run. Args: properties: Configuration metadata for the run. Yields: An active BaseTrackerRun instance for logging metrics. \"\"\" ... BaseTrackerRun Bases: ABC Abstract base class representing an active tracking session (run). This object is responsible for the actual logging of metrics, parameters, during train or inference run. Source code in d9d/tracker/base.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 class BaseTrackerRun ( abc . ABC ): \"\"\" Abstract base class representing an active tracking session (run). This object is responsible for the actual logging of metrics, parameters, during train or inference run. \"\"\" @abc . abstractmethod def set_step ( self , step : int ): \"\"\" Updates the global step counter for subsequent logs. Args: step: The current step index (e.g., iteration number). \"\"\" ... @abc . abstractmethod def set_context ( self , context : dict [ str , str ]): \"\"\" Sets a persistent context dictionary for subsequent logs. These context values (tags) will be attached to every metric logged until changed. Args: context: A dictionary of tag names and values. \"\"\" ... @abc . abstractmethod def scalar ( self , name : str , value : float , context : dict [ str , str ] | None = None ): \"\"\" Logs a scalar value. Args: name: The name of the metric. value: The scalar value to log. context: Optional ephemeral context specific to this metric event. Merged with global context if present. \"\"\" ... @abc . abstractmethod def bins ( self , name : str , values : torch . Tensor , context : dict [ str , str ] | None = None ): \"\"\" Logs a distribution/histogram of values. Args: name: The name of the metric. values: A tensor containing the population of values to bin. context: Optional ephemeral context specific to this metric event. Merged with global context if present. \"\"\" ... bins ( name , values , context = None ) abstractmethod Logs a distribution/histogram of values. Parameters: Name Type Description Default name str The name of the metric. required values Tensor A tensor containing the population of values to bin. required context dict [ str , str ] | None Optional ephemeral context specific to this metric event. Merged with global context if present. None Source code in d9d/tracker/base.py 55 56 57 58 59 60 61 62 63 64 65 66 @abc . abstractmethod def bins ( self , name : str , values : torch . Tensor , context : dict [ str , str ] | None = None ): \"\"\" Logs a distribution/histogram of values. Args: name: The name of the metric. values: A tensor containing the population of values to bin. context: Optional ephemeral context specific to this metric event. Merged with global context if present. \"\"\" ... scalar ( name , value , context = None ) abstractmethod Logs a scalar value. Parameters: Name Type Description Default name str The name of the metric. required value float The scalar value to log. required context dict [ str , str ] | None Optional ephemeral context specific to this metric event. Merged with global context if present. None Source code in d9d/tracker/base.py 42 43 44 45 46 47 48 49 50 51 52 53 @abc . abstractmethod def scalar ( self , name : str , value : float , context : dict [ str , str ] | None = None ): \"\"\" Logs a scalar value. Args: name: The name of the metric. value: The scalar value to log. context: Optional ephemeral context specific to this metric event. Merged with global context if present. \"\"\" ... set_context ( context ) abstractmethod Sets a persistent context dictionary for subsequent logs. These context values (tags) will be attached to every metric logged until changed. Parameters: Name Type Description Default context dict [ str , str ] A dictionary of tag names and values. required Source code in d9d/tracker/base.py 29 30 31 32 33 34 35 36 37 38 39 40 @abc . abstractmethod def set_context ( self , context : dict [ str , str ]): \"\"\" Sets a persistent context dictionary for subsequent logs. These context values (tags) will be attached to every metric logged until changed. Args: context: A dictionary of tag names and values. \"\"\" ... set_step ( step ) abstractmethod Updates the global step counter for subsequent logs. Parameters: Name Type Description Default step int The current step index (e.g., iteration number). required Source code in d9d/tracker/base.py 19 20 21 22 23 24 25 26 27 @abc . abstractmethod def set_step ( self , step : int ): \"\"\" Updates the global step counter for subsequent logs. Args: step: The current step index (e.g., iteration number). \"\"\" ... RunConfig Bases: BaseModel Configuration for initializing a specific logged run. Attributes: Name Type Description name str The display name of the experiment. description str | None An optional description of the experiment. hparams dict [ str , Any ] A dictionary of hyperparameters to log at the start of the run. Source code in d9d/tracker/base.py 69 70 71 72 73 74 75 76 77 78 79 80 81 class RunConfig ( BaseModel ): \"\"\" Configuration for initializing a specific logged run. Attributes: name: The display name of the experiment. description: An optional description of the experiment. hparams: A dictionary of hyperparameters to log at the start of the run. \"\"\" name : str description : str | None hparams : dict [ str , Any ] = Field ( default_factory = dict ) tracker_from_config ( config ) Instantiates a specific tracker implementation based on the configuration. Based on the 'provider' field in the config, this function selects the appropriate backend (e.g., Aim, Null). It handles checking for missing dependencies for optional backends. Parameters: Name Type Description Default config AnyTrackerConfig A specific tracker configuration object. required Returns: Type Description BaseTracker An initialized BaseTracker instance. Raises: Type Description ImportError If the dependencies for the requested provider are not installed. Source code in d9d/tracker/factory.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def tracker_from_config ( config : AnyTrackerConfig ) -> BaseTracker : \"\"\" Instantiates a specific tracker implementation based on the configuration. Based on the 'provider' field in the config, this function selects the appropriate backend (e.g., Aim, Null). It handles checking for missing dependencies for optional backends. Args: config: A specific tracker configuration object. Returns: An initialized BaseTracker instance. Raises: ImportError: If the dependencies for the requested provider are not installed. \"\"\" tracker_type = _MAP [ type ( config )] if isinstance ( tracker_type , _TrackerImportFailed ): raise ImportError ( f \"The tracker configuration { config . provider } could not be loaded - \" f \"ensure these dependencies are installed: { tracker_type . dependency } \" ) from tracker_type.exception return tracker_type . from_config ( config )","title":"Experiment Tracking"},{"location":"internals/tracker_integration/#about","text":"Warning: If you are utilizing the standard d9d training infrastructure, you do not need to call these functions manually. The framework automatically handles tracking based on configuration. This package is primarily intended for users extending d9d . The d9d.tracker package provides a unified, configuration-driven interface for logging metrics, hyperparameters, and distributions during training. It abstracts the specific backend (such as Aim or simple console logging) behind a common API. This coupled with a pydantic configuration system allows users to switch logging backends via configuration files without changing a single line of training loop code. Crucially, the tracker is State Aware . It implements the PyTorch Stateful protocol, ensuring that if a training job is interrupted and resumed, the tracker automatically re-attaches to the existing experiment run rather than creating a fragmented new one.","title":"About"},{"location":"internals/tracker_integration/#architecture-separation-of-concerns","text":"The module splits tracking logic into two distinct phases: The Tracker (Factory/Manager) : Represented by BaseTracker . This object persists throughout the lifecycle of the application. It holds configuration (where to save logs) and state (the ID of the current run). It is responsible for creating \"Runs\". The Run (Session) : Represented by BaseTrackerRun . This is a context-managed object active only during the actual training loop. It handles the set_step , scalar , and bins operations. There is also factory method called tracker_from_config that can create a BaseTracker object based on Pydantic configuration.","title":"Architecture Separation of Concerns"},{"location":"internals/tracker_integration/#adding-a-new-tracker","text":"To support a new logging backend (e.g., Weights & Biases, MLFlow), you need to implement three components and register them in the factory.","title":"Adding a New Tracker"},{"location":"internals/tracker_integration/#the-configuration","text":"Create a Pydantic model for your tracker's settings. Functionally, it must contain a provider literal field which acts as the discriminator for the polymorphic deserialization. from typing import Literal from pydantic import BaseModel class WandbConfig ( BaseModel ): provider : Literal [ 'wandb' ] = 'wandb' project : str entity : str | None = None","title":"The Configuration"},{"location":"internals/tracker_integration/#the-run-handler","text":"Implement BaseTrackerRun . This class maps d9d calls ( scalar , bins ) to the specific calls of your backend SDK. from d9d.tracker import BaseTrackerRun class WandbRun ( BaseTrackerRun ): def __init__ ( self , run_obj ): self . _run = run_obj self . _step = 0 def set_step ( self , step : int ): self . _step = step # ... implement scalar(), bins(), etc. to call self._run.log()","title":"The Run Handler"},{"location":"internals/tracker_integration/#the-tracker-factory","text":"Implement BaseTracker . This handles initialization and state persistence (resuming). from contextlib import contextmanager from d9d.tracker import BaseTracker , RunConfig class WandbTracker ( BaseTracker [ WandbConfig ]): def __init__ ( self , config : WandbConfig ): self . config = config self . run_id = None # State to persist def state_dict ( self ): # This is saved to the checkpoint return { \"run_id\" : self . run_id } def load_state_dict ( self , state_dict ): # This is restored from the checkpoint self . run_id = state_dict . get ( \"run_id\" ) @contextmanager def open ( self , props : RunConfig ): # Logic to init e.g. wandb.init(id=self.run_id, resume=\"allow\", ...) # self.run_id = ... # yield WandbRun(...) # cleanup if necessary","title":"The Tracker Factory"},{"location":"internals/tracker_integration/#registration","text":"To make tracker_from_config recognize your new tracker, you must modify d9d/tracker/factory.py . Add your config to AnyTrackerConfig type alias: AnyTrackerConfig = Annotated [ AimConfig | NullTrackerConfig | WandbConfig , # <--- Add here Field ( discriminator = 'provider' ) ] Register the mapping in _MAP (wrapping imports in try/except is recommended if the SDK is an optional dependency): try : from .provider.wandb.tracker import WandbTracker _MAP [ WandbConfig ] = WandbTracker except ImportError as e : _MAP [ WandbConfig ] = _TrackerImportFailed ( 'wandb' , e )","title":"Registration"},{"location":"internals/tracker_integration/#d9d.tracker","text":"Package providing a unified interface for experiment tracking and logging.","title":"tracker"},{"location":"internals/tracker_integration/#d9d.tracker.BaseTracker","text":"Bases: ABC , Stateful , Generic [ TConfig ] Abstract base class for a tracker backend factory. This class manages the lifecycle of runs and integration with the distributed checkpointing system to ensure experiment continuity (e.g., resuming the same run hash after a restart). Source code in d9d/tracker/base.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 class BaseTracker ( abc . ABC , Stateful , Generic [ TConfig ]): \"\"\" Abstract base class for a tracker backend factory. This class manages the lifecycle of runs and integration with the distributed checkpointing system to ensure experiment continuity (e.g., resuming the same run hash after a restart). \"\"\" @contextmanager @abc . abstractmethod def open ( self , properties : RunConfig ) -> Generator [ BaseTrackerRun , None , None ]: \"\"\" Context manager that initiates and manages an experiment run. Args: properties: Configuration metadata for the run. Yields: An active BaseTrackerRun instance for logging metrics. \"\"\" ... @classmethod @abc . abstractmethod def from_config ( cls , config : TConfig ) -> Self : \"\"\" Factory method to create a tracker instance from a configuration object. Args: config: The backend-specific configuration object. Returns: An initialized instance of the tracker. \"\"\" ...","title":"BaseTracker"},{"location":"internals/tracker_integration/#d9d.tracker.BaseTracker.from_config","text":"Factory method to create a tracker instance from a configuration object. Parameters: Name Type Description Default config TConfig The backend-specific configuration object. required Returns: Type Description Self An initialized instance of the tracker. Source code in d9d/tracker/base.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 @classmethod @abc . abstractmethod def from_config ( cls , config : TConfig ) -> Self : \"\"\" Factory method to create a tracker instance from a configuration object. Args: config: The backend-specific configuration object. Returns: An initialized instance of the tracker. \"\"\" ...","title":"from_config"},{"location":"internals/tracker_integration/#d9d.tracker.BaseTracker.open","text":"Context manager that initiates and manages an experiment run. Parameters: Name Type Description Default properties RunConfig Configuration metadata for the run. required Yields: Type Description BaseTrackerRun An active BaseTrackerRun instance for logging metrics. Source code in d9d/tracker/base.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 @contextmanager @abc . abstractmethod def open ( self , properties : RunConfig ) -> Generator [ BaseTrackerRun , None , None ]: \"\"\" Context manager that initiates and manages an experiment run. Args: properties: Configuration metadata for the run. Yields: An active BaseTrackerRun instance for logging metrics. \"\"\" ...","title":"open"},{"location":"internals/tracker_integration/#d9d.tracker.BaseTrackerRun","text":"Bases: ABC Abstract base class representing an active tracking session (run). This object is responsible for the actual logging of metrics, parameters, during train or inference run. Source code in d9d/tracker/base.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 class BaseTrackerRun ( abc . ABC ): \"\"\" Abstract base class representing an active tracking session (run). This object is responsible for the actual logging of metrics, parameters, during train or inference run. \"\"\" @abc . abstractmethod def set_step ( self , step : int ): \"\"\" Updates the global step counter for subsequent logs. Args: step: The current step index (e.g., iteration number). \"\"\" ... @abc . abstractmethod def set_context ( self , context : dict [ str , str ]): \"\"\" Sets a persistent context dictionary for subsequent logs. These context values (tags) will be attached to every metric logged until changed. Args: context: A dictionary of tag names and values. \"\"\" ... @abc . abstractmethod def scalar ( self , name : str , value : float , context : dict [ str , str ] | None = None ): \"\"\" Logs a scalar value. Args: name: The name of the metric. value: The scalar value to log. context: Optional ephemeral context specific to this metric event. Merged with global context if present. \"\"\" ... @abc . abstractmethod def bins ( self , name : str , values : torch . Tensor , context : dict [ str , str ] | None = None ): \"\"\" Logs a distribution/histogram of values. Args: name: The name of the metric. values: A tensor containing the population of values to bin. context: Optional ephemeral context specific to this metric event. Merged with global context if present. \"\"\" ...","title":"BaseTrackerRun"},{"location":"internals/tracker_integration/#d9d.tracker.BaseTrackerRun.bins","text":"Logs a distribution/histogram of values. Parameters: Name Type Description Default name str The name of the metric. required values Tensor A tensor containing the population of values to bin. required context dict [ str , str ] | None Optional ephemeral context specific to this metric event. Merged with global context if present. None Source code in d9d/tracker/base.py 55 56 57 58 59 60 61 62 63 64 65 66 @abc . abstractmethod def bins ( self , name : str , values : torch . Tensor , context : dict [ str , str ] | None = None ): \"\"\" Logs a distribution/histogram of values. Args: name: The name of the metric. values: A tensor containing the population of values to bin. context: Optional ephemeral context specific to this metric event. Merged with global context if present. \"\"\" ...","title":"bins"},{"location":"internals/tracker_integration/#d9d.tracker.BaseTrackerRun.scalar","text":"Logs a scalar value. Parameters: Name Type Description Default name str The name of the metric. required value float The scalar value to log. required context dict [ str , str ] | None Optional ephemeral context specific to this metric event. Merged with global context if present. None Source code in d9d/tracker/base.py 42 43 44 45 46 47 48 49 50 51 52 53 @abc . abstractmethod def scalar ( self , name : str , value : float , context : dict [ str , str ] | None = None ): \"\"\" Logs a scalar value. Args: name: The name of the metric. value: The scalar value to log. context: Optional ephemeral context specific to this metric event. Merged with global context if present. \"\"\" ...","title":"scalar"},{"location":"internals/tracker_integration/#d9d.tracker.BaseTrackerRun.set_context","text":"Sets a persistent context dictionary for subsequent logs. These context values (tags) will be attached to every metric logged until changed. Parameters: Name Type Description Default context dict [ str , str ] A dictionary of tag names and values. required Source code in d9d/tracker/base.py 29 30 31 32 33 34 35 36 37 38 39 40 @abc . abstractmethod def set_context ( self , context : dict [ str , str ]): \"\"\" Sets a persistent context dictionary for subsequent logs. These context values (tags) will be attached to every metric logged until changed. Args: context: A dictionary of tag names and values. \"\"\" ...","title":"set_context"},{"location":"internals/tracker_integration/#d9d.tracker.BaseTrackerRun.set_step","text":"Updates the global step counter for subsequent logs. Parameters: Name Type Description Default step int The current step index (e.g., iteration number). required Source code in d9d/tracker/base.py 19 20 21 22 23 24 25 26 27 @abc . abstractmethod def set_step ( self , step : int ): \"\"\" Updates the global step counter for subsequent logs. Args: step: The current step index (e.g., iteration number). \"\"\" ...","title":"set_step"},{"location":"internals/tracker_integration/#d9d.tracker.RunConfig","text":"Bases: BaseModel Configuration for initializing a specific logged run. Attributes: Name Type Description name str The display name of the experiment. description str | None An optional description of the experiment. hparams dict [ str , Any ] A dictionary of hyperparameters to log at the start of the run. Source code in d9d/tracker/base.py 69 70 71 72 73 74 75 76 77 78 79 80 81 class RunConfig ( BaseModel ): \"\"\" Configuration for initializing a specific logged run. Attributes: name: The display name of the experiment. description: An optional description of the experiment. hparams: A dictionary of hyperparameters to log at the start of the run. \"\"\" name : str description : str | None hparams : dict [ str , Any ] = Field ( default_factory = dict )","title":"RunConfig"},{"location":"internals/tracker_integration/#d9d.tracker.tracker_from_config","text":"Instantiates a specific tracker implementation based on the configuration. Based on the 'provider' field in the config, this function selects the appropriate backend (e.g., Aim, Null). It handles checking for missing dependencies for optional backends. Parameters: Name Type Description Default config AnyTrackerConfig A specific tracker configuration object. required Returns: Type Description BaseTracker An initialized BaseTracker instance. Raises: Type Description ImportError If the dependencies for the requested provider are not installed. Source code in d9d/tracker/factory.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def tracker_from_config ( config : AnyTrackerConfig ) -> BaseTracker : \"\"\" Instantiates a specific tracker implementation based on the configuration. Based on the 'provider' field in the config, this function selects the appropriate backend (e.g., Aim, Null). It handles checking for missing dependencies for optional backends. Args: config: A specific tracker configuration object. Returns: An initialized BaseTracker instance. Raises: ImportError: If the dependencies for the requested provider are not installed. \"\"\" tracker_type = _MAP [ type ( config )] if isinstance ( tracker_type , _TrackerImportFailed ): raise ImportError ( f \"The tracker configuration { config . provider } could not be loaded - \" f \"ensure these dependencies are installed: { tracker_type . dependency } \" ) from tracker_type.exception return tracker_type . from_config ( config )","title":"tracker_from_config"},{"location":"lr_scheduler/piecewise/","text":"About The d9d.lr_scheduler.piecewise module provides a flexible, builder-based system for constructing piecewise learning rate schedules. Instead of writing custom LRScheduler subclasses, manual functions for LambdaLR for every variation of piecewise schedule (i.e. \"Warmup + Hold + Decay\"), you can construct any such a schedule declaratively by chaining phases together. Usage Examples Python API Here is how to create a standard \"Linear Warmup + Hold + Cosine Decay\" schedule: import torch from d9d.lr_scheduler.piecewise import * optimizer : torch . optim . Optimizer = ... total_steps : int = 1000 # Define Schedule # 1. Start at 0.0 # 2. Linear warmup to 1.0*LR over 100 steps # 3. Stay at 1.0 * LR until 50% of training steps # 3. Cosine decay to 0.1 (10% of LR) for the rest of training scheduler = ( piecewise_schedule ( initial_multiplier = 0.0 , total_steps = total_steps ) . for_steps ( 100 , target_multiplier = 1.0 , curve = CurveLinear ()) . until_percentage ( 0.5 , target_multiplier = 1.0 , curve = CurveLinear ()) . fill_rest ( target_multiplier = 0.1 , curve = CurveCosine ()) . build ( optimizer ) ) Pydantic API import json import torch from d9d.lr_scheduler.piecewise import PiecewiseSchedulerConfig , piecewise_scheduler_from_config optimizer : torch . optim . Optimizer = ... total_steps : int = 1000 raw_config_json = \"\"\" { \"initial_multiplier\": 0.0, \"phases\": [ { \"mode\": \"steps\", \"steps\": 100, \"target_multiplier\": 1.0, \"curve\": { \"type\": \"linear\" } }, { \"mode\": \"rest\", \"target_multiplier\": 0.1, \"curve\": { \"type\": \"cosine\" } } ] } \"\"\" scheduler_config = PiecewiseSchedulerConfig . model_validate_json ( raw_config_json ) scheduler = piecewise_scheduler_from_config ( config = scheduler_config , optimizer = optimizer , total_steps = total_steps ) Available Curves The following curve classes are available to interpolate values between phases: Curve Class Curve Config Description CurveLinear \"linear\" Standard straight-line interpolation. CurveCosine \"cosine\" Half-period cosine interpolation (Cosine Annealing). CurvePoly(power) \"poly\" Polynomial interpolation. power=1 is linear, power=2 is quadratic, etc. CurveExponential \"exponential\" Exponential (log-linear) interpolation. API Reference d9d.lr_scheduler.piecewise Implements flexible piecewise learning rate schedules via a builder pattern. CurveBase Bases: ABC Abstract base class for interpolation curves used in scheduling. Source code in d9d/lr_scheduler/piecewise/curves.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class CurveBase ( abc . ABC ): \"\"\" Abstract base class for interpolation curves used in scheduling. \"\"\" @abc . abstractmethod def compute ( self , start : float , end : float , step_p : float ) -> float : \"\"\" Calculates the interpolated value. Args: start: The value at the beginning of the phase. end: The value at the end of the phase. step_p: Progress fraction through the phase (0.0 to 1.0). Returns: The interpolated value. \"\"\" compute ( start , end , step_p ) abstractmethod Calculates the interpolated value. Parameters: Name Type Description Default start float The value at the beginning of the phase. required end float The value at the end of the phase. required step_p float Progress fraction through the phase (0.0 to 1.0). required Returns: Type Description float The interpolated value. Source code in d9d/lr_scheduler/piecewise/curves.py 10 11 12 13 14 15 16 17 18 19 20 21 22 @abc . abstractmethod def compute ( self , start : float , end : float , step_p : float ) -> float : \"\"\" Calculates the interpolated value. Args: start: The value at the beginning of the phase. end: The value at the end of the phase. step_p: Progress fraction through the phase (0.0 to 1.0). Returns: The interpolated value. \"\"\" CurveCosine Bases: CurveBase Interpolates using a cosine annealing schedule (half-period cosine). Source code in d9d/lr_scheduler/piecewise/curves.py 34 35 36 37 38 39 40 41 class CurveCosine ( CurveBase ): \"\"\" Interpolates using a cosine annealing schedule (half-period cosine). \"\"\" def compute ( self , start : float , end : float , step_p : float ) -> float : cos_out = ( 1 + math . cos ( math . pi * step_p )) / 2 return end + ( start - end ) * cos_out CurveExponential Bases: CurveBase Interpolates exponentially between start and end values (log-space linear). Source code in d9d/lr_scheduler/piecewise/curves.py 64 65 66 67 68 69 70 71 72 73 74 75 class CurveExponential ( CurveBase ): \"\"\" Interpolates exponentially between start and end values (log-space linear). \"\"\" def compute ( self , start : float , end : float , step_p : float ) -> float : eps = 1e-8 safe_start = max ( start , eps ) safe_end = max ( end , eps ) out_log = math . log ( safe_start ) + ( math . log ( safe_end ) - math . log ( safe_start )) * step_p return math . exp ( out_log ) CurveLinear Bases: CurveBase Linearly interpolates between start and end values. Source code in d9d/lr_scheduler/piecewise/curves.py 25 26 27 28 29 30 31 class CurveLinear ( CurveBase ): \"\"\" Linearly interpolates between start and end values. \"\"\" def compute ( self , start : float , end : float , step_p : float ) -> float : return start + ( end - start ) * step_p CurvePoly Bases: CurveBase Interpolates using a polynomial function. Source code in d9d/lr_scheduler/piecewise/curves.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 class CurvePoly ( CurveBase ): \"\"\" Interpolates using a polynomial function. \"\"\" def __init__ ( self , power : float ): \"\"\" Constructs a polynomial curve. Args: power: The exponent of the polynomial. 1.0 is linear, 2.0 is quadratic, etc. \"\"\" self . _power = power def compute ( self , start : float , end : float , step_p : float ) -> float : p_transformed = step_p ** self . _power return start + ( end - start ) * p_transformed __init__ ( power ) Constructs a polynomial curve. Parameters: Name Type Description Default power float The exponent of the polynomial. 1.0 is linear, 2.0 is quadratic, etc. required Source code in d9d/lr_scheduler/piecewise/curves.py 49 50 51 52 53 54 55 56 57 def __init__ ( self , power : float ): \"\"\" Constructs a polynomial curve. Args: power: The exponent of the polynomial. 1.0 is linear, 2.0 is quadratic, etc. \"\"\" self . _power = power PiecewiseSchedulerConfig Bases: BaseModel Declarative configuration for a piecewise learning rate scheduler. Attributes: Name Type Description initial_multiplier float The starting learning rate multiplier. phases list [ PhaseConfig ] A sequential list of phase configurations. Source code in d9d/lr_scheduler/piecewise/config.py 130 131 132 133 134 135 136 137 138 139 140 class PiecewiseSchedulerConfig ( BaseModel ): \"\"\" Declarative configuration for a piecewise learning rate scheduler. Attributes: initial_multiplier: The starting learning rate multiplier. phases: A sequential list of phase configurations. \"\"\" initial_multiplier : float phases : list [ PhaseConfig ] piecewise_schedule ( initial_multiplier , total_steps = None ) Entry point for creating a piecewise learning rate schedule. Parameters: Name Type Description Default initial_multiplier float The initial learning rate multiplier. required total_steps int | None Total training steps. Required for percentage-based scheduling. None Returns: Type Description PiecewiseScheduleBuilder A builder instance to configure phases. Source code in d9d/lr_scheduler/piecewise/builder.py 129 130 131 132 133 134 135 136 137 138 139 140 141 def piecewise_schedule ( initial_multiplier : float , total_steps : int | None = None ) -> PiecewiseScheduleBuilder : \"\"\" Entry point for creating a piecewise learning rate schedule. Args: initial_multiplier: The initial learning rate multiplier. total_steps: Total training steps. Required for percentage-based scheduling. Returns: A builder instance to configure phases. \"\"\" return PiecewiseScheduleBuilder ( initial_multiplier = initial_multiplier , total_steps = total_steps ) piecewise_scheduler_from_config ( config , optimizer , total_steps ) Constructs a PyTorch scheduler from the provided configuration. Parameters: Name Type Description Default config PiecewiseSchedulerConfig The scheduler configuration. required optimizer Optimizer The optimizer to wrap. required total_steps int | None The total number of training steps. Required if using percentage-based phases. required Returns: Type Description LRSchedulerProtocol A configured learning rate scheduler. Source code in d9d/lr_scheduler/piecewise/config.py 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 def piecewise_scheduler_from_config ( config : PiecewiseSchedulerConfig , optimizer : Optimizer , total_steps : int | None ) -> LRSchedulerProtocol : \"\"\" Constructs a PyTorch scheduler from the provided configuration. Args: config: The scheduler configuration. optimizer: The optimizer to wrap. total_steps: The total number of training steps. Required if using percentage-based phases. Returns: A configured learning rate scheduler. \"\"\" builder = piecewise_schedule ( config . initial_multiplier , total_steps ) for phase in config . phases : curve = curve_from_config ( phase . curve ) match phase : case StepPhaseConfig (): builder . for_steps ( phase . steps , phase . target_multiplier , curve ) case PercentagePhaseConfig (): builder . until_percentage ( phase . percentage , phase . target_multiplier , curve ) case RestPhaseConfig (): builder . fill_rest ( phase . target_multiplier , curve ) return builder . build ( optimizer )","title":"Piecewise Scheduler"},{"location":"lr_scheduler/piecewise/#about","text":"The d9d.lr_scheduler.piecewise module provides a flexible, builder-based system for constructing piecewise learning rate schedules. Instead of writing custom LRScheduler subclasses, manual functions for LambdaLR for every variation of piecewise schedule (i.e. \"Warmup + Hold + Decay\"), you can construct any such a schedule declaratively by chaining phases together.","title":"About"},{"location":"lr_scheduler/piecewise/#usage-examples","text":"","title":"Usage Examples"},{"location":"lr_scheduler/piecewise/#python-api","text":"Here is how to create a standard \"Linear Warmup + Hold + Cosine Decay\" schedule: import torch from d9d.lr_scheduler.piecewise import * optimizer : torch . optim . Optimizer = ... total_steps : int = 1000 # Define Schedule # 1. Start at 0.0 # 2. Linear warmup to 1.0*LR over 100 steps # 3. Stay at 1.0 * LR until 50% of training steps # 3. Cosine decay to 0.1 (10% of LR) for the rest of training scheduler = ( piecewise_schedule ( initial_multiplier = 0.0 , total_steps = total_steps ) . for_steps ( 100 , target_multiplier = 1.0 , curve = CurveLinear ()) . until_percentage ( 0.5 , target_multiplier = 1.0 , curve = CurveLinear ()) . fill_rest ( target_multiplier = 0.1 , curve = CurveCosine ()) . build ( optimizer ) )","title":"Python API"},{"location":"lr_scheduler/piecewise/#pydantic-api","text":"import json import torch from d9d.lr_scheduler.piecewise import PiecewiseSchedulerConfig , piecewise_scheduler_from_config optimizer : torch . optim . Optimizer = ... total_steps : int = 1000 raw_config_json = \"\"\" { \"initial_multiplier\": 0.0, \"phases\": [ { \"mode\": \"steps\", \"steps\": 100, \"target_multiplier\": 1.0, \"curve\": { \"type\": \"linear\" } }, { \"mode\": \"rest\", \"target_multiplier\": 0.1, \"curve\": { \"type\": \"cosine\" } } ] } \"\"\" scheduler_config = PiecewiseSchedulerConfig . model_validate_json ( raw_config_json ) scheduler = piecewise_scheduler_from_config ( config = scheduler_config , optimizer = optimizer , total_steps = total_steps )","title":"Pydantic API"},{"location":"lr_scheduler/piecewise/#available-curves","text":"The following curve classes are available to interpolate values between phases: Curve Class Curve Config Description CurveLinear \"linear\" Standard straight-line interpolation. CurveCosine \"cosine\" Half-period cosine interpolation (Cosine Annealing). CurvePoly(power) \"poly\" Polynomial interpolation. power=1 is linear, power=2 is quadratic, etc. CurveExponential \"exponential\" Exponential (log-linear) interpolation.","title":"Available Curves"},{"location":"lr_scheduler/piecewise/#api-reference","text":"","title":"API Reference"},{"location":"lr_scheduler/piecewise/#d9d.lr_scheduler.piecewise","text":"Implements flexible piecewise learning rate schedules via a builder pattern.","title":"piecewise"},{"location":"lr_scheduler/piecewise/#d9d.lr_scheduler.piecewise.CurveBase","text":"Bases: ABC Abstract base class for interpolation curves used in scheduling. Source code in d9d/lr_scheduler/piecewise/curves.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class CurveBase ( abc . ABC ): \"\"\" Abstract base class for interpolation curves used in scheduling. \"\"\" @abc . abstractmethod def compute ( self , start : float , end : float , step_p : float ) -> float : \"\"\" Calculates the interpolated value. Args: start: The value at the beginning of the phase. end: The value at the end of the phase. step_p: Progress fraction through the phase (0.0 to 1.0). Returns: The interpolated value. \"\"\"","title":"CurveBase"},{"location":"lr_scheduler/piecewise/#d9d.lr_scheduler.piecewise.CurveBase.compute","text":"Calculates the interpolated value. Parameters: Name Type Description Default start float The value at the beginning of the phase. required end float The value at the end of the phase. required step_p float Progress fraction through the phase (0.0 to 1.0). required Returns: Type Description float The interpolated value. Source code in d9d/lr_scheduler/piecewise/curves.py 10 11 12 13 14 15 16 17 18 19 20 21 22 @abc . abstractmethod def compute ( self , start : float , end : float , step_p : float ) -> float : \"\"\" Calculates the interpolated value. Args: start: The value at the beginning of the phase. end: The value at the end of the phase. step_p: Progress fraction through the phase (0.0 to 1.0). Returns: The interpolated value. \"\"\"","title":"compute"},{"location":"lr_scheduler/piecewise/#d9d.lr_scheduler.piecewise.CurveCosine","text":"Bases: CurveBase Interpolates using a cosine annealing schedule (half-period cosine). Source code in d9d/lr_scheduler/piecewise/curves.py 34 35 36 37 38 39 40 41 class CurveCosine ( CurveBase ): \"\"\" Interpolates using a cosine annealing schedule (half-period cosine). \"\"\" def compute ( self , start : float , end : float , step_p : float ) -> float : cos_out = ( 1 + math . cos ( math . pi * step_p )) / 2 return end + ( start - end ) * cos_out","title":"CurveCosine"},{"location":"lr_scheduler/piecewise/#d9d.lr_scheduler.piecewise.CurveExponential","text":"Bases: CurveBase Interpolates exponentially between start and end values (log-space linear). Source code in d9d/lr_scheduler/piecewise/curves.py 64 65 66 67 68 69 70 71 72 73 74 75 class CurveExponential ( CurveBase ): \"\"\" Interpolates exponentially between start and end values (log-space linear). \"\"\" def compute ( self , start : float , end : float , step_p : float ) -> float : eps = 1e-8 safe_start = max ( start , eps ) safe_end = max ( end , eps ) out_log = math . log ( safe_start ) + ( math . log ( safe_end ) - math . log ( safe_start )) * step_p return math . exp ( out_log )","title":"CurveExponential"},{"location":"lr_scheduler/piecewise/#d9d.lr_scheduler.piecewise.CurveLinear","text":"Bases: CurveBase Linearly interpolates between start and end values. Source code in d9d/lr_scheduler/piecewise/curves.py 25 26 27 28 29 30 31 class CurveLinear ( CurveBase ): \"\"\" Linearly interpolates between start and end values. \"\"\" def compute ( self , start : float , end : float , step_p : float ) -> float : return start + ( end - start ) * step_p","title":"CurveLinear"},{"location":"lr_scheduler/piecewise/#d9d.lr_scheduler.piecewise.CurvePoly","text":"Bases: CurveBase Interpolates using a polynomial function. Source code in d9d/lr_scheduler/piecewise/curves.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 class CurvePoly ( CurveBase ): \"\"\" Interpolates using a polynomial function. \"\"\" def __init__ ( self , power : float ): \"\"\" Constructs a polynomial curve. Args: power: The exponent of the polynomial. 1.0 is linear, 2.0 is quadratic, etc. \"\"\" self . _power = power def compute ( self , start : float , end : float , step_p : float ) -> float : p_transformed = step_p ** self . _power return start + ( end - start ) * p_transformed","title":"CurvePoly"},{"location":"lr_scheduler/piecewise/#d9d.lr_scheduler.piecewise.CurvePoly.__init__","text":"Constructs a polynomial curve. Parameters: Name Type Description Default power float The exponent of the polynomial. 1.0 is linear, 2.0 is quadratic, etc. required Source code in d9d/lr_scheduler/piecewise/curves.py 49 50 51 52 53 54 55 56 57 def __init__ ( self , power : float ): \"\"\" Constructs a polynomial curve. Args: power: The exponent of the polynomial. 1.0 is linear, 2.0 is quadratic, etc. \"\"\" self . _power = power","title":"__init__"},{"location":"lr_scheduler/piecewise/#d9d.lr_scheduler.piecewise.PiecewiseSchedulerConfig","text":"Bases: BaseModel Declarative configuration for a piecewise learning rate scheduler. Attributes: Name Type Description initial_multiplier float The starting learning rate multiplier. phases list [ PhaseConfig ] A sequential list of phase configurations. Source code in d9d/lr_scheduler/piecewise/config.py 130 131 132 133 134 135 136 137 138 139 140 class PiecewiseSchedulerConfig ( BaseModel ): \"\"\" Declarative configuration for a piecewise learning rate scheduler. Attributes: initial_multiplier: The starting learning rate multiplier. phases: A sequential list of phase configurations. \"\"\" initial_multiplier : float phases : list [ PhaseConfig ]","title":"PiecewiseSchedulerConfig"},{"location":"lr_scheduler/piecewise/#d9d.lr_scheduler.piecewise.piecewise_schedule","text":"Entry point for creating a piecewise learning rate schedule. Parameters: Name Type Description Default initial_multiplier float The initial learning rate multiplier. required total_steps int | None Total training steps. Required for percentage-based scheduling. None Returns: Type Description PiecewiseScheduleBuilder A builder instance to configure phases. Source code in d9d/lr_scheduler/piecewise/builder.py 129 130 131 132 133 134 135 136 137 138 139 140 141 def piecewise_schedule ( initial_multiplier : float , total_steps : int | None = None ) -> PiecewiseScheduleBuilder : \"\"\" Entry point for creating a piecewise learning rate schedule. Args: initial_multiplier: The initial learning rate multiplier. total_steps: Total training steps. Required for percentage-based scheduling. Returns: A builder instance to configure phases. \"\"\" return PiecewiseScheduleBuilder ( initial_multiplier = initial_multiplier , total_steps = total_steps )","title":"piecewise_schedule"},{"location":"lr_scheduler/piecewise/#d9d.lr_scheduler.piecewise.piecewise_scheduler_from_config","text":"Constructs a PyTorch scheduler from the provided configuration. Parameters: Name Type Description Default config PiecewiseSchedulerConfig The scheduler configuration. required optimizer Optimizer The optimizer to wrap. required total_steps int | None The total number of training steps. Required if using percentage-based phases. required Returns: Type Description LRSchedulerProtocol A configured learning rate scheduler. Source code in d9d/lr_scheduler/piecewise/config.py 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 def piecewise_scheduler_from_config ( config : PiecewiseSchedulerConfig , optimizer : Optimizer , total_steps : int | None ) -> LRSchedulerProtocol : \"\"\" Constructs a PyTorch scheduler from the provided configuration. Args: config: The scheduler configuration. optimizer: The optimizer to wrap. total_steps: The total number of training steps. Required if using percentage-based phases. Returns: A configured learning rate scheduler. \"\"\" builder = piecewise_schedule ( config . initial_multiplier , total_steps ) for phase in config . phases : curve = curve_from_config ( phase . curve ) match phase : case StepPhaseConfig (): builder . for_steps ( phase . steps , phase . target_multiplier , curve ) case PercentagePhaseConfig (): builder . until_percentage ( phase . percentage , phase . target_multiplier , curve ) case RestPhaseConfig (): builder . fill_rest ( phase . target_multiplier , curve ) return builder . build ( optimizer )","title":"piecewise_scheduler_from_config"},{"location":"lr_scheduler/visualization/","text":"About It is often difficult to mentally visualize complex multiphase learning rate schedules. To address this, d9d allows you to visualize the resulting learning rate curve interactively using plotly . Usage Example The visualize_lr_scheduler function takes a factory function that constructs your scheduler, simulates a training run, and plots the learning rate history. import torch from d9d.lr_scheduler.visualizer import visualize_lr_scheduler from d9d.lr_scheduler.piecewise import piecewise_schedule , CurveLinear , CurveCosine def create_scheduler ( optimizer : torch . optim . Optimizer ): return ( piecewise_schedule ( initial_multiplier = 0.0 , total_steps = 100 ) . for_steps ( 10 , 1.0 , CurveLinear ()) . fill_rest ( 0.0 , CurveCosine ()) . build ( optimizer ) ) # Opens an interactive plot in browser/notebook visualize_lr_scheduler ( factory = create_scheduler , num_steps = 100 , # Duration to simulate init_lr = 1e-3 # Base LR to visualize ) API Reference d9d.lr_scheduler.visualizer visualize_lr_scheduler ( factory , num_steps , init_lr = 1.0 ) Visualizes the learning rate schedule using Plotly. This function simulates the training process for num_steps to record the LR changes and generates an interactive plot. Parameters: Name Type Description Default factory SchedulerFactory A callable that accepts an Optimizer and returns an LRScheduler. required num_steps int The number of steps to simulate. required init_lr float The initial learning rate to set on the dummy optimizer. 1.0 Raises: Type Description ImportError If the plotly library is not installed. Source code in d9d/lr_scheduler/visualizer.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def visualize_lr_scheduler ( factory : SchedulerFactory , num_steps : int , init_lr : float = 1.0 ): \"\"\" Visualizes the learning rate schedule using Plotly. This function simulates the training process for `num_steps` to record the LR changes and generates an interactive plot. Args: factory: A callable that accepts an Optimizer and returns an LRScheduler. num_steps: The number of steps to simulate. init_lr: The initial learning rate to set on the dummy optimizer. Raises: ImportError: If the `plotly` library is not installed. \"\"\" try : import plotly.graph_objects as go # noqa: PLC0415 except ImportError as e : raise ImportError ( \"You have to install `plotly` dependency to use scheduler visualization\" ) from e lrs = _get_history ( factory , num_steps , init_lr ) steps = list ( range ( num_steps )) fig = go . Figure () fig . add_trace ( go . Scatter ( x = steps , y = lrs , mode = \"lines\" , name = \"Learning Rate\" , line = { \"color\" : \"#636EFA\" , \"width\" : 3 }, hovertemplate = \"<b>Step:</b> % {x} <br><b>LR:</b> % {y:.6f} <extra></extra>\" , ) ) fig . update_layout ( title = { \"text\" : \"Scheduler\" , \"y\" : 0.95 , \"x\" : 0.5 , \"xanchor\" : \"center\" , \"yanchor\" : \"top\" }, xaxis_title = \"Steps\" , yaxis_title = \"Learning Rate\" , template = \"plotly_white\" , hovermode = \"x unified\" , height = 500 , ) fig . show ()","title":"Visualization"},{"location":"lr_scheduler/visualization/#about","text":"It is often difficult to mentally visualize complex multiphase learning rate schedules. To address this, d9d allows you to visualize the resulting learning rate curve interactively using plotly .","title":"About"},{"location":"lr_scheduler/visualization/#usage-example","text":"The visualize_lr_scheduler function takes a factory function that constructs your scheduler, simulates a training run, and plots the learning rate history. import torch from d9d.lr_scheduler.visualizer import visualize_lr_scheduler from d9d.lr_scheduler.piecewise import piecewise_schedule , CurveLinear , CurveCosine def create_scheduler ( optimizer : torch . optim . Optimizer ): return ( piecewise_schedule ( initial_multiplier = 0.0 , total_steps = 100 ) . for_steps ( 10 , 1.0 , CurveLinear ()) . fill_rest ( 0.0 , CurveCosine ()) . build ( optimizer ) ) # Opens an interactive plot in browser/notebook visualize_lr_scheduler ( factory = create_scheduler , num_steps = 100 , # Duration to simulate init_lr = 1e-3 # Base LR to visualize )","title":"Usage Example"},{"location":"lr_scheduler/visualization/#api-reference","text":"","title":"API Reference"},{"location":"lr_scheduler/visualization/#d9d.lr_scheduler.visualizer","text":"","title":"visualizer"},{"location":"lr_scheduler/visualization/#d9d.lr_scheduler.visualizer.visualize_lr_scheduler","text":"Visualizes the learning rate schedule using Plotly. This function simulates the training process for num_steps to record the LR changes and generates an interactive plot. Parameters: Name Type Description Default factory SchedulerFactory A callable that accepts an Optimizer and returns an LRScheduler. required num_steps int The number of steps to simulate. required init_lr float The initial learning rate to set on the dummy optimizer. 1.0 Raises: Type Description ImportError If the plotly library is not installed. Source code in d9d/lr_scheduler/visualizer.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def visualize_lr_scheduler ( factory : SchedulerFactory , num_steps : int , init_lr : float = 1.0 ): \"\"\" Visualizes the learning rate schedule using Plotly. This function simulates the training process for `num_steps` to record the LR changes and generates an interactive plot. Args: factory: A callable that accepts an Optimizer and returns an LRScheduler. num_steps: The number of steps to simulate. init_lr: The initial learning rate to set on the dummy optimizer. Raises: ImportError: If the `plotly` library is not installed. \"\"\" try : import plotly.graph_objects as go # noqa: PLC0415 except ImportError as e : raise ImportError ( \"You have to install `plotly` dependency to use scheduler visualization\" ) from e lrs = _get_history ( factory , num_steps , init_lr ) steps = list ( range ( num_steps )) fig = go . Figure () fig . add_trace ( go . Scatter ( x = steps , y = lrs , mode = \"lines\" , name = \"Learning Rate\" , line = { \"color\" : \"#636EFA\" , \"width\" : 3 }, hovertemplate = \"<b>Step:</b> % {x} <br><b>LR:</b> % {y:.6f} <extra></extra>\" , ) ) fig . update_layout ( title = { \"text\" : \"Scheduler\" , \"y\" : 0.95 , \"x\" : 0.5 , \"xanchor\" : \"center\" , \"yanchor\" : \"top\" }, xaxis_title = \"Steps\" , yaxis_title = \"Learning Rate\" , template = \"plotly_white\" , hovermode = \"x unified\" , height = 500 , ) fig . show ()","title":"visualize_lr_scheduler"},{"location":"metric/0_index/","text":"About The d9d.metric package provides a unified interface for tracking, accumulating, and synchronizing statistics (such as Accuracy) across a distributed environment. Why and How The Single-GPU Trap Some practitioners coming from single-GPU training or standard data science backgrounds are used to workflows relying on good-old CPU-based libraries such as scikit-learn : # Typical single-node pattern loss_val = loss_fn ( pred , target ) . item () # <--- CPU Sync Point 1 history . append ( loss_val ) # ... later ... avg = np . mean ( history ) # <--- CPU Sync Point 2 sklearn . metrics . f1_score ( all_preds , all_targets ) In a large-scale distributed environment, this approach causes critical failures: Pipeline Stalls : Calling .item() or .cpu() forces a synchronization that waits for the GPU to finish. This destroys the pipelining efficiency required for training large models. Out-of-Memory Errors : Accumulating prediction history for many steps in a Python list will rapidly exhaust RAM. No Synchronization - Partial View : Rank 0 only sees its own data shard. Logging loss from Rank 0 is misleading. So, we have to do something with metric implementations to be performant and accurate. The d9d Solution This package addresses issues described above by providing a Metric interface that is: Distributed Aware : Each metric knows how to synchronize its state across an ND-parallel environment via the sync method. Async Compatible : While Metric implementations themselves can remain simple and synchronous, they are designed to be driven by the AsyncMetricCollector . This wrapper offloads the synchronization and computation to a side-stream, allowing the main training loop to continue while metrics are being reduced. Stateful : Metrics implement the torch.distributed.checkpoint.stateful.Stateful interface, allowing their state to be checkpointed seamlessly. Clear : Unlike some other libraries, d9d's Metric is a lightweight interface. It has no hidden state accounting or complex contracts. Just implement the interface and ensure you don't break the lifecycle. The Metric Lifecycle A Metric in d9d follows a specific lifecycle: Update : Happens every train step. Data is aggregated locally on the GPU using methods like .add_() . No communication occurs here. Sync : Happens at the logging interval. The metric aggregates data across the world (e.g. all_reduce ). Compute : Calculates the final scalar (e.g., dividing total loss by total samples) using the synchronized data. Reset : Clears the internal state for the next logging window. Usage Examples Basic Usage Typically, you want to just instantiate and update metrics within your TrainTask object. See related examples in Trainer documentation. Manual Usage You may want to use d9d metrics manually, without using the Trainer object. When used directly, the sync() method is blocking by default. You may call it within torch.cuda.stream(...) to overlap with computations. import torch from d9d.metric.impl import WeightedMeanMetric from d9d.core.dist_context import DistributedContext # 1. Initialize metric = WeightedMeanMetric () metric . to ( \"cuda\" ) dataloader = ... dist_ctx = ... # 2. Training Loop for step , batch in enumerate ( dataloader ): # ... forward, backward ... loss = ... num_tokens = ... # Update local state (No communication, cheap) metric . update ( values = loss , weights = num_tokens ) # 3. Synchronize & Compute # This will block until all ranks finish all_reduce metric . sync ( dist_ctx ) print ( f \"Global Average Loss: { metric . compute () } \" ) # 4. Reset for next epoch metric . reset () d9d.metric Distributed metric abstractions and implementations. Metric Bases: ABC , Stateful , Generic [ TComputeResult ] Abstract base class for all metrics. Metrics track statistics over time (e.g., during training) and can be synchronized across distributed processes. They also support state persistence via the Stateful interface. Source code in d9d/metric/abc.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 class Metric ( abc . ABC , Stateful , Generic [ TComputeResult ]): \"\"\" Abstract base class for all metrics. Metrics track statistics over time (e.g., during training) and can be synchronized across distributed processes. They also support state persistence via the Stateful interface. \"\"\" @abc . abstractmethod def update ( self , * args : Any , ** kwargs : Any ): \"\"\" Updates the metric state with a new batch of data. Args: *args: Positional arguments required by the specific metric implementation. **kwargs: Keyword arguments required by the specific metric implementation. \"\"\" @abc . abstractmethod def sync ( self , dist_context : DistributedContext ): \"\"\" Synchronizes the metric state across distributed processes. This method aggregates statistics from all ranks (e.g., via all-reduce) to ensure the metric state is consistent globally. Args: dist_context: The distributed context. \"\"\" @abc . abstractmethod def compute ( self ) -> TComputeResult : \"\"\" Computes the current value of the metric. Returns: The computed metric result (of type `TComputeResult`). This can be a single `torch.Tensor` or `PyTree` structure (dict, list, etc.) containing tensors, depending on how the subclass was typed. \"\"\" @abc . abstractmethod def reset ( self ): \"\"\" Resets the internal state of the metric to the initial values. \"\"\" def to ( self , device : str | torch . device | int ): \"\"\" Moves a metric state to a specified device. Args: device: The device to move the metric state to. \"\"\" compute () abstractmethod Computes the current value of the metric. Returns: Type Description TComputeResult The computed metric result (of type TComputeResult ). This can be a single torch.Tensor or PyTree structure (dict, list, etc.) containing tensors, depending on how the subclass was typed. Source code in d9d/metric/abc.py 44 45 46 47 48 49 50 51 52 53 @abc . abstractmethod def compute ( self ) -> TComputeResult : \"\"\" Computes the current value of the metric. Returns: The computed metric result (of type `TComputeResult`). This can be a single `torch.Tensor` or `PyTree` structure (dict, list, etc.) containing tensors, depending on how the subclass was typed. \"\"\" reset () abstractmethod Resets the internal state of the metric to the initial values. Source code in d9d/metric/abc.py 55 56 57 58 59 @abc . abstractmethod def reset ( self ): \"\"\" Resets the internal state of the metric to the initial values. \"\"\" sync ( dist_context ) abstractmethod Synchronizes the metric state across distributed processes. This method aggregates statistics from all ranks (e.g., via all-reduce) to ensure the metric state is consistent globally. Parameters: Name Type Description Default dist_context DistributedContext The distributed context. required Source code in d9d/metric/abc.py 32 33 34 35 36 37 38 39 40 41 42 @abc . abstractmethod def sync ( self , dist_context : DistributedContext ): \"\"\" Synchronizes the metric state across distributed processes. This method aggregates statistics from all ranks (e.g., via all-reduce) to ensure the metric state is consistent globally. Args: dist_context: The distributed context. \"\"\" to ( device ) Moves a metric state to a specified device. Parameters: Name Type Description Default device str | device | int The device to move the metric state to. required Source code in d9d/metric/abc.py 61 62 63 64 65 66 67 def to ( self , device : str | torch . device | int ): \"\"\" Moves a metric state to a specified device. Args: device: The device to move the metric state to. \"\"\" update ( * args , ** kwargs ) abstractmethod Updates the metric state with a new batch of data. Parameters: Name Type Description Default *args Any Positional arguments required by the specific metric implementation. () **kwargs Any Keyword arguments required by the specific metric implementation. {} Source code in d9d/metric/abc.py 22 23 24 25 26 27 28 29 30 @abc . abstractmethod def update ( self , * args : Any , ** kwargs : Any ): \"\"\" Updates the metric state with a new batch of data. Args: *args: Positional arguments required by the specific metric implementation. **kwargs: Keyword arguments required by the specific metric implementation. \"\"\"","title":"Metrics"},{"location":"metric/0_index/#about","text":"The d9d.metric package provides a unified interface for tracking, accumulating, and synchronizing statistics (such as Accuracy) across a distributed environment.","title":"About"},{"location":"metric/0_index/#why-and-how","text":"","title":"Why and How"},{"location":"metric/0_index/#the-single-gpu-trap","text":"Some practitioners coming from single-GPU training or standard data science backgrounds are used to workflows relying on good-old CPU-based libraries such as scikit-learn : # Typical single-node pattern loss_val = loss_fn ( pred , target ) . item () # <--- CPU Sync Point 1 history . append ( loss_val ) # ... later ... avg = np . mean ( history ) # <--- CPU Sync Point 2 sklearn . metrics . f1_score ( all_preds , all_targets ) In a large-scale distributed environment, this approach causes critical failures: Pipeline Stalls : Calling .item() or .cpu() forces a synchronization that waits for the GPU to finish. This destroys the pipelining efficiency required for training large models. Out-of-Memory Errors : Accumulating prediction history for many steps in a Python list will rapidly exhaust RAM. No Synchronization - Partial View : Rank 0 only sees its own data shard. Logging loss from Rank 0 is misleading. So, we have to do something with metric implementations to be performant and accurate.","title":"The Single-GPU Trap"},{"location":"metric/0_index/#the-d9d-solution","text":"This package addresses issues described above by providing a Metric interface that is: Distributed Aware : Each metric knows how to synchronize its state across an ND-parallel environment via the sync method. Async Compatible : While Metric implementations themselves can remain simple and synchronous, they are designed to be driven by the AsyncMetricCollector . This wrapper offloads the synchronization and computation to a side-stream, allowing the main training loop to continue while metrics are being reduced. Stateful : Metrics implement the torch.distributed.checkpoint.stateful.Stateful interface, allowing their state to be checkpointed seamlessly. Clear : Unlike some other libraries, d9d's Metric is a lightweight interface. It has no hidden state accounting or complex contracts. Just implement the interface and ensure you don't break the lifecycle.","title":"The d9d Solution"},{"location":"metric/0_index/#the-metric-lifecycle","text":"A Metric in d9d follows a specific lifecycle: Update : Happens every train step. Data is aggregated locally on the GPU using methods like .add_() . No communication occurs here. Sync : Happens at the logging interval. The metric aggregates data across the world (e.g. all_reduce ). Compute : Calculates the final scalar (e.g., dividing total loss by total samples) using the synchronized data. Reset : Clears the internal state for the next logging window.","title":"The Metric Lifecycle"},{"location":"metric/0_index/#usage-examples","text":"","title":"Usage Examples"},{"location":"metric/0_index/#basic-usage","text":"Typically, you want to just instantiate and update metrics within your TrainTask object. See related examples in Trainer documentation.","title":"Basic Usage"},{"location":"metric/0_index/#manual-usage","text":"You may want to use d9d metrics manually, without using the Trainer object. When used directly, the sync() method is blocking by default. You may call it within torch.cuda.stream(...) to overlap with computations. import torch from d9d.metric.impl import WeightedMeanMetric from d9d.core.dist_context import DistributedContext # 1. Initialize metric = WeightedMeanMetric () metric . to ( \"cuda\" ) dataloader = ... dist_ctx = ... # 2. Training Loop for step , batch in enumerate ( dataloader ): # ... forward, backward ... loss = ... num_tokens = ... # Update local state (No communication, cheap) metric . update ( values = loss , weights = num_tokens ) # 3. Synchronize & Compute # This will block until all ranks finish all_reduce metric . sync ( dist_ctx ) print ( f \"Global Average Loss: { metric . compute () } \" ) # 4. Reset for next epoch metric . reset ()","title":"Manual Usage"},{"location":"metric/0_index/#d9d.metric","text":"Distributed metric abstractions and implementations.","title":"metric"},{"location":"metric/0_index/#d9d.metric.Metric","text":"Bases: ABC , Stateful , Generic [ TComputeResult ] Abstract base class for all metrics. Metrics track statistics over time (e.g., during training) and can be synchronized across distributed processes. They also support state persistence via the Stateful interface. Source code in d9d/metric/abc.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 class Metric ( abc . ABC , Stateful , Generic [ TComputeResult ]): \"\"\" Abstract base class for all metrics. Metrics track statistics over time (e.g., during training) and can be synchronized across distributed processes. They also support state persistence via the Stateful interface. \"\"\" @abc . abstractmethod def update ( self , * args : Any , ** kwargs : Any ): \"\"\" Updates the metric state with a new batch of data. Args: *args: Positional arguments required by the specific metric implementation. **kwargs: Keyword arguments required by the specific metric implementation. \"\"\" @abc . abstractmethod def sync ( self , dist_context : DistributedContext ): \"\"\" Synchronizes the metric state across distributed processes. This method aggregates statistics from all ranks (e.g., via all-reduce) to ensure the metric state is consistent globally. Args: dist_context: The distributed context. \"\"\" @abc . abstractmethod def compute ( self ) -> TComputeResult : \"\"\" Computes the current value of the metric. Returns: The computed metric result (of type `TComputeResult`). This can be a single `torch.Tensor` or `PyTree` structure (dict, list, etc.) containing tensors, depending on how the subclass was typed. \"\"\" @abc . abstractmethod def reset ( self ): \"\"\" Resets the internal state of the metric to the initial values. \"\"\" def to ( self , device : str | torch . device | int ): \"\"\" Moves a metric state to a specified device. Args: device: The device to move the metric state to. \"\"\"","title":"Metric"},{"location":"metric/0_index/#d9d.metric.Metric.compute","text":"Computes the current value of the metric. Returns: Type Description TComputeResult The computed metric result (of type TComputeResult ). This can be a single torch.Tensor or PyTree structure (dict, list, etc.) containing tensors, depending on how the subclass was typed. Source code in d9d/metric/abc.py 44 45 46 47 48 49 50 51 52 53 @abc . abstractmethod def compute ( self ) -> TComputeResult : \"\"\" Computes the current value of the metric. Returns: The computed metric result (of type `TComputeResult`). This can be a single `torch.Tensor` or `PyTree` structure (dict, list, etc.) containing tensors, depending on how the subclass was typed. \"\"\"","title":"compute"},{"location":"metric/0_index/#d9d.metric.Metric.reset","text":"Resets the internal state of the metric to the initial values. Source code in d9d/metric/abc.py 55 56 57 58 59 @abc . abstractmethod def reset ( self ): \"\"\" Resets the internal state of the metric to the initial values. \"\"\"","title":"reset"},{"location":"metric/0_index/#d9d.metric.Metric.sync","text":"Synchronizes the metric state across distributed processes. This method aggregates statistics from all ranks (e.g., via all-reduce) to ensure the metric state is consistent globally. Parameters: Name Type Description Default dist_context DistributedContext The distributed context. required Source code in d9d/metric/abc.py 32 33 34 35 36 37 38 39 40 41 42 @abc . abstractmethod def sync ( self , dist_context : DistributedContext ): \"\"\" Synchronizes the metric state across distributed processes. This method aggregates statistics from all ranks (e.g., via all-reduce) to ensure the metric state is consistent globally. Args: dist_context: The distributed context. \"\"\"","title":"sync"},{"location":"metric/0_index/#d9d.metric.Metric.to","text":"Moves a metric state to a specified device. Parameters: Name Type Description Default device str | device | int The device to move the metric state to. required Source code in d9d/metric/abc.py 61 62 63 64 65 66 67 def to ( self , device : str | torch . device | int ): \"\"\" Moves a metric state to a specified device. Args: device: The device to move the metric state to. \"\"\"","title":"to"},{"location":"metric/0_index/#d9d.metric.Metric.update","text":"Updates the metric state with a new batch of data. Parameters: Name Type Description Default *args Any Positional arguments required by the specific metric implementation. () **kwargs Any Keyword arguments required by the specific metric implementation. {} Source code in d9d/metric/abc.py 22 23 24 25 26 27 28 29 30 @abc . abstractmethod def update ( self , * args : Any , ** kwargs : Any ): \"\"\" Updates the metric state with a new batch of data. Args: *args: Positional arguments required by the specific metric implementation. **kwargs: Keyword arguments required by the specific metric implementation. \"\"\"","title":"update"},{"location":"metric/custom/","text":"The d9d framework allows you to implement custom metrics by adhering to the Metric interface. Design Guidelines Metric implementations usually follow this design: GPU Residency : Metrics accumulate data directly on the GPU tensors to avoid CPU-GPU synchronization. Linearly Additive States : Instead of storing unstable averages (e.g., \"Current Accuracy\"), store raw accumulation counts like \"Total Correct\" and \"Total Samples\". These are mathematically safe to sum via all_reduce . Helper Components We provide the d9d.metric.component package to simplify implementation: MetricAccumulator : A helper object that handles the boilerplate of maintaining Local vs Synchronized versions of a metric state tensor. It supports standard reduction operations like Sum, Max, and Min. Examples MaxMetric Below is an example of a MaxMetric that tracks the maximum value seen across all ranks using the MetricAccumulator helper. import torch import torch.distributed as dist from typing import Any from d9d.metric import Metric from d9d.metric.component import MetricAccumulator , MetricReduceOp from d9d.core.dist_context import DistributedContext class MaxMetric ( Metric [ torch . Tensor ]): def __init__ ( self ): # Initialize accumulator with -inf self . _max_val = MetricAccumulator ( torch . tensor ( float ( '-inf' )), reduce_op = MetricReduceOp . max ) def update ( self , value : torch . Tensor ): # Update local max (No communication) self . _max_val . update ( value ) def sync ( self , dist_context : DistributedContext ): # Perform all_reduce across the world self . _max_val . sync () def compute ( self ) -> torch . Tensor : # Return the synchronized value return self . _max_val . value def reset ( self ): self . _max_val . reset () def to ( self , device : str | torch . device | int ): self . _max_val . to ( device ) # Stateful Protocol for Checkpointing def state_dict ( self ) -> dict [ str , Any ]: return { 'max_val' : self . _max_val . state_dict ()} def load_state_dict ( self , state_dict : dict [ str , Any ]) -> None : self . _max_val . load_state_dict ( state_dict [ 'max_val' ]) d9d.metric.component Reusable components for building distributed metrics. MetricAccumulator Bases: Stateful Helper class to track a distributed metric state. This class manages two copies of the state: a 'local' copy that is updated locally on every step, and a 'synchronized' copy that is populated during the sync phase via distributed reduction (all-reduce). Source code in d9d/metric/component/accumulator.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 class MetricAccumulator ( Stateful ): \"\"\"Helper class to track a distributed metric state. This class manages two copies of the state: a 'local' copy that is updated locally on every step, and a 'synchronized' copy that is populated during the sync phase via distributed reduction (all-reduce). \"\"\" def __init__ ( self , initial_value : torch . Tensor , reduce_op : MetricReduceOp = MetricReduceOp . sum ): \"\"\"Constructs MetricAccumulator object. Args: initial_value: Tensor representing the starting value (e.g., 0 for sum, -inf for max). This tensor determines the device and dtype of the accumulator. reduce_op: The reduction operation to use during updates and synchronization. \"\"\" self . _initial = initial_value . clone () self . _local = initial_value . clone () self . _synchronized = initial_value . clone () self . _reduce_op = reduce_op self . _is_synchronized = False def update ( self , value : torch . Tensor | float | bool ): \"\"\"Updates the local accumulator with a new value. This operation is performed in-place on the local tensor using the configured reduction operation (e.g., add for Sum, max for Max). It marks the accumulator as not synchronized. Args: value: The value to accumulate. \"\"\" _accumulate_inplace_ ( self . _reduce_op , self . _local , value ) self . _is_synchronized = False def sync ( self ): \"\"\"Synchronizes the accumulator across the default distributed process group. This method acts as a blocking barrier. It copies the local state to a buffer and performs an `all_reduce` collective operation. \"\"\" self . _synchronized . copy_ ( self . _local ) dist . all_reduce ( self . _synchronized , op = _torch_reduce_op_for ( self . _reduce_op )) self . _is_synchronized = True @property def value ( self ) -> torch . Tensor : \"\"\"Returns the current accumulated value. Returns: The global synchronized value if `sync()` was called recently, otherwise the local accumulated value. \"\"\" return self . _synchronized if self . _is_synchronized else self . _local def reset ( self ): \"\"\"Resets the accumulator to its initial state.\"\"\" self . _local . copy_ ( self . _initial ) self . _is_synchronized = False def to ( self , device : str | torch . device | int ): \"\"\"Moves internal tensors to the specified device. Args: device: Target device. \"\"\" self . _local = self . _local . to ( device ) self . _synchronized = self . _synchronized . to ( device ) def state_dict ( self ) -> dict [ str , Any ]: \"\"\"Returns the serialized state of the accumulator. Returns: Dictionary containing local and synchronized tensors and status flags. \"\"\" return { \"local\" : self . _local , \"synchronized\" : self . _synchronized , \"is_synchronized\" : self . _is_synchronized } def load_state_dict ( self , state_dict : dict [ str , Any ]) -> None : \"\"\"Restores the accumulator state from a checkpoint. Args: state_dict: Dictionary containing state to load. \"\"\" self . _local = state_dict [ \"local\" ] self . _synchronized = state_dict [ \"synchronized\" ] self . _is_synchronized = state_dict [ \"is_synchronized\" ] value property Returns the current accumulated value. Returns: Type Description Tensor The global synchronized value if sync() was called recently, Tensor otherwise the local accumulated value. __init__ ( initial_value , reduce_op = MetricReduceOp . sum ) Constructs MetricAccumulator object. Parameters: Name Type Description Default initial_value Tensor Tensor representing the starting value (e.g., 0 for sum, -inf for max). This tensor determines the device and dtype of the accumulator. required reduce_op MetricReduceOp The reduction operation to use during updates and synchronization. sum Source code in d9d/metric/component/accumulator.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def __init__ ( self , initial_value : torch . Tensor , reduce_op : MetricReduceOp = MetricReduceOp . sum ): \"\"\"Constructs MetricAccumulator object. Args: initial_value: Tensor representing the starting value (e.g., 0 for sum, -inf for max). This tensor determines the device and dtype of the accumulator. reduce_op: The reduction operation to use during updates and synchronization. \"\"\" self . _initial = initial_value . clone () self . _local = initial_value . clone () self . _synchronized = initial_value . clone () self . _reduce_op = reduce_op self . _is_synchronized = False load_state_dict ( state_dict ) Restores the accumulator state from a checkpoint. Parameters: Name Type Description Default state_dict dict [ str , Any ] Dictionary containing state to load. required Source code in d9d/metric/component/accumulator.py 132 133 134 135 136 137 138 139 140 141 def load_state_dict ( self , state_dict : dict [ str , Any ]) -> None : \"\"\"Restores the accumulator state from a checkpoint. Args: state_dict: Dictionary containing state to load. \"\"\" self . _local = state_dict [ \"local\" ] self . _synchronized = state_dict [ \"synchronized\" ] self . _is_synchronized = state_dict [ \"is_synchronized\" ] reset () Resets the accumulator to its initial state. Source code in d9d/metric/component/accumulator.py 106 107 108 109 110 111 def reset ( self ): \"\"\"Resets the accumulator to its initial state.\"\"\" self . _local . copy_ ( self . _initial ) self . _is_synchronized = False state_dict () Returns the serialized state of the accumulator. Returns: Type Description dict [ str , Any ] Dictionary containing local and synchronized tensors and status flags. Source code in d9d/metric/component/accumulator.py 123 124 125 126 127 128 129 130 def state_dict ( self ) -> dict [ str , Any ]: \"\"\"Returns the serialized state of the accumulator. Returns: Dictionary containing local and synchronized tensors and status flags. \"\"\" return { \"local\" : self . _local , \"synchronized\" : self . _synchronized , \"is_synchronized\" : self . _is_synchronized } sync () Synchronizes the accumulator across the default distributed process group. This method acts as a blocking barrier. It copies the local state to a buffer and performs an all_reduce collective operation. Source code in d9d/metric/component/accumulator.py 83 84 85 86 87 88 89 90 91 92 93 def sync ( self ): \"\"\"Synchronizes the accumulator across the default distributed process group. This method acts as a blocking barrier. It copies the local state to a buffer and performs an `all_reduce` collective operation. \"\"\" self . _synchronized . copy_ ( self . _local ) dist . all_reduce ( self . _synchronized , op = _torch_reduce_op_for ( self . _reduce_op )) self . _is_synchronized = True to ( device ) Moves internal tensors to the specified device. Parameters: Name Type Description Default device str | device | int Target device. required Source code in d9d/metric/component/accumulator.py 113 114 115 116 117 118 119 120 121 def to ( self , device : str | torch . device | int ): \"\"\"Moves internal tensors to the specified device. Args: device: Target device. \"\"\" self . _local = self . _local . to ( device ) self . _synchronized = self . _synchronized . to ( device ) update ( value ) Updates the local accumulator with a new value. This operation is performed in-place on the local tensor using the configured reduction operation (e.g., add for Sum, max for Max). It marks the accumulator as not synchronized. Parameters: Name Type Description Default value Tensor | float | bool The value to accumulate. required Source code in d9d/metric/component/accumulator.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def update ( self , value : torch . Tensor | float | bool ): \"\"\"Updates the local accumulator with a new value. This operation is performed in-place on the local tensor using the configured reduction operation (e.g., add for Sum, max for Max). It marks the accumulator as not synchronized. Args: value: The value to accumulate. \"\"\" _accumulate_inplace_ ( self . _reduce_op , self . _local , value ) self . _is_synchronized = False","title":"Custom Metrics"},{"location":"metric/custom/#design-guidelines","text":"Metric implementations usually follow this design: GPU Residency : Metrics accumulate data directly on the GPU tensors to avoid CPU-GPU synchronization. Linearly Additive States : Instead of storing unstable averages (e.g., \"Current Accuracy\"), store raw accumulation counts like \"Total Correct\" and \"Total Samples\". These are mathematically safe to sum via all_reduce .","title":"Design Guidelines"},{"location":"metric/custom/#helper-components","text":"We provide the d9d.metric.component package to simplify implementation: MetricAccumulator : A helper object that handles the boilerplate of maintaining Local vs Synchronized versions of a metric state tensor. It supports standard reduction operations like Sum, Max, and Min.","title":"Helper Components"},{"location":"metric/custom/#examples","text":"","title":"Examples"},{"location":"metric/custom/#maxmetric","text":"Below is an example of a MaxMetric that tracks the maximum value seen across all ranks using the MetricAccumulator helper. import torch import torch.distributed as dist from typing import Any from d9d.metric import Metric from d9d.metric.component import MetricAccumulator , MetricReduceOp from d9d.core.dist_context import DistributedContext class MaxMetric ( Metric [ torch . Tensor ]): def __init__ ( self ): # Initialize accumulator with -inf self . _max_val = MetricAccumulator ( torch . tensor ( float ( '-inf' )), reduce_op = MetricReduceOp . max ) def update ( self , value : torch . Tensor ): # Update local max (No communication) self . _max_val . update ( value ) def sync ( self , dist_context : DistributedContext ): # Perform all_reduce across the world self . _max_val . sync () def compute ( self ) -> torch . Tensor : # Return the synchronized value return self . _max_val . value def reset ( self ): self . _max_val . reset () def to ( self , device : str | torch . device | int ): self . _max_val . to ( device ) # Stateful Protocol for Checkpointing def state_dict ( self ) -> dict [ str , Any ]: return { 'max_val' : self . _max_val . state_dict ()} def load_state_dict ( self , state_dict : dict [ str , Any ]) -> None : self . _max_val . load_state_dict ( state_dict [ 'max_val' ])","title":"MaxMetric"},{"location":"metric/custom/#d9d.metric.component","text":"Reusable components for building distributed metrics.","title":"component"},{"location":"metric/custom/#d9d.metric.component.MetricAccumulator","text":"Bases: Stateful Helper class to track a distributed metric state. This class manages two copies of the state: a 'local' copy that is updated locally on every step, and a 'synchronized' copy that is populated during the sync phase via distributed reduction (all-reduce). Source code in d9d/metric/component/accumulator.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 class MetricAccumulator ( Stateful ): \"\"\"Helper class to track a distributed metric state. This class manages two copies of the state: a 'local' copy that is updated locally on every step, and a 'synchronized' copy that is populated during the sync phase via distributed reduction (all-reduce). \"\"\" def __init__ ( self , initial_value : torch . Tensor , reduce_op : MetricReduceOp = MetricReduceOp . sum ): \"\"\"Constructs MetricAccumulator object. Args: initial_value: Tensor representing the starting value (e.g., 0 for sum, -inf for max). This tensor determines the device and dtype of the accumulator. reduce_op: The reduction operation to use during updates and synchronization. \"\"\" self . _initial = initial_value . clone () self . _local = initial_value . clone () self . _synchronized = initial_value . clone () self . _reduce_op = reduce_op self . _is_synchronized = False def update ( self , value : torch . Tensor | float | bool ): \"\"\"Updates the local accumulator with a new value. This operation is performed in-place on the local tensor using the configured reduction operation (e.g., add for Sum, max for Max). It marks the accumulator as not synchronized. Args: value: The value to accumulate. \"\"\" _accumulate_inplace_ ( self . _reduce_op , self . _local , value ) self . _is_synchronized = False def sync ( self ): \"\"\"Synchronizes the accumulator across the default distributed process group. This method acts as a blocking barrier. It copies the local state to a buffer and performs an `all_reduce` collective operation. \"\"\" self . _synchronized . copy_ ( self . _local ) dist . all_reduce ( self . _synchronized , op = _torch_reduce_op_for ( self . _reduce_op )) self . _is_synchronized = True @property def value ( self ) -> torch . Tensor : \"\"\"Returns the current accumulated value. Returns: The global synchronized value if `sync()` was called recently, otherwise the local accumulated value. \"\"\" return self . _synchronized if self . _is_synchronized else self . _local def reset ( self ): \"\"\"Resets the accumulator to its initial state.\"\"\" self . _local . copy_ ( self . _initial ) self . _is_synchronized = False def to ( self , device : str | torch . device | int ): \"\"\"Moves internal tensors to the specified device. Args: device: Target device. \"\"\" self . _local = self . _local . to ( device ) self . _synchronized = self . _synchronized . to ( device ) def state_dict ( self ) -> dict [ str , Any ]: \"\"\"Returns the serialized state of the accumulator. Returns: Dictionary containing local and synchronized tensors and status flags. \"\"\" return { \"local\" : self . _local , \"synchronized\" : self . _synchronized , \"is_synchronized\" : self . _is_synchronized } def load_state_dict ( self , state_dict : dict [ str , Any ]) -> None : \"\"\"Restores the accumulator state from a checkpoint. Args: state_dict: Dictionary containing state to load. \"\"\" self . _local = state_dict [ \"local\" ] self . _synchronized = state_dict [ \"synchronized\" ] self . _is_synchronized = state_dict [ \"is_synchronized\" ]","title":"MetricAccumulator"},{"location":"metric/custom/#d9d.metric.component.MetricAccumulator.value","text":"Returns the current accumulated value. Returns: Type Description Tensor The global synchronized value if sync() was called recently, Tensor otherwise the local accumulated value.","title":"value"},{"location":"metric/custom/#d9d.metric.component.MetricAccumulator.__init__","text":"Constructs MetricAccumulator object. Parameters: Name Type Description Default initial_value Tensor Tensor representing the starting value (e.g., 0 for sum, -inf for max). This tensor determines the device and dtype of the accumulator. required reduce_op MetricReduceOp The reduction operation to use during updates and synchronization. sum Source code in d9d/metric/component/accumulator.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def __init__ ( self , initial_value : torch . Tensor , reduce_op : MetricReduceOp = MetricReduceOp . sum ): \"\"\"Constructs MetricAccumulator object. Args: initial_value: Tensor representing the starting value (e.g., 0 for sum, -inf for max). This tensor determines the device and dtype of the accumulator. reduce_op: The reduction operation to use during updates and synchronization. \"\"\" self . _initial = initial_value . clone () self . _local = initial_value . clone () self . _synchronized = initial_value . clone () self . _reduce_op = reduce_op self . _is_synchronized = False","title":"__init__"},{"location":"metric/custom/#d9d.metric.component.MetricAccumulator.load_state_dict","text":"Restores the accumulator state from a checkpoint. Parameters: Name Type Description Default state_dict dict [ str , Any ] Dictionary containing state to load. required Source code in d9d/metric/component/accumulator.py 132 133 134 135 136 137 138 139 140 141 def load_state_dict ( self , state_dict : dict [ str , Any ]) -> None : \"\"\"Restores the accumulator state from a checkpoint. Args: state_dict: Dictionary containing state to load. \"\"\" self . _local = state_dict [ \"local\" ] self . _synchronized = state_dict [ \"synchronized\" ] self . _is_synchronized = state_dict [ \"is_synchronized\" ]","title":"load_state_dict"},{"location":"metric/custom/#d9d.metric.component.MetricAccumulator.reset","text":"Resets the accumulator to its initial state. Source code in d9d/metric/component/accumulator.py 106 107 108 109 110 111 def reset ( self ): \"\"\"Resets the accumulator to its initial state.\"\"\" self . _local . copy_ ( self . _initial ) self . _is_synchronized = False","title":"reset"},{"location":"metric/custom/#d9d.metric.component.MetricAccumulator.state_dict","text":"Returns the serialized state of the accumulator. Returns: Type Description dict [ str , Any ] Dictionary containing local and synchronized tensors and status flags. Source code in d9d/metric/component/accumulator.py 123 124 125 126 127 128 129 130 def state_dict ( self ) -> dict [ str , Any ]: \"\"\"Returns the serialized state of the accumulator. Returns: Dictionary containing local and synchronized tensors and status flags. \"\"\" return { \"local\" : self . _local , \"synchronized\" : self . _synchronized , \"is_synchronized\" : self . _is_synchronized }","title":"state_dict"},{"location":"metric/custom/#d9d.metric.component.MetricAccumulator.sync","text":"Synchronizes the accumulator across the default distributed process group. This method acts as a blocking barrier. It copies the local state to a buffer and performs an all_reduce collective operation. Source code in d9d/metric/component/accumulator.py 83 84 85 86 87 88 89 90 91 92 93 def sync ( self ): \"\"\"Synchronizes the accumulator across the default distributed process group. This method acts as a blocking barrier. It copies the local state to a buffer and performs an `all_reduce` collective operation. \"\"\" self . _synchronized . copy_ ( self . _local ) dist . all_reduce ( self . _synchronized , op = _torch_reduce_op_for ( self . _reduce_op )) self . _is_synchronized = True","title":"sync"},{"location":"metric/custom/#d9d.metric.component.MetricAccumulator.to","text":"Moves internal tensors to the specified device. Parameters: Name Type Description Default device str | device | int Target device. required Source code in d9d/metric/component/accumulator.py 113 114 115 116 117 118 119 120 121 def to ( self , device : str | torch . device | int ): \"\"\"Moves internal tensors to the specified device. Args: device: Target device. \"\"\" self . _local = self . _local . to ( device ) self . _synchronized = self . _synchronized . to ( device )","title":"to"},{"location":"metric/custom/#d9d.metric.component.MetricAccumulator.update","text":"Updates the local accumulator with a new value. This operation is performed in-place on the local tensor using the configured reduction operation (e.g., add for Sum, max for Max). It marks the accumulator as not synchronized. Parameters: Name Type Description Default value Tensor | float | bool The value to accumulate. required Source code in d9d/metric/component/accumulator.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def update ( self , value : torch . Tensor | float | bool ): \"\"\"Updates the local accumulator with a new value. This operation is performed in-place on the local tensor using the configured reduction operation (e.g., add for Sum, max for Max). It marks the accumulator as not synchronized. Args: value: The value to accumulate. \"\"\" _accumulate_inplace_ ( self . _reduce_op , self . _local , value ) self . _is_synchronized = False","title":"update"},{"location":"metric/implemented/","text":"The d9d.metric.impl package contains standard metric implementations ready for use in distributed training. d9d.metric.impl BinaryAUROCMetric Bases: Metric [ Tensor ] Computes approximated AUROC for binary classification using histograms. Standard AUROC computation requires storing the entire history of predictions to sort and rank them. This implementation solves the memory constraint by discretizing predictions into histograms. This method employs a frequency-based sketching approach. It relies on the observation that the AUROC can be approximated by computing the area shared or separated by the probability density functions of the positive and negative classes. We maintain two separate histograms for positive and negative samples and apply the trapezoidal rule to estimate the area. References Albakour et al., \"Fast and memory efficient AUC-ROC approximation for Stream Learning\", 2021. https://www.researchgate.net/publication/353020448_Fast_and_memory_efficient_AUC-ROC_approximation_for_Stream_Learning Source code in d9d/metric/impl/auroc.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 class BinaryAUROCMetric ( Metric [ torch . Tensor ]): \"\"\"Computes approximated AUROC for binary classification using histograms. Standard AUROC computation requires storing the entire history of predictions to sort and rank them. This implementation solves the memory constraint by discretizing predictions into histograms. This method employs a frequency-based sketching approach. It relies on the observation that the AUROC can be approximated by computing the area shared or separated by the probability density functions of the positive and negative classes. We maintain two separate histograms for positive and negative samples and apply the trapezoidal rule to estimate the area. References: Albakour et al., \"Fast and memory efficient AUC-ROC approximation for Stream Learning\", 2021. https://www.researchgate.net/publication/353020448_Fast_and_memory_efficient_AUC-ROC_approximation_for_Stream_Learning \"\"\" def __init__ ( self , num_bins : int = 10000 ): \"\"\"Constructs the BinaryAUROCMetric object. Args: num_bins: Number of bins for histogram approximation. This parameter controls the trade-off between memory consumption and approximation accuracy. \"\"\" self . _num_bins = num_bins self . _device : str | torch . device | int = \"cpu\" shape = ( num_bins ,) self . _pos_hist = MetricAccumulator ( torch . zeros ( shape , dtype = torch . float32 )) self . _neg_hist = MetricAccumulator ( torch . zeros ( shape , dtype = torch . float32 )) def update ( self , probs : torch . Tensor , labels : torch . Tensor ): \"\"\"Updates the metric statistics with a new batch of predictions. Args: probs: Predicted probabilities in range [0, 1]. labels: Ground truth binary labels. Raises: ValueError: If `probs` or `labels` have different number of elements. \"\"\" probs = probs . reshape ( - 1 ) labels = labels . reshape ( - 1 ) if probs . numel () != labels . numel (): raise ValueError ( \"Predictions and labels should have the same number of elements\" ) bins = ( probs * self . _num_bins ) . long () . clamp ( 0 , self . _num_bins - 1 ) pos_batch = torch . zeros ( self . _num_bins , device = self . _device , dtype = torch . float32 ) neg_batch = torch . zeros ( self . _num_bins , device = self . _device , dtype = torch . float32 ) pos_batch . index_add_ ( 0 , bins , labels . float ()) neg_batch . index_add_ ( 0 , bins , ( 1 - labels ) . float ()) self . _pos_hist . update ( pos_batch ) self . _neg_hist . update ( neg_batch ) def sync ( self , dist_context : DistributedContext ): self . _pos_hist . sync () self . _neg_hist . sync () def compute ( self ) -> torch . Tensor : pos_hist = self . _pos_hist . value neg_hist = self . _neg_hist . value return _compute_histogram_auroc ( pos_hist , neg_hist ) def reset ( self ): self . _pos_hist . reset () self . _neg_hist . reset () def to ( self , device : str | torch . device | int ): self . _device = device self . _pos_hist . to ( device ) self . _neg_hist . to ( device ) def state_dict ( self ) -> dict [ str , Any ]: return { \"pos_hist\" : self . _pos_hist . state_dict (), \"neg_hist\" : self . _neg_hist . state_dict ()} def load_state_dict ( self , state_dict : dict [ str , Any ]): self . _pos_hist . load_state_dict ( state_dict [ \"pos_hist\" ]) self . _neg_hist . load_state_dict ( state_dict [ \"neg_hist\" ]) __init__ ( num_bins = 10000 ) Constructs the BinaryAUROCMetric object. Parameters: Name Type Description Default num_bins int Number of bins for histogram approximation. This parameter controls the trade-off between memory consumption and approximation accuracy. 10000 Source code in d9d/metric/impl/auroc.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def __init__ ( self , num_bins : int = 10000 ): \"\"\"Constructs the BinaryAUROCMetric object. Args: num_bins: Number of bins for histogram approximation. This parameter controls the trade-off between memory consumption and approximation accuracy. \"\"\" self . _num_bins = num_bins self . _device : str | torch . device | int = \"cpu\" shape = ( num_bins ,) self . _pos_hist = MetricAccumulator ( torch . zeros ( shape , dtype = torch . float32 )) self . _neg_hist = MetricAccumulator ( torch . zeros ( shape , dtype = torch . float32 )) update ( probs , labels ) Updates the metric statistics with a new batch of predictions. Parameters: Name Type Description Default probs Tensor Predicted probabilities in range [0, 1]. required labels Tensor Ground truth binary labels. required Raises: Type Description ValueError If probs or labels have different number of elements. Source code in d9d/metric/impl/auroc.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def update ( self , probs : torch . Tensor , labels : torch . Tensor ): \"\"\"Updates the metric statistics with a new batch of predictions. Args: probs: Predicted probabilities in range [0, 1]. labels: Ground truth binary labels. Raises: ValueError: If `probs` or `labels` have different number of elements. \"\"\" probs = probs . reshape ( - 1 ) labels = labels . reshape ( - 1 ) if probs . numel () != labels . numel (): raise ValueError ( \"Predictions and labels should have the same number of elements\" ) bins = ( probs * self . _num_bins ) . long () . clamp ( 0 , self . _num_bins - 1 ) pos_batch = torch . zeros ( self . _num_bins , device = self . _device , dtype = torch . float32 ) neg_batch = torch . zeros ( self . _num_bins , device = self . _device , dtype = torch . float32 ) pos_batch . index_add_ ( 0 , bins , labels . float ()) neg_batch . index_add_ ( 0 , bins , ( 1 - labels ) . float ()) self . _pos_hist . update ( pos_batch ) self . _neg_hist . update ( neg_batch ) SumMetric Bases: Metric [ Tensor ] Computes the sum of input values. Source code in d9d/metric/impl/sum.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 class SumMetric ( Metric [ torch . Tensor ]): \"\"\" Computes the sum of input values. \"\"\" def __init__ ( self ): \"\"\"Constructs a SumMetric object.\"\"\" self . _accumulator = MetricAccumulator ( torch . scalar_tensor ( 0 , dtype = torch . float32 )) def update ( self , value : torch . Tensor ): \"\"\" Updates the metric state by adding the sum of the input value. Args: value: A tensor whose sum will be added to the accumulator. \"\"\" self . _accumulator . update ( value . sum ()) def sync ( self , dist_context : DistributedContext ): self . _accumulator . sync () def compute ( self ) -> torch . Tensor : return self . _accumulator . value def reset ( self ): self . _accumulator . reset () def to ( self , device : str | torch . device | int ): self . _accumulator . to ( device ) def state_dict ( self ) -> dict [ str , Any ]: return { \"accumulator\" : self . _accumulator . state_dict ()} def load_state_dict ( self , state_dict : dict [ str , Any ]) -> None : self . _accumulator . load_state_dict ( state_dict [ \"accumulator\" ]) __init__ () Constructs a SumMetric object. Source code in d9d/metric/impl/sum.py 15 16 17 def __init__ ( self ): \"\"\"Constructs a SumMetric object.\"\"\" self . _accumulator = MetricAccumulator ( torch . scalar_tensor ( 0 , dtype = torch . float32 )) update ( value ) Updates the metric state by adding the sum of the input value. Parameters: Name Type Description Default value Tensor A tensor whose sum will be added to the accumulator. required Source code in d9d/metric/impl/sum.py 19 20 21 22 23 24 25 26 def update ( self , value : torch . Tensor ): \"\"\" Updates the metric state by adding the sum of the input value. Args: value: A tensor whose sum will be added to the accumulator. \"\"\" self . _accumulator . update ( value . sum ()) WeightedMeanMetric Bases: Metric [ Tensor ] Computes the weighted mean of values. Tracks the sum of weighted values and the sum of weights. Source code in d9d/metric/impl/mean.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 class WeightedMeanMetric ( Metric [ torch . Tensor ]): \"\"\" Computes the weighted mean of values. Tracks the sum of weighted values and the sum of weights. \"\"\" def __init__ ( self ): \"\"\"Constructs a WeightedMeanMetric object.\"\"\" self . _value = MetricAccumulator ( torch . scalar_tensor ( 0 , dtype = torch . float32 )) self . _weight = MetricAccumulator ( torch . scalar_tensor ( 0 , dtype = torch . float32 )) def update ( self , values : torch . Tensor , weights : torch . Tensor ): self . _value . update (( values * weights ) . sum ()) self . _weight . update ( weights . sum ()) def sync ( self , dist_context : DistributedContext ): self . _value . sync () self . _weight . sync () def compute ( self ) -> torch . Tensor : return self . _value . value / self . _weight . value def reset ( self ): self . _value . reset () self . _weight . reset () def to ( self , device : str | torch . device | int ): self . _value . to ( device ) self . _weight . to ( device ) @property def accumulated_weight ( self ) -> torch . Tensor : \"\"\" Returns the total weight accumulated so far. Returns: Scalar tensor with total weight. \"\"\" return self . _weight . value def state_dict ( self ) -> dict [ str , Any ]: return { \"value\" : self . _value . state_dict (), \"weight\" : self . _weight . state_dict ()} def load_state_dict ( self , state_dict : dict [ str , Any ]) -> None : self . _value . load_state_dict ( state_dict [ \"value\" ]) self . _weight . load_state_dict ( state_dict [ \"weight\" ]) accumulated_weight property Returns the total weight accumulated so far. Returns: Type Description Tensor Scalar tensor with total weight. __init__ () Constructs a WeightedMeanMetric object. Source code in d9d/metric/impl/mean.py 17 18 19 20 21 def __init__ ( self ): \"\"\"Constructs a WeightedMeanMetric object.\"\"\" self . _value = MetricAccumulator ( torch . scalar_tensor ( 0 , dtype = torch . float32 )) self . _weight = MetricAccumulator ( torch . scalar_tensor ( 0 , dtype = torch . float32 ))","title":"Implemented Metrics"},{"location":"metric/implemented/#d9d.metric.impl","text":"","title":"impl"},{"location":"metric/implemented/#d9d.metric.impl.BinaryAUROCMetric","text":"Bases: Metric [ Tensor ] Computes approximated AUROC for binary classification using histograms. Standard AUROC computation requires storing the entire history of predictions to sort and rank them. This implementation solves the memory constraint by discretizing predictions into histograms. This method employs a frequency-based sketching approach. It relies on the observation that the AUROC can be approximated by computing the area shared or separated by the probability density functions of the positive and negative classes. We maintain two separate histograms for positive and negative samples and apply the trapezoidal rule to estimate the area. References Albakour et al., \"Fast and memory efficient AUC-ROC approximation for Stream Learning\", 2021. https://www.researchgate.net/publication/353020448_Fast_and_memory_efficient_AUC-ROC_approximation_for_Stream_Learning Source code in d9d/metric/impl/auroc.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 class BinaryAUROCMetric ( Metric [ torch . Tensor ]): \"\"\"Computes approximated AUROC for binary classification using histograms. Standard AUROC computation requires storing the entire history of predictions to sort and rank them. This implementation solves the memory constraint by discretizing predictions into histograms. This method employs a frequency-based sketching approach. It relies on the observation that the AUROC can be approximated by computing the area shared or separated by the probability density functions of the positive and negative classes. We maintain two separate histograms for positive and negative samples and apply the trapezoidal rule to estimate the area. References: Albakour et al., \"Fast and memory efficient AUC-ROC approximation for Stream Learning\", 2021. https://www.researchgate.net/publication/353020448_Fast_and_memory_efficient_AUC-ROC_approximation_for_Stream_Learning \"\"\" def __init__ ( self , num_bins : int = 10000 ): \"\"\"Constructs the BinaryAUROCMetric object. Args: num_bins: Number of bins for histogram approximation. This parameter controls the trade-off between memory consumption and approximation accuracy. \"\"\" self . _num_bins = num_bins self . _device : str | torch . device | int = \"cpu\" shape = ( num_bins ,) self . _pos_hist = MetricAccumulator ( torch . zeros ( shape , dtype = torch . float32 )) self . _neg_hist = MetricAccumulator ( torch . zeros ( shape , dtype = torch . float32 )) def update ( self , probs : torch . Tensor , labels : torch . Tensor ): \"\"\"Updates the metric statistics with a new batch of predictions. Args: probs: Predicted probabilities in range [0, 1]. labels: Ground truth binary labels. Raises: ValueError: If `probs` or `labels` have different number of elements. \"\"\" probs = probs . reshape ( - 1 ) labels = labels . reshape ( - 1 ) if probs . numel () != labels . numel (): raise ValueError ( \"Predictions and labels should have the same number of elements\" ) bins = ( probs * self . _num_bins ) . long () . clamp ( 0 , self . _num_bins - 1 ) pos_batch = torch . zeros ( self . _num_bins , device = self . _device , dtype = torch . float32 ) neg_batch = torch . zeros ( self . _num_bins , device = self . _device , dtype = torch . float32 ) pos_batch . index_add_ ( 0 , bins , labels . float ()) neg_batch . index_add_ ( 0 , bins , ( 1 - labels ) . float ()) self . _pos_hist . update ( pos_batch ) self . _neg_hist . update ( neg_batch ) def sync ( self , dist_context : DistributedContext ): self . _pos_hist . sync () self . _neg_hist . sync () def compute ( self ) -> torch . Tensor : pos_hist = self . _pos_hist . value neg_hist = self . _neg_hist . value return _compute_histogram_auroc ( pos_hist , neg_hist ) def reset ( self ): self . _pos_hist . reset () self . _neg_hist . reset () def to ( self , device : str | torch . device | int ): self . _device = device self . _pos_hist . to ( device ) self . _neg_hist . to ( device ) def state_dict ( self ) -> dict [ str , Any ]: return { \"pos_hist\" : self . _pos_hist . state_dict (), \"neg_hist\" : self . _neg_hist . state_dict ()} def load_state_dict ( self , state_dict : dict [ str , Any ]): self . _pos_hist . load_state_dict ( state_dict [ \"pos_hist\" ]) self . _neg_hist . load_state_dict ( state_dict [ \"neg_hist\" ])","title":"BinaryAUROCMetric"},{"location":"metric/implemented/#d9d.metric.impl.BinaryAUROCMetric.__init__","text":"Constructs the BinaryAUROCMetric object. Parameters: Name Type Description Default num_bins int Number of bins for histogram approximation. This parameter controls the trade-off between memory consumption and approximation accuracy. 10000 Source code in d9d/metric/impl/auroc.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def __init__ ( self , num_bins : int = 10000 ): \"\"\"Constructs the BinaryAUROCMetric object. Args: num_bins: Number of bins for histogram approximation. This parameter controls the trade-off between memory consumption and approximation accuracy. \"\"\" self . _num_bins = num_bins self . _device : str | torch . device | int = \"cpu\" shape = ( num_bins ,) self . _pos_hist = MetricAccumulator ( torch . zeros ( shape , dtype = torch . float32 )) self . _neg_hist = MetricAccumulator ( torch . zeros ( shape , dtype = torch . float32 ))","title":"__init__"},{"location":"metric/implemented/#d9d.metric.impl.BinaryAUROCMetric.update","text":"Updates the metric statistics with a new batch of predictions. Parameters: Name Type Description Default probs Tensor Predicted probabilities in range [0, 1]. required labels Tensor Ground truth binary labels. required Raises: Type Description ValueError If probs or labels have different number of elements. Source code in d9d/metric/impl/auroc.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def update ( self , probs : torch . Tensor , labels : torch . Tensor ): \"\"\"Updates the metric statistics with a new batch of predictions. Args: probs: Predicted probabilities in range [0, 1]. labels: Ground truth binary labels. Raises: ValueError: If `probs` or `labels` have different number of elements. \"\"\" probs = probs . reshape ( - 1 ) labels = labels . reshape ( - 1 ) if probs . numel () != labels . numel (): raise ValueError ( \"Predictions and labels should have the same number of elements\" ) bins = ( probs * self . _num_bins ) . long () . clamp ( 0 , self . _num_bins - 1 ) pos_batch = torch . zeros ( self . _num_bins , device = self . _device , dtype = torch . float32 ) neg_batch = torch . zeros ( self . _num_bins , device = self . _device , dtype = torch . float32 ) pos_batch . index_add_ ( 0 , bins , labels . float ()) neg_batch . index_add_ ( 0 , bins , ( 1 - labels ) . float ()) self . _pos_hist . update ( pos_batch ) self . _neg_hist . update ( neg_batch )","title":"update"},{"location":"metric/implemented/#d9d.metric.impl.SumMetric","text":"Bases: Metric [ Tensor ] Computes the sum of input values. Source code in d9d/metric/impl/sum.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 class SumMetric ( Metric [ torch . Tensor ]): \"\"\" Computes the sum of input values. \"\"\" def __init__ ( self ): \"\"\"Constructs a SumMetric object.\"\"\" self . _accumulator = MetricAccumulator ( torch . scalar_tensor ( 0 , dtype = torch . float32 )) def update ( self , value : torch . Tensor ): \"\"\" Updates the metric state by adding the sum of the input value. Args: value: A tensor whose sum will be added to the accumulator. \"\"\" self . _accumulator . update ( value . sum ()) def sync ( self , dist_context : DistributedContext ): self . _accumulator . sync () def compute ( self ) -> torch . Tensor : return self . _accumulator . value def reset ( self ): self . _accumulator . reset () def to ( self , device : str | torch . device | int ): self . _accumulator . to ( device ) def state_dict ( self ) -> dict [ str , Any ]: return { \"accumulator\" : self . _accumulator . state_dict ()} def load_state_dict ( self , state_dict : dict [ str , Any ]) -> None : self . _accumulator . load_state_dict ( state_dict [ \"accumulator\" ])","title":"SumMetric"},{"location":"metric/implemented/#d9d.metric.impl.SumMetric.__init__","text":"Constructs a SumMetric object. Source code in d9d/metric/impl/sum.py 15 16 17 def __init__ ( self ): \"\"\"Constructs a SumMetric object.\"\"\" self . _accumulator = MetricAccumulator ( torch . scalar_tensor ( 0 , dtype = torch . float32 ))","title":"__init__"},{"location":"metric/implemented/#d9d.metric.impl.SumMetric.update","text":"Updates the metric state by adding the sum of the input value. Parameters: Name Type Description Default value Tensor A tensor whose sum will be added to the accumulator. required Source code in d9d/metric/impl/sum.py 19 20 21 22 23 24 25 26 def update ( self , value : torch . Tensor ): \"\"\" Updates the metric state by adding the sum of the input value. Args: value: A tensor whose sum will be added to the accumulator. \"\"\" self . _accumulator . update ( value . sum ())","title":"update"},{"location":"metric/implemented/#d9d.metric.impl.WeightedMeanMetric","text":"Bases: Metric [ Tensor ] Computes the weighted mean of values. Tracks the sum of weighted values and the sum of weights. Source code in d9d/metric/impl/mean.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 class WeightedMeanMetric ( Metric [ torch . Tensor ]): \"\"\" Computes the weighted mean of values. Tracks the sum of weighted values and the sum of weights. \"\"\" def __init__ ( self ): \"\"\"Constructs a WeightedMeanMetric object.\"\"\" self . _value = MetricAccumulator ( torch . scalar_tensor ( 0 , dtype = torch . float32 )) self . _weight = MetricAccumulator ( torch . scalar_tensor ( 0 , dtype = torch . float32 )) def update ( self , values : torch . Tensor , weights : torch . Tensor ): self . _value . update (( values * weights ) . sum ()) self . _weight . update ( weights . sum ()) def sync ( self , dist_context : DistributedContext ): self . _value . sync () self . _weight . sync () def compute ( self ) -> torch . Tensor : return self . _value . value / self . _weight . value def reset ( self ): self . _value . reset () self . _weight . reset () def to ( self , device : str | torch . device | int ): self . _value . to ( device ) self . _weight . to ( device ) @property def accumulated_weight ( self ) -> torch . Tensor : \"\"\" Returns the total weight accumulated so far. Returns: Scalar tensor with total weight. \"\"\" return self . _weight . value def state_dict ( self ) -> dict [ str , Any ]: return { \"value\" : self . _value . state_dict (), \"weight\" : self . _weight . state_dict ()} def load_state_dict ( self , state_dict : dict [ str , Any ]) -> None : self . _value . load_state_dict ( state_dict [ \"value\" ]) self . _weight . load_state_dict ( state_dict [ \"weight\" ])","title":"WeightedMeanMetric"},{"location":"metric/implemented/#d9d.metric.impl.WeightedMeanMetric.accumulated_weight","text":"Returns the total weight accumulated so far. Returns: Type Description Tensor Scalar tensor with total weight.","title":"accumulated_weight"},{"location":"metric/implemented/#d9d.metric.impl.WeightedMeanMetric.__init__","text":"Constructs a WeightedMeanMetric object. Source code in d9d/metric/impl/mean.py 17 18 19 20 21 def __init__ ( self ): \"\"\"Constructs a WeightedMeanMetric object.\"\"\" self . _value = MetricAccumulator ( torch . scalar_tensor ( 0 , dtype = torch . float32 )) self . _weight = MetricAccumulator ( torch . scalar_tensor ( 0 , dtype = torch . float32 ))","title":"__init__"},{"location":"model_states/io/","text":"About The d9d.model_state.io package handles the reading and writing of model checkpoints. We use checkpoint format that is compatible with HuggingFace format. This format is characterized by using sharded model-00001-of-XXXXX.safetensors .safetensors files for storing parameter tensors along with model.safetensors.index.json file containing the metadata. It is tightly integrated with the d9d.model_state.mapper framework to allow for Streamed Transformation - converting model architectures on-the-fly during IO without loading the entire model into memory. Core Concepts Why Support Transformations In d9d all the model state input/output logic is natively integrated with mapping and transforming model states. Such a combined system acts as a powerful abstraction layer that decouples the checkpoint architecture (how weights are stored on disk) from the model architecture (how weights are used in PyTorch code). This integration is critical for: Native HuggingFace Compatibility : You can use highly optimized, custom model implementations (e.g., using a single packed qkv_proj tensor) while reading directly from standard community checkpoints (which typically store q_proj , k_proj , and v_proj separately). The mapper handles the reshaping and stacking on-the-fly during the read stream. This eliminates the need for maintaining separate \"conversion scripts\" or storing duplicate, converted copies of large models. Runtime Structure Adapting (e.g., LoRA) : When injecting adapters like LoRA, the runtime model structure changes - often wrapping original layers. For example, a standard some_linear.weight on disk might need to be loaded into some_linear.orig.weight in memory. Instead of loading the full state dict and manually patching keys (which spikes memory), the mapper reroutes these keys without the need of materializing the model weights fully . How Loading Works Standard model loading involves loading a huge dictionary into CPU RAM, filtering and processing it, and moving the results to GPU. This approach is ineffective since it requires a lot of CPU-GPU transfers, consumes high amount of memory and involves duplicate work across different pipeline parallel workers. d9d proposes a different approach: Streaming & Eviction : Tensors are loaded in streamed manner and therefore kept in memory only when needed. Once a mapper group (e.g., \"stack Q, K, V\") is executed, the source tensors are immediately evicted from memory. Topology-Aware Loading : Instead of blindly loading all the files, the reader inspects the ModelStateMapper . It calculates exactly which files contain the required inputs. How Saving Works Standard model saving often requires gathering all parameters to a single rank (causing OOM) or manual orchestration of file names and indices across hundreds of GPUs. d9d's approach automates the checkpoint exporting lifecycle for large-scale distributed setups: Streaming & Eviction : Tensors are saved in streamed manner and therefore kept in memory only when needed. Once a mapper group (e.g., \"stack Q, K, V\") is executed, the source tensors are immediately evicted from memory. Target tensors are kept in memory only before they are flushed to respective .safetensors files. Distributed Awareness : In addition to providing local model exporting, we provide distributed-aware export functions. The writer natively understands distributed topologies (via ProcessGroup or DeviceMesh ). In Pipeline Parallel scenarios, it identifies which rank holds the specific stage master copy, ensuring that parameters are written exactly once without race conditions or duplication. Usage Examples These examples provide information primarily how to load and write model states in a pass-through way. If you want to see examples of complex model state mapping, please refer to ModelStateMapper documentation. Raw I/O - Streamed Loading This example shows how to load a model without spiking memory usage. from pathlib import Path import torch from d9d.model_state.io.reader import read_model_state from d9d.model_state.mapper.leaf import ModelStateMapperStackTensors from d9d.model_state.mapper.adapters import identity_mapper_from_module # Define the mapper (Topology) # in this example we will load all the parameters the model contains mapper = identity_mapper_from_module ( model ) # Start the stream # 'src_dir' must contain safetensors files and model.safetensors.index.json loader_stream = read_model_state ( src_dir = Path ( \"./checkpoint\" ), mapper = mapper , device = \"cpu\" # or \"cuda:0\" ) # Iterate through the transformed results state_dict = {} for name , tensor in loader_stream : print ( f \"Loaded and transformed: { name } -> { tensor . shape } \" ) state_dict [ name ] = tensor Raw I/O - Streamed Saving Saving a model locally, automatically splitting into 1 GB shards. from pathlib import Path from d9d.model_state.io.writer import write_model_state_local from d9d.model_state.mapper.adapters import identity_mapper_from_module # 1. Create a generator for your model states state_generator = model . named_parameters () # 2. Define a mapper (Identity if no transformation is needed during save) mapper = identity_mapper_from_module ( model ) # 3. Write # This handles sharding and metadata file creation automatically write_model_state_local ( dest_dir = Path ( \"./output_checkpoint\" ), mapper = mapper , state_generator = state_generator , shard_size_gb = 1.0 # Split files if they exceed 1GB ) Raw I/O - Distributed Load-Transform-Save One of the most powerful features of d9d is the ability to perform Offline Checkpoint Conversion using a distributed cluster. If you have a massive checkpoint in Format A (e.g., HuggingFace) and need to convert it to Format B (e.g., a custom Training format with packed QKV), you don't need a single machine with 1TB RAM. instead, you can spin up 8 GPUs, have each GPU process 1/8th of the keys in parallel, and write a new sharded checkpoint. import torch.distributed as dist from pathlib import Path from d9d.model_state.io import read_model_state , write_model_state_distributed from d9d.model_state.mapper.compose import ModelStateMapperShard from d9d.model_state.mapper.adapters import identity_mapper_from_mapper_outputs # 1. Initialize distributed environment dist . init_process_group ( \"nccl\" ) rank = dist . get_rank () world_size = dist . get_world_size () # 2. Define the global transformation logic # This describes how the ENTIRE model should be converted. # e.g., \"Stack Q,K,V\", \"Rename MLP\", \"Load everying else as-is\" mapper = build_my_fancy_custom_mapper () # 3. Shard the workload # We wrap the mapper to restrict execution. # Rank 0 will only process the first 1/N dependency groups, Rank 1 the next, etc. # This ensures that no two ranks load/process/save the same tensors. local_work_mapper = ModelStateMapperShard ( sub_mapper = mapper , total_shards = world_size , current_shard = rank ) # 4. Define saving topology # The 'read_model_state' generator below will yield tensors that have # ALREADY been transformed to their target names/shapes. # The writer just needs to accept these new keys and save them. # We generate this identity mapper automatically from the output signature. writer_mapper = identity_mapper_from_mapper_outputs ( local_work_mapper ) # 5. Execute the pipeline # - Reader: Loads specific source files, transforms, yields new tensors. # - Writer: Receives new tensors, saves to '*.safetensors' files with temporary names # - Finalizer: Rank 0 creates 'model.safetensors.index.json' covering all ranks and renames .safetensors files to their final names. write_model_state_distributed ( dest_dir = Path ( \"./converted_checkpoint\" ), mapper = writer_mapper , state_generator = read_model_state ( src_dir = Path ( \"./original_checkpoint\" ), mapper = local_work_mapper , # Defines what to load and transformations device = \"cuda\" , show_progress = False # Disable read bars to avoid stderr spam ), process_group = dist . distributed_c10d . _get_default_group (), shard_size_gb = 4.0 , show_progress = True # Master rank will show global save progress ) PyTorch Module I/O - Streamed Loading Loading a checkpoint where disk keys exactly match model keys. identity_mapper_from_module ensures only existing model parameters are loaded. from pathlib import Path from d9d.model_state.io import load_model_state from d9d.model_state.mapper.adapters import identity_mapper_from_module # 1. Setup Model (e.g., empty or on meta device) model = ... # 2. Create Identity Topology # This tells d9d: \"Load every key that exists in 'model' as is.\" mapper = identity_mapper_from_module ( model ) # 3. Stream & Inject load_model_state ( src_dir = Path ( \"./checkpoints/v1\" ), mapper = mapper , device = \"cuda\" , model = model ) PyTorch Module I/O - Streaming Saving (DeviceMesh) Saves a model in a complex ND Parallel environment using PyTorch DeviceMesh . This features: DTensor Gathering : Automatically gathers DTensor shards from the mesh into full tensors before writing. Concurrency Within PP Rank : In a Data/Tensor/... parallel setup, multiple GPUs hold replicated or sharded copies of the same parameters. This function uses the DeviceMesh to ensure that only the \"canonical\" PP replica (DP Rank 0, TP Rank 0, ...) writes to disk, preventing write conflicts. Concurrency Across PP Ranks : Each PP rank writes the data into its own files. After all the PP ranks finish writing, PP Rank 0 merges the metadata from different PP ranks into a single global checkpoint index file. from pathlib import Path from torch.distributed.device_mesh import init_device_mesh from d9d.model_state.io import save_model_state_pipeline_parallel from d9d.model_state.mapper.compose import ModelStateMapperParallel from d9d.model_state.mapper.adapters import identity_mapper_from_module # 1. Setup 3D Mesh # pp=2 (Pipeline), dp=2 (Data), tp=2 (Tensor) mesh = init_device_mesh ( \"cuda\" , ( 2 , 2 , 2 ), mesh_dim_names = ( \"pp\" , \"dp\" , \"tp\" )) # 2. Define Model Stages # In this example, each PP rank manages two distinct parts of the model. my_stages = [ TransformerStage ( ... ), TransformerStage ( ... )] # 3. Create Topology # Since this rank manages multiple modules, we create a Parallel mapper # to combine the requirements of all stages. mapper = ModelStateMapperParallel ([ identity_mapper_from_module ( stage ) for stage in my_stages ]) # 4. Save # The system inspects the mesh. It identifies if the current rank is # the \"Master\" for the provided stages (i.e., dp_rank=0, tp_rank=0). # If so, it gathers DTensors and writes. If not, it skips writing # but participates in the collective gather. save_model_state_pipeline_parallel ( dest_dir = Path ( \"./checkpoint\" ), mapper = mapper , device_mesh = mesh , pipeline_dim_name = \"pp\" , models = my_stages , shard_size_gb = 4.0 ) d9d.model_state.io load_model_state ( src_dir , mapper , device , model , show_progress = True ) High-level utility to stream a checkpoint directly into a PyTorch module. This function orchestrates the full loading lifecycle: Topology Mapping: Uses mapper to rename/stack/reshape on-disk states to model states. Automatic Distribution: If the model contains DTensor s, the loaded local tensors are automatically sharded/replicated to match the model's placement schema. Streaming Read & Inject: After loading and transforming a model state, it will be injected into model using load_state_dict(...) . NOTICE: Only states specified in mapper will be loaded! You can use d9d.model_state.mapper.adapters.identity_mapper_from_module(module) to create a mapper that will load every model state without changing it. Parameters: Name Type Description Default src_dir Path Directory containing .safetensors and index files. required mapper ModelStateMapper The topology defining how mapping from disk keys to model keys works. required device str The device to load tensors onto (usually \"cpu\" or \"cuda\"). required model Module The model instance to load weights into. required show_progress bool Whether to display the loading progress bar. True Source code in d9d/model_state/io/module_reader.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def load_model_state ( src_dir : Path , mapper : ModelStateMapper , device : str , model : nn . Module , show_progress : bool = True , ): \"\"\" High-level utility to stream a checkpoint directly into a PyTorch module. This function orchestrates the full loading lifecycle: 1. Topology Mapping: Uses `mapper` to rename/stack/reshape on-disk states to model states. 2. Automatic Distribution: If the `model` contains `DTensor`s, the loaded local tensors are automatically sharded/replicated to match the model's placement schema. 3. Streaming Read & Inject: After loading and transforming a model state, it will be injected into `model` using `load_state_dict(...)`. NOTICE: Only states specified in `mapper` will be loaded! You can use `d9d.model_state.mapper.adapters.identity_mapper_from_module(module)` to create a mapper that will load every model state without changing it. Args: src_dir: Directory containing .safetensors and index files. mapper: The topology defining how mapping from disk keys to model keys works. device: The device to load tensors onto (usually \"cpu\" or \"cuda\"). model: The model instance to load weights into. show_progress: Whether to display the loading progress bar. \"\"\" for state_name , state_value in read_model_state ( src_dir = src_dir , mapper = _augment_mapper_for_injection ( model , mapper ), device = device , show_progress = show_progress ): model . load_state_dict ({ state_name : state_value }, strict = False ) read_model_state ( src_dir , mapper , device , show_progress = True ) Reads a model checkpoint from disk, transforming it on-the-fly according to the state mapper. This function uses a streaming approach. It analyzes the mapper to determine which files need to be loaded. Tensors are loaded into memory only when needed and evicted immediately after the mapper processes them. Parameters: Name Type Description Default src_dir Path The directory containing .safetensors files and model.safetensors.index.json file. required mapper ModelStateMapper The transformation graph defining how to map on-disk keys to output keys. required device str The device to load tensors onto (e.g., \"cpu\", \"cuda:0\"). required show_progress bool Whether to display a progress bar. True Yields: Type Description Iterable [ tuple [ str , Tensor ]] A tuple containing the transformed parameter name and its tensor value. Source code in d9d/model_state/io/reader.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def read_model_state ( src_dir : Path , mapper : ModelStateMapper , device : str , show_progress : bool = True ) -> Iterable [ tuple [ str , torch . Tensor ]]: \"\"\" Reads a model checkpoint from disk, transforming it on-the-fly according to the state mapper. This function uses a streaming approach. It analyzes the mapper to determine which files need to be loaded. Tensors are loaded into memory only when needed and evicted immediately after the mapper processes them. Args: src_dir: The directory containing .safetensors files and `model.safetensors.index.json` file. mapper: The transformation graph defining how to map on-disk keys to output keys. device: The device to load tensors onto (e.g., \"cpu\", \"cuda:0\"). show_progress: Whether to display a progress bar. Yields: A tuple containing the transformed parameter name and its tensor value. \"\"\" yield from _StateLoadingFlow ( src_dir = src_dir , device = device , mapper = mapper , show_progress = show_progress ) . load () save_model_state ( dest_dir , mapper , model , shard_size_gb = 4.0 , show_progress = True ) High-level utility to save a PyTorch model to disk on a single process. NOTICE: Only states specified in mapper will be saved! You can use d9d.model_state.mapper.adapters.identity_mapper_from_module(module) to create a mapper that will save every model state without changing it. Parameters: Name Type Description Default dest_dir Path The directory to save .safetensors shards and index. required mapper ModelStateMapper Topology defining how model keys map to disk keys. required model Module The PyTorch module to save. required shard_size_gb float Max size per shard file in Gigabytes. 4.0 show_progress bool Whether to display a progress bar. True Source code in d9d/model_state/io/module_writer.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def save_model_state ( dest_dir : Path , mapper : ModelStateMapper , model : nn . Module , shard_size_gb : float = 4.0 , show_progress : bool = True ): \"\"\" High-level utility to save a PyTorch model to disk on a **single** process. NOTICE: Only states specified in `mapper` will be saved! You can use `d9d.model_state.mapper.adapters.identity_mapper_from_module(module)` to create a mapper that will save every model state without changing it. Args: dest_dir: The directory to save .safetensors shards and index. mapper: Topology defining how model keys map to disk keys. model: The PyTorch module to save. shard_size_gb: Max size per shard file in Gigabytes. show_progress: Whether to display a progress bar. \"\"\" write_model_state_local ( dest_dir = dest_dir , mapper = _augment_mapper_for_extraction ([ model ], mapper ), state_generator = _state_generator ([ model ]), shard_size_gb = shard_size_gb , show_progress = show_progress , ) save_model_state_pipeline_parallel ( dest_dir , mapper , device_mesh , pipeline_dim_name , models , shard_size_gb = 4.0 , show_progress = True ) High-level utility to save a model in a Distributed Pipeline Parallel environment to disk. Features: Auto-Gather : Converts DTensor parameters to full tensors before saving. Distribution Awareness : Uses the device_mesh to ensure that for a given pipeline stage, only the master rank writes the checkpoint, preventing Write-After-Write conflicts. Index Merging : Aggregates metadata from all independent pipeline stages into one global index file. NOTICE: Only states specified in mapper will be saved! You can use d9d.model_state.mapper.adapters.identity_mapper_from_module(module) to create a mapper that will save every model state without changing it. Parameters: Name Type Description Default dest_dir Path directory to save .safetensors shards and index file. required mapper ModelStateMapper Topology defining how model keys map to disk keys. required device_mesh DeviceMesh The cluster topology mesh. required pipeline_dim_name str The specific dimension name in the mesh used for pipelining. required models list [ Module ] A list of modules (pipeline stages) processed by this PP rank. required shard_size_gb float Max size per shard file in Gigabytes. 4.0 show_progress bool Whether to display a progress bar. True Source code in d9d/model_state/io/module_writer.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def save_model_state_pipeline_parallel ( dest_dir : Path , mapper : ModelStateMapper , device_mesh : DeviceMesh , pipeline_dim_name : str , models : list [ nn . Module ], shard_size_gb : float = 4.0 , show_progress : bool = True , ): \"\"\" High-level utility to save a model in a Distributed Pipeline Parallel environment to disk. Features: 1. **Auto-Gather**: Converts `DTensor` parameters to full tensors before saving. 2. **Distribution Awareness**: Uses the `device_mesh` to ensure that for a given pipeline stage, only the master rank writes the checkpoint, preventing Write-After-Write conflicts. 3. **Index Merging**: Aggregates metadata from all independent pipeline stages into one global index file. NOTICE: Only states specified in `mapper` will be saved! You can use `d9d.model_state.mapper.adapters.identity_mapper_from_module(module)` to create a mapper that will save every model state without changing it. Args: dest_dir: directory to save .safetensors shards and index file. mapper: Topology defining how model keys map to disk keys. device_mesh: The cluster topology mesh. pipeline_dim_name: The specific dimension name in the mesh used for pipelining. models: A list of modules (pipeline stages) processed by this PP rank. shard_size_gb: Max size per shard file in Gigabytes. show_progress: Whether to display a progress bar. \"\"\" write_model_state_pipeline_parallel ( dest_dir = dest_dir , mapper = _augment_mapper_for_extraction ( models , mapper ), state_generator = _state_generator ( models ), device_mesh = device_mesh , pipeline_dim_name = pipeline_dim_name , shard_size_gb = shard_size_gb , show_progress = show_progress , ) write_model_state_distributed ( dest_dir , mapper , state_generator , process_group , shard_size_gb = 4.0 , show_progress = True ) Saves model states in a distributed setup (multiple processes). This function uses a streaming approach. It analyzes the mapper to determine which files need to be saved. Tensors are loaded into memory only when needed and evicted immediately after the mapper processes them. Each rank writes its own shard. Rank 0 gathers indices and finalizes the checkpoint. Parameters: Name Type Description Default dest_dir Path Destination directory. required mapper ModelStateMapper Mapping to apply to states before saving. required state_generator Iterable [ tuple [ str , Tensor ]] Stream of (name, tensor) pairs from the model. required process_group ProcessGroup The distributed process group. required shard_size_gb float Maximum shard size in GB. 4.0 show_progress bool Whether to show the progress bar. True Source code in d9d/model_state/io/writer.py 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 def write_model_state_distributed ( dest_dir : Path , mapper : ModelStateMapper , state_generator : Iterable [ tuple [ str , torch . Tensor ]], process_group : ProcessGroup , shard_size_gb : float = 4.0 , show_progress : bool = True , ): \"\"\" Saves model states in a distributed setup (multiple processes). This function uses a streaming approach. It analyzes the mapper to determine which files need to be saved. Tensors are loaded into memory only when needed and evicted immediately after the mapper processes them. Each rank writes its own shard. Rank 0 gathers indices and finalizes the checkpoint. Args: dest_dir: Destination directory. mapper: Mapping to apply to states before saving. state_generator: Stream of (name, tensor) pairs from the model. process_group: The distributed process group. shard_size_gb: Maximum shard size in GB. show_progress: Whether to show the progress bar. \"\"\" current_idx = _StateWritingFlowLocal ( dest_dir = dest_dir , mapper = mapper , shard_size_gb = shard_size_gb , show_progress = show_progress , sharding_rank = process_group . rank (), is_current_process_rank_master = True , ) . write ( state_generator = state_generator ) gather_idx = all_gather_object ( current_idx , process_group ) gather_idx_filter = [ x for x in gather_idx if x is not None ] if process_group . rank () == 0 : _finalize_master ( dest_dir , gather_idx_filter ) write_model_state_local ( dest_dir , mapper , state_generator , shard_size_gb = 4.0 , show_progress = True ) Saves model states to disk in a single local process. This function uses a streaming approach. It analyzes the mapper to determine which files need to be saved. Tensors are loaded into memory only when needed and evicted immediately after the mapper processes them. Parameters: Name Type Description Default dest_dir Path Destination directory. required mapper ModelStateMapper Mapping to apply to states before saving. required state_generator Iterable [ tuple [ str , Tensor ]] Stream of (name, tensor) pairs to save. required shard_size_gb float Maximum size of a single .safetensors file in GB. 4.0 show_progress bool Whether to show the progress bar. True Source code in d9d/model_state/io/writer.py 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 def write_model_state_local ( dest_dir : Path , mapper : ModelStateMapper , state_generator : Iterable [ tuple [ str , torch . Tensor ]], shard_size_gb : float = 4.0 , show_progress : bool = True , ): \"\"\" Saves model states to disk in a single local process. This function uses a streaming approach. It analyzes the mapper to determine which files need to be saved. Tensors are loaded into memory only when needed and evicted immediately after the mapper processes them. Args: dest_dir: Destination directory. mapper: Mapping to apply to states before saving. state_generator: Stream of (name, tensor) pairs to save. shard_size_gb: Maximum size of a single .safetensors file in GB. show_progress: Whether to show the progress bar. \"\"\" idx = _StateWritingFlowLocal ( dest_dir = dest_dir , mapper = mapper , shard_size_gb = shard_size_gb , show_progress = show_progress , sharding_rank = 0 , is_current_process_rank_master = True , ) . write ( state_generator = state_generator ) idx = cast ( ModelStateIndex , idx ) # we are sure is_current_process_rank_master=True _finalize_master ( dest_dir , [ idx ]) write_model_state_pipeline_parallel ( dest_dir , mapper , state_generator , device_mesh , pipeline_dim_name , shard_size_gb = 4.0 , show_progress = True ) Saves model states in a complex ND distributed training setting. This function uses a streaming approach. It analyzes the mapper to determine which files need to be saved. Tensors are loaded into memory only when needed and evicted immediately after the mapper processes them. This handles Pipeline Parallelism by ensuring that only one rank per pipeline stage actually writes data to disk to avoid duplication. Parameters: Name Type Description Default dest_dir Path Destination directory. required mapper ModelStateMapper Mapping to apply to states before saving. required state_generator Iterable [ tuple [ str , Tensor ]] Stream of (name, tensor) pairs from the model. required device_mesh DeviceMesh The PyTorch DeviceMesh representing the cluster layout. required pipeline_dim_name str The name of the mesh dimension responsible for pipeline parallelism. required shard_size_gb float Maximum shard size in GB. 4.0 show_progress bool Whether to show the progress bar. True Source code in d9d/model_state/io/writer.py 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 def write_model_state_pipeline_parallel ( dest_dir : Path , mapper : ModelStateMapper , state_generator : Iterable [ tuple [ str , torch . Tensor ]], device_mesh : DeviceMesh , pipeline_dim_name : str , shard_size_gb : float = 4.0 , show_progress : bool = True , ): \"\"\" Saves model states in a complex ND distributed training setting. This function uses a streaming approach. It analyzes the mapper to determine which files need to be saved. Tensors are loaded into memory only when needed and evicted immediately after the mapper processes them. This handles Pipeline Parallelism by ensuring that only one rank per pipeline stage actually writes data to disk to avoid duplication. Args: dest_dir: Destination directory. mapper: Mapping to apply to states before saving. state_generator: Stream of (name, tensor) pairs from the model. device_mesh: The PyTorch DeviceMesh representing the cluster layout. pipeline_dim_name: The name of the mesh dimension responsible for pipeline parallelism. shard_size_gb: Maximum shard size in GB. show_progress: Whether to show the progress bar. \"\"\" pipeline_rank = device_mesh [ pipeline_dim_name ] . get_rank () mesh_dim_names = device_mesh . mesh_dim_names coords = device_mesh . get_coordinate () if mesh_dim_names is None or coords is None : raise ValueError ( \"Cannot save state using a DeviceMesh with no dim names or coords\" ) non_pipeline_coord_sum = sum ( coord for name , coord in zip ( mesh_dim_names , coords , strict = True ) if name != pipeline_dim_name ) master_within_pipeline_rank = non_pipeline_coord_sum == 0 current_idx = _StateWritingFlowLocal ( dest_dir = dest_dir , mapper = mapper , shard_size_gb = shard_size_gb , show_progress = show_progress , sharding_rank = pipeline_rank , is_current_process_rank_master = master_within_pipeline_rank , ) . write ( state_generator = state_generator ) gather_idx = all_gather_object ( current_idx , device_mesh . get_group ( 0 )) gather_idx_filter = [ x for x in gather_idx if x is not None ] if pipeline_rank == 0 and master_within_pipeline_rank : _finalize_master ( dest_dir , gather_idx_filter )","title":"Model State I/O"},{"location":"model_states/io/#about","text":"The d9d.model_state.io package handles the reading and writing of model checkpoints. We use checkpoint format that is compatible with HuggingFace format. This format is characterized by using sharded model-00001-of-XXXXX.safetensors .safetensors files for storing parameter tensors along with model.safetensors.index.json file containing the metadata. It is tightly integrated with the d9d.model_state.mapper framework to allow for Streamed Transformation - converting model architectures on-the-fly during IO without loading the entire model into memory.","title":"About"},{"location":"model_states/io/#core-concepts","text":"","title":"Core Concepts"},{"location":"model_states/io/#why-support-transformations","text":"In d9d all the model state input/output logic is natively integrated with mapping and transforming model states. Such a combined system acts as a powerful abstraction layer that decouples the checkpoint architecture (how weights are stored on disk) from the model architecture (how weights are used in PyTorch code). This integration is critical for: Native HuggingFace Compatibility : You can use highly optimized, custom model implementations (e.g., using a single packed qkv_proj tensor) while reading directly from standard community checkpoints (which typically store q_proj , k_proj , and v_proj separately). The mapper handles the reshaping and stacking on-the-fly during the read stream. This eliminates the need for maintaining separate \"conversion scripts\" or storing duplicate, converted copies of large models. Runtime Structure Adapting (e.g., LoRA) : When injecting adapters like LoRA, the runtime model structure changes - often wrapping original layers. For example, a standard some_linear.weight on disk might need to be loaded into some_linear.orig.weight in memory. Instead of loading the full state dict and manually patching keys (which spikes memory), the mapper reroutes these keys without the need of materializing the model weights fully .","title":"Why Support Transformations"},{"location":"model_states/io/#how-loading-works","text":"Standard model loading involves loading a huge dictionary into CPU RAM, filtering and processing it, and moving the results to GPU. This approach is ineffective since it requires a lot of CPU-GPU transfers, consumes high amount of memory and involves duplicate work across different pipeline parallel workers. d9d proposes a different approach: Streaming & Eviction : Tensors are loaded in streamed manner and therefore kept in memory only when needed. Once a mapper group (e.g., \"stack Q, K, V\") is executed, the source tensors are immediately evicted from memory. Topology-Aware Loading : Instead of blindly loading all the files, the reader inspects the ModelStateMapper . It calculates exactly which files contain the required inputs.","title":"How Loading Works"},{"location":"model_states/io/#how-saving-works","text":"Standard model saving often requires gathering all parameters to a single rank (causing OOM) or manual orchestration of file names and indices across hundreds of GPUs. d9d's approach automates the checkpoint exporting lifecycle for large-scale distributed setups: Streaming & Eviction : Tensors are saved in streamed manner and therefore kept in memory only when needed. Once a mapper group (e.g., \"stack Q, K, V\") is executed, the source tensors are immediately evicted from memory. Target tensors are kept in memory only before they are flushed to respective .safetensors files. Distributed Awareness : In addition to providing local model exporting, we provide distributed-aware export functions. The writer natively understands distributed topologies (via ProcessGroup or DeviceMesh ). In Pipeline Parallel scenarios, it identifies which rank holds the specific stage master copy, ensuring that parameters are written exactly once without race conditions or duplication.","title":"How Saving Works"},{"location":"model_states/io/#usage-examples","text":"These examples provide information primarily how to load and write model states in a pass-through way. If you want to see examples of complex model state mapping, please refer to ModelStateMapper documentation.","title":"Usage Examples"},{"location":"model_states/io/#raw-io-streamed-loading","text":"This example shows how to load a model without spiking memory usage. from pathlib import Path import torch from d9d.model_state.io.reader import read_model_state from d9d.model_state.mapper.leaf import ModelStateMapperStackTensors from d9d.model_state.mapper.adapters import identity_mapper_from_module # Define the mapper (Topology) # in this example we will load all the parameters the model contains mapper = identity_mapper_from_module ( model ) # Start the stream # 'src_dir' must contain safetensors files and model.safetensors.index.json loader_stream = read_model_state ( src_dir = Path ( \"./checkpoint\" ), mapper = mapper , device = \"cpu\" # or \"cuda:0\" ) # Iterate through the transformed results state_dict = {} for name , tensor in loader_stream : print ( f \"Loaded and transformed: { name } -> { tensor . shape } \" ) state_dict [ name ] = tensor","title":"Raw I/O - Streamed Loading"},{"location":"model_states/io/#raw-io-streamed-saving","text":"Saving a model locally, automatically splitting into 1 GB shards. from pathlib import Path from d9d.model_state.io.writer import write_model_state_local from d9d.model_state.mapper.adapters import identity_mapper_from_module # 1. Create a generator for your model states state_generator = model . named_parameters () # 2. Define a mapper (Identity if no transformation is needed during save) mapper = identity_mapper_from_module ( model ) # 3. Write # This handles sharding and metadata file creation automatically write_model_state_local ( dest_dir = Path ( \"./output_checkpoint\" ), mapper = mapper , state_generator = state_generator , shard_size_gb = 1.0 # Split files if they exceed 1GB )","title":"Raw I/O - Streamed Saving"},{"location":"model_states/io/#raw-io-distributed-load-transform-save","text":"One of the most powerful features of d9d is the ability to perform Offline Checkpoint Conversion using a distributed cluster. If you have a massive checkpoint in Format A (e.g., HuggingFace) and need to convert it to Format B (e.g., a custom Training format with packed QKV), you don't need a single machine with 1TB RAM. instead, you can spin up 8 GPUs, have each GPU process 1/8th of the keys in parallel, and write a new sharded checkpoint. import torch.distributed as dist from pathlib import Path from d9d.model_state.io import read_model_state , write_model_state_distributed from d9d.model_state.mapper.compose import ModelStateMapperShard from d9d.model_state.mapper.adapters import identity_mapper_from_mapper_outputs # 1. Initialize distributed environment dist . init_process_group ( \"nccl\" ) rank = dist . get_rank () world_size = dist . get_world_size () # 2. Define the global transformation logic # This describes how the ENTIRE model should be converted. # e.g., \"Stack Q,K,V\", \"Rename MLP\", \"Load everying else as-is\" mapper = build_my_fancy_custom_mapper () # 3. Shard the workload # We wrap the mapper to restrict execution. # Rank 0 will only process the first 1/N dependency groups, Rank 1 the next, etc. # This ensures that no two ranks load/process/save the same tensors. local_work_mapper = ModelStateMapperShard ( sub_mapper = mapper , total_shards = world_size , current_shard = rank ) # 4. Define saving topology # The 'read_model_state' generator below will yield tensors that have # ALREADY been transformed to their target names/shapes. # The writer just needs to accept these new keys and save them. # We generate this identity mapper automatically from the output signature. writer_mapper = identity_mapper_from_mapper_outputs ( local_work_mapper ) # 5. Execute the pipeline # - Reader: Loads specific source files, transforms, yields new tensors. # - Writer: Receives new tensors, saves to '*.safetensors' files with temporary names # - Finalizer: Rank 0 creates 'model.safetensors.index.json' covering all ranks and renames .safetensors files to their final names. write_model_state_distributed ( dest_dir = Path ( \"./converted_checkpoint\" ), mapper = writer_mapper , state_generator = read_model_state ( src_dir = Path ( \"./original_checkpoint\" ), mapper = local_work_mapper , # Defines what to load and transformations device = \"cuda\" , show_progress = False # Disable read bars to avoid stderr spam ), process_group = dist . distributed_c10d . _get_default_group (), shard_size_gb = 4.0 , show_progress = True # Master rank will show global save progress )","title":"Raw I/O - Distributed Load-Transform-Save"},{"location":"model_states/io/#pytorch-module-io-streamed-loading","text":"Loading a checkpoint where disk keys exactly match model keys. identity_mapper_from_module ensures only existing model parameters are loaded. from pathlib import Path from d9d.model_state.io import load_model_state from d9d.model_state.mapper.adapters import identity_mapper_from_module # 1. Setup Model (e.g., empty or on meta device) model = ... # 2. Create Identity Topology # This tells d9d: \"Load every key that exists in 'model' as is.\" mapper = identity_mapper_from_module ( model ) # 3. Stream & Inject load_model_state ( src_dir = Path ( \"./checkpoints/v1\" ), mapper = mapper , device = \"cuda\" , model = model )","title":"PyTorch Module I/O - Streamed Loading"},{"location":"model_states/io/#pytorch-module-io-streaming-saving-devicemesh","text":"Saves a model in a complex ND Parallel environment using PyTorch DeviceMesh . This features: DTensor Gathering : Automatically gathers DTensor shards from the mesh into full tensors before writing. Concurrency Within PP Rank : In a Data/Tensor/... parallel setup, multiple GPUs hold replicated or sharded copies of the same parameters. This function uses the DeviceMesh to ensure that only the \"canonical\" PP replica (DP Rank 0, TP Rank 0, ...) writes to disk, preventing write conflicts. Concurrency Across PP Ranks : Each PP rank writes the data into its own files. After all the PP ranks finish writing, PP Rank 0 merges the metadata from different PP ranks into a single global checkpoint index file. from pathlib import Path from torch.distributed.device_mesh import init_device_mesh from d9d.model_state.io import save_model_state_pipeline_parallel from d9d.model_state.mapper.compose import ModelStateMapperParallel from d9d.model_state.mapper.adapters import identity_mapper_from_module # 1. Setup 3D Mesh # pp=2 (Pipeline), dp=2 (Data), tp=2 (Tensor) mesh = init_device_mesh ( \"cuda\" , ( 2 , 2 , 2 ), mesh_dim_names = ( \"pp\" , \"dp\" , \"tp\" )) # 2. Define Model Stages # In this example, each PP rank manages two distinct parts of the model. my_stages = [ TransformerStage ( ... ), TransformerStage ( ... )] # 3. Create Topology # Since this rank manages multiple modules, we create a Parallel mapper # to combine the requirements of all stages. mapper = ModelStateMapperParallel ([ identity_mapper_from_module ( stage ) for stage in my_stages ]) # 4. Save # The system inspects the mesh. It identifies if the current rank is # the \"Master\" for the provided stages (i.e., dp_rank=0, tp_rank=0). # If so, it gathers DTensors and writes. If not, it skips writing # but participates in the collective gather. save_model_state_pipeline_parallel ( dest_dir = Path ( \"./checkpoint\" ), mapper = mapper , device_mesh = mesh , pipeline_dim_name = \"pp\" , models = my_stages , shard_size_gb = 4.0 )","title":"PyTorch Module I/O - Streaming Saving (DeviceMesh)"},{"location":"model_states/io/#d9d.model_state.io","text":"","title":"io"},{"location":"model_states/io/#d9d.model_state.io.load_model_state","text":"High-level utility to stream a checkpoint directly into a PyTorch module. This function orchestrates the full loading lifecycle: Topology Mapping: Uses mapper to rename/stack/reshape on-disk states to model states. Automatic Distribution: If the model contains DTensor s, the loaded local tensors are automatically sharded/replicated to match the model's placement schema. Streaming Read & Inject: After loading and transforming a model state, it will be injected into model using load_state_dict(...) . NOTICE: Only states specified in mapper will be loaded! You can use d9d.model_state.mapper.adapters.identity_mapper_from_module(module) to create a mapper that will load every model state without changing it. Parameters: Name Type Description Default src_dir Path Directory containing .safetensors and index files. required mapper ModelStateMapper The topology defining how mapping from disk keys to model keys works. required device str The device to load tensors onto (usually \"cpu\" or \"cuda\"). required model Module The model instance to load weights into. required show_progress bool Whether to display the loading progress bar. True Source code in d9d/model_state/io/module_reader.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def load_model_state ( src_dir : Path , mapper : ModelStateMapper , device : str , model : nn . Module , show_progress : bool = True , ): \"\"\" High-level utility to stream a checkpoint directly into a PyTorch module. This function orchestrates the full loading lifecycle: 1. Topology Mapping: Uses `mapper` to rename/stack/reshape on-disk states to model states. 2. Automatic Distribution: If the `model` contains `DTensor`s, the loaded local tensors are automatically sharded/replicated to match the model's placement schema. 3. Streaming Read & Inject: After loading and transforming a model state, it will be injected into `model` using `load_state_dict(...)`. NOTICE: Only states specified in `mapper` will be loaded! You can use `d9d.model_state.mapper.adapters.identity_mapper_from_module(module)` to create a mapper that will load every model state without changing it. Args: src_dir: Directory containing .safetensors and index files. mapper: The topology defining how mapping from disk keys to model keys works. device: The device to load tensors onto (usually \"cpu\" or \"cuda\"). model: The model instance to load weights into. show_progress: Whether to display the loading progress bar. \"\"\" for state_name , state_value in read_model_state ( src_dir = src_dir , mapper = _augment_mapper_for_injection ( model , mapper ), device = device , show_progress = show_progress ): model . load_state_dict ({ state_name : state_value }, strict = False )","title":"load_model_state"},{"location":"model_states/io/#d9d.model_state.io.read_model_state","text":"Reads a model checkpoint from disk, transforming it on-the-fly according to the state mapper. This function uses a streaming approach. It analyzes the mapper to determine which files need to be loaded. Tensors are loaded into memory only when needed and evicted immediately after the mapper processes them. Parameters: Name Type Description Default src_dir Path The directory containing .safetensors files and model.safetensors.index.json file. required mapper ModelStateMapper The transformation graph defining how to map on-disk keys to output keys. required device str The device to load tensors onto (e.g., \"cpu\", \"cuda:0\"). required show_progress bool Whether to display a progress bar. True Yields: Type Description Iterable [ tuple [ str , Tensor ]] A tuple containing the transformed parameter name and its tensor value. Source code in d9d/model_state/io/reader.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def read_model_state ( src_dir : Path , mapper : ModelStateMapper , device : str , show_progress : bool = True ) -> Iterable [ tuple [ str , torch . Tensor ]]: \"\"\" Reads a model checkpoint from disk, transforming it on-the-fly according to the state mapper. This function uses a streaming approach. It analyzes the mapper to determine which files need to be loaded. Tensors are loaded into memory only when needed and evicted immediately after the mapper processes them. Args: src_dir: The directory containing .safetensors files and `model.safetensors.index.json` file. mapper: The transformation graph defining how to map on-disk keys to output keys. device: The device to load tensors onto (e.g., \"cpu\", \"cuda:0\"). show_progress: Whether to display a progress bar. Yields: A tuple containing the transformed parameter name and its tensor value. \"\"\" yield from _StateLoadingFlow ( src_dir = src_dir , device = device , mapper = mapper , show_progress = show_progress ) . load ()","title":"read_model_state"},{"location":"model_states/io/#d9d.model_state.io.save_model_state","text":"High-level utility to save a PyTorch model to disk on a single process. NOTICE: Only states specified in mapper will be saved! You can use d9d.model_state.mapper.adapters.identity_mapper_from_module(module) to create a mapper that will save every model state without changing it. Parameters: Name Type Description Default dest_dir Path The directory to save .safetensors shards and index. required mapper ModelStateMapper Topology defining how model keys map to disk keys. required model Module The PyTorch module to save. required shard_size_gb float Max size per shard file in Gigabytes. 4.0 show_progress bool Whether to display a progress bar. True Source code in d9d/model_state/io/module_writer.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def save_model_state ( dest_dir : Path , mapper : ModelStateMapper , model : nn . Module , shard_size_gb : float = 4.0 , show_progress : bool = True ): \"\"\" High-level utility to save a PyTorch model to disk on a **single** process. NOTICE: Only states specified in `mapper` will be saved! You can use `d9d.model_state.mapper.adapters.identity_mapper_from_module(module)` to create a mapper that will save every model state without changing it. Args: dest_dir: The directory to save .safetensors shards and index. mapper: Topology defining how model keys map to disk keys. model: The PyTorch module to save. shard_size_gb: Max size per shard file in Gigabytes. show_progress: Whether to display a progress bar. \"\"\" write_model_state_local ( dest_dir = dest_dir , mapper = _augment_mapper_for_extraction ([ model ], mapper ), state_generator = _state_generator ([ model ]), shard_size_gb = shard_size_gb , show_progress = show_progress , )","title":"save_model_state"},{"location":"model_states/io/#d9d.model_state.io.save_model_state_pipeline_parallel","text":"High-level utility to save a model in a Distributed Pipeline Parallel environment to disk. Features: Auto-Gather : Converts DTensor parameters to full tensors before saving. Distribution Awareness : Uses the device_mesh to ensure that for a given pipeline stage, only the master rank writes the checkpoint, preventing Write-After-Write conflicts. Index Merging : Aggregates metadata from all independent pipeline stages into one global index file. NOTICE: Only states specified in mapper will be saved! You can use d9d.model_state.mapper.adapters.identity_mapper_from_module(module) to create a mapper that will save every model state without changing it. Parameters: Name Type Description Default dest_dir Path directory to save .safetensors shards and index file. required mapper ModelStateMapper Topology defining how model keys map to disk keys. required device_mesh DeviceMesh The cluster topology mesh. required pipeline_dim_name str The specific dimension name in the mesh used for pipelining. required models list [ Module ] A list of modules (pipeline stages) processed by this PP rank. required shard_size_gb float Max size per shard file in Gigabytes. 4.0 show_progress bool Whether to display a progress bar. True Source code in d9d/model_state/io/module_writer.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def save_model_state_pipeline_parallel ( dest_dir : Path , mapper : ModelStateMapper , device_mesh : DeviceMesh , pipeline_dim_name : str , models : list [ nn . Module ], shard_size_gb : float = 4.0 , show_progress : bool = True , ): \"\"\" High-level utility to save a model in a Distributed Pipeline Parallel environment to disk. Features: 1. **Auto-Gather**: Converts `DTensor` parameters to full tensors before saving. 2. **Distribution Awareness**: Uses the `device_mesh` to ensure that for a given pipeline stage, only the master rank writes the checkpoint, preventing Write-After-Write conflicts. 3. **Index Merging**: Aggregates metadata from all independent pipeline stages into one global index file. NOTICE: Only states specified in `mapper` will be saved! You can use `d9d.model_state.mapper.adapters.identity_mapper_from_module(module)` to create a mapper that will save every model state without changing it. Args: dest_dir: directory to save .safetensors shards and index file. mapper: Topology defining how model keys map to disk keys. device_mesh: The cluster topology mesh. pipeline_dim_name: The specific dimension name in the mesh used for pipelining. models: A list of modules (pipeline stages) processed by this PP rank. shard_size_gb: Max size per shard file in Gigabytes. show_progress: Whether to display a progress bar. \"\"\" write_model_state_pipeline_parallel ( dest_dir = dest_dir , mapper = _augment_mapper_for_extraction ( models , mapper ), state_generator = _state_generator ( models ), device_mesh = device_mesh , pipeline_dim_name = pipeline_dim_name , shard_size_gb = shard_size_gb , show_progress = show_progress , )","title":"save_model_state_pipeline_parallel"},{"location":"model_states/io/#d9d.model_state.io.write_model_state_distributed","text":"Saves model states in a distributed setup (multiple processes). This function uses a streaming approach. It analyzes the mapper to determine which files need to be saved. Tensors are loaded into memory only when needed and evicted immediately after the mapper processes them. Each rank writes its own shard. Rank 0 gathers indices and finalizes the checkpoint. Parameters: Name Type Description Default dest_dir Path Destination directory. required mapper ModelStateMapper Mapping to apply to states before saving. required state_generator Iterable [ tuple [ str , Tensor ]] Stream of (name, tensor) pairs from the model. required process_group ProcessGroup The distributed process group. required shard_size_gb float Maximum shard size in GB. 4.0 show_progress bool Whether to show the progress bar. True Source code in d9d/model_state/io/writer.py 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 def write_model_state_distributed ( dest_dir : Path , mapper : ModelStateMapper , state_generator : Iterable [ tuple [ str , torch . Tensor ]], process_group : ProcessGroup , shard_size_gb : float = 4.0 , show_progress : bool = True , ): \"\"\" Saves model states in a distributed setup (multiple processes). This function uses a streaming approach. It analyzes the mapper to determine which files need to be saved. Tensors are loaded into memory only when needed and evicted immediately after the mapper processes them. Each rank writes its own shard. Rank 0 gathers indices and finalizes the checkpoint. Args: dest_dir: Destination directory. mapper: Mapping to apply to states before saving. state_generator: Stream of (name, tensor) pairs from the model. process_group: The distributed process group. shard_size_gb: Maximum shard size in GB. show_progress: Whether to show the progress bar. \"\"\" current_idx = _StateWritingFlowLocal ( dest_dir = dest_dir , mapper = mapper , shard_size_gb = shard_size_gb , show_progress = show_progress , sharding_rank = process_group . rank (), is_current_process_rank_master = True , ) . write ( state_generator = state_generator ) gather_idx = all_gather_object ( current_idx , process_group ) gather_idx_filter = [ x for x in gather_idx if x is not None ] if process_group . rank () == 0 : _finalize_master ( dest_dir , gather_idx_filter )","title":"write_model_state_distributed"},{"location":"model_states/io/#d9d.model_state.io.write_model_state_local","text":"Saves model states to disk in a single local process. This function uses a streaming approach. It analyzes the mapper to determine which files need to be saved. Tensors are loaded into memory only when needed and evicted immediately after the mapper processes them. Parameters: Name Type Description Default dest_dir Path Destination directory. required mapper ModelStateMapper Mapping to apply to states before saving. required state_generator Iterable [ tuple [ str , Tensor ]] Stream of (name, tensor) pairs to save. required shard_size_gb float Maximum size of a single .safetensors file in GB. 4.0 show_progress bool Whether to show the progress bar. True Source code in d9d/model_state/io/writer.py 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 def write_model_state_local ( dest_dir : Path , mapper : ModelStateMapper , state_generator : Iterable [ tuple [ str , torch . Tensor ]], shard_size_gb : float = 4.0 , show_progress : bool = True , ): \"\"\" Saves model states to disk in a single local process. This function uses a streaming approach. It analyzes the mapper to determine which files need to be saved. Tensors are loaded into memory only when needed and evicted immediately after the mapper processes them. Args: dest_dir: Destination directory. mapper: Mapping to apply to states before saving. state_generator: Stream of (name, tensor) pairs to save. shard_size_gb: Maximum size of a single .safetensors file in GB. show_progress: Whether to show the progress bar. \"\"\" idx = _StateWritingFlowLocal ( dest_dir = dest_dir , mapper = mapper , shard_size_gb = shard_size_gb , show_progress = show_progress , sharding_rank = 0 , is_current_process_rank_master = True , ) . write ( state_generator = state_generator ) idx = cast ( ModelStateIndex , idx ) # we are sure is_current_process_rank_master=True _finalize_master ( dest_dir , [ idx ])","title":"write_model_state_local"},{"location":"model_states/io/#d9d.model_state.io.write_model_state_pipeline_parallel","text":"Saves model states in a complex ND distributed training setting. This function uses a streaming approach. It analyzes the mapper to determine which files need to be saved. Tensors are loaded into memory only when needed and evicted immediately after the mapper processes them. This handles Pipeline Parallelism by ensuring that only one rank per pipeline stage actually writes data to disk to avoid duplication. Parameters: Name Type Description Default dest_dir Path Destination directory. required mapper ModelStateMapper Mapping to apply to states before saving. required state_generator Iterable [ tuple [ str , Tensor ]] Stream of (name, tensor) pairs from the model. required device_mesh DeviceMesh The PyTorch DeviceMesh representing the cluster layout. required pipeline_dim_name str The name of the mesh dimension responsible for pipeline parallelism. required shard_size_gb float Maximum shard size in GB. 4.0 show_progress bool Whether to show the progress bar. True Source code in d9d/model_state/io/writer.py 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 def write_model_state_pipeline_parallel ( dest_dir : Path , mapper : ModelStateMapper , state_generator : Iterable [ tuple [ str , torch . Tensor ]], device_mesh : DeviceMesh , pipeline_dim_name : str , shard_size_gb : float = 4.0 , show_progress : bool = True , ): \"\"\" Saves model states in a complex ND distributed training setting. This function uses a streaming approach. It analyzes the mapper to determine which files need to be saved. Tensors are loaded into memory only when needed and evicted immediately after the mapper processes them. This handles Pipeline Parallelism by ensuring that only one rank per pipeline stage actually writes data to disk to avoid duplication. Args: dest_dir: Destination directory. mapper: Mapping to apply to states before saving. state_generator: Stream of (name, tensor) pairs from the model. device_mesh: The PyTorch DeviceMesh representing the cluster layout. pipeline_dim_name: The name of the mesh dimension responsible for pipeline parallelism. shard_size_gb: Maximum shard size in GB. show_progress: Whether to show the progress bar. \"\"\" pipeline_rank = device_mesh [ pipeline_dim_name ] . get_rank () mesh_dim_names = device_mesh . mesh_dim_names coords = device_mesh . get_coordinate () if mesh_dim_names is None or coords is None : raise ValueError ( \"Cannot save state using a DeviceMesh with no dim names or coords\" ) non_pipeline_coord_sum = sum ( coord for name , coord in zip ( mesh_dim_names , coords , strict = True ) if name != pipeline_dim_name ) master_within_pipeline_rank = non_pipeline_coord_sum == 0 current_idx = _StateWritingFlowLocal ( dest_dir = dest_dir , mapper = mapper , shard_size_gb = shard_size_gb , show_progress = show_progress , sharding_rank = pipeline_rank , is_current_process_rank_master = master_within_pipeline_rank , ) . write ( state_generator = state_generator ) gather_idx = all_gather_object ( current_idx , device_mesh . get_group ( 0 )) gather_idx_filter = [ x for x in gather_idx if x is not None ] if pipeline_rank == 0 and master_within_pipeline_rank : _finalize_master ( dest_dir , gather_idx_filter )","title":"write_model_state_pipeline_parallel"},{"location":"model_states/mapper/","text":"About The d9d.model_state.mapper package solves the complexity of working with model checkpoints by providing a declarative, graph-based framework for transforming model states . Core Concept Loading large-scale models is rarely a simple 1-to-1 key matching operation. You often face challenges such as: Naming Mismatches : HuggingFace uses model.layers.0 , your custom model uses transformer.h.0 . Shape Mismatches : The checkpoint stores Q , K , and V separately, but your model implementation expects a stacked QKV tensor. Scale : The checkpoint is 500GB. You cannot load the whole dictionary on every GPU to process it. Instead of writing a manual loop that loads tensors and blindly modifies them, this framework treats state transformation as a Directed Acyclic Graph (DAG) . Such a declarative approach makes it available for d9d to perform complex transform-save and transform-load operations effectively in a streamed manner without loading the whole checkpoint into memory. Usage Examples Pass-through Mapping for PyTorch Module If you simply want to load a checkpoint where keys match the model definition (standard load_state_dict behavior), but want to utilize d9d's streaming/sharding capabilities. import torch.nn as nn from d9d.model_state.mapper.adapters import identity_mapper_from_module # Define your PyTorch model model = nn . Sequential ( nn . Linear ( 10 , 10 ), nn . ReLU (), nn . Linear ( 10 , 5 ) ) # Automatically generate a mapper based on the model's actual parameter names # This creates Identity mappers for \"0.weight\", \"0.bias\", \"2.weight\", \"2.bias\" mapper = identity_mapper_from_module ( model ) Using Leaf Mappers This example demonstrates using leaf mappers to handle common mismatch scenario: merging separate Query/Key/Value tensors into a single tensor. import torch from d9d.model_state.mapper.leaf import ( ModelStateMapperRename , ModelStateMapperStackTensors ) # Stacking Tensors # Scenario: Checkpoint has separate Q, K, V linear layers, we need one QKV tensor stack_mapper = ModelStateMapperStackTensors ( source_names = [ \"attn.q.weight\" , \"attn.k.weight\" , \"attn.v.weight\" ], target_name = \"attn.qkv.weight\" , stack_dim = 0 ) # To show what this mapper needs: print ( stack_mapper . state_dependency_groups ()) # Output: {StateGroup(inputs={'attn.q.weight', ...}, outputs={'attn.qkv.weight'})} # To actually execute: dummy_data = { \"attn.q.weight\" : torch . randn ( 64 , 64 ), \"attn.k.weight\" : torch . randn ( 64 , 64 ), \"attn.v.weight\" : torch . randn ( 64 , 64 ), } result = stack_mapper . apply ( dummy_data ) print ( result [ \"attn.qkv.weight\" ] . shape ) # Output: torch.Size([3, 64, 64]) Composing Complex Pipelines Converting an entire model state requires processing multiple keys in parallel, and potentially chaining operations (e.g., Rename then Stack). from d9d.model_state.mapper.compose import ModelStateMapperSequential , ModelStateMapperParallel from d9d.model_state.mapper.leaf import ModelStateMapperRename , ModelStateMapperStackTensors # Define a transformation pipeline mapper = ModelStateMapperSequential ([ # Step 1: Rename keys to standard format ModelStateMapperParallel ([ ModelStateMapperRename ( \"bert.encoder.layer.0.attention.self.query.weight\" , \"layer.0.q\" ), ModelStateMapperRename ( \"bert.encoder.layer.0.attention.self.key.weight\" , \"layer.0.k\" ), ModelStateMapperRename ( \"bert.encoder.layer.0.attention.self.value.weight\" , \"layer.0.v\" ), ]), # Step 2: Stack them into a specialized attention tensor ModelStateMapperStackTensors ( source_names = [ \"layer.0.q\" , \"layer.0.k\" , \"layer.0.v\" ], target_name = \"layer.0.qkv\" , stack_dim = 0 ) ]) d9d.model_state.mapper This package provides core components of the state mapping system. ModelStateMapper Bases: ABC The abstract base class for all model state transformation operations. This class serves as the interface between the definition of a transformation topology and the actual execution of tensor operations. It enforces a Declarative vs. Imperative separation of concerns: Declarative (Topology): Through state_dependency_groups() , the mapper announces what it intends to do without handling any data. This allows the system to build execution graphs, validate chains, detect collisions, and shard tasks before allocating memory. Imperative (Execution): Through apply() , the mapper performs the actual logic (PyTorch operations) on model states. Source code in d9d/model_state/mapper/abc.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 class ModelStateMapper ( abc . ABC ): \"\"\" The abstract base class for all model state transformation operations. This class serves as the interface between the definition of a transformation topology and the actual execution of tensor operations. It enforces a Declarative vs. Imperative separation of concerns: 1. Declarative (Topology): Through `state_dependency_groups()`, the mapper announces *what* it intends to do without handling any data. This allows the system to build execution graphs, validate chains, detect collisions, and shard tasks *before* allocating memory. 2. Imperative (Execution): Through `apply()`, the mapper performs the actual logic (PyTorch operations) on model states. \"\"\" @abc . abstractmethod def state_dependency_groups ( self ) -> frozenset [ StateGroup ]: \"\"\" Calculates and returns the set of independent dependency groups this mapper handles. Returns: A frozenset of `StateGroup` objects. Each group represents a disjoint operation. For example, a mapper that renames ten independent tensors would return ten distinct `StateGroup` objects, allowing them to be sharded or processed individually. \"\"\" ... @abc . abstractmethod def apply ( self , group : dict [ str , torch . Tensor ]) -> dict [ str , torch . Tensor ]: \"\"\" Executes the transformation logic on a specific dictionary of tensors. The orchestration system guarantees that the `group` dictionary passed here contains all keys listed in the `inputs` of the active `StateGroup`. Implementation of this method should guarantee that the result will contain all keys listed in the `outputs`. Args: group: A dictionary containing the source data. Keys match `StateGroup.inputs`. Returns: A dictionary containing the transformed data. Keys must strictly match `StateGroup.outputs`. \"\"\" ... apply ( group ) abstractmethod Executes the transformation logic on a specific dictionary of tensors. The orchestration system guarantees that the group dictionary passed here contains all keys listed in the inputs of the active StateGroup . Implementation of this method should guarantee that the result will contain all keys listed in the outputs . Parameters: Name Type Description Default group dict [ str , Tensor ] A dictionary containing the source data. Keys match StateGroup.inputs . required Returns: Type Description dict [ str , Tensor ] A dictionary containing the transformed data. Keys must strictly match StateGroup.outputs . Source code in d9d/model_state/mapper/abc.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 @abc . abstractmethod def apply ( self , group : dict [ str , torch . Tensor ]) -> dict [ str , torch . Tensor ]: \"\"\" Executes the transformation logic on a specific dictionary of tensors. The orchestration system guarantees that the `group` dictionary passed here contains all keys listed in the `inputs` of the active `StateGroup`. Implementation of this method should guarantee that the result will contain all keys listed in the `outputs`. Args: group: A dictionary containing the source data. Keys match `StateGroup.inputs`. Returns: A dictionary containing the transformed data. Keys must strictly match `StateGroup.outputs`. \"\"\" ... state_dependency_groups () abstractmethod Calculates and returns the set of independent dependency groups this mapper handles. Returns: Type Description frozenset [ StateGroup ] A frozenset of StateGroup objects. Each group frozenset [ StateGroup ] represents a disjoint operation. For example, a mapper that renames ten frozenset [ StateGroup ] independent tensors would return ten distinct StateGroup objects, frozenset [ StateGroup ] allowing them to be sharded or processed individually. Source code in d9d/model_state/mapper/abc.py 40 41 42 43 44 45 46 47 48 49 50 51 @abc . abstractmethod def state_dependency_groups ( self ) -> frozenset [ StateGroup ]: \"\"\" Calculates and returns the set of independent dependency groups this mapper handles. Returns: A frozenset of `StateGroup` objects. Each group represents a disjoint operation. For example, a mapper that renames ten independent tensors would return ten distinct `StateGroup` objects, allowing them to be sharded or processed individually. \"\"\" ... StateGroup dataclass Represents an atomic unit of dependency in the model state transformation graph. A StateGroup defines a strict contract between a set of input keys (source) and a set of output keys (destination). Attributes: Name Type Description inputs frozenset [ str ] The complete set of keys required from the source state dictionary to satisfy this dependency. outputs frozenset [ str ] The complete set of keys that will be produced as a result of this transformation. Source code in d9d/model_state/mapper/abc.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 @dataclasses . dataclass ( frozen = True ) class StateGroup : \"\"\" Represents an atomic unit of dependency in the model state transformation graph. A `StateGroup` defines a strict contract between a set of input keys (source) and a set of output keys (destination). Attributes: inputs: The complete set of keys required from the source state dictionary to satisfy this dependency. outputs: The complete set of keys that will be produced as a result of this transformation. \"\"\" inputs : frozenset [ str ] outputs : frozenset [ str ] d9d.model_state.mapper.adapters This package provides utility functions that are used to create simple ModelStateMapper instances from objects such as PyTorch modules or other StateMappers identity_mapper_from_mapper_outputs ( mapper ) Creates an identity mapper covering all outputs produced by the provided mapper. This function inspects the state_dependency_groups() of the input mapper , extracts every key listed in the outputs set of each group, and creates a corresponding ModelStateMapperIdentity for it. Parameters: Name Type Description Default mapper ModelStateMapper The mapper whose output signature will be inspected to generate the new identity mapper. required Returns: Type Description ModelStateMapper A composite mapper that acts as a pass-through for every key produced by the source mapper . Source code in d9d/model_state/mapper/adapters/mapper.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def identity_mapper_from_mapper_outputs ( mapper : ModelStateMapper ) -> ModelStateMapper : \"\"\" Creates an identity mapper covering all outputs produced by the provided mapper. This function inspects the `state_dependency_groups()` of the input `mapper`, extracts every key listed in the `outputs` set of each group, and creates a corresponding `ModelStateMapperIdentity` for it. Args: mapper: The mapper whose output signature will be inspected to generate the new identity mapper. Returns: A composite mapper that acts as a pass-through for every key produced by the source `mapper`. \"\"\" mappers : list [ ModelStateMapper ] = [] for state_group in mapper . state_dependency_groups (): for output_name in state_group . outputs : mappers . append ( ModelStateMapperIdentity ( output_name )) return ModelStateMapperParallel ( mappers ) identity_mapper_from_module ( module ) Creates an identity mapper for every parameter in a single PyTorch module. It is useful when you want to define a \"pass-through\" pipeline where the source checkpoint keys are expected to exactly match the model's current parameter names (standard load_state_dict behavior). Parameters: Name Type Description Default module Module The instantiated PyTorch model to inspect. required Source code in d9d/model_state/mapper/adapters/module.py 8 9 10 11 12 13 14 15 16 17 18 19 20 def identity_mapper_from_module ( module : nn . Module ) -> ModelStateMapper : \"\"\" Creates an identity mapper for every parameter in a single PyTorch module. It is useful when you want to define a \"pass-through\" pipeline where the source checkpoint keys are expected to exactly match the model's current parameter names (standard `load_state_dict` behavior). Args: module: The instantiated PyTorch model to inspect. \"\"\" return ModelStateMapperParallel ([ ModelStateMapperIdentity ( key ) for key in module . state_dict ()]) d9d.model_state.mapper.compose Complex state mappers are built using composition. This package provides ModelStateMapper implementations that are composed of other mappers. ModelStateMapperParallel Bases: ModelStateMapper Executes a list of states mappers independently alongside each other. This class aggregates multiple mappers into a single logical unit. It enforces strict isolation between the mappers: no two mappers can consume the same input key (input collision) or produce the same output key (output collision). During execution ( apply ), it routes the specific subset of the input dictionary to the sub-mapper responsible for those keys. Source code in d9d/model_state/mapper/compose/parallel.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 class ModelStateMapperParallel ( ModelStateMapper ): \"\"\" Executes a list of states mappers independently alongside each other. This class aggregates multiple mappers into a single logical unit. It enforces strict isolation between the mappers: no two mappers can consume the same input key (input collision) or produce the same output key (output collision). During execution (`apply`), it routes the specific subset of the input dictionary to the sub-mapper responsible for those keys. \"\"\" def __init__ ( self , mappers : Sequence [ ModelStateMapper ]): mappers_lst = filter_empty_mappers ( mappers ) all_groups = set () inputs_to_mapper = {} seen_inputs : set [ str ] = set () seen_outputs : set [ str ] = set () for mapper in mappers_lst : sub_groups = mapper . state_dependency_groups () for sub_group in sub_groups : if not seen_inputs . isdisjoint ( sub_group . inputs ): raise ValueError ( f \"Found a colliding input group: { sub_group . inputs } \" ) seen_inputs . update ( sub_group . inputs ) if not seen_outputs . isdisjoint ( sub_group . outputs ): raise ValueError ( f \"Found colliding output keys: { sub_group . outputs } \" ) seen_outputs . update ( sub_group . outputs ) all_groups . add ( sub_group ) inputs_to_mapper [ sub_group . inputs ] = mapper self . _all_groups = frozenset ( all_groups ) self . _inputs_to_mapper = inputs_to_mapper def state_dependency_groups ( self ) -> frozenset [ StateGroup ]: return self . _all_groups def apply ( self , group : dict [ str , torch . Tensor ]) -> dict [ str , torch . Tensor ]: group_keys = frozenset ( group . keys ()) if group_keys not in self . _inputs_to_mapper : raise ValueError ( \"Tried to run a parallel mapper with undefined group. Perhaps you sent groups that are not isolated?\" ) return self . _inputs_to_mapper [ group_keys ] . apply ( group ) ModelStateMapperSequential Bases: ModelStateMapper Executes a list of mappers in a specific sequence (pipeline). This class manages the data flow from one mapper to the next. It abstracts away intermediate states, exposing only the inputs required by the first relevant stage and the outputs produced by the final relevant stage. Key Features: Gap Filling : Automatically injects Identity mappers if a tensor needs to pass through a stage without modification to reach a later stage or the final output. Group Merging : Computes the net dependency graph. If Stage A requires 'x' and produces 'y', and Stage B requires 'y' and produces 'z', the Sequential mapper reports a single group {x} -> {z} . Source code in d9d/model_state/mapper/compose/sequential.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 class ModelStateMapperSequential ( ModelStateMapper ): \"\"\" Executes a list of mappers in a specific sequence (pipeline). This class manages the data flow from one mapper to the next. It abstracts away intermediate states, exposing only the inputs required by the first relevant stage and the outputs produced by the final relevant stage. Key Features: 1. **Gap Filling**: Automatically injects `Identity` mappers if a tensor needs to pass through a stage without modification to reach a later stage or the final output. 2. **Group Merging**: Computes the net dependency graph. If Stage A requires 'x' and produces 'y', and Stage B requires 'y' and produces 'z', the Sequential mapper reports a single group `{x} -> {z}`. \"\"\" def __init__ ( self , mappers : list [ ModelStateMapper ]): mappers = filter_empty_mappers ( mappers ) if not mappers : raise ValueError ( \"Mappers list cannot be empty.\" ) mappers = self . _fill_gaps ( mappers ) self . _groups = self . _compute_pipeline_groups ( mappers ) self . _mappers = mappers @staticmethod def _fill_gaps ( mappers : list [ ModelStateMapper ]) -> list [ ModelStateMapper ]: mappers = mappers . copy () # propagate inputs from bottom to top for stage_i in range ( 1 , len ( mappers ))[:: - 1 ]: groups_current = mappers [ stage_i ] . state_dependency_groups () groups_prev = mappers [ stage_i - 1 ] . state_dependency_groups () current_stage_requires = frozenset . union ( * ( x . inputs for x in groups_current )) prev_stage_produces = frozenset . union ( * ( x . outputs for x in groups_prev )) needs_to_pass_through = current_stage_requires - prev_stage_produces mappers [ stage_i - 1 ] = ModelStateMapperParallel ( [ mappers [ stage_i - 1 ]] + [ ModelStateMapperIdentity ( x ) for x in needs_to_pass_through ] ) # propagate outputs from top to bottom for stage_i in range ( 0 , len ( mappers ) - 1 ): groups_current = mappers [ stage_i ] . state_dependency_groups () groups_next = mappers [ stage_i + 1 ] . state_dependency_groups () current_stage_produces = frozenset . union ( * ( x . outputs for x in groups_current )) next_stage_requires = frozenset . union ( * ( x . inputs for x in groups_next )) needs_to_pass_through = current_stage_produces - next_stage_requires mappers [ stage_i + 1 ] = ModelStateMapperParallel ( [ mappers [ stage_i + 1 ]] + [ ModelStateMapperIdentity ( x ) for x in needs_to_pass_through ] ) return mappers @staticmethod def _compute_pipeline_groups ( mappers : list [ ModelStateMapper ]) -> frozenset [ StateGroup ]: outputs_depend_on_inputs = {} # given a fully connected graph, we can just go upwards for last_group_traced in mappers [ - 1 ] . state_dependency_groups (): required_inputs = last_group_traced . inputs for mapper_i in range ( 0 , len ( mappers ) - 1 )[:: - 1 ]: next_visit_groups = [ x for x in mappers [ mapper_i ] . state_dependency_groups () if not x . outputs . isdisjoint ( required_inputs ) ] required_inputs = frozenset . union ( * ( x . inputs for x in next_visit_groups )) outputs_depend_on_inputs [ last_group_traced . outputs ] = required_inputs return ModelStateMapperSequential . _merge_groups ( list ( outputs_depend_on_inputs . items ())) @staticmethod def _merge_groups ( groups : Sequence [ tuple [ AbstractSet [ str ], AbstractSet [ str ]]]) -> frozenset [ StateGroup ]: saved_groups : list [ tuple [ set [ str ], set [ str ]]] = [] saved_groups_modified = True while saved_groups_modified : saved_groups_modified = False for output_names , input_names in groups : was_new_group_created = False for group in saved_groups : if group [ 0 ] . intersection ( input_names ) or group [ 1 ] . intersection ( output_names ): group [ 0 ] . update ( input_names ) group [ 1 ] . update ( output_names ) was_new_group_created = True saved_groups_modified = True if not was_new_group_created : saved_groups . append (( set ( input_names ), set ( output_names ))) groups = saved_groups saved_groups = [] return frozenset ( StateGroup ( inputs = frozenset ( x [ 0 ]), outputs = frozenset ( x [ 1 ])) for x in groups ) def state_dependency_groups ( self ) -> frozenset [ StateGroup ]: return self . _groups def apply ( self , group : dict [ str , torch . Tensor ]) -> dict [ str , torch . Tensor ]: current_state = group next_state = {} for mapper in self . _mappers : for deps in mapper . state_dependency_groups (): if not deps . inputs <= current_state . keys (): continue next_state . update ( mapper . apply ({ k : v for k , v in current_state . items () if k in deps . inputs })) current_state = next_state next_state = {} return current_state ModelStateMapperShard Bases: ModelStateMapper Wraps another state mapper and restricts its execution to a specific subset (shard) of dependency groups. This is primarily used for parallelizing model loading across multiple processes or nodes. By assigning a different current_shard index to each process, the total set of tensors required by the sub_mapper is split evenly, preventing every process from loading the entire checkpoint. Source code in d9d/model_state/mapper/compose/shard.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class ModelStateMapperShard ( ModelStateMapper ): \"\"\" Wraps another state mapper and restricts its execution to a specific subset (shard) of dependency groups. This is primarily used for parallelizing model loading across multiple processes or nodes. By assigning a different `current_shard` index to each process, the total set of tensors required by the `sub_mapper` is split evenly, preventing every process from loading the entire checkpoint. \"\"\" def __init__ ( self , sub_mapper : ModelStateMapper , total_shards : int , current_shard : int ): self . _groups = self . _shard_groups ( sub_mapper . state_dependency_groups (), n_shards = total_shards , shard = current_shard ) self . _sub_mapper = sub_mapper self . _total_shards = total_shards self . _current_shard = current_shard @staticmethod def _shard_groups ( groups : frozenset [ StateGroup ], n_shards : int , shard : int ) -> frozenset [ StateGroup ]: groups_sorted = sorted ( groups , key = lambda x : sorted ( x . inputs )) groups_shard = [ x for i , x in enumerate ( groups_sorted ) if i % n_shards == shard ] return frozenset ( groups_shard ) def state_dependency_groups ( self ) -> frozenset [ StateGroup ]: return self . _groups def apply ( self , group : dict [ str , torch . Tensor ]) -> dict [ str , torch . Tensor ]: return self . _sub_mapper . apply ( group ) filter_empty_mappers ( mappers ) Filters out mappers that have no effect (no inputs and no outputs). Parameters: Name Type Description Default mappers Sequence [ ModelStateMapper ] The list of mappers to filter. required Returns: Type Description list [ ModelStateMapper ] A new list containing only active mappers. Source code in d9d/model_state/mapper/compose/helper.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def filter_empty_mappers ( mappers : Sequence [ ModelStateMapper ]) -> list [ ModelStateMapper ]: \"\"\" Filters out mappers that have no effect (no inputs and no outputs). Args: mappers: The list of mappers to filter. Returns: A new list containing only active mappers. \"\"\" result = [] for mapper in mappers : for group in mapper . state_dependency_groups (): if len ( group . inputs ) > 0 or len ( group . outputs ) > 0 : result . append ( mapper ) break return result d9d.model_state.mapper.leaf This package provides leaf mapper implementations. ModelStateMapperDistribute Bases: ModelStateMapper Converts a single local Tensor object into a DTensor object with specified device_mesh and placements . Source code in d9d/model_state/mapper/leaf/dtensor.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 class ModelStateMapperDistribute ( ModelStateMapper ): \"\"\" Converts a single local Tensor object into a DTensor object with specified `device_mesh` and `placements`. \"\"\" def __init__ ( self , name : str , device_mesh : DeviceMesh | None , placements : Sequence [ Placement ] | None ): self . _name = name self . _device_mesh = device_mesh self . _placements = placements def state_dependency_groups ( self ) -> frozenset [ StateGroup ]: return frozenset ([ StateGroup ( inputs = frozenset ([ self . _name ]), outputs = frozenset ([ self . _name ]))]) def apply ( self , group : dict [ str , torch . Tensor ]) -> dict [ str , torch . Tensor ]: return { self . _name : distribute_tensor ( group [ self . _name ], device_mesh = self . _device_mesh , placements = self . _placements , src_data_rank = None , # do not communicate here ) } ModelStateMapperGatherFullTensor Bases: ModelStateMapper Gathers a single DTensor object into a full Tensor object. Source code in d9d/model_state/mapper/leaf/dtensor.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 class ModelStateMapperGatherFullTensor ( ModelStateMapper ): \"\"\" Gathers a single DTensor object into a full Tensor object. \"\"\" def __init__ ( self , name : str ): self . _name = name def state_dependency_groups ( self ) -> frozenset [ StateGroup ]: return frozenset ([ StateGroup ( inputs = frozenset ([ self . _name ]), outputs = frozenset ([ self . _name ]))]) def apply ( self , group : dict [ str , torch . Tensor ]) -> dict [ str , torch . Tensor ]: tensor = group [ self . _name ] if not isinstance ( tensor , DTensor ): raise ValueError ( \"Cannot gather anything but DTensor\" ) return { self . _name : tensor . full_tensor ()} ModelStateMapperIdentity Bases: ModelStateMapper Passes a single state tensor through unchanged. Source code in d9d/model_state/mapper/leaf/identity.py 6 7 8 9 10 11 12 13 14 15 16 17 18 class ModelStateMapperIdentity ( ModelStateMapper ): \"\"\" Passes a single state tensor through unchanged. \"\"\" def __init__ ( self , name : str ): self . _name = name def state_dependency_groups ( self ) -> frozenset [ StateGroup ]: return frozenset ([ StateGroup ( inputs = frozenset ([ self . _name ]), outputs = frozenset ([ self . _name ]))]) def apply ( self , group : dict [ str , torch . Tensor ]) -> dict [ str , torch . Tensor ]: return group ModelStateMapperRename Bases: ModelStateMapper Renames a single state tensor from name_from to name_to . Source code in d9d/model_state/mapper/leaf/rename.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class ModelStateMapperRename ( ModelStateMapper ): \"\"\" Renames a single state tensor from `name_from` to `name_to`. \"\"\" def __init__ ( self , name_from : str , name_to : str ): self . _name_from = name_from self . _name_to = name_to def state_dependency_groups ( self ) -> frozenset [ StateGroup ]: return frozenset ([ StateGroup ( inputs = frozenset ([ self . _name_from ]), outputs = frozenset ([ self . _name_to ]))]) def apply ( self , group : dict [ str , torch . Tensor ]) -> dict [ str , torch . Tensor ]: return { self . _name_to : group [ self . _name_from ]} ModelStateMapperSelectChildModules Bases: ModelStateMapper Selects a set of keys belonging to a specific parent module (prefix) and renames them by removing that prefix. This is effectively a batch rename operation that \"hoists\" parameters from a submodule scope to the current scope. Source code in d9d/model_state/mapper/leaf/select_child.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class ModelStateMapperSelectChildModules ( ModelStateMapper ): \"\"\" Selects a set of keys belonging to a specific parent module (prefix) and renames them by removing that prefix. This is effectively a batch rename operation that \"hoists\" parameters from a submodule scope to the current scope. \"\"\" def __init__ ( self , base_names : list [ str ], parent_name : str ): self . _base_names = base_names self . _parent_prefix = f \" { parent_name } .\" def state_dependency_groups ( self ) -> frozenset [ StateGroup ]: return frozenset ( [ StateGroup ( inputs = frozenset ([ self . _parent_prefix + name ]), outputs = frozenset ([ name ])) for name in self . _base_names ] ) def apply ( self , group : dict [ str , torch . Tensor ]) -> dict [ str , torch . Tensor ]: name , value = next ( iter ( group . items ())) if name . startswith ( self . _parent_prefix ): return { name [ len ( self . _parent_prefix ) :]: value } else : return {} ModelStateMapperStackTensors Bases: ModelStateMapper Stacks multiple input tensors with names source_names into a single output tensor with name target_name producing new stack_dim dimension. Source code in d9d/model_state/mapper/leaf/stack.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class ModelStateMapperStackTensors ( ModelStateMapper ): \"\"\" Stacks multiple input tensors with names `source_names` into a single output tensor with name `target_name` producing new `stack_dim` dimension. \"\"\" def __init__ ( self , source_names : list [ str ], target_name : str , stack_dim : int ): self . _source_names = source_names self . _target_name = target_name self . _stack_dim = stack_dim def state_dependency_groups ( self ) -> frozenset [ StateGroup ]: return frozenset ([ StateGroup ( inputs = frozenset ( self . _source_names ), outputs = frozenset ([ self . _target_name ]))]) def apply ( self , group : dict [ str , torch . Tensor ]) -> dict [ str , torch . Tensor ]: source_tensors = [ group [ name ] for name in self . _source_names ] return { self . _target_name : torch . stack ( source_tensors , dim = self . _stack_dim )}","title":"Model State Mapper"},{"location":"model_states/mapper/#about","text":"The d9d.model_state.mapper package solves the complexity of working with model checkpoints by providing a declarative, graph-based framework for transforming model states .","title":"About"},{"location":"model_states/mapper/#core-concept","text":"Loading large-scale models is rarely a simple 1-to-1 key matching operation. You often face challenges such as: Naming Mismatches : HuggingFace uses model.layers.0 , your custom model uses transformer.h.0 . Shape Mismatches : The checkpoint stores Q , K , and V separately, but your model implementation expects a stacked QKV tensor. Scale : The checkpoint is 500GB. You cannot load the whole dictionary on every GPU to process it. Instead of writing a manual loop that loads tensors and blindly modifies them, this framework treats state transformation as a Directed Acyclic Graph (DAG) . Such a declarative approach makes it available for d9d to perform complex transform-save and transform-load operations effectively in a streamed manner without loading the whole checkpoint into memory.","title":"Core Concept"},{"location":"model_states/mapper/#usage-examples","text":"","title":"Usage Examples"},{"location":"model_states/mapper/#pass-through-mapping-for-pytorch-module","text":"If you simply want to load a checkpoint where keys match the model definition (standard load_state_dict behavior), but want to utilize d9d's streaming/sharding capabilities. import torch.nn as nn from d9d.model_state.mapper.adapters import identity_mapper_from_module # Define your PyTorch model model = nn . Sequential ( nn . Linear ( 10 , 10 ), nn . ReLU (), nn . Linear ( 10 , 5 ) ) # Automatically generate a mapper based on the model's actual parameter names # This creates Identity mappers for \"0.weight\", \"0.bias\", \"2.weight\", \"2.bias\" mapper = identity_mapper_from_module ( model )","title":"Pass-through Mapping for PyTorch Module"},{"location":"model_states/mapper/#using-leaf-mappers","text":"This example demonstrates using leaf mappers to handle common mismatch scenario: merging separate Query/Key/Value tensors into a single tensor. import torch from d9d.model_state.mapper.leaf import ( ModelStateMapperRename , ModelStateMapperStackTensors ) # Stacking Tensors # Scenario: Checkpoint has separate Q, K, V linear layers, we need one QKV tensor stack_mapper = ModelStateMapperStackTensors ( source_names = [ \"attn.q.weight\" , \"attn.k.weight\" , \"attn.v.weight\" ], target_name = \"attn.qkv.weight\" , stack_dim = 0 ) # To show what this mapper needs: print ( stack_mapper . state_dependency_groups ()) # Output: {StateGroup(inputs={'attn.q.weight', ...}, outputs={'attn.qkv.weight'})} # To actually execute: dummy_data = { \"attn.q.weight\" : torch . randn ( 64 , 64 ), \"attn.k.weight\" : torch . randn ( 64 , 64 ), \"attn.v.weight\" : torch . randn ( 64 , 64 ), } result = stack_mapper . apply ( dummy_data ) print ( result [ \"attn.qkv.weight\" ] . shape ) # Output: torch.Size([3, 64, 64])","title":"Using Leaf Mappers"},{"location":"model_states/mapper/#composing-complex-pipelines","text":"Converting an entire model state requires processing multiple keys in parallel, and potentially chaining operations (e.g., Rename then Stack). from d9d.model_state.mapper.compose import ModelStateMapperSequential , ModelStateMapperParallel from d9d.model_state.mapper.leaf import ModelStateMapperRename , ModelStateMapperStackTensors # Define a transformation pipeline mapper = ModelStateMapperSequential ([ # Step 1: Rename keys to standard format ModelStateMapperParallel ([ ModelStateMapperRename ( \"bert.encoder.layer.0.attention.self.query.weight\" , \"layer.0.q\" ), ModelStateMapperRename ( \"bert.encoder.layer.0.attention.self.key.weight\" , \"layer.0.k\" ), ModelStateMapperRename ( \"bert.encoder.layer.0.attention.self.value.weight\" , \"layer.0.v\" ), ]), # Step 2: Stack them into a specialized attention tensor ModelStateMapperStackTensors ( source_names = [ \"layer.0.q\" , \"layer.0.k\" , \"layer.0.v\" ], target_name = \"layer.0.qkv\" , stack_dim = 0 ) ])","title":"Composing Complex Pipelines"},{"location":"model_states/mapper/#d9d.model_state.mapper","text":"This package provides core components of the state mapping system.","title":"mapper"},{"location":"model_states/mapper/#d9d.model_state.mapper.ModelStateMapper","text":"Bases: ABC The abstract base class for all model state transformation operations. This class serves as the interface between the definition of a transformation topology and the actual execution of tensor operations. It enforces a Declarative vs. Imperative separation of concerns: Declarative (Topology): Through state_dependency_groups() , the mapper announces what it intends to do without handling any data. This allows the system to build execution graphs, validate chains, detect collisions, and shard tasks before allocating memory. Imperative (Execution): Through apply() , the mapper performs the actual logic (PyTorch operations) on model states. Source code in d9d/model_state/mapper/abc.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 class ModelStateMapper ( abc . ABC ): \"\"\" The abstract base class for all model state transformation operations. This class serves as the interface between the definition of a transformation topology and the actual execution of tensor operations. It enforces a Declarative vs. Imperative separation of concerns: 1. Declarative (Topology): Through `state_dependency_groups()`, the mapper announces *what* it intends to do without handling any data. This allows the system to build execution graphs, validate chains, detect collisions, and shard tasks *before* allocating memory. 2. Imperative (Execution): Through `apply()`, the mapper performs the actual logic (PyTorch operations) on model states. \"\"\" @abc . abstractmethod def state_dependency_groups ( self ) -> frozenset [ StateGroup ]: \"\"\" Calculates and returns the set of independent dependency groups this mapper handles. Returns: A frozenset of `StateGroup` objects. Each group represents a disjoint operation. For example, a mapper that renames ten independent tensors would return ten distinct `StateGroup` objects, allowing them to be sharded or processed individually. \"\"\" ... @abc . abstractmethod def apply ( self , group : dict [ str , torch . Tensor ]) -> dict [ str , torch . Tensor ]: \"\"\" Executes the transformation logic on a specific dictionary of tensors. The orchestration system guarantees that the `group` dictionary passed here contains all keys listed in the `inputs` of the active `StateGroup`. Implementation of this method should guarantee that the result will contain all keys listed in the `outputs`. Args: group: A dictionary containing the source data. Keys match `StateGroup.inputs`. Returns: A dictionary containing the transformed data. Keys must strictly match `StateGroup.outputs`. \"\"\" ...","title":"ModelStateMapper"},{"location":"model_states/mapper/#d9d.model_state.mapper.ModelStateMapper.apply","text":"Executes the transformation logic on a specific dictionary of tensors. The orchestration system guarantees that the group dictionary passed here contains all keys listed in the inputs of the active StateGroup . Implementation of this method should guarantee that the result will contain all keys listed in the outputs . Parameters: Name Type Description Default group dict [ str , Tensor ] A dictionary containing the source data. Keys match StateGroup.inputs . required Returns: Type Description dict [ str , Tensor ] A dictionary containing the transformed data. Keys must strictly match StateGroup.outputs . Source code in d9d/model_state/mapper/abc.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 @abc . abstractmethod def apply ( self , group : dict [ str , torch . Tensor ]) -> dict [ str , torch . Tensor ]: \"\"\" Executes the transformation logic on a specific dictionary of tensors. The orchestration system guarantees that the `group` dictionary passed here contains all keys listed in the `inputs` of the active `StateGroup`. Implementation of this method should guarantee that the result will contain all keys listed in the `outputs`. Args: group: A dictionary containing the source data. Keys match `StateGroup.inputs`. Returns: A dictionary containing the transformed data. Keys must strictly match `StateGroup.outputs`. \"\"\" ...","title":"apply"},{"location":"model_states/mapper/#d9d.model_state.mapper.ModelStateMapper.state_dependency_groups","text":"Calculates and returns the set of independent dependency groups this mapper handles. Returns: Type Description frozenset [ StateGroup ] A frozenset of StateGroup objects. Each group frozenset [ StateGroup ] represents a disjoint operation. For example, a mapper that renames ten frozenset [ StateGroup ] independent tensors would return ten distinct StateGroup objects, frozenset [ StateGroup ] allowing them to be sharded or processed individually. Source code in d9d/model_state/mapper/abc.py 40 41 42 43 44 45 46 47 48 49 50 51 @abc . abstractmethod def state_dependency_groups ( self ) -> frozenset [ StateGroup ]: \"\"\" Calculates and returns the set of independent dependency groups this mapper handles. Returns: A frozenset of `StateGroup` objects. Each group represents a disjoint operation. For example, a mapper that renames ten independent tensors would return ten distinct `StateGroup` objects, allowing them to be sharded or processed individually. \"\"\" ...","title":"state_dependency_groups"},{"location":"model_states/mapper/#d9d.model_state.mapper.StateGroup","text":"Represents an atomic unit of dependency in the model state transformation graph. A StateGroup defines a strict contract between a set of input keys (source) and a set of output keys (destination). Attributes: Name Type Description inputs frozenset [ str ] The complete set of keys required from the source state dictionary to satisfy this dependency. outputs frozenset [ str ] The complete set of keys that will be produced as a result of this transformation. Source code in d9d/model_state/mapper/abc.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 @dataclasses . dataclass ( frozen = True ) class StateGroup : \"\"\" Represents an atomic unit of dependency in the model state transformation graph. A `StateGroup` defines a strict contract between a set of input keys (source) and a set of output keys (destination). Attributes: inputs: The complete set of keys required from the source state dictionary to satisfy this dependency. outputs: The complete set of keys that will be produced as a result of this transformation. \"\"\" inputs : frozenset [ str ] outputs : frozenset [ str ]","title":"StateGroup"},{"location":"model_states/mapper/#d9d.model_state.mapper.adapters","text":"This package provides utility functions that are used to create simple ModelStateMapper instances from objects such as PyTorch modules or other StateMappers","title":"adapters"},{"location":"model_states/mapper/#d9d.model_state.mapper.adapters.identity_mapper_from_mapper_outputs","text":"Creates an identity mapper covering all outputs produced by the provided mapper. This function inspects the state_dependency_groups() of the input mapper , extracts every key listed in the outputs set of each group, and creates a corresponding ModelStateMapperIdentity for it. Parameters: Name Type Description Default mapper ModelStateMapper The mapper whose output signature will be inspected to generate the new identity mapper. required Returns: Type Description ModelStateMapper A composite mapper that acts as a pass-through for every key produced by the source mapper . Source code in d9d/model_state/mapper/adapters/mapper.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def identity_mapper_from_mapper_outputs ( mapper : ModelStateMapper ) -> ModelStateMapper : \"\"\" Creates an identity mapper covering all outputs produced by the provided mapper. This function inspects the `state_dependency_groups()` of the input `mapper`, extracts every key listed in the `outputs` set of each group, and creates a corresponding `ModelStateMapperIdentity` for it. Args: mapper: The mapper whose output signature will be inspected to generate the new identity mapper. Returns: A composite mapper that acts as a pass-through for every key produced by the source `mapper`. \"\"\" mappers : list [ ModelStateMapper ] = [] for state_group in mapper . state_dependency_groups (): for output_name in state_group . outputs : mappers . append ( ModelStateMapperIdentity ( output_name )) return ModelStateMapperParallel ( mappers )","title":"identity_mapper_from_mapper_outputs"},{"location":"model_states/mapper/#d9d.model_state.mapper.adapters.identity_mapper_from_module","text":"Creates an identity mapper for every parameter in a single PyTorch module. It is useful when you want to define a \"pass-through\" pipeline where the source checkpoint keys are expected to exactly match the model's current parameter names (standard load_state_dict behavior). Parameters: Name Type Description Default module Module The instantiated PyTorch model to inspect. required Source code in d9d/model_state/mapper/adapters/module.py 8 9 10 11 12 13 14 15 16 17 18 19 20 def identity_mapper_from_module ( module : nn . Module ) -> ModelStateMapper : \"\"\" Creates an identity mapper for every parameter in a single PyTorch module. It is useful when you want to define a \"pass-through\" pipeline where the source checkpoint keys are expected to exactly match the model's current parameter names (standard `load_state_dict` behavior). Args: module: The instantiated PyTorch model to inspect. \"\"\" return ModelStateMapperParallel ([ ModelStateMapperIdentity ( key ) for key in module . state_dict ()])","title":"identity_mapper_from_module"},{"location":"model_states/mapper/#d9d.model_state.mapper.compose","text":"Complex state mappers are built using composition. This package provides ModelStateMapper implementations that are composed of other mappers.","title":"compose"},{"location":"model_states/mapper/#d9d.model_state.mapper.compose.ModelStateMapperParallel","text":"Bases: ModelStateMapper Executes a list of states mappers independently alongside each other. This class aggregates multiple mappers into a single logical unit. It enforces strict isolation between the mappers: no two mappers can consume the same input key (input collision) or produce the same output key (output collision). During execution ( apply ), it routes the specific subset of the input dictionary to the sub-mapper responsible for those keys. Source code in d9d/model_state/mapper/compose/parallel.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 class ModelStateMapperParallel ( ModelStateMapper ): \"\"\" Executes a list of states mappers independently alongside each other. This class aggregates multiple mappers into a single logical unit. It enforces strict isolation between the mappers: no two mappers can consume the same input key (input collision) or produce the same output key (output collision). During execution (`apply`), it routes the specific subset of the input dictionary to the sub-mapper responsible for those keys. \"\"\" def __init__ ( self , mappers : Sequence [ ModelStateMapper ]): mappers_lst = filter_empty_mappers ( mappers ) all_groups = set () inputs_to_mapper = {} seen_inputs : set [ str ] = set () seen_outputs : set [ str ] = set () for mapper in mappers_lst : sub_groups = mapper . state_dependency_groups () for sub_group in sub_groups : if not seen_inputs . isdisjoint ( sub_group . inputs ): raise ValueError ( f \"Found a colliding input group: { sub_group . inputs } \" ) seen_inputs . update ( sub_group . inputs ) if not seen_outputs . isdisjoint ( sub_group . outputs ): raise ValueError ( f \"Found colliding output keys: { sub_group . outputs } \" ) seen_outputs . update ( sub_group . outputs ) all_groups . add ( sub_group ) inputs_to_mapper [ sub_group . inputs ] = mapper self . _all_groups = frozenset ( all_groups ) self . _inputs_to_mapper = inputs_to_mapper def state_dependency_groups ( self ) -> frozenset [ StateGroup ]: return self . _all_groups def apply ( self , group : dict [ str , torch . Tensor ]) -> dict [ str , torch . Tensor ]: group_keys = frozenset ( group . keys ()) if group_keys not in self . _inputs_to_mapper : raise ValueError ( \"Tried to run a parallel mapper with undefined group. Perhaps you sent groups that are not isolated?\" ) return self . _inputs_to_mapper [ group_keys ] . apply ( group )","title":"ModelStateMapperParallel"},{"location":"model_states/mapper/#d9d.model_state.mapper.compose.ModelStateMapperSequential","text":"Bases: ModelStateMapper Executes a list of mappers in a specific sequence (pipeline). This class manages the data flow from one mapper to the next. It abstracts away intermediate states, exposing only the inputs required by the first relevant stage and the outputs produced by the final relevant stage. Key Features: Gap Filling : Automatically injects Identity mappers if a tensor needs to pass through a stage without modification to reach a later stage or the final output. Group Merging : Computes the net dependency graph. If Stage A requires 'x' and produces 'y', and Stage B requires 'y' and produces 'z', the Sequential mapper reports a single group {x} -> {z} . Source code in d9d/model_state/mapper/compose/sequential.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 class ModelStateMapperSequential ( ModelStateMapper ): \"\"\" Executes a list of mappers in a specific sequence (pipeline). This class manages the data flow from one mapper to the next. It abstracts away intermediate states, exposing only the inputs required by the first relevant stage and the outputs produced by the final relevant stage. Key Features: 1. **Gap Filling**: Automatically injects `Identity` mappers if a tensor needs to pass through a stage without modification to reach a later stage or the final output. 2. **Group Merging**: Computes the net dependency graph. If Stage A requires 'x' and produces 'y', and Stage B requires 'y' and produces 'z', the Sequential mapper reports a single group `{x} -> {z}`. \"\"\" def __init__ ( self , mappers : list [ ModelStateMapper ]): mappers = filter_empty_mappers ( mappers ) if not mappers : raise ValueError ( \"Mappers list cannot be empty.\" ) mappers = self . _fill_gaps ( mappers ) self . _groups = self . _compute_pipeline_groups ( mappers ) self . _mappers = mappers @staticmethod def _fill_gaps ( mappers : list [ ModelStateMapper ]) -> list [ ModelStateMapper ]: mappers = mappers . copy () # propagate inputs from bottom to top for stage_i in range ( 1 , len ( mappers ))[:: - 1 ]: groups_current = mappers [ stage_i ] . state_dependency_groups () groups_prev = mappers [ stage_i - 1 ] . state_dependency_groups () current_stage_requires = frozenset . union ( * ( x . inputs for x in groups_current )) prev_stage_produces = frozenset . union ( * ( x . outputs for x in groups_prev )) needs_to_pass_through = current_stage_requires - prev_stage_produces mappers [ stage_i - 1 ] = ModelStateMapperParallel ( [ mappers [ stage_i - 1 ]] + [ ModelStateMapperIdentity ( x ) for x in needs_to_pass_through ] ) # propagate outputs from top to bottom for stage_i in range ( 0 , len ( mappers ) - 1 ): groups_current = mappers [ stage_i ] . state_dependency_groups () groups_next = mappers [ stage_i + 1 ] . state_dependency_groups () current_stage_produces = frozenset . union ( * ( x . outputs for x in groups_current )) next_stage_requires = frozenset . union ( * ( x . inputs for x in groups_next )) needs_to_pass_through = current_stage_produces - next_stage_requires mappers [ stage_i + 1 ] = ModelStateMapperParallel ( [ mappers [ stage_i + 1 ]] + [ ModelStateMapperIdentity ( x ) for x in needs_to_pass_through ] ) return mappers @staticmethod def _compute_pipeline_groups ( mappers : list [ ModelStateMapper ]) -> frozenset [ StateGroup ]: outputs_depend_on_inputs = {} # given a fully connected graph, we can just go upwards for last_group_traced in mappers [ - 1 ] . state_dependency_groups (): required_inputs = last_group_traced . inputs for mapper_i in range ( 0 , len ( mappers ) - 1 )[:: - 1 ]: next_visit_groups = [ x for x in mappers [ mapper_i ] . state_dependency_groups () if not x . outputs . isdisjoint ( required_inputs ) ] required_inputs = frozenset . union ( * ( x . inputs for x in next_visit_groups )) outputs_depend_on_inputs [ last_group_traced . outputs ] = required_inputs return ModelStateMapperSequential . _merge_groups ( list ( outputs_depend_on_inputs . items ())) @staticmethod def _merge_groups ( groups : Sequence [ tuple [ AbstractSet [ str ], AbstractSet [ str ]]]) -> frozenset [ StateGroup ]: saved_groups : list [ tuple [ set [ str ], set [ str ]]] = [] saved_groups_modified = True while saved_groups_modified : saved_groups_modified = False for output_names , input_names in groups : was_new_group_created = False for group in saved_groups : if group [ 0 ] . intersection ( input_names ) or group [ 1 ] . intersection ( output_names ): group [ 0 ] . update ( input_names ) group [ 1 ] . update ( output_names ) was_new_group_created = True saved_groups_modified = True if not was_new_group_created : saved_groups . append (( set ( input_names ), set ( output_names ))) groups = saved_groups saved_groups = [] return frozenset ( StateGroup ( inputs = frozenset ( x [ 0 ]), outputs = frozenset ( x [ 1 ])) for x in groups ) def state_dependency_groups ( self ) -> frozenset [ StateGroup ]: return self . _groups def apply ( self , group : dict [ str , torch . Tensor ]) -> dict [ str , torch . Tensor ]: current_state = group next_state = {} for mapper in self . _mappers : for deps in mapper . state_dependency_groups (): if not deps . inputs <= current_state . keys (): continue next_state . update ( mapper . apply ({ k : v for k , v in current_state . items () if k in deps . inputs })) current_state = next_state next_state = {} return current_state","title":"ModelStateMapperSequential"},{"location":"model_states/mapper/#d9d.model_state.mapper.compose.ModelStateMapperShard","text":"Bases: ModelStateMapper Wraps another state mapper and restricts its execution to a specific subset (shard) of dependency groups. This is primarily used for parallelizing model loading across multiple processes or nodes. By assigning a different current_shard index to each process, the total set of tensors required by the sub_mapper is split evenly, preventing every process from loading the entire checkpoint. Source code in d9d/model_state/mapper/compose/shard.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class ModelStateMapperShard ( ModelStateMapper ): \"\"\" Wraps another state mapper and restricts its execution to a specific subset (shard) of dependency groups. This is primarily used for parallelizing model loading across multiple processes or nodes. By assigning a different `current_shard` index to each process, the total set of tensors required by the `sub_mapper` is split evenly, preventing every process from loading the entire checkpoint. \"\"\" def __init__ ( self , sub_mapper : ModelStateMapper , total_shards : int , current_shard : int ): self . _groups = self . _shard_groups ( sub_mapper . state_dependency_groups (), n_shards = total_shards , shard = current_shard ) self . _sub_mapper = sub_mapper self . _total_shards = total_shards self . _current_shard = current_shard @staticmethod def _shard_groups ( groups : frozenset [ StateGroup ], n_shards : int , shard : int ) -> frozenset [ StateGroup ]: groups_sorted = sorted ( groups , key = lambda x : sorted ( x . inputs )) groups_shard = [ x for i , x in enumerate ( groups_sorted ) if i % n_shards == shard ] return frozenset ( groups_shard ) def state_dependency_groups ( self ) -> frozenset [ StateGroup ]: return self . _groups def apply ( self , group : dict [ str , torch . Tensor ]) -> dict [ str , torch . Tensor ]: return self . _sub_mapper . apply ( group )","title":"ModelStateMapperShard"},{"location":"model_states/mapper/#d9d.model_state.mapper.compose.filter_empty_mappers","text":"Filters out mappers that have no effect (no inputs and no outputs). Parameters: Name Type Description Default mappers Sequence [ ModelStateMapper ] The list of mappers to filter. required Returns: Type Description list [ ModelStateMapper ] A new list containing only active mappers. Source code in d9d/model_state/mapper/compose/helper.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def filter_empty_mappers ( mappers : Sequence [ ModelStateMapper ]) -> list [ ModelStateMapper ]: \"\"\" Filters out mappers that have no effect (no inputs and no outputs). Args: mappers: The list of mappers to filter. Returns: A new list containing only active mappers. \"\"\" result = [] for mapper in mappers : for group in mapper . state_dependency_groups (): if len ( group . inputs ) > 0 or len ( group . outputs ) > 0 : result . append ( mapper ) break return result","title":"filter_empty_mappers"},{"location":"model_states/mapper/#d9d.model_state.mapper.leaf","text":"This package provides leaf mapper implementations.","title":"leaf"},{"location":"model_states/mapper/#d9d.model_state.mapper.leaf.ModelStateMapperDistribute","text":"Bases: ModelStateMapper Converts a single local Tensor object into a DTensor object with specified device_mesh and placements . Source code in d9d/model_state/mapper/leaf/dtensor.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 class ModelStateMapperDistribute ( ModelStateMapper ): \"\"\" Converts a single local Tensor object into a DTensor object with specified `device_mesh` and `placements`. \"\"\" def __init__ ( self , name : str , device_mesh : DeviceMesh | None , placements : Sequence [ Placement ] | None ): self . _name = name self . _device_mesh = device_mesh self . _placements = placements def state_dependency_groups ( self ) -> frozenset [ StateGroup ]: return frozenset ([ StateGroup ( inputs = frozenset ([ self . _name ]), outputs = frozenset ([ self . _name ]))]) def apply ( self , group : dict [ str , torch . Tensor ]) -> dict [ str , torch . Tensor ]: return { self . _name : distribute_tensor ( group [ self . _name ], device_mesh = self . _device_mesh , placements = self . _placements , src_data_rank = None , # do not communicate here ) }","title":"ModelStateMapperDistribute"},{"location":"model_states/mapper/#d9d.model_state.mapper.leaf.ModelStateMapperGatherFullTensor","text":"Bases: ModelStateMapper Gathers a single DTensor object into a full Tensor object. Source code in d9d/model_state/mapper/leaf/dtensor.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 class ModelStateMapperGatherFullTensor ( ModelStateMapper ): \"\"\" Gathers a single DTensor object into a full Tensor object. \"\"\" def __init__ ( self , name : str ): self . _name = name def state_dependency_groups ( self ) -> frozenset [ StateGroup ]: return frozenset ([ StateGroup ( inputs = frozenset ([ self . _name ]), outputs = frozenset ([ self . _name ]))]) def apply ( self , group : dict [ str , torch . Tensor ]) -> dict [ str , torch . Tensor ]: tensor = group [ self . _name ] if not isinstance ( tensor , DTensor ): raise ValueError ( \"Cannot gather anything but DTensor\" ) return { self . _name : tensor . full_tensor ()}","title":"ModelStateMapperGatherFullTensor"},{"location":"model_states/mapper/#d9d.model_state.mapper.leaf.ModelStateMapperIdentity","text":"Bases: ModelStateMapper Passes a single state tensor through unchanged. Source code in d9d/model_state/mapper/leaf/identity.py 6 7 8 9 10 11 12 13 14 15 16 17 18 class ModelStateMapperIdentity ( ModelStateMapper ): \"\"\" Passes a single state tensor through unchanged. \"\"\" def __init__ ( self , name : str ): self . _name = name def state_dependency_groups ( self ) -> frozenset [ StateGroup ]: return frozenset ([ StateGroup ( inputs = frozenset ([ self . _name ]), outputs = frozenset ([ self . _name ]))]) def apply ( self , group : dict [ str , torch . Tensor ]) -> dict [ str , torch . Tensor ]: return group","title":"ModelStateMapperIdentity"},{"location":"model_states/mapper/#d9d.model_state.mapper.leaf.ModelStateMapperRename","text":"Bases: ModelStateMapper Renames a single state tensor from name_from to name_to . Source code in d9d/model_state/mapper/leaf/rename.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class ModelStateMapperRename ( ModelStateMapper ): \"\"\" Renames a single state tensor from `name_from` to `name_to`. \"\"\" def __init__ ( self , name_from : str , name_to : str ): self . _name_from = name_from self . _name_to = name_to def state_dependency_groups ( self ) -> frozenset [ StateGroup ]: return frozenset ([ StateGroup ( inputs = frozenset ([ self . _name_from ]), outputs = frozenset ([ self . _name_to ]))]) def apply ( self , group : dict [ str , torch . Tensor ]) -> dict [ str , torch . Tensor ]: return { self . _name_to : group [ self . _name_from ]}","title":"ModelStateMapperRename"},{"location":"model_states/mapper/#d9d.model_state.mapper.leaf.ModelStateMapperSelectChildModules","text":"Bases: ModelStateMapper Selects a set of keys belonging to a specific parent module (prefix) and renames them by removing that prefix. This is effectively a batch rename operation that \"hoists\" parameters from a submodule scope to the current scope. Source code in d9d/model_state/mapper/leaf/select_child.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class ModelStateMapperSelectChildModules ( ModelStateMapper ): \"\"\" Selects a set of keys belonging to a specific parent module (prefix) and renames them by removing that prefix. This is effectively a batch rename operation that \"hoists\" parameters from a submodule scope to the current scope. \"\"\" def __init__ ( self , base_names : list [ str ], parent_name : str ): self . _base_names = base_names self . _parent_prefix = f \" { parent_name } .\" def state_dependency_groups ( self ) -> frozenset [ StateGroup ]: return frozenset ( [ StateGroup ( inputs = frozenset ([ self . _parent_prefix + name ]), outputs = frozenset ([ name ])) for name in self . _base_names ] ) def apply ( self , group : dict [ str , torch . Tensor ]) -> dict [ str , torch . Tensor ]: name , value = next ( iter ( group . items ())) if name . startswith ( self . _parent_prefix ): return { name [ len ( self . _parent_prefix ) :]: value } else : return {}","title":"ModelStateMapperSelectChildModules"},{"location":"model_states/mapper/#d9d.model_state.mapper.leaf.ModelStateMapperStackTensors","text":"Bases: ModelStateMapper Stacks multiple input tensors with names source_names into a single output tensor with name target_name producing new stack_dim dimension. Source code in d9d/model_state/mapper/leaf/stack.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class ModelStateMapperStackTensors ( ModelStateMapper ): \"\"\" Stacks multiple input tensors with names `source_names` into a single output tensor with name `target_name` producing new `stack_dim` dimension. \"\"\" def __init__ ( self , source_names : list [ str ], target_name : str , stack_dim : int ): self . _source_names = source_names self . _target_name = target_name self . _stack_dim = stack_dim def state_dependency_groups ( self ) -> frozenset [ StateGroup ]: return frozenset ([ StateGroup ( inputs = frozenset ( self . _source_names ), outputs = frozenset ([ self . _target_name ]))]) def apply ( self , group : dict [ str , torch . Tensor ]) -> dict [ str , torch . Tensor ]: source_tensors = [ group [ name ] for name in self . _source_names ] return { self . _target_name : torch . stack ( source_tensors , dim = self . _stack_dim )}","title":"ModelStateMapperStackTensors"},{"location":"models/1_model_design/","text":"Bring Your own Model d9d does not enforce you to use its model implementations. You are eager to use own custom implementations of any model you want, optionally using high-performant d9d's building blocks. Just make sure to follow the main design principles described below. Main Principles d9d opts for a \"white-box\" approach to modelling. We avoid heavy abstraction layers in favor of readable, standard PyTorch code. No LayerSpecs Some distributed frameworks force users to define models via metadata specification objects to inject wrapping logic (like FSDP or Checkpointing) automatically. This makes debugging difficult. In d9d, you write standard nn.Module classes. Use nn.linear , nn.RMSNorm , or d9d's optimized blocks directly. Distributed wrapping logic is handled transparently, maintaining the standard PyTorch look and feel. Explicit Composition We avoid creating \"Uber-Modules\" - single, massive classes (e.g., GenericTransformerBlock ) that handle every possible architectural variation (MoE, Dense, Post-Norm, Pre-Norm, Parallel Dense-Attention) via dozens of flags and parameters. Instead, d9d promotes explicit composition like HuggingFace Transformers does. This composition makes the call stack distinct and the logic for a specific architecture easy to trace. Pipelining-Aware Models Please see Pipelining API . Late Initialization Constructing a large model on a single GPU (or even CPU RAM) often leads to immediate Out-Of-Memory (OOM) errors. d9d solves this via the ModuleLateInit protocol. It is safe to use modules implementing this protocol with d9d's native Trainer framework. The Trainer will instantiate modules on the meta device (consuming no memory), lay out the distributed topology and sharding strategy. Only then reset_parameters() is called to materialize model weights without allocating unnecessary things. Reference Implementations For reference implementations, please see Qwen3-MoE . d9d.module.base Defines structural protocols and base classes for PyTorch modules used within the d9d framework. ModuleLateInit Bases: Protocol Protocol for modules that support late parameter initialization. Source code in d9d/module/base/late_init.py 5 6 7 8 9 10 @typing . runtime_checkable class ModuleLateInit ( Protocol ): \"\"\"Protocol for modules that support late parameter initialization.\"\"\" def reset_parameters ( self ): \"\"\"Resets the module parameters (i.e. performs random initialization).\"\"\" reset_parameters () Resets the module parameters (i.e. performs random initialization). Source code in d9d/module/base/late_init.py 9 10 def reset_parameters ( self ): \"\"\"Resets the module parameters (i.e. performs random initialization).\"\"\"","title":"Model Design"},{"location":"models/1_model_design/#bring-your-own-model","text":"d9d does not enforce you to use its model implementations. You are eager to use own custom implementations of any model you want, optionally using high-performant d9d's building blocks. Just make sure to follow the main design principles described below.","title":"Bring Your own Model"},{"location":"models/1_model_design/#main-principles","text":"d9d opts for a \"white-box\" approach to modelling. We avoid heavy abstraction layers in favor of readable, standard PyTorch code.","title":"Main Principles"},{"location":"models/1_model_design/#no-layerspecs","text":"Some distributed frameworks force users to define models via metadata specification objects to inject wrapping logic (like FSDP or Checkpointing) automatically. This makes debugging difficult. In d9d, you write standard nn.Module classes. Use nn.linear , nn.RMSNorm , or d9d's optimized blocks directly. Distributed wrapping logic is handled transparently, maintaining the standard PyTorch look and feel.","title":"No LayerSpecs"},{"location":"models/1_model_design/#explicit-composition","text":"We avoid creating \"Uber-Modules\" - single, massive classes (e.g., GenericTransformerBlock ) that handle every possible architectural variation (MoE, Dense, Post-Norm, Pre-Norm, Parallel Dense-Attention) via dozens of flags and parameters. Instead, d9d promotes explicit composition like HuggingFace Transformers does. This composition makes the call stack distinct and the logic for a specific architecture easy to trace.","title":"Explicit Composition"},{"location":"models/1_model_design/#pipelining-aware-models","text":"Please see Pipelining API .","title":"Pipelining-Aware Models"},{"location":"models/1_model_design/#late-initialization","text":"Constructing a large model on a single GPU (or even CPU RAM) often leads to immediate Out-Of-Memory (OOM) errors. d9d solves this via the ModuleLateInit protocol. It is safe to use modules implementing this protocol with d9d's native Trainer framework. The Trainer will instantiate modules on the meta device (consuming no memory), lay out the distributed topology and sharding strategy. Only then reset_parameters() is called to materialize model weights without allocating unnecessary things.","title":"Late Initialization"},{"location":"models/1_model_design/#reference-implementations","text":"For reference implementations, please see Qwen3-MoE .","title":"Reference Implementations"},{"location":"models/1_model_design/#d9d.module.base","text":"Defines structural protocols and base classes for PyTorch modules used within the d9d framework.","title":"base"},{"location":"models/1_model_design/#d9d.module.base.ModuleLateInit","text":"Bases: Protocol Protocol for modules that support late parameter initialization. Source code in d9d/module/base/late_init.py 5 6 7 8 9 10 @typing . runtime_checkable class ModuleLateInit ( Protocol ): \"\"\"Protocol for modules that support late parameter initialization.\"\"\" def reset_parameters ( self ): \"\"\"Resets the module parameters (i.e. performs random initialization).\"\"\"","title":"ModuleLateInit"},{"location":"models/1_model_design/#d9d.module.base.ModuleLateInit.reset_parameters","text":"Resets the module parameters (i.e. performs random initialization). Source code in d9d/module/base/late_init.py 9 10 def reset_parameters ( self ): \"\"\"Resets the module parameters (i.e. performs random initialization).\"\"\"","title":"reset_parameters"},{"location":"models/2_horizontal_parallelism/","text":"About The d9d.module.parallelism package provides high-level strategies for distributing model execution across device meshes. These strategies are \"Horizontal\" in the sense that they function within a specific stage of a pipeline (intra-layer parallelism), as opposed to Pipeline Parallelism which is \"Vertical\" (inter-layer). Design DTensor-First Architecture d9d enforces a DTensor-first philosophy. We mandate that every trainable parameter in the distributed environment be represented as a torch.distributed.tensor.DTensor . This constraint simplifies the system architecture significantly: Universal Checkpointing : The checkpointing engine does not need to know about specific parallel strategies (like \"This is DP\" or \"This is TP\"). It simply inspects the DTensor.placements attribute to automatically determine how to gather, deduplicate, and save tensors. Native Synchronization : Gradient synchronization for replicated parameters is handled entirely by the d9d internals , that now knows which tensor dimensions are Replicated. Composition over Monoliths We explicitly reject monolithic wrappers like torch.nn.parallel.DistributedDataParallel (DDP). While DDP is efficient for pure Data Parallelism, it acts as a \"black box\" that assumes ownership of the entire model execution loop. Instead, d9d relies on PyTorch's parallelize_module API . This allows for fine-grained, per-submodule parallelism decisions: Layer A can use Tensor Parallelism (Row/Col wise). Layer B (e.g., a Router) can use Replicate Parallelism . Layer C (e.g., MLP) can use Expert Parallelism . By treating \"Data Parallelism\" simply as another tiling strategy (\"Replicate\") within the Tensor Parallel system, we achieve a unified interface for ND parallelism. Strategies Replicate Parallelism parallelize_replicate implements Replicate Parallelism . It replicates parameters across the mesh. Used for Data Parallelism or Context Parallelism. During the forward pass, it installs hooks that temporarily \"unwrap\" DTensor parameters into standard, local torch.Tensor objects. This allows standard PyTorch operations and custom kernels to run without modification, while accessing module's state dict and parameters still yields DTensor objects. Expert Parallelism (MoE) Mixture of Experts (MoE) requires a unique parallel strategy where: 1. Experts are sharded across the ep_shard mesh dimension (each GPU holds a subset of experts), optionally replicating along ep_replicate . 2. Routers are replicated (all GPUs have the same routing logic). parallelize_expert_parallel applies sharding to MoELayer modules. It shards the GroupedLinear weights along the expert dimension. Simultaneously, it effectively applies parallelize_replicate to the router. Fully Sharded Data Parallel (FSDP) parallelize_fsdp provides a thin wrapper around PyTorch's native fully_shard . Difference from standard FSDP: Standard FSDP averages gradients across the mesh (Sum / WorldSize) by default. d9d's wrapper forces the gradients being summed rather than averaged . This is required for our gradient accumulation logic that is handled externally. parallelize_fsdp strictly requires a 1D DeviceMesh. To use it in multi-dimensional meshes (e.g., combining Replication and Sharding), use parallelize_hsdp or apply parallelize_replicate to the other dimensions manually first. Hybrid Sharded Data Parallel (HSDP) parallelize_hsdp is a high-level composite strategy for mixing Full Sharding with Replicate Parallel. parallelize_hsdp accepts a multi-dimensional mesh and a target shard_dim . It identifies all dimensions other than shard_dim as Replication Dimensions . It applies parallelize_replicate to the replication dimensions. It applies parallelize_fsdp to the specific sharding dimension. Usage Examples Replicate Parallelism import torch from d9d.core.dist_context import DistributedContext , DENSE_DOMAIN from d9d.module.parallelism.api import parallelize_replicate # 1. Create a Distributed Context ctx : DistributedContext = ... # 2. Get Dense Domain Mesh dense_mesh = ctx . mesh_for ( DENSE_DOMAIN ) # pp x dp_replicate x dp_cp_shard x cp_replicate x tp # 2. Define Model model = MyCustomLayer ( ... ) # 3. Parallelize parallelize_replicate ( model , dense_mesh [[ 'dp_replicate' , 'cp_replicate' ]]) Applying Expert Parallelism import torch from d9d.core.dist_context import DistributedContext , EXPERT_DOMAIN from d9d.module.parallelism.api import parallelize_expert_parallel from d9d.module.block.moe import MoELayer # 1. Create a Distributed Context ctx : DistributedContext = ... # 2. Get Expert Domain Mesh expert_mesh = ctx . mesh_for ( EXPERT_DOMAIN ) # pp x ep_replicate x ep_shard # 3. Define Model model = MoELayer ( ... ) # 4. Parallelize parallelize_expert_parallel ( model , mesh_experts = expert_mesh [[ 'ep_replicate' , 'ep_shard' ]], expert_shard_dim = 'ep_shard' ) Applying FSDP import torch from d9d.core.dist_context import DistributedContext , DENSE_DOMAIN from d9d.module.parallelism.api import parallelize_fsdp , parallelize_replicate # 1. Create a Distributed Context ctx : DistributedContext = ... # 2. Define Model model = MyCustomLayer ( ... ) # 3. Get Dense Domain Mesh dense_mesh = ctx . mesh_for ( DENSE_DOMAIN ) # 4. Parallelize parallelize_fsdp ( model , mesh = dense_mesh [ 'dp_cp_shard' ] ) Applying HSDP import torch from d9d.core.dist_context import DistributedContext , DENSE_DOMAIN from d9d.module.parallelism.api import parallelize_hsdp # 1. Create a Distributed Context ctx : DistributedContext = ... # 2. Get Mesh dense_mesh = ctx . mesh_for ( DENSE_DOMAIN ) # pp x dp_replicate x dp_cp_shard x cp_replicate x tp # 3. Define Model model = MyCustomLayer ( ... ) # 4. Parallelize parallelize_hsdp ( model , mesh = dense_mesh [ \"dp_replicate\" , \"dp_cp_shard\" , \"cp_replicate\" ], shard_dim = \"dp_cp_shard\" ) d9d.module.parallelism.api Horizontal parallelism strategies and utilities for d9d modules. This package provides high-level helper functions to apply specific distributed parallelism strategies to PyTorch modules compatible with the d9d ecosystem. parallelize_expert_parallel ( module , mesh_experts , expert_shard_dim = 'ep_shard' ) Applies Expert Parallelism to a MoE layer. This function configures the provided Mixture of Experts layer for distributed execution. It partitions the sparse experts across the specified dimension of the device mesh (Expert Parallelism) and replicates along other dims. Simultaneously, it configures the router to be fully replicated across the mesh. Parameters: Name Type Description Default module MoELayer The MoE layer instance to parallelize. required mesh_experts DeviceMesh The device mesh containing the expert parallel resources. required expert_shard_dim str The name of the mesh dimension where experts should be sharded. 'ep_shard' Source code in d9d/module/parallelism/api/expert_parallel.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def parallelize_expert_parallel ( module : MoELayer , mesh_experts : DeviceMesh , expert_shard_dim : str = \"ep_shard\" ): \"\"\" Applies Expert Parallelism to a MoE layer. This function configures the provided Mixture of Experts layer for distributed execution. It partitions the sparse experts across the specified dimension of the device mesh (Expert Parallelism) and replicates along other dims. Simultaneously, it configures the router to be fully replicated across the mesh. Args: module: The MoE layer instance to parallelize. mesh_experts: The device mesh containing the expert parallel resources. expert_shard_dim: The name of the mesh dimension where experts should be sharded. \"\"\" parallelize_module ( module , mesh_experts , ShardMoESparseExpertsParallel ( shard_dim_name = expert_shard_dim )) parallelize_module ( module . router , mesh_experts , ToLocalParallel ( param_placement = tuple ( Replicate () for _ in range ( mesh_experts . ndim )), grad_placement = tuple ( Replicate () for _ in range ( mesh_experts . ndim )), ), ) parallelize_fsdp ( module , mesh , * args , ** kwargs ) Applies Fully Sharded Data Parallel (FSDP) with forced gradient summation. This function wraps the provided module with PyTorch's fully_shard API using the specified device mesh. Unlike standard FSDP usage, this function explicitly configures the module to sum gradients across the mesh instead of averaging them and disables internal all-sum-reduce hooks. This is intended for d9d to handle gradient normalization and reduction across replicas externally. Parameters: Name Type Description Default module Module The module to shard. required mesh DeviceMesh The device mesh over which to shard the module. required *args Any Additional positional arguments passed to fully_shard . () **kwargs Any Additional keyword arguments passed to fully_shard . {} Source code in d9d/module/parallelism/api/fully_sharded.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def parallelize_fsdp ( module : nn . Module , mesh : DeviceMesh , * args : Any , ** kwargs : Any ): \"\"\" Applies Fully Sharded Data Parallel (FSDP) with forced gradient summation. This function wraps the provided module with PyTorch's ``fully_shard`` API using the specified device mesh. Unlike standard FSDP usage, this function explicitly configures the module to sum gradients across the mesh instead of averaging them and disables internal all-sum-reduce hooks. This is intended for d9d to handle gradient normalization and reduction across replicas externally. Args: module: The module to shard. mesh: The device mesh over which to shard the module. *args: Additional positional arguments passed to ``fully_shard``. **kwargs: Additional keyword arguments passed to ``fully_shard``. \"\"\" if mesh . ndim != 1 : raise ValueError ( \"FSDP mesh should contain exactly one dimension - for HSDP, please apply parallelize_replicate(...) first!\" ) fully_shard ( module , * args , mesh = mesh , ** kwargs ) if not isinstance ( module , FSDPModule ): raise RuntimeError ( \"Torch FSDP did not convert the module into FSDPModule\" ) _force_fsdp_grad_reduction_policy ( module ) parallelize_hsdp ( module , mesh , shard_dim = 'dp_cp_shard' , * fsdp_args , ** fsdp_kwargs ) Applies Hybrid Sharded Data Parallelism (HSDP) to a module. This function decomposes the provided device mesh into sharding dimensions and replication dimensions. It applies replication parallelism across the replication dimensions and Fully Sharded Data Parallelism (FSDP) across the specified shard dimension. Parameters: Name Type Description Default module Module The module to parallelize. required mesh DeviceMesh The device mesh over which to distribute the module. required shard_dim str The name of the mesh dimension used for FSDP sharding. Any dimension in the mesh not matching this name will be treated as a replication dimension. 'dp_cp_shard' *fsdp_args Any Positional arguments passed to the underlying FSDP parallelizer. () **fsdp_kwargs Any Keyword arguments passed to the underlying FSDP parallelizer. {} Raises: Type Description ValueError If the device mesh does not have named dimensions. Source code in d9d/module/parallelism/api/hybrid_sharded.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def parallelize_hsdp ( module : nn . Module , mesh : DeviceMesh , shard_dim : str = \"dp_cp_shard\" , * fsdp_args : Any , ** fsdp_kwargs : Any ): \"\"\" Applies Hybrid Sharded Data Parallelism (HSDP) to a module. This function decomposes the provided device mesh into sharding dimensions and replication dimensions. It applies replication parallelism across the replication dimensions and Fully Sharded Data Parallelism (FSDP) across the specified shard dimension. Args: module: The module to parallelize. mesh: The device mesh over which to distribute the module. shard_dim: The name of the mesh dimension used for FSDP sharding. Any dimension in the mesh not matching this name will be treated as a replication dimension. *fsdp_args: Positional arguments passed to the underlying FSDP parallelizer. **fsdp_kwargs: Keyword arguments passed to the underlying FSDP parallelizer. Raises: ValueError: If the device mesh does not have named dimensions. \"\"\" replicate_dims = mesh . mesh_dim_names if replicate_dims is None : raise ValueError ( \"Cannot use with unnamed device meshes\" ) replicate_dims = tuple ( x for x in replicate_dims if x != shard_dim and mesh [ x ] . size () > 1 ) if len ( replicate_dims ) > 0 : parallelize_replicate ( module , mesh [ replicate_dims ]) if mesh [ shard_dim ] . size () != 1 : parallelize_fsdp ( module , mesh [ shard_dim ], * fsdp_args , ** fsdp_kwargs ) parallelize_replicate ( module , mesh ) Applies replicated parallelism to the module. This function configures the provided module to be fully replicated across the given device mesh. It utilizes the ToLocalParallel style, which manages DTensor wrapping for parameters and gradients (via Replicate placements) while ensuring that the underlying computation sees standard local tensors during the forward pass. This approach is effectively Data Parallelism managed via the DTensor APIs, allowing seamless integration of modules that require local tensor inputs into a broader distributed mesh context. Parameters: Name Type Description Default module Module The module to parallelize. required mesh DeviceMesh The device mesh over which to replicate the module. required Source code in d9d/module/parallelism/api/replicate_parallel.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def parallelize_replicate ( module : nn . Module , mesh : DeviceMesh , ): \"\"\" Applies replicated parallelism to the module. This function configures the provided module to be fully replicated across the given device mesh. It utilizes the ``ToLocalParallel`` style, which manages ``DTensor`` wrapping for parameters and gradients (via ``Replicate`` placements) while ensuring that the underlying computation sees standard local tensors during the forward pass. This approach is effectively Data Parallelism managed via the DTensor APIs, allowing seamless integration of modules that require local tensor inputs into a broader distributed mesh context. Args: module: The module to parallelize. mesh: The device mesh over which to replicate the module. \"\"\" parallelize_module ( module , mesh , ToLocalParallel ( param_placement = tuple ( Replicate () for _ in range ( mesh . ndim )), grad_placement = tuple ( Replicate () for _ in range ( mesh . ndim )), ), ) d9d.module.parallelism.style ShardMoESparseExpertsParallel Bases: ParallelStyle Parallel style that shards MoE experts across a specific mesh dimension. This style is designed for MoELayer instances using GroupedLinear for experts. It splits the experts across the specified dimension of the device mesh (Expert Parallelism). Other dimensions in the mesh treat the parameters as Replicated. It also initializes the necessary distributed communication groups within the MoE layer to handle token dispatching. Source code in d9d/module/parallelism/style/shard_experts.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 class ShardMoESparseExpertsParallel ( ParallelStyle ): \"\"\" Parallel style that shards MoE experts across a specific mesh dimension. This style is designed for ``MoELayer`` instances using ``GroupedLinear`` for experts. It splits the experts across the specified dimension of the device mesh (Expert Parallelism). Other dimensions in the mesh treat the parameters as Replicated. It also initializes the necessary distributed communication groups within the MoE layer to handle token dispatching. \"\"\" def __init__ ( self , shard_dim_name : str ): self . _shard_dim_name = shard_dim_name def _partition_experts ( self , module_name : str , mod : nn . Module , device_mesh : DeviceMesh ): if not isinstance ( mod , GroupedLinear ): raise TypeError ( \"This plan should be applied only on GroupedLinear\" ) mesh_dim_names = device_mesh . mesh_dim_names if mesh_dim_names is None : raise ValueError ( \"This plan should be applied only on named DeviceMeshes\" ) placements = [ Shard ( 0 ) if dim_name == self . _shard_dim_name else Replicate () for dim_name in mesh_dim_names ] weight = nn . Parameter ( distribute_tensor ( mod . weight , device_mesh , placements ), requires_grad = mod . weight . requires_grad ) mod . weight = weight def _apply ( self , module : nn . Module , device_mesh : DeviceMesh ) -> nn . Module : if not isinstance ( module , MoELayer ): raise TypeError ( \"This plan should be applied only on MoELayer\" ) module . enable_distributed_communicator ( device_mesh . get_group ( self . _shard_dim_name )) for submod in module . modules (): if isinstance ( submod , GroupedLinear ): distribute_module ( submod , device_mesh , self . _partition_experts ) return module ToLocalParallel Bases: ParallelStyle Parallel style that distributes parameters and gradients but executes with local tensors. This style wraps standard tensor distribution (via DTensor ) but injects runtime hooks to temporarily unwrap DTensor parameters into local torch.Tensor during the forward pass. This is useful for parallel strategies (like Replicate) where the underlying calculation logic is not DTensor-aware, but the parameters must remain distributed for gradient synchronization and for distributed checkpointing. Source code in d9d/module/parallelism/style/to_local.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 class ToLocalParallel ( ParallelStyle ): \"\"\" Parallel style that distributes parameters and gradients but executes with local tensors. This style wraps standard tensor distribution (via ``DTensor``) but injects runtime hooks to temporarily unwrap ``DTensor`` parameters into local ``torch.Tensor`` during the forward pass. This is useful for parallel strategies (like Replicate) where the underlying calculation logic is not DTensor-aware, but the parameters must remain distributed for gradient synchronization and for distributed checkpointing. \"\"\" def __init__ ( self , param_placement : tuple [ Placement , ... ], grad_placement : tuple [ Placement , ... ]): \"\"\" Constructs ToLocalParallel object. Args: param_placement: Tuple of placements defining how parameters are distributed. grad_placement: Tuple of placements defining how gradients are synchronized. \"\"\" self . _grad_placement = grad_placement self . _param_placement = param_placement def _distribute_params ( self , name : str , module : nn . Module , device_mesh : DeviceMesh ): for param_name , param in module . named_parameters ( recurse = False ): new_param = nn . Parameter ( distribute_tensor ( param . data , device_mesh , self . _param_placement ), requires_grad = param . requires_grad ) module . register_parameter ( param_name , new_param ) def _apply ( self , module : nn . Module , device_mesh : DeviceMesh ): patched_classes = {} original_classes = {} for submod_name , submod in module . named_modules (): param_names = [ name for name , p in submod . named_parameters ( recurse = False )] patched_classes [ submod_name ] = _build_to_local_patched_class ( submod , self . _grad_placement , param_names ) original_classes [ submod_name ] = submod . __class__ distribute_module ( submod , device_mesh , self . _distribute_params ) module . register_forward_pre_hook ( _ModulePatch ( patched_classes )) module . register_forward_hook ( _ModulePatch ( original_classes )) __init__ ( param_placement , grad_placement ) Constructs ToLocalParallel object. Parameters: Name Type Description Default param_placement tuple [ Placement , ...] Tuple of placements defining how parameters are distributed. required grad_placement tuple [ Placement , ...] Tuple of placements defining how gradients are synchronized. required Source code in d9d/module/parallelism/style/to_local.py 45 46 47 48 49 50 51 52 53 54 55 def __init__ ( self , param_placement : tuple [ Placement , ... ], grad_placement : tuple [ Placement , ... ]): \"\"\" Constructs ToLocalParallel object. Args: param_placement: Tuple of placements defining how parameters are distributed. grad_placement: Tuple of placements defining how gradients are synchronized. \"\"\" self . _grad_placement = grad_placement self . _param_placement = param_placement","title":"Horizontal Parallelism"},{"location":"models/2_horizontal_parallelism/#about","text":"The d9d.module.parallelism package provides high-level strategies for distributing model execution across device meshes. These strategies are \"Horizontal\" in the sense that they function within a specific stage of a pipeline (intra-layer parallelism), as opposed to Pipeline Parallelism which is \"Vertical\" (inter-layer).","title":"About"},{"location":"models/2_horizontal_parallelism/#design","text":"","title":"Design"},{"location":"models/2_horizontal_parallelism/#dtensor-first-architecture","text":"d9d enforces a DTensor-first philosophy. We mandate that every trainable parameter in the distributed environment be represented as a torch.distributed.tensor.DTensor . This constraint simplifies the system architecture significantly: Universal Checkpointing : The checkpointing engine does not need to know about specific parallel strategies (like \"This is DP\" or \"This is TP\"). It simply inspects the DTensor.placements attribute to automatically determine how to gather, deduplicate, and save tensors. Native Synchronization : Gradient synchronization for replicated parameters is handled entirely by the d9d internals , that now knows which tensor dimensions are Replicated.","title":"DTensor-First Architecture"},{"location":"models/2_horizontal_parallelism/#composition-over-monoliths","text":"We explicitly reject monolithic wrappers like torch.nn.parallel.DistributedDataParallel (DDP). While DDP is efficient for pure Data Parallelism, it acts as a \"black box\" that assumes ownership of the entire model execution loop. Instead, d9d relies on PyTorch's parallelize_module API . This allows for fine-grained, per-submodule parallelism decisions: Layer A can use Tensor Parallelism (Row/Col wise). Layer B (e.g., a Router) can use Replicate Parallelism . Layer C (e.g., MLP) can use Expert Parallelism . By treating \"Data Parallelism\" simply as another tiling strategy (\"Replicate\") within the Tensor Parallel system, we achieve a unified interface for ND parallelism.","title":"Composition over Monoliths"},{"location":"models/2_horizontal_parallelism/#strategies","text":"","title":"Strategies"},{"location":"models/2_horizontal_parallelism/#replicate-parallelism","text":"parallelize_replicate implements Replicate Parallelism . It replicates parameters across the mesh. Used for Data Parallelism or Context Parallelism. During the forward pass, it installs hooks that temporarily \"unwrap\" DTensor parameters into standard, local torch.Tensor objects. This allows standard PyTorch operations and custom kernels to run without modification, while accessing module's state dict and parameters still yields DTensor objects.","title":"Replicate Parallelism"},{"location":"models/2_horizontal_parallelism/#expert-parallelism-moe","text":"Mixture of Experts (MoE) requires a unique parallel strategy where: 1. Experts are sharded across the ep_shard mesh dimension (each GPU holds a subset of experts), optionally replicating along ep_replicate . 2. Routers are replicated (all GPUs have the same routing logic). parallelize_expert_parallel applies sharding to MoELayer modules. It shards the GroupedLinear weights along the expert dimension. Simultaneously, it effectively applies parallelize_replicate to the router.","title":"Expert Parallelism (MoE)"},{"location":"models/2_horizontal_parallelism/#fully-sharded-data-parallel-fsdp","text":"parallelize_fsdp provides a thin wrapper around PyTorch's native fully_shard . Difference from standard FSDP: Standard FSDP averages gradients across the mesh (Sum / WorldSize) by default. d9d's wrapper forces the gradients being summed rather than averaged . This is required for our gradient accumulation logic that is handled externally. parallelize_fsdp strictly requires a 1D DeviceMesh. To use it in multi-dimensional meshes (e.g., combining Replication and Sharding), use parallelize_hsdp or apply parallelize_replicate to the other dimensions manually first.","title":"Fully Sharded Data Parallel (FSDP)"},{"location":"models/2_horizontal_parallelism/#hybrid-sharded-data-parallel-hsdp","text":"parallelize_hsdp is a high-level composite strategy for mixing Full Sharding with Replicate Parallel. parallelize_hsdp accepts a multi-dimensional mesh and a target shard_dim . It identifies all dimensions other than shard_dim as Replication Dimensions . It applies parallelize_replicate to the replication dimensions. It applies parallelize_fsdp to the specific sharding dimension.","title":"Hybrid Sharded Data Parallel (HSDP)"},{"location":"models/2_horizontal_parallelism/#usage-examples","text":"","title":"Usage Examples"},{"location":"models/2_horizontal_parallelism/#replicate-parallelism_1","text":"import torch from d9d.core.dist_context import DistributedContext , DENSE_DOMAIN from d9d.module.parallelism.api import parallelize_replicate # 1. Create a Distributed Context ctx : DistributedContext = ... # 2. Get Dense Domain Mesh dense_mesh = ctx . mesh_for ( DENSE_DOMAIN ) # pp x dp_replicate x dp_cp_shard x cp_replicate x tp # 2. Define Model model = MyCustomLayer ( ... ) # 3. Parallelize parallelize_replicate ( model , dense_mesh [[ 'dp_replicate' , 'cp_replicate' ]])","title":"Replicate Parallelism"},{"location":"models/2_horizontal_parallelism/#applying-expert-parallelism","text":"import torch from d9d.core.dist_context import DistributedContext , EXPERT_DOMAIN from d9d.module.parallelism.api import parallelize_expert_parallel from d9d.module.block.moe import MoELayer # 1. Create a Distributed Context ctx : DistributedContext = ... # 2. Get Expert Domain Mesh expert_mesh = ctx . mesh_for ( EXPERT_DOMAIN ) # pp x ep_replicate x ep_shard # 3. Define Model model = MoELayer ( ... ) # 4. Parallelize parallelize_expert_parallel ( model , mesh_experts = expert_mesh [[ 'ep_replicate' , 'ep_shard' ]], expert_shard_dim = 'ep_shard' )","title":"Applying Expert Parallelism"},{"location":"models/2_horizontal_parallelism/#applying-fsdp","text":"import torch from d9d.core.dist_context import DistributedContext , DENSE_DOMAIN from d9d.module.parallelism.api import parallelize_fsdp , parallelize_replicate # 1. Create a Distributed Context ctx : DistributedContext = ... # 2. Define Model model = MyCustomLayer ( ... ) # 3. Get Dense Domain Mesh dense_mesh = ctx . mesh_for ( DENSE_DOMAIN ) # 4. Parallelize parallelize_fsdp ( model , mesh = dense_mesh [ 'dp_cp_shard' ] )","title":"Applying FSDP"},{"location":"models/2_horizontal_parallelism/#applying-hsdp","text":"import torch from d9d.core.dist_context import DistributedContext , DENSE_DOMAIN from d9d.module.parallelism.api import parallelize_hsdp # 1. Create a Distributed Context ctx : DistributedContext = ... # 2. Get Mesh dense_mesh = ctx . mesh_for ( DENSE_DOMAIN ) # pp x dp_replicate x dp_cp_shard x cp_replicate x tp # 3. Define Model model = MyCustomLayer ( ... ) # 4. Parallelize parallelize_hsdp ( model , mesh = dense_mesh [ \"dp_replicate\" , \"dp_cp_shard\" , \"cp_replicate\" ], shard_dim = \"dp_cp_shard\" )","title":"Applying HSDP"},{"location":"models/2_horizontal_parallelism/#d9d.module.parallelism.api","text":"Horizontal parallelism strategies and utilities for d9d modules. This package provides high-level helper functions to apply specific distributed parallelism strategies to PyTorch modules compatible with the d9d ecosystem.","title":"api"},{"location":"models/2_horizontal_parallelism/#d9d.module.parallelism.api.parallelize_expert_parallel","text":"Applies Expert Parallelism to a MoE layer. This function configures the provided Mixture of Experts layer for distributed execution. It partitions the sparse experts across the specified dimension of the device mesh (Expert Parallelism) and replicates along other dims. Simultaneously, it configures the router to be fully replicated across the mesh. Parameters: Name Type Description Default module MoELayer The MoE layer instance to parallelize. required mesh_experts DeviceMesh The device mesh containing the expert parallel resources. required expert_shard_dim str The name of the mesh dimension where experts should be sharded. 'ep_shard' Source code in d9d/module/parallelism/api/expert_parallel.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def parallelize_expert_parallel ( module : MoELayer , mesh_experts : DeviceMesh , expert_shard_dim : str = \"ep_shard\" ): \"\"\" Applies Expert Parallelism to a MoE layer. This function configures the provided Mixture of Experts layer for distributed execution. It partitions the sparse experts across the specified dimension of the device mesh (Expert Parallelism) and replicates along other dims. Simultaneously, it configures the router to be fully replicated across the mesh. Args: module: The MoE layer instance to parallelize. mesh_experts: The device mesh containing the expert parallel resources. expert_shard_dim: The name of the mesh dimension where experts should be sharded. \"\"\" parallelize_module ( module , mesh_experts , ShardMoESparseExpertsParallel ( shard_dim_name = expert_shard_dim )) parallelize_module ( module . router , mesh_experts , ToLocalParallel ( param_placement = tuple ( Replicate () for _ in range ( mesh_experts . ndim )), grad_placement = tuple ( Replicate () for _ in range ( mesh_experts . ndim )), ), )","title":"parallelize_expert_parallel"},{"location":"models/2_horizontal_parallelism/#d9d.module.parallelism.api.parallelize_fsdp","text":"Applies Fully Sharded Data Parallel (FSDP) with forced gradient summation. This function wraps the provided module with PyTorch's fully_shard API using the specified device mesh. Unlike standard FSDP usage, this function explicitly configures the module to sum gradients across the mesh instead of averaging them and disables internal all-sum-reduce hooks. This is intended for d9d to handle gradient normalization and reduction across replicas externally. Parameters: Name Type Description Default module Module The module to shard. required mesh DeviceMesh The device mesh over which to shard the module. required *args Any Additional positional arguments passed to fully_shard . () **kwargs Any Additional keyword arguments passed to fully_shard . {} Source code in d9d/module/parallelism/api/fully_sharded.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def parallelize_fsdp ( module : nn . Module , mesh : DeviceMesh , * args : Any , ** kwargs : Any ): \"\"\" Applies Fully Sharded Data Parallel (FSDP) with forced gradient summation. This function wraps the provided module with PyTorch's ``fully_shard`` API using the specified device mesh. Unlike standard FSDP usage, this function explicitly configures the module to sum gradients across the mesh instead of averaging them and disables internal all-sum-reduce hooks. This is intended for d9d to handle gradient normalization and reduction across replicas externally. Args: module: The module to shard. mesh: The device mesh over which to shard the module. *args: Additional positional arguments passed to ``fully_shard``. **kwargs: Additional keyword arguments passed to ``fully_shard``. \"\"\" if mesh . ndim != 1 : raise ValueError ( \"FSDP mesh should contain exactly one dimension - for HSDP, please apply parallelize_replicate(...) first!\" ) fully_shard ( module , * args , mesh = mesh , ** kwargs ) if not isinstance ( module , FSDPModule ): raise RuntimeError ( \"Torch FSDP did not convert the module into FSDPModule\" ) _force_fsdp_grad_reduction_policy ( module )","title":"parallelize_fsdp"},{"location":"models/2_horizontal_parallelism/#d9d.module.parallelism.api.parallelize_hsdp","text":"Applies Hybrid Sharded Data Parallelism (HSDP) to a module. This function decomposes the provided device mesh into sharding dimensions and replication dimensions. It applies replication parallelism across the replication dimensions and Fully Sharded Data Parallelism (FSDP) across the specified shard dimension. Parameters: Name Type Description Default module Module The module to parallelize. required mesh DeviceMesh The device mesh over which to distribute the module. required shard_dim str The name of the mesh dimension used for FSDP sharding. Any dimension in the mesh not matching this name will be treated as a replication dimension. 'dp_cp_shard' *fsdp_args Any Positional arguments passed to the underlying FSDP parallelizer. () **fsdp_kwargs Any Keyword arguments passed to the underlying FSDP parallelizer. {} Raises: Type Description ValueError If the device mesh does not have named dimensions. Source code in d9d/module/parallelism/api/hybrid_sharded.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def parallelize_hsdp ( module : nn . Module , mesh : DeviceMesh , shard_dim : str = \"dp_cp_shard\" , * fsdp_args : Any , ** fsdp_kwargs : Any ): \"\"\" Applies Hybrid Sharded Data Parallelism (HSDP) to a module. This function decomposes the provided device mesh into sharding dimensions and replication dimensions. It applies replication parallelism across the replication dimensions and Fully Sharded Data Parallelism (FSDP) across the specified shard dimension. Args: module: The module to parallelize. mesh: The device mesh over which to distribute the module. shard_dim: The name of the mesh dimension used for FSDP sharding. Any dimension in the mesh not matching this name will be treated as a replication dimension. *fsdp_args: Positional arguments passed to the underlying FSDP parallelizer. **fsdp_kwargs: Keyword arguments passed to the underlying FSDP parallelizer. Raises: ValueError: If the device mesh does not have named dimensions. \"\"\" replicate_dims = mesh . mesh_dim_names if replicate_dims is None : raise ValueError ( \"Cannot use with unnamed device meshes\" ) replicate_dims = tuple ( x for x in replicate_dims if x != shard_dim and mesh [ x ] . size () > 1 ) if len ( replicate_dims ) > 0 : parallelize_replicate ( module , mesh [ replicate_dims ]) if mesh [ shard_dim ] . size () != 1 : parallelize_fsdp ( module , mesh [ shard_dim ], * fsdp_args , ** fsdp_kwargs )","title":"parallelize_hsdp"},{"location":"models/2_horizontal_parallelism/#d9d.module.parallelism.api.parallelize_replicate","text":"Applies replicated parallelism to the module. This function configures the provided module to be fully replicated across the given device mesh. It utilizes the ToLocalParallel style, which manages DTensor wrapping for parameters and gradients (via Replicate placements) while ensuring that the underlying computation sees standard local tensors during the forward pass. This approach is effectively Data Parallelism managed via the DTensor APIs, allowing seamless integration of modules that require local tensor inputs into a broader distributed mesh context. Parameters: Name Type Description Default module Module The module to parallelize. required mesh DeviceMesh The device mesh over which to replicate the module. required Source code in d9d/module/parallelism/api/replicate_parallel.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def parallelize_replicate ( module : nn . Module , mesh : DeviceMesh , ): \"\"\" Applies replicated parallelism to the module. This function configures the provided module to be fully replicated across the given device mesh. It utilizes the ``ToLocalParallel`` style, which manages ``DTensor`` wrapping for parameters and gradients (via ``Replicate`` placements) while ensuring that the underlying computation sees standard local tensors during the forward pass. This approach is effectively Data Parallelism managed via the DTensor APIs, allowing seamless integration of modules that require local tensor inputs into a broader distributed mesh context. Args: module: The module to parallelize. mesh: The device mesh over which to replicate the module. \"\"\" parallelize_module ( module , mesh , ToLocalParallel ( param_placement = tuple ( Replicate () for _ in range ( mesh . ndim )), grad_placement = tuple ( Replicate () for _ in range ( mesh . ndim )), ), )","title":"parallelize_replicate"},{"location":"models/2_horizontal_parallelism/#d9d.module.parallelism.style","text":"","title":"style"},{"location":"models/2_horizontal_parallelism/#d9d.module.parallelism.style.ShardMoESparseExpertsParallel","text":"Bases: ParallelStyle Parallel style that shards MoE experts across a specific mesh dimension. This style is designed for MoELayer instances using GroupedLinear for experts. It splits the experts across the specified dimension of the device mesh (Expert Parallelism). Other dimensions in the mesh treat the parameters as Replicated. It also initializes the necessary distributed communication groups within the MoE layer to handle token dispatching. Source code in d9d/module/parallelism/style/shard_experts.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 class ShardMoESparseExpertsParallel ( ParallelStyle ): \"\"\" Parallel style that shards MoE experts across a specific mesh dimension. This style is designed for ``MoELayer`` instances using ``GroupedLinear`` for experts. It splits the experts across the specified dimension of the device mesh (Expert Parallelism). Other dimensions in the mesh treat the parameters as Replicated. It also initializes the necessary distributed communication groups within the MoE layer to handle token dispatching. \"\"\" def __init__ ( self , shard_dim_name : str ): self . _shard_dim_name = shard_dim_name def _partition_experts ( self , module_name : str , mod : nn . Module , device_mesh : DeviceMesh ): if not isinstance ( mod , GroupedLinear ): raise TypeError ( \"This plan should be applied only on GroupedLinear\" ) mesh_dim_names = device_mesh . mesh_dim_names if mesh_dim_names is None : raise ValueError ( \"This plan should be applied only on named DeviceMeshes\" ) placements = [ Shard ( 0 ) if dim_name == self . _shard_dim_name else Replicate () for dim_name in mesh_dim_names ] weight = nn . Parameter ( distribute_tensor ( mod . weight , device_mesh , placements ), requires_grad = mod . weight . requires_grad ) mod . weight = weight def _apply ( self , module : nn . Module , device_mesh : DeviceMesh ) -> nn . Module : if not isinstance ( module , MoELayer ): raise TypeError ( \"This plan should be applied only on MoELayer\" ) module . enable_distributed_communicator ( device_mesh . get_group ( self . _shard_dim_name )) for submod in module . modules (): if isinstance ( submod , GroupedLinear ): distribute_module ( submod , device_mesh , self . _partition_experts ) return module","title":"ShardMoESparseExpertsParallel"},{"location":"models/2_horizontal_parallelism/#d9d.module.parallelism.style.ToLocalParallel","text":"Bases: ParallelStyle Parallel style that distributes parameters and gradients but executes with local tensors. This style wraps standard tensor distribution (via DTensor ) but injects runtime hooks to temporarily unwrap DTensor parameters into local torch.Tensor during the forward pass. This is useful for parallel strategies (like Replicate) where the underlying calculation logic is not DTensor-aware, but the parameters must remain distributed for gradient synchronization and for distributed checkpointing. Source code in d9d/module/parallelism/style/to_local.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 class ToLocalParallel ( ParallelStyle ): \"\"\" Parallel style that distributes parameters and gradients but executes with local tensors. This style wraps standard tensor distribution (via ``DTensor``) but injects runtime hooks to temporarily unwrap ``DTensor`` parameters into local ``torch.Tensor`` during the forward pass. This is useful for parallel strategies (like Replicate) where the underlying calculation logic is not DTensor-aware, but the parameters must remain distributed for gradient synchronization and for distributed checkpointing. \"\"\" def __init__ ( self , param_placement : tuple [ Placement , ... ], grad_placement : tuple [ Placement , ... ]): \"\"\" Constructs ToLocalParallel object. Args: param_placement: Tuple of placements defining how parameters are distributed. grad_placement: Tuple of placements defining how gradients are synchronized. \"\"\" self . _grad_placement = grad_placement self . _param_placement = param_placement def _distribute_params ( self , name : str , module : nn . Module , device_mesh : DeviceMesh ): for param_name , param in module . named_parameters ( recurse = False ): new_param = nn . Parameter ( distribute_tensor ( param . data , device_mesh , self . _param_placement ), requires_grad = param . requires_grad ) module . register_parameter ( param_name , new_param ) def _apply ( self , module : nn . Module , device_mesh : DeviceMesh ): patched_classes = {} original_classes = {} for submod_name , submod in module . named_modules (): param_names = [ name for name , p in submod . named_parameters ( recurse = False )] patched_classes [ submod_name ] = _build_to_local_patched_class ( submod , self . _grad_placement , param_names ) original_classes [ submod_name ] = submod . __class__ distribute_module ( submod , device_mesh , self . _distribute_params ) module . register_forward_pre_hook ( _ModulePatch ( patched_classes )) module . register_forward_hook ( _ModulePatch ( original_classes ))","title":"ToLocalParallel"},{"location":"models/2_horizontal_parallelism/#d9d.module.parallelism.style.ToLocalParallel.__init__","text":"Constructs ToLocalParallel object. Parameters: Name Type Description Default param_placement tuple [ Placement , ...] Tuple of placements defining how parameters are distributed. required grad_placement tuple [ Placement , ...] Tuple of placements defining how gradients are synchronized. required Source code in d9d/module/parallelism/style/to_local.py 45 46 47 48 49 50 51 52 53 54 55 def __init__ ( self , param_placement : tuple [ Placement , ... ], grad_placement : tuple [ Placement , ... ]): \"\"\" Constructs ToLocalParallel object. Args: param_placement: Tuple of placements defining how parameters are distributed. grad_placement: Tuple of placements defining how gradients are synchronized. \"\"\" self . _grad_placement = grad_placement self . _param_placement = param_placement","title":"__init__"},{"location":"models/3_pipeline_parallelism/","text":"The d9d Approach d9d implements a modern, highly modular pipelining engine designed for performance, stability and customization. Dynamic Shapes & Algorithmic Shape Inference To run P2P (Point-to-Point) communication, the receiver must know the shape of the incoming tensor to pre-allocate buffers. d9d asks your model to implement a lightweight protocol ( ModuleSupportsPipelining ) to calculate stage input and output shapes from batch input shapes mathematically, without performing a heavy forward pass or doing a distributed graph tracing. This allows supporting Dynamic Shapes (e.g., varying sequence lengths) efficiently across runs. Construction Consistency (No Patching) A common anti-pattern in distributed training is \"Instantiate-then-Delete\": creating a huge model on CPU/Meta device and then hacking it apart del model.layers[N:] . We reject this pattern because of: Fragility : Changes to model architecture require changes to the external slicing script. Leaky Abstractions : Forward methods become full of if self.layer is not None . Invalid States : The model object exists in a \"zombie\" state until sliced. In d9d, models are Pipeline-Aware . Each pipeline rank constructs only the sub-graph it owns. The object returned is compliant, complete, and valid immediately. Making Models Compatible The Protocol Implementing the Protocol To use Pipeline Parallelism in d9d, your model must implement the d9d.pipelining.api.ModuleSupportsPipelining protocol to allow the framework to manage memory and buffer allocations. Forward Compatibility Pipelined models currently only support outputting a dictionary ( dict[str, torch.Tensor] ). However, we plan to support arbitrary PyTrees in further releases. The keys in the dictionary returned by your forward method must strictly match the keys in the dictionary calculated by infer_stage_outputs_from_pipeline_inputs . The named arguments accepted by your forward method must strictly match the infer_stage_inputs_from_pipeline_inputs . This allows the communication handler to map tensor names to P2P buffers deterministically. Example Below is a skeleton of a Transformer-like model implemented for d9d pipelining. import torch from torch import nn from d9d.pipelining.api import PipelineStageInfo , distribute_layers_for_pipeline_stage class MyModelChunk ( nn . Module ): def __init__ ( self , stage : PipelineStageInfo , config ): super () . __init__ () self . stage = stage self . config = config # 1. Determine what layers live here self . start_layer , self . end_layer = distribute_layers_for_pipeline_stage ( config . n_layers , num_virtual_layers_pre = 1 , num_virtual_layers_post = 1 , stage = stage ) # 2. Build sub-modules (using ModuleDict - for compatibility) self . layers = nn . ModuleDict ({ str ( layer ): TransformerBlock ( ... ) for layer in range ( self . start_layer , self . end_layer ) }) # Only build embeddings on first stage if stage . is_current_stage_first : self . embed = nn . Embedding ( ... ) # Only build head on last stage if stage . is_current_stage_last : self . head = nn . Linear ( ... ) def forward ( self , input_ids = None , hidden_states = None ): # Run embeddings only on first stage if self . stage . is_current_stage_first : x = self . embed ( input_ids ) else : x = hidden_states # Run local layers for layer_idx in range ( self . start_layer , self . end_layer ): x = self . layers [ str ( layer_idx )]( x ) outputs = { \"hidden_states\" : x } # Last stage logic if self . stage . is_current_stage_last : logits = self . head ( x ) outputs [ 'logits' ] = logits return outputs # --- Protocol Implementation --- def infer_stage_inputs_from_pipeline_inputs ( self , inputs : dict [ str , torch . Tensor ], n_microbatches : int ): batch_size = inputs [ 'input_ids' ] . shape [ 0 ] micro_batch_size = batch_size // n_microbatches seq_len = inputs [ 'input_ids' ] . shape [ 1 ] if self . stage . is_current_stage_first : # First stage receives raw input IDs return { \"input_ids\" : torch . empty (( micro_batch_size , seq_len ), dtype = torch . long )} else : # Intermediate stages receive hidden states from previous stage return { \"hidden_states\" : torch . empty (( micro_batch_size , seq_len , self . hidden_dim ))} def infer_stage_outputs_from_pipeline_inputs ( self , inputs : dict [ str , torch . Tensor ], n_microbatches : int ): batch_size = inputs [ 'input_ids' ] . shape [ 0 ] micro_batch_size = batch_size // n_microbatches seq_len = inputs [ 'input_ids' ] . shape [ 1 ] outputs = { \"hidden_states\" : torch . empty (( micro_batch_size , seq_len , self . config . hidden_dim ))} if self . stage . is_current_stage_last : # Last stage outputs logits too outputs [ \"logits\" ] = torch . empty (( micro_batch_size , seq_len , self . config . vocab_size )) return outputs Using the Pipeline Supported Schedules Example JSON Description {\"schedule\": \"inference\"} Configuration for inference-only pipeline execution. Runs all forward passes sequentially without any backward passes. {\"schedule\": \"gpipe\"} Standard GPipe execution. Assumes a single stage per rank and processes all microbatches for the forward pass before switching to the backward pass. {\"schedule\": \"looped_bfs\", \"num_stages_per_rank\": 2} Looped Breadth-First Search execution. Supports multiple stages per rank (virtualization) and executes all work for a specific stage before moving to the next. {\"schedule\": \"1f1b\", \"num_stages_per_rank\": 1, \"zero_bubble\": true} Interleaved 1F1B and Interleaved Zero Bubble execution. Supports multiple stages per rank. Handles sharding backward passes to dI and dW when zero_bubble is enabled. {\"schedule\": \"zero_bubble_v\"} Zero Bubble V (ZBV) execution. A specialized V-shape topology schedule that splits backward passes into Input and Weight gradients. Requires exactly 2 stages per rank. {\"schedule\": \"dual_pipe_v\"} DualPipeV execution. A bidirectional pipeline schedule for high-throughput training using V-shape topology and reciprocal forward/backward scheduling. Batch Sharding Pipelining works by splitting the input batch into N microbatches. By default, d9d assumes all input and output tensors should be split along dimension 0. However, if your inputs require different sharding strategy, you can customize this via PipelineShardingSpec . Please see the sharding utils docs . from d9d.pipelining.api import PipelineShardingSpec from d9d.core.sharding import ShardingSpec from torch.distributed.tensor import Shard # Example: Split 'images' on dim 1, but replicate 'camera_angles' across all microbatches my_spec = PipelineShardingSpec ( input_data = { \"images\" : Shard ( 1 ), \"camera_angles\" : None } # input_kwargs can be defined similarly ) Usage within the Trainer Pipelining is available in the Trainer framework. When configuring the Trainer, simply provide an AnyPipelineScheduleConfig in your training arguments. The Trainer handles the construction of the schedule and the distribution of layers automatically. Advanced - Manual Usage If you want to use pipelining outside the Trainer (e.g., custom loops), you use the build_schedule factory. The build_schedule function requires a Model Provider logic. Instead of passing an instantiated model, you pass a function that accepts PipelineStageInfo and returns the nn.Module for that stage. This ensures construction consistency. from torch import Tensor from torch.distributed.tensor import Shard import torch.nn.functional as F from d9d.core.dist_context import DistributedContext from d9d.core.sharding import shard_tree from d9d.pipelining.factory import build_schedule , PipelineSchedule1F1BConfig from d9d.pipelining.api import PipelineShardingSpec # 0. Define an object that manages loss calculation across steps class PipelineLossHandler : def __init__ ( self , num_microbatches : int ): self . _shard_spec = { 'target' : Shard ( 0 ) } self . _num_microbatches = num_microbatches self . _targets = None def set_targets ( self , targets : Tensor ): self . _targets = shard_tree ( { 'target' : targets }, sharding_spec = self . _shard_spec , num_shards = self . _num_microbatches , enforce_even_split = True ) def compute_loss ( self , outputs : dict [ str , Tensor ], microbatch_idx : int ): # Implement any custom logic here current_target = self . _targets [ microbatch_idx ] return F . cross_entropy ( outputs [ 'logits' ] . view ( - 1 , outputs [ 'logits' ] . shape [ - 1 ]), current_target . view ( - 1 )) # 1. Define configuration dist_context : DistributedContext = ... model_config = ... n_microbatches = 32 schedule_config = PipelineSchedule1F1BConfig ( num_stages_per_rank = 4 , # 4 Virtual stages per rank zero_bubble = True # Enable ZB1P optimization ) # 2. Build the schedule, model shards and loss compute function loss_handler = PipelineLossHandler ( num_microbatches = n_microbatches ) schedule_info , modules = build_schedule ( dist_context = dist_context , n_microbatches = 32 , schedule_config = schedule_config , model_provider = lambda stage : MyModelChunk ( stage , model_config ), # Factory function callback = loss_handler . compute_loss ) # 3. Execution # The schedule object exposes a simple step API inputs = { \"input_ids\" : ... } # Full batch loss_handler . set_targets ( ... ) # Set targets for full batch schedule_info . schedule . configure_buffers ( # Pre-allocate buffers inputs , kwargs = {}, sharding_spec = PipelineShardingSpec () ) schedule_info . schedule . step ( inputs , kwargs = {}) d9d.pipelining.api Pipelining API that is intended to be accessible by end user. PipelineLossFn = Callable [[ dict [ str , torch . Tensor ], int ], torch . Tensor ] module-attribute Callback function type for calculating loss in the final pipeline stage. Parameters: Name Type Description Default outputs A dictionary mapping output names to tensors produced by the model. required microbatch_idx The index of the current micro-batch being processed. required Returns: Type Description The computed loss tensor (scalar). PipelineResultFn = Callable [[ dict [ str , torch . Tensor ], int ], Any ] module-attribute Callback function type for handling results from a final pipeline stage. Parameters: Name Type Description Default outputs A dictionary mapping output names to tensors produced by the stage. required microbatch_idx The index of the current micro-batch being processed. required Returns: Type Description Anything - not used. ModuleSupportsPipelining Bases: Protocol Protocol for modules that support pipeline parallelism metadata inference. Classes implementing this protocol enable the framework to pre-calculate tensor shapes and types required for inter-stage communication (p2p) without executing the full forward pass. Source code in d9d/pipelining/api/module.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 @typing . runtime_checkable class ModuleSupportsPipelining ( typing . Protocol ): \"\"\" Protocol for modules that support pipeline parallelism metadata inference. Classes implementing this protocol enable the framework to pre-calculate tensor shapes and types required for inter-stage communication (p2p) without executing the full forward pass. \"\"\" def infer_stage_inputs_from_pipeline_inputs ( self , inputs : dict [ str , torch . Tensor ], n_microbatches : int ) -> dict [ str , torch . Tensor ]: \"\"\" Infers the input tensors metadata for the current pipeline stage based on global batch inputs. Args: inputs: Global inputs for the pipeline. n_microbatches: Number of microbatches the global batch is split into. Returns: Dictionary of input tensors expected by this specific stage locally. \"\"\" ... def infer_stage_outputs_from_pipeline_inputs ( self , inputs : dict [ str , torch . Tensor ], n_microbatches : int ) -> dict [ str , torch . Tensor ]: \"\"\" Infers the output tensors metadata for the current pipeline stage based on global batch inputs. Args: inputs: Global inputs for the pipeline (typically a batch). n_microbatches: Number of microbatches the global batch is split into. Returns: Dictionary of output tensors produced by this specific stage locally. \"\"\" ... infer_stage_inputs_from_pipeline_inputs ( inputs , n_microbatches ) Infers the input tensors metadata for the current pipeline stage based on global batch inputs. Parameters: Name Type Description Default inputs dict [ str , Tensor ] Global inputs for the pipeline. required n_microbatches int Number of microbatches the global batch is split into. required Returns: Type Description dict [ str , Tensor ] Dictionary of input tensors expected by this specific stage locally. Source code in d9d/pipelining/api/module.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def infer_stage_inputs_from_pipeline_inputs ( self , inputs : dict [ str , torch . Tensor ], n_microbatches : int ) -> dict [ str , torch . Tensor ]: \"\"\" Infers the input tensors metadata for the current pipeline stage based on global batch inputs. Args: inputs: Global inputs for the pipeline. n_microbatches: Number of microbatches the global batch is split into. Returns: Dictionary of input tensors expected by this specific stage locally. \"\"\" ... infer_stage_outputs_from_pipeline_inputs ( inputs , n_microbatches ) Infers the output tensors metadata for the current pipeline stage based on global batch inputs. Parameters: Name Type Description Default inputs dict [ str , Tensor ] Global inputs for the pipeline (typically a batch). required n_microbatches int Number of microbatches the global batch is split into. required Returns: Type Description dict [ str , Tensor ] Dictionary of output tensors produced by this specific stage locally. Source code in d9d/pipelining/api/module.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def infer_stage_outputs_from_pipeline_inputs ( self , inputs : dict [ str , torch . Tensor ], n_microbatches : int ) -> dict [ str , torch . Tensor ]: \"\"\" Infers the output tensors metadata for the current pipeline stage based on global batch inputs. Args: inputs: Global inputs for the pipeline (typically a batch). n_microbatches: Number of microbatches the global batch is split into. Returns: Dictionary of output tensors produced by this specific stage locally. \"\"\" ... PipelineSchedule Bases: ABC Abstract base class defining the interface for pipeline execution schedules. Source code in d9d/pipelining/api/schedule.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 class PipelineSchedule ( abc . ABC ): \"\"\"Abstract base class defining the interface for pipeline execution schedules.\"\"\" @abc . abstractmethod def configure_buffers ( self , inputs : dict [ str , torch . Tensor ], kwargs : dict [ str , Any ], sharding_spec : PipelineShardingSpec | None ): \"\"\" Configures internal state and buffers based on input shapes. This method allows the schedule to pre-allocate memory or setup sharding specifications based on the structure of the input data before execution begins. Args: inputs: A dictionary of input tensors. kwargs: A dictionary of keyword arguments. sharding_spec: A specification defining how inputs and kwargs should be split into micro-batches. If None, assumes standard split-by-zero-dim behavior. \"\"\" ... @abc . abstractmethod def step ( self , inputs : dict [ str , torch . Tensor ], kwargs : dict [ str , Any ]): \"\"\" Executes a single pipeline step using the provided inputs. This typically involves distributing inputs across microbatches, executing forward and backward passes according to the specific schedule logic, and handling communications between stages. Args: inputs: A dictionary of global input tensors. kwargs: A dictionary of global keyword arguments. \"\"\" ... configure_buffers ( inputs , kwargs , sharding_spec ) abstractmethod Configures internal state and buffers based on input shapes. This method allows the schedule to pre-allocate memory or setup sharding specifications based on the structure of the input data before execution begins. Parameters: Name Type Description Default inputs dict [ str , Tensor ] A dictionary of input tensors. required kwargs dict [ str , Any ] A dictionary of keyword arguments. required sharding_spec PipelineShardingSpec | None A specification defining how inputs and kwargs should be split into micro-batches. If None, assumes standard split-by-zero-dim behavior. required Source code in d9d/pipelining/api/schedule.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 @abc . abstractmethod def configure_buffers ( self , inputs : dict [ str , torch . Tensor ], kwargs : dict [ str , Any ], sharding_spec : PipelineShardingSpec | None ): \"\"\" Configures internal state and buffers based on input shapes. This method allows the schedule to pre-allocate memory or setup sharding specifications based on the structure of the input data before execution begins. Args: inputs: A dictionary of input tensors. kwargs: A dictionary of keyword arguments. sharding_spec: A specification defining how inputs and kwargs should be split into micro-batches. If None, assumes standard split-by-zero-dim behavior. \"\"\" ... step ( inputs , kwargs ) abstractmethod Executes a single pipeline step using the provided inputs. This typically involves distributing inputs across microbatches, executing forward and backward passes according to the specific schedule logic, and handling communications between stages. Args: inputs: A dictionary of global input tensors. kwargs: A dictionary of global keyword arguments. Source code in d9d/pipelining/api/schedule.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 @abc . abstractmethod def step ( self , inputs : dict [ str , torch . Tensor ], kwargs : dict [ str , Any ]): \"\"\" Executes a single pipeline step using the provided inputs. This typically involves distributing inputs across microbatches, executing forward and backward passes according to the specific schedule logic, and handling communications between stages. Args: inputs: A dictionary of global input tensors. kwargs: A dictionary of global keyword arguments. \"\"\" ... PipelineStageInfo dataclass Holds information about the current position within the distributed pipeline. Attributes: Name Type Description current_stage int The 0-based index of the current pipeline stage. num_stages int The total number of stages in the pipeline. Source code in d9d/pipelining/api/module.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 @dataclasses . dataclass class PipelineStageInfo : \"\"\" Holds information about the current position within the distributed pipeline. Attributes: current_stage: The 0-based index of the current pipeline stage. num_stages: The total number of stages in the pipeline. \"\"\" current_stage : int num_stages : int @property def is_current_stage_first ( self ) -> bool : \"\"\" Determines if this is the first stage in the pipeline. Returns: True if current_stage is 0. \"\"\" return self . current_stage == 0 @property def is_current_stage_last ( self ) -> bool : \"\"\" Determines if this is the last stage in the pipeline. Returns: True if current_stage is the last index. \"\"\" return self . current_stage == self . num_stages - 1 is_current_stage_first property Determines if this is the first stage in the pipeline. Returns: Type Description bool True if current_stage is 0. is_current_stage_last property Determines if this is the last stage in the pipeline. Returns: Type Description bool True if current_stage is the last index. distribute_layers_for_pipeline_stage ( num_layers , num_virtual_layers_pre , num_virtual_layers_post , stage ) Calculates the layer index range for a specific pipeline stage. This function distributes a given number of layers across multiple pipeline stages as evenly as possible. It accounts for additional, non-layer computational load on the first and last stages (e.g., embeddings and the LM head) by using the concept of 'virtual layers' to reserve capacity. Parameters: Name Type Description Default num_layers int The total number of primary model layers to be distributed (e.g., the transformer blocks). required num_virtual_layers_pre int The number of 'virtual' layers representing the computational cost of modules on the first stage, before the main layers (e.g., token and positional embeddings). required num_virtual_layers_post int The number of 'virtual' layers representing the computational cost of modules on the last stage, after the main layers (e.g., the final layer normalization and LM head). required stage PipelineStageInfo An object containing total stages and current stage index. required Returns: Type Description tuple [ int , int ] A tuple (start_index, end_index), representing the slice of layers for the given stage. The start_index is inclusive and the end_index is exclusive. Raises: Type Description ValueError If the pipeline configuration results in a stage having zero or negative layers assigned (pipeline too long for the model size). Source code in d9d/pipelining/api/module.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def distribute_layers_for_pipeline_stage ( num_layers : int , num_virtual_layers_pre : int , num_virtual_layers_post : int , stage : PipelineStageInfo ) -> tuple [ int , int ]: \"\"\" Calculates the layer index range for a specific pipeline stage. This function distributes a given number of layers across multiple pipeline stages as evenly as possible. It accounts for additional, non-layer computational load on the first and last stages (e.g., embeddings and the LM head) by using the concept of 'virtual layers' to reserve capacity. Args: num_layers: The total number of primary model layers to be distributed (e.g., the transformer blocks). num_virtual_layers_pre: The number of 'virtual' layers representing the computational cost of modules on the *first* stage, before the main layers (e.g., token and positional embeddings). num_virtual_layers_post: The number of 'virtual' layers representing the computational cost of modules on the *last* stage, after the main layers (e.g., the final layer normalization and LM head). stage: An object containing total stages and current stage index. Returns: A tuple (start_index, end_index), representing the slice of layers for the given stage. The start_index is inclusive and the end_index is exclusive. Raises: ValueError: If the pipeline configuration results in a stage having zero or negative layers assigned (pipeline too long for the model size). \"\"\" num_layers_virtual = num_layers + num_virtual_layers_pre + num_virtual_layers_post base_layers_per_stage = num_layers_virtual // stage . num_stages extra_layers = num_layers_virtual % stage . num_stages layer_count_per_stage = [] for proposed_stage_i in range ( stage . num_stages ): proposed_stage = PipelineStageInfo ( num_stages = stage . num_stages , current_stage = proposed_stage_i ) layers = base_layers_per_stage + 1 if proposed_stage_i < extra_layers else base_layers_per_stage adjustment = 0 if proposed_stage . is_current_stage_first : adjustment += num_virtual_layers_pre if proposed_stage . is_current_stage_last : adjustment += num_virtual_layers_post actual_layers = layers - adjustment if actual_layers <= 0 : raise ValueError ( f \"Tried to distribute layers, but got { actual_layers } on \" f \"stage { proposed_stage . current_stage } . Perhaps the pipeline is too long for this model?\" ) layer_count_per_stage . append ( actual_layers ) start_layer_id = sum ( layer_count_per_stage [: stage . current_stage ]) num_layers_in_stage = layer_count_per_stage [ stage . current_stage ] return start_layer_id , start_layer_id + num_layers_in_stage d9d.pipelining.factory AnyPipelineScheduleConfig = Annotated [ PipelineScheduleInferenceConfig | PipelineScheduleGPipeConfig | PipelineScheduleLoopedBFSConfig | PipelineSchedule1F1BConfig | PipelineScheduleZeroBubbleVConfig | PipelineScheduleDualPipeVConfig , Field ( discriminator = 'schedule' )] module-attribute Union of all supported pipeline schedule configuration types. This type alias uses a Pydantic discriminator on the schedule field to allow polymorphic validation and serialization of specific schedule configs (e.g. Inference, GPipe, 1F1B, ZeroBubble, etc.). PipelineSchedule1F1BConfig Bases: BaseModel Configuration for Interleaved 1F1B and Interleaved Zero Bubble execution. Supports assigning multiple stages per rank and sharding backward to dI and dW to reduce pipeline bubbles. Source code in d9d/pipelining/factory/config.py 40 41 42 43 44 45 46 47 48 49 50 51 class PipelineSchedule1F1BConfig ( BaseModel ): \"\"\" Configuration for Interleaved 1F1B and Interleaved Zero Bubble execution. Supports assigning multiple stages per rank and sharding backward to dI and dW to reduce pipeline bubbles. \"\"\" schedule : Literal [ \"1f1b\" ] = \"1f1b\" num_stages_per_rank : int zero_bubble : bool PipelineScheduleDualPipeVConfig Bases: BaseModel Configuration for DualPipeV execution. A bidirectional pipeline schedule for high-throughput training, utilizing V-shape topology and reciprocal forward/backward scheduling. Source code in d9d/pipelining/factory/config.py 65 66 67 68 69 70 71 72 73 class PipelineScheduleDualPipeVConfig ( BaseModel ): \"\"\" Configuration for DualPipeV execution. A bidirectional pipeline schedule for high-throughput training, utilizing V-shape topology and reciprocal forward/backward scheduling. \"\"\" schedule : Literal [ \"dual_pipe_v\" ] = \"dual_pipe_v\" PipelineScheduleGPipeConfig Bases: BaseModel Configuration for GPipe execution. This assumes a single stage per rank and processes all microbatches for the forward pass before switching to the backward pass. Source code in d9d/pipelining/factory/config.py 16 17 18 19 20 21 22 23 24 class PipelineScheduleGPipeConfig ( BaseModel ): \"\"\" Configuration for GPipe execution. This assumes a single stage per rank and processes all microbatches for the forward pass before switching to the backward pass. \"\"\" schedule : Literal [ \"gpipe\" ] = \"gpipe\" PipelineScheduleInferenceConfig Bases: BaseModel Configuration for inference-only pipeline execution. This schedule runs all forward passes sequentially without any backward passes. Source code in d9d/pipelining/factory/config.py 6 7 8 9 10 11 12 13 class PipelineScheduleInferenceConfig ( BaseModel ): \"\"\" Configuration for inference-only pipeline execution. This schedule runs all forward passes sequentially without any backward passes. \"\"\" schedule : Literal [ \"inference\" ] = \"inference\" PipelineScheduleLoopedBFSConfig Bases: BaseModel Configuration for Looped Breadth-First Search execution. Similar to GPipe, but supports multiple stages per rank (virtualization). It executes all available work for a specific stage before moving to the next. Source code in d9d/pipelining/factory/config.py 27 28 29 30 31 32 33 34 35 36 37 class PipelineScheduleLoopedBFSConfig ( BaseModel ): \"\"\" Configuration for Looped Breadth-First Search execution. Similar to GPipe, but supports multiple stages per rank (virtualization). It executes all available work for a specific stage before moving to the next. \"\"\" schedule : Literal [ \"looped_bfs\" ] = \"looped_bfs\" num_stages_per_rank : int PipelineScheduleZeroBubbleVConfig Bases: BaseModel Configuration for Zero Bubble V (ZBV) execution. A specialized V-shape topology schedule that splits backward passes into Input and Weight gradients to maximize overlap. Requires exactly 2 stages per rank. Source code in d9d/pipelining/factory/config.py 54 55 56 57 58 59 60 61 62 class PipelineScheduleZeroBubbleVConfig ( BaseModel ): \"\"\" Configuration for Zero Bubble V (ZBV) execution. A specialized V-shape topology schedule that splits backward passes into Input and Weight gradients to maximize overlap. Requires exactly 2 stages per rank. \"\"\" schedule : Literal [ \"zero_bubble_v\" ] = \"zero_bubble_v\" build_schedule ( dist_context , n_microbatches , schedule_config , model_provider , callback ) Constructs the pipeline schedule and instantiates model stages. This function coordinates the creation of the pipeline. If the context is distributed, it builds a parallel schedule ( PipelineScheduleExecutor ) by calculating topology and creating stages for the current rank. If the context is local, it builds an offline schedule ( OfflinePipelineExecutor ) for direct execution. Parameters: Name Type Description Default dist_context DistributedContext The distributed context. required n_microbatches int Number of microbatches per global step. required schedule_config AnyPipelineScheduleConfig Configuration object determining the schedule strategy. required model_provider Callable [[ PipelineStageInfo ], Module ] A factory function that accepts stage info and returns an nn.Module for that specific stage. required callback PipelineLossFn | PipelineResultFn Callback either computing loss function (if training) or just processing pipeline outputs (if not training). required Returns: Type Description PipelineScheduleInfo A tuple containing the schedule info (executor and metadata) and a list list [ Module ] of local PyTorch modules created for this rank. Source code in d9d/pipelining/factory/factory.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def build_schedule ( dist_context : DistributedContext , n_microbatches : int , schedule_config : AnyPipelineScheduleConfig , model_provider : Callable [[ PipelineStageInfo ], nn . Module ], callback : PipelineLossFn | PipelineResultFn , ) -> tuple [ PipelineScheduleInfo , list [ nn . Module ]]: \"\"\" Constructs the pipeline schedule and instantiates model stages. This function coordinates the creation of the pipeline. If the context is distributed, it builds a parallel schedule (`PipelineScheduleExecutor`) by calculating topology and creating stages for the current rank. If the context is local, it builds an offline schedule (`OfflinePipelineExecutor`) for direct execution. Args: dist_context: The distributed context. n_microbatches: Number of microbatches per global step. schedule_config: Configuration object determining the schedule strategy. model_provider: A factory function that accepts stage info and returns an `nn.Module` for that specific stage. callback: Callback either computing loss function (if training) or just processing pipeline outputs (if not training). Returns: A tuple containing the schedule info (executor and metadata) and a list of local PyTorch modules created for this rank. \"\"\" if dist_context . mesh_params . is_distributed : return _build_schedule_distributed ( dist_context = dist_context , n_microbatches = n_microbatches , schedule_config = schedule_config , model_provider = model_provider , callback = callback , ) else : return _build_schedule_local ( schedule_config = schedule_config , model_provider = model_provider , callback = callback )","title":"Pipeline Parallelism"},{"location":"models/3_pipeline_parallelism/#the-d9d-approach","text":"d9d implements a modern, highly modular pipelining engine designed for performance, stability and customization.","title":"The d9d Approach"},{"location":"models/3_pipeline_parallelism/#dynamic-shapes-algorithmic-shape-inference","text":"To run P2P (Point-to-Point) communication, the receiver must know the shape of the incoming tensor to pre-allocate buffers. d9d asks your model to implement a lightweight protocol ( ModuleSupportsPipelining ) to calculate stage input and output shapes from batch input shapes mathematically, without performing a heavy forward pass or doing a distributed graph tracing. This allows supporting Dynamic Shapes (e.g., varying sequence lengths) efficiently across runs.","title":"Dynamic Shapes &amp; Algorithmic Shape Inference"},{"location":"models/3_pipeline_parallelism/#construction-consistency-no-patching","text":"A common anti-pattern in distributed training is \"Instantiate-then-Delete\": creating a huge model on CPU/Meta device and then hacking it apart del model.layers[N:] . We reject this pattern because of: Fragility : Changes to model architecture require changes to the external slicing script. Leaky Abstractions : Forward methods become full of if self.layer is not None . Invalid States : The model object exists in a \"zombie\" state until sliced. In d9d, models are Pipeline-Aware . Each pipeline rank constructs only the sub-graph it owns. The object returned is compliant, complete, and valid immediately.","title":"Construction Consistency (No Patching)"},{"location":"models/3_pipeline_parallelism/#making-models-compatible","text":"","title":"Making Models Compatible"},{"location":"models/3_pipeline_parallelism/#the-protocol","text":"Implementing the Protocol To use Pipeline Parallelism in d9d, your model must implement the d9d.pipelining.api.ModuleSupportsPipelining protocol to allow the framework to manage memory and buffer allocations. Forward Compatibility Pipelined models currently only support outputting a dictionary ( dict[str, torch.Tensor] ). However, we plan to support arbitrary PyTrees in further releases. The keys in the dictionary returned by your forward method must strictly match the keys in the dictionary calculated by infer_stage_outputs_from_pipeline_inputs . The named arguments accepted by your forward method must strictly match the infer_stage_inputs_from_pipeline_inputs . This allows the communication handler to map tensor names to P2P buffers deterministically.","title":"The Protocol"},{"location":"models/3_pipeline_parallelism/#example","text":"Below is a skeleton of a Transformer-like model implemented for d9d pipelining. import torch from torch import nn from d9d.pipelining.api import PipelineStageInfo , distribute_layers_for_pipeline_stage class MyModelChunk ( nn . Module ): def __init__ ( self , stage : PipelineStageInfo , config ): super () . __init__ () self . stage = stage self . config = config # 1. Determine what layers live here self . start_layer , self . end_layer = distribute_layers_for_pipeline_stage ( config . n_layers , num_virtual_layers_pre = 1 , num_virtual_layers_post = 1 , stage = stage ) # 2. Build sub-modules (using ModuleDict - for compatibility) self . layers = nn . ModuleDict ({ str ( layer ): TransformerBlock ( ... ) for layer in range ( self . start_layer , self . end_layer ) }) # Only build embeddings on first stage if stage . is_current_stage_first : self . embed = nn . Embedding ( ... ) # Only build head on last stage if stage . is_current_stage_last : self . head = nn . Linear ( ... ) def forward ( self , input_ids = None , hidden_states = None ): # Run embeddings only on first stage if self . stage . is_current_stage_first : x = self . embed ( input_ids ) else : x = hidden_states # Run local layers for layer_idx in range ( self . start_layer , self . end_layer ): x = self . layers [ str ( layer_idx )]( x ) outputs = { \"hidden_states\" : x } # Last stage logic if self . stage . is_current_stage_last : logits = self . head ( x ) outputs [ 'logits' ] = logits return outputs # --- Protocol Implementation --- def infer_stage_inputs_from_pipeline_inputs ( self , inputs : dict [ str , torch . Tensor ], n_microbatches : int ): batch_size = inputs [ 'input_ids' ] . shape [ 0 ] micro_batch_size = batch_size // n_microbatches seq_len = inputs [ 'input_ids' ] . shape [ 1 ] if self . stage . is_current_stage_first : # First stage receives raw input IDs return { \"input_ids\" : torch . empty (( micro_batch_size , seq_len ), dtype = torch . long )} else : # Intermediate stages receive hidden states from previous stage return { \"hidden_states\" : torch . empty (( micro_batch_size , seq_len , self . hidden_dim ))} def infer_stage_outputs_from_pipeline_inputs ( self , inputs : dict [ str , torch . Tensor ], n_microbatches : int ): batch_size = inputs [ 'input_ids' ] . shape [ 0 ] micro_batch_size = batch_size // n_microbatches seq_len = inputs [ 'input_ids' ] . shape [ 1 ] outputs = { \"hidden_states\" : torch . empty (( micro_batch_size , seq_len , self . config . hidden_dim ))} if self . stage . is_current_stage_last : # Last stage outputs logits too outputs [ \"logits\" ] = torch . empty (( micro_batch_size , seq_len , self . config . vocab_size )) return outputs","title":"Example"},{"location":"models/3_pipeline_parallelism/#using-the-pipeline","text":"","title":"Using the Pipeline"},{"location":"models/3_pipeline_parallelism/#supported-schedules","text":"Example JSON Description {\"schedule\": \"inference\"} Configuration for inference-only pipeline execution. Runs all forward passes sequentially without any backward passes. {\"schedule\": \"gpipe\"} Standard GPipe execution. Assumes a single stage per rank and processes all microbatches for the forward pass before switching to the backward pass. {\"schedule\": \"looped_bfs\", \"num_stages_per_rank\": 2} Looped Breadth-First Search execution. Supports multiple stages per rank (virtualization) and executes all work for a specific stage before moving to the next. {\"schedule\": \"1f1b\", \"num_stages_per_rank\": 1, \"zero_bubble\": true} Interleaved 1F1B and Interleaved Zero Bubble execution. Supports multiple stages per rank. Handles sharding backward passes to dI and dW when zero_bubble is enabled. {\"schedule\": \"zero_bubble_v\"} Zero Bubble V (ZBV) execution. A specialized V-shape topology schedule that splits backward passes into Input and Weight gradients. Requires exactly 2 stages per rank. {\"schedule\": \"dual_pipe_v\"} DualPipeV execution. A bidirectional pipeline schedule for high-throughput training using V-shape topology and reciprocal forward/backward scheduling.","title":"Supported Schedules"},{"location":"models/3_pipeline_parallelism/#batch-sharding","text":"Pipelining works by splitting the input batch into N microbatches. By default, d9d assumes all input and output tensors should be split along dimension 0. However, if your inputs require different sharding strategy, you can customize this via PipelineShardingSpec . Please see the sharding utils docs . from d9d.pipelining.api import PipelineShardingSpec from d9d.core.sharding import ShardingSpec from torch.distributed.tensor import Shard # Example: Split 'images' on dim 1, but replicate 'camera_angles' across all microbatches my_spec = PipelineShardingSpec ( input_data = { \"images\" : Shard ( 1 ), \"camera_angles\" : None } # input_kwargs can be defined similarly )","title":"Batch Sharding"},{"location":"models/3_pipeline_parallelism/#usage-within-the-trainer","text":"Pipelining is available in the Trainer framework. When configuring the Trainer, simply provide an AnyPipelineScheduleConfig in your training arguments. The Trainer handles the construction of the schedule and the distribution of layers automatically.","title":"Usage within the Trainer"},{"location":"models/3_pipeline_parallelism/#advanced-manual-usage","text":"If you want to use pipelining outside the Trainer (e.g., custom loops), you use the build_schedule factory. The build_schedule function requires a Model Provider logic. Instead of passing an instantiated model, you pass a function that accepts PipelineStageInfo and returns the nn.Module for that stage. This ensures construction consistency. from torch import Tensor from torch.distributed.tensor import Shard import torch.nn.functional as F from d9d.core.dist_context import DistributedContext from d9d.core.sharding import shard_tree from d9d.pipelining.factory import build_schedule , PipelineSchedule1F1BConfig from d9d.pipelining.api import PipelineShardingSpec # 0. Define an object that manages loss calculation across steps class PipelineLossHandler : def __init__ ( self , num_microbatches : int ): self . _shard_spec = { 'target' : Shard ( 0 ) } self . _num_microbatches = num_microbatches self . _targets = None def set_targets ( self , targets : Tensor ): self . _targets = shard_tree ( { 'target' : targets }, sharding_spec = self . _shard_spec , num_shards = self . _num_microbatches , enforce_even_split = True ) def compute_loss ( self , outputs : dict [ str , Tensor ], microbatch_idx : int ): # Implement any custom logic here current_target = self . _targets [ microbatch_idx ] return F . cross_entropy ( outputs [ 'logits' ] . view ( - 1 , outputs [ 'logits' ] . shape [ - 1 ]), current_target . view ( - 1 )) # 1. Define configuration dist_context : DistributedContext = ... model_config = ... n_microbatches = 32 schedule_config = PipelineSchedule1F1BConfig ( num_stages_per_rank = 4 , # 4 Virtual stages per rank zero_bubble = True # Enable ZB1P optimization ) # 2. Build the schedule, model shards and loss compute function loss_handler = PipelineLossHandler ( num_microbatches = n_microbatches ) schedule_info , modules = build_schedule ( dist_context = dist_context , n_microbatches = 32 , schedule_config = schedule_config , model_provider = lambda stage : MyModelChunk ( stage , model_config ), # Factory function callback = loss_handler . compute_loss ) # 3. Execution # The schedule object exposes a simple step API inputs = { \"input_ids\" : ... } # Full batch loss_handler . set_targets ( ... ) # Set targets for full batch schedule_info . schedule . configure_buffers ( # Pre-allocate buffers inputs , kwargs = {}, sharding_spec = PipelineShardingSpec () ) schedule_info . schedule . step ( inputs , kwargs = {})","title":"Advanced - Manual Usage"},{"location":"models/3_pipeline_parallelism/#d9d.pipelining.api","text":"Pipelining API that is intended to be accessible by end user.","title":"api"},{"location":"models/3_pipeline_parallelism/#d9d.pipelining.api.PipelineLossFn","text":"Callback function type for calculating loss in the final pipeline stage. Parameters: Name Type Description Default outputs A dictionary mapping output names to tensors produced by the model. required microbatch_idx The index of the current micro-batch being processed. required Returns: Type Description The computed loss tensor (scalar).","title":"PipelineLossFn"},{"location":"models/3_pipeline_parallelism/#d9d.pipelining.api.PipelineResultFn","text":"Callback function type for handling results from a final pipeline stage. Parameters: Name Type Description Default outputs A dictionary mapping output names to tensors produced by the stage. required microbatch_idx The index of the current micro-batch being processed. required Returns: Type Description Anything - not used.","title":"PipelineResultFn"},{"location":"models/3_pipeline_parallelism/#d9d.pipelining.api.ModuleSupportsPipelining","text":"Bases: Protocol Protocol for modules that support pipeline parallelism metadata inference. Classes implementing this protocol enable the framework to pre-calculate tensor shapes and types required for inter-stage communication (p2p) without executing the full forward pass. Source code in d9d/pipelining/api/module.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 @typing . runtime_checkable class ModuleSupportsPipelining ( typing . Protocol ): \"\"\" Protocol for modules that support pipeline parallelism metadata inference. Classes implementing this protocol enable the framework to pre-calculate tensor shapes and types required for inter-stage communication (p2p) without executing the full forward pass. \"\"\" def infer_stage_inputs_from_pipeline_inputs ( self , inputs : dict [ str , torch . Tensor ], n_microbatches : int ) -> dict [ str , torch . Tensor ]: \"\"\" Infers the input tensors metadata for the current pipeline stage based on global batch inputs. Args: inputs: Global inputs for the pipeline. n_microbatches: Number of microbatches the global batch is split into. Returns: Dictionary of input tensors expected by this specific stage locally. \"\"\" ... def infer_stage_outputs_from_pipeline_inputs ( self , inputs : dict [ str , torch . Tensor ], n_microbatches : int ) -> dict [ str , torch . Tensor ]: \"\"\" Infers the output tensors metadata for the current pipeline stage based on global batch inputs. Args: inputs: Global inputs for the pipeline (typically a batch). n_microbatches: Number of microbatches the global batch is split into. Returns: Dictionary of output tensors produced by this specific stage locally. \"\"\" ...","title":"ModuleSupportsPipelining"},{"location":"models/3_pipeline_parallelism/#d9d.pipelining.api.ModuleSupportsPipelining.infer_stage_inputs_from_pipeline_inputs","text":"Infers the input tensors metadata for the current pipeline stage based on global batch inputs. Parameters: Name Type Description Default inputs dict [ str , Tensor ] Global inputs for the pipeline. required n_microbatches int Number of microbatches the global batch is split into. required Returns: Type Description dict [ str , Tensor ] Dictionary of input tensors expected by this specific stage locally. Source code in d9d/pipelining/api/module.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def infer_stage_inputs_from_pipeline_inputs ( self , inputs : dict [ str , torch . Tensor ], n_microbatches : int ) -> dict [ str , torch . Tensor ]: \"\"\" Infers the input tensors metadata for the current pipeline stage based on global batch inputs. Args: inputs: Global inputs for the pipeline. n_microbatches: Number of microbatches the global batch is split into. Returns: Dictionary of input tensors expected by this specific stage locally. \"\"\" ...","title":"infer_stage_inputs_from_pipeline_inputs"},{"location":"models/3_pipeline_parallelism/#d9d.pipelining.api.ModuleSupportsPipelining.infer_stage_outputs_from_pipeline_inputs","text":"Infers the output tensors metadata for the current pipeline stage based on global batch inputs. Parameters: Name Type Description Default inputs dict [ str , Tensor ] Global inputs for the pipeline (typically a batch). required n_microbatches int Number of microbatches the global batch is split into. required Returns: Type Description dict [ str , Tensor ] Dictionary of output tensors produced by this specific stage locally. Source code in d9d/pipelining/api/module.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def infer_stage_outputs_from_pipeline_inputs ( self , inputs : dict [ str , torch . Tensor ], n_microbatches : int ) -> dict [ str , torch . Tensor ]: \"\"\" Infers the output tensors metadata for the current pipeline stage based on global batch inputs. Args: inputs: Global inputs for the pipeline (typically a batch). n_microbatches: Number of microbatches the global batch is split into. Returns: Dictionary of output tensors produced by this specific stage locally. \"\"\" ...","title":"infer_stage_outputs_from_pipeline_inputs"},{"location":"models/3_pipeline_parallelism/#d9d.pipelining.api.PipelineSchedule","text":"Bases: ABC Abstract base class defining the interface for pipeline execution schedules. Source code in d9d/pipelining/api/schedule.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 class PipelineSchedule ( abc . ABC ): \"\"\"Abstract base class defining the interface for pipeline execution schedules.\"\"\" @abc . abstractmethod def configure_buffers ( self , inputs : dict [ str , torch . Tensor ], kwargs : dict [ str , Any ], sharding_spec : PipelineShardingSpec | None ): \"\"\" Configures internal state and buffers based on input shapes. This method allows the schedule to pre-allocate memory or setup sharding specifications based on the structure of the input data before execution begins. Args: inputs: A dictionary of input tensors. kwargs: A dictionary of keyword arguments. sharding_spec: A specification defining how inputs and kwargs should be split into micro-batches. If None, assumes standard split-by-zero-dim behavior. \"\"\" ... @abc . abstractmethod def step ( self , inputs : dict [ str , torch . Tensor ], kwargs : dict [ str , Any ]): \"\"\" Executes a single pipeline step using the provided inputs. This typically involves distributing inputs across microbatches, executing forward and backward passes according to the specific schedule logic, and handling communications between stages. Args: inputs: A dictionary of global input tensors. kwargs: A dictionary of global keyword arguments. \"\"\" ...","title":"PipelineSchedule"},{"location":"models/3_pipeline_parallelism/#d9d.pipelining.api.PipelineSchedule.configure_buffers","text":"Configures internal state and buffers based on input shapes. This method allows the schedule to pre-allocate memory or setup sharding specifications based on the structure of the input data before execution begins. Parameters: Name Type Description Default inputs dict [ str , Tensor ] A dictionary of input tensors. required kwargs dict [ str , Any ] A dictionary of keyword arguments. required sharding_spec PipelineShardingSpec | None A specification defining how inputs and kwargs should be split into micro-batches. If None, assumes standard split-by-zero-dim behavior. required Source code in d9d/pipelining/api/schedule.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 @abc . abstractmethod def configure_buffers ( self , inputs : dict [ str , torch . Tensor ], kwargs : dict [ str , Any ], sharding_spec : PipelineShardingSpec | None ): \"\"\" Configures internal state and buffers based on input shapes. This method allows the schedule to pre-allocate memory or setup sharding specifications based on the structure of the input data before execution begins. Args: inputs: A dictionary of input tensors. kwargs: A dictionary of keyword arguments. sharding_spec: A specification defining how inputs and kwargs should be split into micro-batches. If None, assumes standard split-by-zero-dim behavior. \"\"\" ...","title":"configure_buffers"},{"location":"models/3_pipeline_parallelism/#d9d.pipelining.api.PipelineSchedule.step","text":"Executes a single pipeline step using the provided inputs. This typically involves distributing inputs across microbatches, executing forward and backward passes according to the specific schedule logic, and handling communications between stages. Args: inputs: A dictionary of global input tensors. kwargs: A dictionary of global keyword arguments. Source code in d9d/pipelining/api/schedule.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 @abc . abstractmethod def step ( self , inputs : dict [ str , torch . Tensor ], kwargs : dict [ str , Any ]): \"\"\" Executes a single pipeline step using the provided inputs. This typically involves distributing inputs across microbatches, executing forward and backward passes according to the specific schedule logic, and handling communications between stages. Args: inputs: A dictionary of global input tensors. kwargs: A dictionary of global keyword arguments. \"\"\" ...","title":"step"},{"location":"models/3_pipeline_parallelism/#d9d.pipelining.api.PipelineStageInfo","text":"Holds information about the current position within the distributed pipeline. Attributes: Name Type Description current_stage int The 0-based index of the current pipeline stage. num_stages int The total number of stages in the pipeline. Source code in d9d/pipelining/api/module.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 @dataclasses . dataclass class PipelineStageInfo : \"\"\" Holds information about the current position within the distributed pipeline. Attributes: current_stage: The 0-based index of the current pipeline stage. num_stages: The total number of stages in the pipeline. \"\"\" current_stage : int num_stages : int @property def is_current_stage_first ( self ) -> bool : \"\"\" Determines if this is the first stage in the pipeline. Returns: True if current_stage is 0. \"\"\" return self . current_stage == 0 @property def is_current_stage_last ( self ) -> bool : \"\"\" Determines if this is the last stage in the pipeline. Returns: True if current_stage is the last index. \"\"\" return self . current_stage == self . num_stages - 1","title":"PipelineStageInfo"},{"location":"models/3_pipeline_parallelism/#d9d.pipelining.api.PipelineStageInfo.is_current_stage_first","text":"Determines if this is the first stage in the pipeline. Returns: Type Description bool True if current_stage is 0.","title":"is_current_stage_first"},{"location":"models/3_pipeline_parallelism/#d9d.pipelining.api.PipelineStageInfo.is_current_stage_last","text":"Determines if this is the last stage in the pipeline. Returns: Type Description bool True if current_stage is the last index.","title":"is_current_stage_last"},{"location":"models/3_pipeline_parallelism/#d9d.pipelining.api.distribute_layers_for_pipeline_stage","text":"Calculates the layer index range for a specific pipeline stage. This function distributes a given number of layers across multiple pipeline stages as evenly as possible. It accounts for additional, non-layer computational load on the first and last stages (e.g., embeddings and the LM head) by using the concept of 'virtual layers' to reserve capacity. Parameters: Name Type Description Default num_layers int The total number of primary model layers to be distributed (e.g., the transformer blocks). required num_virtual_layers_pre int The number of 'virtual' layers representing the computational cost of modules on the first stage, before the main layers (e.g., token and positional embeddings). required num_virtual_layers_post int The number of 'virtual' layers representing the computational cost of modules on the last stage, after the main layers (e.g., the final layer normalization and LM head). required stage PipelineStageInfo An object containing total stages and current stage index. required Returns: Type Description tuple [ int , int ] A tuple (start_index, end_index), representing the slice of layers for the given stage. The start_index is inclusive and the end_index is exclusive. Raises: Type Description ValueError If the pipeline configuration results in a stage having zero or negative layers assigned (pipeline too long for the model size). Source code in d9d/pipelining/api/module.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def distribute_layers_for_pipeline_stage ( num_layers : int , num_virtual_layers_pre : int , num_virtual_layers_post : int , stage : PipelineStageInfo ) -> tuple [ int , int ]: \"\"\" Calculates the layer index range for a specific pipeline stage. This function distributes a given number of layers across multiple pipeline stages as evenly as possible. It accounts for additional, non-layer computational load on the first and last stages (e.g., embeddings and the LM head) by using the concept of 'virtual layers' to reserve capacity. Args: num_layers: The total number of primary model layers to be distributed (e.g., the transformer blocks). num_virtual_layers_pre: The number of 'virtual' layers representing the computational cost of modules on the *first* stage, before the main layers (e.g., token and positional embeddings). num_virtual_layers_post: The number of 'virtual' layers representing the computational cost of modules on the *last* stage, after the main layers (e.g., the final layer normalization and LM head). stage: An object containing total stages and current stage index. Returns: A tuple (start_index, end_index), representing the slice of layers for the given stage. The start_index is inclusive and the end_index is exclusive. Raises: ValueError: If the pipeline configuration results in a stage having zero or negative layers assigned (pipeline too long for the model size). \"\"\" num_layers_virtual = num_layers + num_virtual_layers_pre + num_virtual_layers_post base_layers_per_stage = num_layers_virtual // stage . num_stages extra_layers = num_layers_virtual % stage . num_stages layer_count_per_stage = [] for proposed_stage_i in range ( stage . num_stages ): proposed_stage = PipelineStageInfo ( num_stages = stage . num_stages , current_stage = proposed_stage_i ) layers = base_layers_per_stage + 1 if proposed_stage_i < extra_layers else base_layers_per_stage adjustment = 0 if proposed_stage . is_current_stage_first : adjustment += num_virtual_layers_pre if proposed_stage . is_current_stage_last : adjustment += num_virtual_layers_post actual_layers = layers - adjustment if actual_layers <= 0 : raise ValueError ( f \"Tried to distribute layers, but got { actual_layers } on \" f \"stage { proposed_stage . current_stage } . Perhaps the pipeline is too long for this model?\" ) layer_count_per_stage . append ( actual_layers ) start_layer_id = sum ( layer_count_per_stage [: stage . current_stage ]) num_layers_in_stage = layer_count_per_stage [ stage . current_stage ] return start_layer_id , start_layer_id + num_layers_in_stage","title":"distribute_layers_for_pipeline_stage"},{"location":"models/3_pipeline_parallelism/#d9d.pipelining.factory","text":"","title":"factory"},{"location":"models/3_pipeline_parallelism/#d9d.pipelining.factory.AnyPipelineScheduleConfig","text":"Union of all supported pipeline schedule configuration types. This type alias uses a Pydantic discriminator on the schedule field to allow polymorphic validation and serialization of specific schedule configs (e.g. Inference, GPipe, 1F1B, ZeroBubble, etc.).","title":"AnyPipelineScheduleConfig"},{"location":"models/3_pipeline_parallelism/#d9d.pipelining.factory.PipelineSchedule1F1BConfig","text":"Bases: BaseModel Configuration for Interleaved 1F1B and Interleaved Zero Bubble execution. Supports assigning multiple stages per rank and sharding backward to dI and dW to reduce pipeline bubbles. Source code in d9d/pipelining/factory/config.py 40 41 42 43 44 45 46 47 48 49 50 51 class PipelineSchedule1F1BConfig ( BaseModel ): \"\"\" Configuration for Interleaved 1F1B and Interleaved Zero Bubble execution. Supports assigning multiple stages per rank and sharding backward to dI and dW to reduce pipeline bubbles. \"\"\" schedule : Literal [ \"1f1b\" ] = \"1f1b\" num_stages_per_rank : int zero_bubble : bool","title":"PipelineSchedule1F1BConfig"},{"location":"models/3_pipeline_parallelism/#d9d.pipelining.factory.PipelineScheduleDualPipeVConfig","text":"Bases: BaseModel Configuration for DualPipeV execution. A bidirectional pipeline schedule for high-throughput training, utilizing V-shape topology and reciprocal forward/backward scheduling. Source code in d9d/pipelining/factory/config.py 65 66 67 68 69 70 71 72 73 class PipelineScheduleDualPipeVConfig ( BaseModel ): \"\"\" Configuration for DualPipeV execution. A bidirectional pipeline schedule for high-throughput training, utilizing V-shape topology and reciprocal forward/backward scheduling. \"\"\" schedule : Literal [ \"dual_pipe_v\" ] = \"dual_pipe_v\"","title":"PipelineScheduleDualPipeVConfig"},{"location":"models/3_pipeline_parallelism/#d9d.pipelining.factory.PipelineScheduleGPipeConfig","text":"Bases: BaseModel Configuration for GPipe execution. This assumes a single stage per rank and processes all microbatches for the forward pass before switching to the backward pass. Source code in d9d/pipelining/factory/config.py 16 17 18 19 20 21 22 23 24 class PipelineScheduleGPipeConfig ( BaseModel ): \"\"\" Configuration for GPipe execution. This assumes a single stage per rank and processes all microbatches for the forward pass before switching to the backward pass. \"\"\" schedule : Literal [ \"gpipe\" ] = \"gpipe\"","title":"PipelineScheduleGPipeConfig"},{"location":"models/3_pipeline_parallelism/#d9d.pipelining.factory.PipelineScheduleInferenceConfig","text":"Bases: BaseModel Configuration for inference-only pipeline execution. This schedule runs all forward passes sequentially without any backward passes. Source code in d9d/pipelining/factory/config.py 6 7 8 9 10 11 12 13 class PipelineScheduleInferenceConfig ( BaseModel ): \"\"\" Configuration for inference-only pipeline execution. This schedule runs all forward passes sequentially without any backward passes. \"\"\" schedule : Literal [ \"inference\" ] = \"inference\"","title":"PipelineScheduleInferenceConfig"},{"location":"models/3_pipeline_parallelism/#d9d.pipelining.factory.PipelineScheduleLoopedBFSConfig","text":"Bases: BaseModel Configuration for Looped Breadth-First Search execution. Similar to GPipe, but supports multiple stages per rank (virtualization). It executes all available work for a specific stage before moving to the next. Source code in d9d/pipelining/factory/config.py 27 28 29 30 31 32 33 34 35 36 37 class PipelineScheduleLoopedBFSConfig ( BaseModel ): \"\"\" Configuration for Looped Breadth-First Search execution. Similar to GPipe, but supports multiple stages per rank (virtualization). It executes all available work for a specific stage before moving to the next. \"\"\" schedule : Literal [ \"looped_bfs\" ] = \"looped_bfs\" num_stages_per_rank : int","title":"PipelineScheduleLoopedBFSConfig"},{"location":"models/3_pipeline_parallelism/#d9d.pipelining.factory.PipelineScheduleZeroBubbleVConfig","text":"Bases: BaseModel Configuration for Zero Bubble V (ZBV) execution. A specialized V-shape topology schedule that splits backward passes into Input and Weight gradients to maximize overlap. Requires exactly 2 stages per rank. Source code in d9d/pipelining/factory/config.py 54 55 56 57 58 59 60 61 62 class PipelineScheduleZeroBubbleVConfig ( BaseModel ): \"\"\" Configuration for Zero Bubble V (ZBV) execution. A specialized V-shape topology schedule that splits backward passes into Input and Weight gradients to maximize overlap. Requires exactly 2 stages per rank. \"\"\" schedule : Literal [ \"zero_bubble_v\" ] = \"zero_bubble_v\"","title":"PipelineScheduleZeroBubbleVConfig"},{"location":"models/3_pipeline_parallelism/#d9d.pipelining.factory.build_schedule","text":"Constructs the pipeline schedule and instantiates model stages. This function coordinates the creation of the pipeline. If the context is distributed, it builds a parallel schedule ( PipelineScheduleExecutor ) by calculating topology and creating stages for the current rank. If the context is local, it builds an offline schedule ( OfflinePipelineExecutor ) for direct execution. Parameters: Name Type Description Default dist_context DistributedContext The distributed context. required n_microbatches int Number of microbatches per global step. required schedule_config AnyPipelineScheduleConfig Configuration object determining the schedule strategy. required model_provider Callable [[ PipelineStageInfo ], Module ] A factory function that accepts stage info and returns an nn.Module for that specific stage. required callback PipelineLossFn | PipelineResultFn Callback either computing loss function (if training) or just processing pipeline outputs (if not training). required Returns: Type Description PipelineScheduleInfo A tuple containing the schedule info (executor and metadata) and a list list [ Module ] of local PyTorch modules created for this rank. Source code in d9d/pipelining/factory/factory.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def build_schedule ( dist_context : DistributedContext , n_microbatches : int , schedule_config : AnyPipelineScheduleConfig , model_provider : Callable [[ PipelineStageInfo ], nn . Module ], callback : PipelineLossFn | PipelineResultFn , ) -> tuple [ PipelineScheduleInfo , list [ nn . Module ]]: \"\"\" Constructs the pipeline schedule and instantiates model stages. This function coordinates the creation of the pipeline. If the context is distributed, it builds a parallel schedule (`PipelineScheduleExecutor`) by calculating topology and creating stages for the current rank. If the context is local, it builds an offline schedule (`OfflinePipelineExecutor`) for direct execution. Args: dist_context: The distributed context. n_microbatches: Number of microbatches per global step. schedule_config: Configuration object determining the schedule strategy. model_provider: A factory function that accepts stage info and returns an `nn.Module` for that specific stage. callback: Callback either computing loss function (if training) or just processing pipeline outputs (if not training). Returns: A tuple containing the schedule info (executor and metadata) and a list of local PyTorch modules created for this rank. \"\"\" if dist_context . mesh_params . is_distributed : return _build_schedule_distributed ( dist_context = dist_context , n_microbatches = n_microbatches , schedule_config = schedule_config , model_provider = model_provider , callback = callback , ) else : return _build_schedule_local ( schedule_config = schedule_config , model_provider = model_provider , callback = callback )","title":"build_schedule"},{"location":"models/4_model_catalogue/","text":"Qwen3-MoE","title":"Model Catalogue"},{"location":"models/qwen3_moe/","text":"About The d9d.module.model.qwen3_moe package implements the Qwen3 Mixture-of-Experts model architecture. The d9d.module.parallelism.model.qwen3_moe package implements default horizontal parallelism strategy for this model. d9d.module.model.qwen3_moe Qwen3MoEForCausalLM Bases: Module , ModuleLateInit , ModuleSupportsPipelining A Qwen3 MoE model wrapped with a Causal Language Modeling head. It is designed to be split across multiple pipeline stages. Source code in d9d/module/model/qwen3_moe/model.py 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 class Qwen3MoEForCausalLM ( nn . Module , ModuleLateInit , ModuleSupportsPipelining ): \"\"\" A Qwen3 MoE model wrapped with a Causal Language Modeling head. It is designed to be split across multiple pipeline stages. \"\"\" def __init__ ( self , params : Qwen3MoEForCausalLMParameters , stage : PipelineStageInfo , hidden_states_snapshot_mode : HiddenStatesAggregationMode , enable_checkpointing : bool , ): \"\"\" Constructs the Qwen3MoEForCausalLM object. Args: params: Full model configuration parameters. stage: Pipeline stage information for this instance. hidden_states_snapshot_mode: Configures intermediate hidden state aggregation & snapshotting mode. enable_checkpointing: Whether to enable activation checkpointing. \"\"\" super () . __init__ () self . model = Qwen3MoEModel ( params . model , stage , hidden_states_snapshot_mode = hidden_states_snapshot_mode , enable_checkpointing = enable_checkpointing , ) if stage . is_current_stage_last : self . lm_head = SplitLanguageModellingHead ( split_vocab_size = params . model . split_vocab_size , split_order = params . model . split_vocab_order , hidden_size = params . model . layer . hidden_size , ) self . _stage = stage self . _hidden_size = params . model . layer . hidden_size def forward ( self , input_ids : torch . Tensor | None = None , hidden_states : torch . Tensor | None = None , position_ids : torch . Tensor | None = None , hidden_states_snapshot : torch . Tensor | None = None , hidden_states_agg_mask : torch . Tensor | None = None , labels : torch . Tensor | None = None , ) -> dict [ str , torch . Tensor ]: \"\"\" Executes the model forward pass. If this is the last stage, it expects `labels` to be provided and computes the cross-entropy loss (returned as 'logps' typically representing per-token loss). Args: input_ids: Input token IDS (for Stage 0). hidden_states: Hidden states from previous stage (for Stage > 0). position_ids: Positional indices for RoPE. hidden_states_snapshot: Intermediate state collector. hidden_states_agg_mask: Mask for state aggregation. labels: Target tokens for loss computation (Last Stage). Returns: Dictionary containing 'hidden_states', optionally 'hidden_states_snapshot', and per-token 'logps' if on the last stage. \"\"\" model_outputs = self . model ( input_ids = input_ids , hidden_states = hidden_states , position_ids = position_ids , hidden_states_snapshot = hidden_states_snapshot , hidden_states_agg_mask = hidden_states_agg_mask , ) if self . _stage . is_current_stage_last : lm_out = self . lm_head ( hidden_states = model_outputs [ \"hidden_states\" ], labels = labels ) model_outputs [ \"logps\" ] = lm_out return model_outputs def reset_parameters ( self ): \"\"\" Resets module parameters. \"\"\" self . model . reset_parameters () if self . _stage . is_current_stage_last : self . lm_head . reset_parameters () def reset_moe_stats ( self ): \"\"\" Resets MoE routing statistics in the backbone. \"\"\" self . model . reset_moe_stats () @property def moe_tokens_per_expert ( self ) -> torch . Tensor : \"\"\" Accesses MoE routing statistics from the backbone. \"\"\" return self . model . moe_tokens_per_expert def infer_stage_inputs_from_pipeline_inputs ( self , inputs : dict [ str , torch . Tensor ], n_microbatches : int ) -> dict [ str , torch . Tensor ]: return self . model . infer_stage_inputs_from_pipeline_inputs ( inputs , n_microbatches ) def infer_stage_outputs_from_pipeline_inputs ( self , inputs : dict [ str , torch . Tensor ], n_microbatches : int ) -> dict [ str , torch . Tensor ]: pp_outputs = self . model . infer_stage_outputs_from_pipeline_inputs ( inputs , n_microbatches ) if self . _stage . is_current_stage_last : pp_outputs [ \"logps\" ] = torch . empty ( inputs [ \"input_ids\" ] . shape , dtype = torch . float32 ) return pp_outputs moe_tokens_per_expert property Accesses MoE routing statistics from the backbone. __init__ ( params , stage , hidden_states_snapshot_mode , enable_checkpointing ) Constructs the Qwen3MoEForCausalLM object. Parameters: Name Type Description Default params Qwen3MoEForCausalLMParameters Full model configuration parameters. required stage PipelineStageInfo Pipeline stage information for this instance. required hidden_states_snapshot_mode HiddenStatesAggregationMode Configures intermediate hidden state aggregation & snapshotting mode. required enable_checkpointing bool Whether to enable activation checkpointing. required Source code in d9d/module/model/qwen3_moe/model.py 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 def __init__ ( self , params : Qwen3MoEForCausalLMParameters , stage : PipelineStageInfo , hidden_states_snapshot_mode : HiddenStatesAggregationMode , enable_checkpointing : bool , ): \"\"\" Constructs the Qwen3MoEForCausalLM object. Args: params: Full model configuration parameters. stage: Pipeline stage information for this instance. hidden_states_snapshot_mode: Configures intermediate hidden state aggregation & snapshotting mode. enable_checkpointing: Whether to enable activation checkpointing. \"\"\" super () . __init__ () self . model = Qwen3MoEModel ( params . model , stage , hidden_states_snapshot_mode = hidden_states_snapshot_mode , enable_checkpointing = enable_checkpointing , ) if stage . is_current_stage_last : self . lm_head = SplitLanguageModellingHead ( split_vocab_size = params . model . split_vocab_size , split_order = params . model . split_vocab_order , hidden_size = params . model . layer . hidden_size , ) self . _stage = stage self . _hidden_size = params . model . layer . hidden_size forward ( input_ids = None , hidden_states = None , position_ids = None , hidden_states_snapshot = None , hidden_states_agg_mask = None , labels = None ) Executes the model forward pass. If this is the last stage, it expects labels to be provided and computes the cross-entropy loss (returned as 'logps' typically representing per-token loss). Parameters: Name Type Description Default input_ids Tensor | None Input token IDS (for Stage 0). None hidden_states Tensor | None Hidden states from previous stage (for Stage > 0). None position_ids Tensor | None Positional indices for RoPE. None hidden_states_snapshot Tensor | None Intermediate state collector. None hidden_states_agg_mask Tensor | None Mask for state aggregation. None labels Tensor | None Target tokens for loss computation (Last Stage). None Returns: Type Description dict [ str , Tensor ] Dictionary containing 'hidden_states', optionally 'hidden_states_snapshot', dict [ str , Tensor ] and per-token 'logps' if on the last stage. Source code in d9d/module/model/qwen3_moe/model.py 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 def forward ( self , input_ids : torch . Tensor | None = None , hidden_states : torch . Tensor | None = None , position_ids : torch . Tensor | None = None , hidden_states_snapshot : torch . Tensor | None = None , hidden_states_agg_mask : torch . Tensor | None = None , labels : torch . Tensor | None = None , ) -> dict [ str , torch . Tensor ]: \"\"\" Executes the model forward pass. If this is the last stage, it expects `labels` to be provided and computes the cross-entropy loss (returned as 'logps' typically representing per-token loss). Args: input_ids: Input token IDS (for Stage 0). hidden_states: Hidden states from previous stage (for Stage > 0). position_ids: Positional indices for RoPE. hidden_states_snapshot: Intermediate state collector. hidden_states_agg_mask: Mask for state aggregation. labels: Target tokens for loss computation (Last Stage). Returns: Dictionary containing 'hidden_states', optionally 'hidden_states_snapshot', and per-token 'logps' if on the last stage. \"\"\" model_outputs = self . model ( input_ids = input_ids , hidden_states = hidden_states , position_ids = position_ids , hidden_states_snapshot = hidden_states_snapshot , hidden_states_agg_mask = hidden_states_agg_mask , ) if self . _stage . is_current_stage_last : lm_out = self . lm_head ( hidden_states = model_outputs [ \"hidden_states\" ], labels = labels ) model_outputs [ \"logps\" ] = lm_out return model_outputs reset_moe_stats () Resets MoE routing statistics in the backbone. Source code in d9d/module/model/qwen3_moe/model.py 327 328 329 330 331 332 def reset_moe_stats ( self ): \"\"\" Resets MoE routing statistics in the backbone. \"\"\" self . model . reset_moe_stats () reset_parameters () Resets module parameters. Source code in d9d/module/model/qwen3_moe/model.py 317 318 319 320 321 322 323 324 325 def reset_parameters ( self ): \"\"\" Resets module parameters. \"\"\" self . model . reset_parameters () if self . _stage . is_current_stage_last : self . lm_head . reset_parameters () Qwen3MoEForCausalLMParameters Bases: BaseModel Configuration parameters for Qwen3 Mixture-of-Experts model with a Causal Language Modeling head. Attributes: Name Type Description model Qwen3MoEParameters The configuration for the underlying Qwen3 MoE model. Source code in d9d/module/model/qwen3_moe/params.py 61 62 63 64 65 66 67 68 69 class Qwen3MoEForCausalLMParameters ( BaseModel ): \"\"\" Configuration parameters for Qwen3 Mixture-of-Experts model with a Causal Language Modeling head. Attributes: model: The configuration for the underlying Qwen3 MoE model. \"\"\" model : Qwen3MoEParameters Qwen3MoEForClassification Bases: Module , ModuleLateInit , ModuleSupportsPipelining A Qwen3 MoE model wrapped with a Sequence/Token Classification head. It is designed to be split across multiple pipeline stages. Source code in d9d/module/model/qwen3_moe/model.py 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 class Qwen3MoEForClassification ( nn . Module , ModuleLateInit , ModuleSupportsPipelining ): \"\"\" A Qwen3 MoE model wrapped with a Sequence/Token Classification head. It is designed to be split across multiple pipeline stages. \"\"\" def __init__ ( self , params : Qwen3MoEForClassificationParameters , stage : PipelineStageInfo , hidden_states_snapshot_mode : HiddenStatesAggregationMode , enable_checkpointing : bool , ): \"\"\" Constructs the Qwen3MoEForClassification object. Args: params: Full model configuration parameters. stage: Pipeline stage information for this instance. hidden_states_snapshot_mode: Configures intermediate hidden state aggregation & snapshotting mode. enable_checkpointing: Whether to enable activation checkpointing. \"\"\" super () . __init__ () self . model = Qwen3MoEModel ( params . model , stage , hidden_states_snapshot_mode = hidden_states_snapshot_mode , enable_checkpointing = enable_checkpointing , ) if stage . is_current_stage_last : self . cls_head = ClassificationHead ( hidden_size = params . model . layer . hidden_size , num_labels = params . num_labels , dropout = params . classifier_dropout , ) self . _stage = stage self . _hidden_size = params . model . layer . hidden_size self . _num_labels = params . num_labels def forward ( self , input_ids : torch . Tensor | None = None , hidden_states : torch . Tensor | None = None , position_ids : torch . Tensor | None = None , hidden_states_snapshot : torch . Tensor | None = None , hidden_states_agg_mask : torch . Tensor | None = None , pooling_mask : torch . Tensor | None = None , ) -> dict [ str , torch . Tensor ]: \"\"\" Executes the classification model forward pass. Args: input_ids: Input token IDS (for Stage 0). hidden_states: Hidden states from previous stage (for Stage > 0). position_ids: Positional indices for RoPE. hidden_states_snapshot: Intermediate state collector. hidden_states_agg_mask: Mask for state aggregation. pooling_mask: Binary mask indicating which token(s) to pool for classification. Note: you can use `d9d.dataset.token_pooling_mask_from_attention_mask` in your Dataset to preallocate the pooling mask from attention mask. Returns: Dictionary containing 'hidden_states', optionally 'hidden_states_snapshot'. If on the last stage, also contains 'scores' (logits) of shape [batch, num_labels]. \"\"\" model_outputs = self . model ( input_ids = input_ids , hidden_states = hidden_states , position_ids = position_ids , hidden_states_snapshot = hidden_states_snapshot , hidden_states_agg_mask = hidden_states_agg_mask , ) if self . _stage . is_current_stage_last : model_outputs [ \"scores\" ] = self . cls_head ( hidden_states = model_outputs [ \"hidden_states\" ], pooling_mask = pooling_mask ) return model_outputs def reset_parameters ( self ): \"\"\" Resets module parameters. \"\"\" self . model . reset_parameters () if self . _stage . is_current_stage_last : self . cls_head . reset_parameters () def reset_moe_stats ( self ): \"\"\" Resets MoE routing statistics in the backbone. \"\"\" self . model . reset_moe_stats () @property def moe_tokens_per_expert ( self ) -> torch . Tensor : \"\"\" Accesses MoE routing statistics from the backbone. \"\"\" return self . model . moe_tokens_per_expert def infer_stage_inputs_from_pipeline_inputs ( self , inputs : dict [ str , torch . Tensor ], n_microbatches : int ) -> dict [ str , torch . Tensor ]: return self . model . infer_stage_inputs_from_pipeline_inputs ( inputs , n_microbatches ) def infer_stage_outputs_from_pipeline_inputs ( self , inputs : dict [ str , torch . Tensor ], n_microbatches : int ) -> dict [ str , torch . Tensor ]: pp_outputs = self . model . infer_stage_outputs_from_pipeline_inputs ( inputs , n_microbatches ) if self . _stage . is_current_stage_last : batch_size = inputs [ \"input_ids\" ] . shape [ 0 ] // n_microbatches pp_outputs [ \"scores\" ] = torch . empty (( batch_size , self . _num_labels ), dtype = torch . float32 ) return pp_outputs moe_tokens_per_expert property Accesses MoE routing statistics from the backbone. __init__ ( params , stage , hidden_states_snapshot_mode , enable_checkpointing ) Constructs the Qwen3MoEForClassification object. Parameters: Name Type Description Default params Qwen3MoEForClassificationParameters Full model configuration parameters. required stage PipelineStageInfo Pipeline stage information for this instance. required hidden_states_snapshot_mode HiddenStatesAggregationMode Configures intermediate hidden state aggregation & snapshotting mode. required enable_checkpointing bool Whether to enable activation checkpointing. required Source code in d9d/module/model/qwen3_moe/model.py 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 def __init__ ( self , params : Qwen3MoEForClassificationParameters , stage : PipelineStageInfo , hidden_states_snapshot_mode : HiddenStatesAggregationMode , enable_checkpointing : bool , ): \"\"\" Constructs the Qwen3MoEForClassification object. Args: params: Full model configuration parameters. stage: Pipeline stage information for this instance. hidden_states_snapshot_mode: Configures intermediate hidden state aggregation & snapshotting mode. enable_checkpointing: Whether to enable activation checkpointing. \"\"\" super () . __init__ () self . model = Qwen3MoEModel ( params . model , stage , hidden_states_snapshot_mode = hidden_states_snapshot_mode , enable_checkpointing = enable_checkpointing , ) if stage . is_current_stage_last : self . cls_head = ClassificationHead ( hidden_size = params . model . layer . hidden_size , num_labels = params . num_labels , dropout = params . classifier_dropout , ) self . _stage = stage self . _hidden_size = params . model . layer . hidden_size self . _num_labels = params . num_labels forward ( input_ids = None , hidden_states = None , position_ids = None , hidden_states_snapshot = None , hidden_states_agg_mask = None , pooling_mask = None ) Executes the classification model forward pass. Parameters: Name Type Description Default input_ids Tensor | None Input token IDS (for Stage 0). None hidden_states Tensor | None Hidden states from previous stage (for Stage > 0). None position_ids Tensor | None Positional indices for RoPE. None hidden_states_snapshot Tensor | None Intermediate state collector. None hidden_states_agg_mask Tensor | None Mask for state aggregation. None pooling_mask Tensor | None Binary mask indicating which token(s) to pool for classification. Note: you can use d9d.dataset.token_pooling_mask_from_attention_mask in your Dataset to preallocate the pooling mask from attention mask. None Returns: Type Description dict [ str , Tensor ] Dictionary containing 'hidden_states', optionally 'hidden_states_snapshot'. If on the last stage, also contains 'scores' (logits) of shape [batch, num_labels]. Source code in d9d/module/model/qwen3_moe/model.py 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 def forward ( self , input_ids : torch . Tensor | None = None , hidden_states : torch . Tensor | None = None , position_ids : torch . Tensor | None = None , hidden_states_snapshot : torch . Tensor | None = None , hidden_states_agg_mask : torch . Tensor | None = None , pooling_mask : torch . Tensor | None = None , ) -> dict [ str , torch . Tensor ]: \"\"\" Executes the classification model forward pass. Args: input_ids: Input token IDS (for Stage 0). hidden_states: Hidden states from previous stage (for Stage > 0). position_ids: Positional indices for RoPE. hidden_states_snapshot: Intermediate state collector. hidden_states_agg_mask: Mask for state aggregation. pooling_mask: Binary mask indicating which token(s) to pool for classification. Note: you can use `d9d.dataset.token_pooling_mask_from_attention_mask` in your Dataset to preallocate the pooling mask from attention mask. Returns: Dictionary containing 'hidden_states', optionally 'hidden_states_snapshot'. If on the last stage, also contains 'scores' (logits) of shape [batch, num_labels]. \"\"\" model_outputs = self . model ( input_ids = input_ids , hidden_states = hidden_states , position_ids = position_ids , hidden_states_snapshot = hidden_states_snapshot , hidden_states_agg_mask = hidden_states_agg_mask , ) if self . _stage . is_current_stage_last : model_outputs [ \"scores\" ] = self . cls_head ( hidden_states = model_outputs [ \"hidden_states\" ], pooling_mask = pooling_mask ) return model_outputs reset_moe_stats () Resets MoE routing statistics in the backbone. Source code in d9d/module/model/qwen3_moe/model.py 452 453 454 455 456 457 def reset_moe_stats ( self ): \"\"\" Resets MoE routing statistics in the backbone. \"\"\" self . model . reset_moe_stats () reset_parameters () Resets module parameters. Source code in d9d/module/model/qwen3_moe/model.py 442 443 444 445 446 447 448 449 450 def reset_parameters ( self ): \"\"\" Resets module parameters. \"\"\" self . model . reset_parameters () if self . _stage . is_current_stage_last : self . cls_head . reset_parameters () Qwen3MoEForClassificationParameters Bases: BaseModel Configuration parameters for Qwen3 Mixture-of-Experts model with a token/sequnce classification head. Attributes: Name Type Description model Qwen3MoEParameters The configuration for the underlying Qwen3 MoE model. num_labels int The number of output labels for classification. classifier_dropout float The dropout probability for the classification head. Source code in d9d/module/model/qwen3_moe/params.py 72 73 74 75 76 77 78 79 80 81 82 83 84 class Qwen3MoEForClassificationParameters ( BaseModel ): \"\"\" Configuration parameters for Qwen3 Mixture-of-Experts model with a token/sequnce classification head. Attributes: model: The configuration for the underlying Qwen3 MoE model. num_labels: The number of output labels for classification. classifier_dropout: The dropout probability for the classification head. \"\"\" model : Qwen3MoEParameters num_labels : int classifier_dropout : float Qwen3MoELayer Bases: Module , ModuleLateInit Implements a single Qwen3 Mixture-of-Experts (MoE) transformer layer. This layer consists of a Grouped Query Attention mechanism followed by an MoE MLP block, with pre-RMSNorm applied before each sub-layer. Source code in d9d/module/model/qwen3_moe/decoder_layer.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 class Qwen3MoELayer ( nn . Module , ModuleLateInit ): \"\"\" Implements a single Qwen3 Mixture-of-Experts (MoE) transformer layer. This layer consists of a Grouped Query Attention mechanism followed by an MoE MLP block, with pre-RMSNorm applied before each sub-layer. \"\"\" def __init__ ( self , params : Qwen3MoELayerParameters ): \"\"\" Constructs a Qwen3MoELayer object. Args: params: Configuration parameters for the layer. \"\"\" super () . __init__ () self . self_attn = GroupedQueryAttention ( hidden_size = params . hidden_size , num_attention_heads = params . num_attention_heads , num_key_value_heads = params . num_key_value_heads , is_causal = True , qk_norm_eps = params . rms_norm_eps , head_dim = params . head_dim , ) self . mlp = MoELayer ( hidden_dim = params . hidden_size , num_grouped_experts = params . num_experts , intermediate_dim_grouped = params . intermediate_size , top_k = params . experts_top_k , router_renormalize_probabilities = True , ) self . input_layernorm = nn . RMSNorm ( params . hidden_size , eps = params . rms_norm_eps ) self . post_attention_layernorm = nn . RMSNorm ( params . hidden_size , eps = params . rms_norm_eps ) def forward ( self , hidden_states : torch . Tensor , position_embeddings : tuple [ torch . Tensor , torch . Tensor ] ) -> torch . Tensor : \"\"\" Performs the forward pass of the MoE layer. Args: hidden_states: Input tensor of shape `(batch, seq_len, hidden_dim)`. position_embeddings: Tuple containing RoPE precomputed embeddings (cos, sin). Returns: Output tensor after attention and MoE blocks, shape `(batch, seq_len, hidden_dim)`. \"\"\" residual = hidden_states hidden_states = self . input_layernorm ( hidden_states ) hidden_states = self . self_attn ( hidden_states = hidden_states , position_embeddings = position_embeddings , attention_mask = None , # no mask for moe decoder ) hidden_states = residual + hidden_states residual = hidden_states hidden_states = self . post_attention_layernorm ( hidden_states ) hidden_states = self . mlp ( hidden_states ) hidden_states = residual + hidden_states return hidden_states def reset_moe_stats ( self ): \"\"\" Resets statistical counters inside the MoE router (e.g., token counts per expert). \"\"\" self . mlp . reset_stats () @property def moe_tokens_per_expert ( self ) -> torch . Tensor : \"\"\" Returns the number of tokens routed to each expert. \"\"\" return self . mlp . tokens_per_expert def reset_parameters ( self ): \"\"\" Resets module parameters. \"\"\" self . self_attn . reset_parameters () self . mlp . reset_parameters () self . input_layernorm . reset_parameters () self . post_attention_layernorm . reset_parameters () moe_tokens_per_expert property Returns the number of tokens routed to each expert. __init__ ( params ) Constructs a Qwen3MoELayer object. Parameters: Name Type Description Default params Qwen3MoELayerParameters Configuration parameters for the layer. required Source code in d9d/module/model/qwen3_moe/decoder_layer.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def __init__ ( self , params : Qwen3MoELayerParameters ): \"\"\" Constructs a Qwen3MoELayer object. Args: params: Configuration parameters for the layer. \"\"\" super () . __init__ () self . self_attn = GroupedQueryAttention ( hidden_size = params . hidden_size , num_attention_heads = params . num_attention_heads , num_key_value_heads = params . num_key_value_heads , is_causal = True , qk_norm_eps = params . rms_norm_eps , head_dim = params . head_dim , ) self . mlp = MoELayer ( hidden_dim = params . hidden_size , num_grouped_experts = params . num_experts , intermediate_dim_grouped = params . intermediate_size , top_k = params . experts_top_k , router_renormalize_probabilities = True , ) self . input_layernorm = nn . RMSNorm ( params . hidden_size , eps = params . rms_norm_eps ) self . post_attention_layernorm = nn . RMSNorm ( params . hidden_size , eps = params . rms_norm_eps ) forward ( hidden_states , position_embeddings ) Performs the forward pass of the MoE layer. Parameters: Name Type Description Default hidden_states Tensor Input tensor of shape (batch, seq_len, hidden_dim) . required position_embeddings tuple [ Tensor , Tensor ] Tuple containing RoPE precomputed embeddings (cos, sin). required Returns: Type Description Tensor Output tensor after attention and MoE blocks, shape (batch, seq_len, hidden_dim) . Source code in d9d/module/model/qwen3_moe/decoder_layer.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def forward ( self , hidden_states : torch . Tensor , position_embeddings : tuple [ torch . Tensor , torch . Tensor ] ) -> torch . Tensor : \"\"\" Performs the forward pass of the MoE layer. Args: hidden_states: Input tensor of shape `(batch, seq_len, hidden_dim)`. position_embeddings: Tuple containing RoPE precomputed embeddings (cos, sin). Returns: Output tensor after attention and MoE blocks, shape `(batch, seq_len, hidden_dim)`. \"\"\" residual = hidden_states hidden_states = self . input_layernorm ( hidden_states ) hidden_states = self . self_attn ( hidden_states = hidden_states , position_embeddings = position_embeddings , attention_mask = None , # no mask for moe decoder ) hidden_states = residual + hidden_states residual = hidden_states hidden_states = self . post_attention_layernorm ( hidden_states ) hidden_states = self . mlp ( hidden_states ) hidden_states = residual + hidden_states return hidden_states reset_moe_stats () Resets statistical counters inside the MoE router (e.g., token counts per expert). Source code in d9d/module/model/qwen3_moe/decoder_layer.py 82 83 84 85 86 87 def reset_moe_stats ( self ): \"\"\" Resets statistical counters inside the MoE router (e.g., token counts per expert). \"\"\" self . mlp . reset_stats () reset_parameters () Resets module parameters. Source code in d9d/module/model/qwen3_moe/decoder_layer.py 97 98 99 100 101 102 103 104 105 def reset_parameters ( self ): \"\"\" Resets module parameters. \"\"\" self . self_attn . reset_parameters () self . mlp . reset_parameters () self . input_layernorm . reset_parameters () self . post_attention_layernorm . reset_parameters () Qwen3MoELayerParameters Bases: BaseModel Configuration parameters for a single Qwen3 MoE layer. Attributes: Name Type Description hidden_size int Dimension of the model's hidden states. intermediate_size int Dimension of the feed-forward hidden state. num_experts int Total number of experts in the MoE layer. experts_top_k int Number of experts to route tokens to. num_attention_heads int Number of attention heads for the query. num_key_value_heads int Number of attention heads for key and value. rms_norm_eps float Epsilon value found in the RMSNorm layers. head_dim int Dimension of a single attention head. Source code in d9d/module/model/qwen3_moe/params.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 class Qwen3MoELayerParameters ( BaseModel ): \"\"\" Configuration parameters for a single Qwen3 MoE layer. Attributes: hidden_size: Dimension of the model's hidden states. intermediate_size: Dimension of the feed-forward hidden state. num_experts: Total number of experts in the MoE layer. experts_top_k: Number of experts to route tokens to. num_attention_heads: Number of attention heads for the query. num_key_value_heads: Number of attention heads for key and value. rms_norm_eps: Epsilon value found in the RMSNorm layers. head_dim: Dimension of a single attention head. \"\"\" hidden_size : int intermediate_size : int num_experts : int experts_top_k : int num_attention_heads : int num_key_value_heads : int rms_norm_eps : float head_dim : int Qwen3MoEModel Bases: Module , ModuleLateInit , ModuleSupportsPipelining The Qwen3 Mixture-of-Experts (MoE) Transformer Decoder backbone. It is designed to be split across multiple pipeline stages. Source code in d9d/module/model/qwen3_moe/model.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 class Qwen3MoEModel ( nn . Module , ModuleLateInit , ModuleSupportsPipelining ): \"\"\" The Qwen3 Mixture-of-Experts (MoE) Transformer Decoder backbone. It is designed to be split across multiple pipeline stages. \"\"\" def __init__ ( self , params : Qwen3MoEParameters , stage : PipelineStageInfo , hidden_states_snapshot_mode : HiddenStatesAggregationMode , enable_checkpointing : bool , ): \"\"\" Constructs the Qwen3MoEModel object. Args: params: Configuration parameters for the full model. stage: Information about the pipeline stage this instance belongs to. hidden_states_snapshot_mode: Configures intermediate hidden state aggregation & snapshotting mode enable_checkpointing: If True, enables activation checkpointing for transformer layers to save memory. \"\"\" super () . __init__ () if stage . is_current_stage_first : self . embed_tokens = SplitTokenEmbeddings ( hidden_size = params . layer . hidden_size , split_vocab_size = params . split_vocab_size , split_order = params . split_vocab_order , ) # we use ModuleDict here to properly handle pipelining and loading weights after the model # was pipelined layer_start , layer_end = distribute_layers_for_pipeline_stage ( num_layers = params . num_hidden_layers , num_virtual_layers_pre = params . pipeline_num_virtual_layers_pre , # embeddings num_virtual_layers_post = params . pipeline_num_virtual_layers_post , # LM head stage = stage , ) self . _num_layers_before = layer_start self . _layers_iter = list ( map ( str , range ( layer_start , layer_end ))) layers = nn . ModuleDict ({ str ( layer_idx ): Qwen3MoELayer ( params = params . layer ) for layer_idx in self . _layers_iter }) self . layers : Mapping [ str , Qwen3MoELayer ] = cast ( Mapping [ str , Qwen3MoELayer ], layers ) self . rope_provider = RotaryEmbeddingProvider ( max_position_ids = params . max_position_ids , rope_base = params . rope_base , head_dim = params . layer . head_dim ) if stage . is_current_stage_last : self . norm = nn . RMSNorm ( normalized_shape = params . layer . hidden_size , eps = params . layer . rms_norm_eps ) self . _stage = stage self . _hidden_states_snapshot_mode = hidden_states_snapshot_mode self . _hidden_size = params . layer . hidden_size self . _enable_checkpointing = enable_checkpointing def output_dtype ( self ) -> torch . dtype : \"\"\" Returns the data type of the model output hidden states. \"\"\" return self . layers [ self . _layers_iter [ 0 ]] . input_layernorm . weight . dtype def forward ( self , input_ids : torch . Tensor | None = None , hidden_states : torch . Tensor | None = None , position_ids : torch . Tensor | None = None , hidden_states_snapshot : torch . Tensor | None = None , hidden_states_agg_mask : torch . Tensor | None = None , ) -> dict [ str , torch . Tensor | None ]: \"\"\" Executes the forward pass for the current pipeline stage. Args: input_ids: Indices of input sequence tokens. Required if this is the first pipeline stage. hidden_states: Hidden states from the previous pipeline stage. Required if this is not the first pipeline stage. position_ids: Indices of positions of each input sequence tokens in the position embeddings. hidden_states_snapshot: Accumulated tensor of aggregated hidden states from previous stages. Used if snapshotting is enabled. hidden_states_agg_mask: Mask used to aggregate hidden states for snapshots. Returns: A dictionary containing: * 'hidden_states': The output of the last layer in this stage. * 'hidden_states_snapshot': (Optional) The updated snapshot tensor. \"\"\" state_aggregator = create_hidden_states_aggregator ( self . _hidden_states_snapshot_mode , hidden_states_agg_mask ) if input_ids is not None : last_hidden_states = self . embed_tokens ( input_ids ) state_aggregator . add_hidden_states ( last_hidden_states ) else : last_hidden_states = hidden_states rope_params = self . rope_provider ( position_ids ) for decoder_layer_name in self . _layers_iter : decoder_layer = self . layers [ decoder_layer_name ] if self . _enable_checkpointing : last_hidden_states = checkpoint ( decoder_layer , last_hidden_states , rope_params , use_reentrant = False ) else : last_hidden_states = decoder_layer ( last_hidden_states , rope_params ) state_aggregator . add_hidden_states ( last_hidden_states ) if self . _stage . is_current_stage_last : last_hidden_states = self . norm ( last_hidden_states ) return { \"hidden_states\" : last_hidden_states , \"hidden_states_snapshot\" : state_aggregator . pack_with_snapshot ( hidden_states_snapshot ), } def reset_moe_stats ( self ): \"\"\" Resets routing statistics for all MoE layers in this stage. \"\"\" for layer_name in self . _layers_iter : self . layers [ layer_name ] . reset_moe_stats () @property def moe_tokens_per_expert ( self ) -> torch . Tensor : \"\"\" Retrieves the number of tokens routed to each expert across all layers. Returns: A tensor of shape (num_local_layers, num_experts) containing counts. \"\"\" return torch . stack ([ self . layers [ layer_name ] . moe_tokens_per_expert for layer_name in self . _layers_iter ], dim = 0 ) def reset_parameters ( self ): \"\"\"Resets module parameters\"\"\" if self . _stage . is_current_stage_first : self . embed_tokens . reset_parameters () self . rope_provider . reset_parameters () for decoder_layer_name in self . _layers_iter : decoder_layer = self . layers [ decoder_layer_name ] decoder_layer . reset_parameters () if self . _stage . is_current_stage_last : self . norm . reset_parameters () def infer_stage_inputs_from_pipeline_inputs ( self , inputs : dict [ str , torch . Tensor ], n_microbatches : int ) -> dict [ str , torch . Tensor ]: input_ids = inputs [ \"input_ids\" ] pp_inputs = {} # for calculation - input ids or prev hidden state if self . _stage . is_current_stage_first : pp_inputs [ \"input_ids\" ] = torch . empty ( ( input_ids . shape [ 0 ] // n_microbatches , input_ids . shape [ 1 ]), dtype = torch . long , device = input_ids . device ) else : pp_inputs [ \"hidden_states\" ] = torch . empty ( ( input_ids . shape [ 0 ] // n_microbatches , input_ids . shape [ 1 ], self . _hidden_size ), dtype = self . output_dtype (), device = input_ids . device , ) if self . _hidden_states_snapshot_mode != HiddenStatesAggregationMode . no : num_layers_before = self . _num_layers_before + 1 # 1 for embedding pp_inputs [ \"hidden_states_snapshot\" ] = torch . empty ( ( num_layers_before , input_ids . shape [ 0 ] // n_microbatches , self . _hidden_size ), dtype = self . output_dtype (), device = input_ids . device , ) return pp_inputs def infer_stage_outputs_from_pipeline_inputs ( self , inputs : dict [ str , torch . Tensor ], n_microbatches : int ) -> dict [ str , torch . Tensor ]: input_ids = inputs [ \"input_ids\" ] # for calculation - last hidden state pp_outputs = { \"hidden_states\" : torch . empty ( ( input_ids . shape [ 0 ] // n_microbatches , input_ids . shape [ 1 ], self . _hidden_size ), dtype = self . output_dtype (), device = input_ids . device , ) } # for state caching if self . _hidden_states_snapshot_mode != HiddenStatesAggregationMode . no : num_layers_before = self . _num_layers_before + 1 num_layers_current = len ( self . layers ) num_layers_after = num_layers_before + num_layers_current pp_outputs [ \"hidden_states_snapshot\" ] = torch . empty ( ( num_layers_after , input_ids . shape [ 0 ] // n_microbatches , self . _hidden_size ), dtype = self . output_dtype (), device = input_ids . device , ) return pp_outputs moe_tokens_per_expert property Retrieves the number of tokens routed to each expert across all layers. Returns: Type Description Tensor A tensor of shape (num_local_layers, num_experts) containing counts. __init__ ( params , stage , hidden_states_snapshot_mode , enable_checkpointing ) Constructs the Qwen3MoEModel object. Parameters: Name Type Description Default params Qwen3MoEParameters Configuration parameters for the full model. required stage PipelineStageInfo Information about the pipeline stage this instance belongs to. required hidden_states_snapshot_mode HiddenStatesAggregationMode Configures intermediate hidden state aggregation & snapshotting mode required enable_checkpointing bool If True, enables activation checkpointing for transformer layers to save memory. required Source code in d9d/module/model/qwen3_moe/model.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def __init__ ( self , params : Qwen3MoEParameters , stage : PipelineStageInfo , hidden_states_snapshot_mode : HiddenStatesAggregationMode , enable_checkpointing : bool , ): \"\"\" Constructs the Qwen3MoEModel object. Args: params: Configuration parameters for the full model. stage: Information about the pipeline stage this instance belongs to. hidden_states_snapshot_mode: Configures intermediate hidden state aggregation & snapshotting mode enable_checkpointing: If True, enables activation checkpointing for transformer layers to save memory. \"\"\" super () . __init__ () if stage . is_current_stage_first : self . embed_tokens = SplitTokenEmbeddings ( hidden_size = params . layer . hidden_size , split_vocab_size = params . split_vocab_size , split_order = params . split_vocab_order , ) # we use ModuleDict here to properly handle pipelining and loading weights after the model # was pipelined layer_start , layer_end = distribute_layers_for_pipeline_stage ( num_layers = params . num_hidden_layers , num_virtual_layers_pre = params . pipeline_num_virtual_layers_pre , # embeddings num_virtual_layers_post = params . pipeline_num_virtual_layers_post , # LM head stage = stage , ) self . _num_layers_before = layer_start self . _layers_iter = list ( map ( str , range ( layer_start , layer_end ))) layers = nn . ModuleDict ({ str ( layer_idx ): Qwen3MoELayer ( params = params . layer ) for layer_idx in self . _layers_iter }) self . layers : Mapping [ str , Qwen3MoELayer ] = cast ( Mapping [ str , Qwen3MoELayer ], layers ) self . rope_provider = RotaryEmbeddingProvider ( max_position_ids = params . max_position_ids , rope_base = params . rope_base , head_dim = params . layer . head_dim ) if stage . is_current_stage_last : self . norm = nn . RMSNorm ( normalized_shape = params . layer . hidden_size , eps = params . layer . rms_norm_eps ) self . _stage = stage self . _hidden_states_snapshot_mode = hidden_states_snapshot_mode self . _hidden_size = params . layer . hidden_size self . _enable_checkpointing = enable_checkpointing forward ( input_ids = None , hidden_states = None , position_ids = None , hidden_states_snapshot = None , hidden_states_agg_mask = None ) Executes the forward pass for the current pipeline stage. Parameters: Name Type Description Default input_ids Tensor | None Indices of input sequence tokens. Required if this is the first pipeline stage. None hidden_states Tensor | None Hidden states from the previous pipeline stage. Required if this is not the first pipeline stage. None position_ids Tensor | None Indices of positions of each input sequence tokens in the position embeddings. None hidden_states_snapshot Tensor | None Accumulated tensor of aggregated hidden states from previous stages. Used if snapshotting is enabled. None hidden_states_agg_mask Tensor | None Mask used to aggregate hidden states for snapshots. None Returns: Type Description dict [ str , Tensor | None] A dictionary containing: * 'hidden_states': The output of the last layer in this stage. * 'hidden_states_snapshot': (Optional) The updated snapshot tensor. Source code in d9d/module/model/qwen3_moe/model.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 def forward ( self , input_ids : torch . Tensor | None = None , hidden_states : torch . Tensor | None = None , position_ids : torch . Tensor | None = None , hidden_states_snapshot : torch . Tensor | None = None , hidden_states_agg_mask : torch . Tensor | None = None , ) -> dict [ str , torch . Tensor | None ]: \"\"\" Executes the forward pass for the current pipeline stage. Args: input_ids: Indices of input sequence tokens. Required if this is the first pipeline stage. hidden_states: Hidden states from the previous pipeline stage. Required if this is not the first pipeline stage. position_ids: Indices of positions of each input sequence tokens in the position embeddings. hidden_states_snapshot: Accumulated tensor of aggregated hidden states from previous stages. Used if snapshotting is enabled. hidden_states_agg_mask: Mask used to aggregate hidden states for snapshots. Returns: A dictionary containing: * 'hidden_states': The output of the last layer in this stage. * 'hidden_states_snapshot': (Optional) The updated snapshot tensor. \"\"\" state_aggregator = create_hidden_states_aggregator ( self . _hidden_states_snapshot_mode , hidden_states_agg_mask ) if input_ids is not None : last_hidden_states = self . embed_tokens ( input_ids ) state_aggregator . add_hidden_states ( last_hidden_states ) else : last_hidden_states = hidden_states rope_params = self . rope_provider ( position_ids ) for decoder_layer_name in self . _layers_iter : decoder_layer = self . layers [ decoder_layer_name ] if self . _enable_checkpointing : last_hidden_states = checkpoint ( decoder_layer , last_hidden_states , rope_params , use_reentrant = False ) else : last_hidden_states = decoder_layer ( last_hidden_states , rope_params ) state_aggregator . add_hidden_states ( last_hidden_states ) if self . _stage . is_current_stage_last : last_hidden_states = self . norm ( last_hidden_states ) return { \"hidden_states\" : last_hidden_states , \"hidden_states_snapshot\" : state_aggregator . pack_with_snapshot ( hidden_states_snapshot ), } output_dtype () Returns the data type of the model output hidden states. Source code in d9d/module/model/qwen3_moe/model.py 82 83 84 85 86 def output_dtype ( self ) -> torch . dtype : \"\"\" Returns the data type of the model output hidden states. \"\"\" return self . layers [ self . _layers_iter [ 0 ]] . input_layernorm . weight . dtype reset_moe_stats () Resets routing statistics for all MoE layers in this stage. Source code in d9d/module/model/qwen3_moe/model.py 144 145 146 147 148 149 150 def reset_moe_stats ( self ): \"\"\" Resets routing statistics for all MoE layers in this stage. \"\"\" for layer_name in self . _layers_iter : self . layers [ layer_name ] . reset_moe_stats () reset_parameters () Resets module parameters Source code in d9d/module/model/qwen3_moe/model.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 def reset_parameters ( self ): \"\"\"Resets module parameters\"\"\" if self . _stage . is_current_stage_first : self . embed_tokens . reset_parameters () self . rope_provider . reset_parameters () for decoder_layer_name in self . _layers_iter : decoder_layer = self . layers [ decoder_layer_name ] decoder_layer . reset_parameters () if self . _stage . is_current_stage_last : self . norm . reset_parameters () Qwen3MoEParameters Bases: BaseModel Configuration parameters for the Qwen3 Mixture-of-Experts model backbone. Attributes: Name Type Description layer Qwen3MoELayerParameters Configuration shared across all transformer layers. num_hidden_layers int The total number of transformer layers. rope_base int Base value for RoPE frequency calculation. max_position_ids int Maximum sequence length. split_vocab_size dict [ str , int ] A dictionary mapping vocabulary segment names to their sizes. split_vocab_order list [ str ] The sequence in which vocabulary splits are correctly ordered. pipeline_num_virtual_layers_pre int The number of 'virtual' layers representing the computational cost of modules on the first stage, before the main layers (e.g., token and positional embeddings). pipeline_num_virtual_layers_post int The number of 'virtual' layers representing the computational cost of modules on the last stage, after the main layers (e.g., the final layer normalization and LM head). Source code in d9d/module/model/qwen3_moe/params.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 class Qwen3MoEParameters ( BaseModel ): \"\"\" Configuration parameters for the Qwen3 Mixture-of-Experts model backbone. Attributes: layer: Configuration shared across all transformer layers. num_hidden_layers: The total number of transformer layers. rope_base: Base value for RoPE frequency calculation. max_position_ids: Maximum sequence length. split_vocab_size: A dictionary mapping vocabulary segment names to their sizes. split_vocab_order: The sequence in which vocabulary splits are correctly ordered. pipeline_num_virtual_layers_pre: The number of 'virtual' layers representing the computational cost of modules on the *first* stage, before the main layers (e.g., token and positional embeddings). pipeline_num_virtual_layers_post: The number of 'virtual' layers representing the computational cost of modules on the *last* stage, after the main layers (e.g., the final layer normalization and LM head). \"\"\" layer : Qwen3MoELayerParameters num_hidden_layers : int rope_base : int max_position_ids : int split_vocab_size : dict [ str , int ] split_vocab_order : list [ str ] pipeline_num_virtual_layers_pre : int = 0 pipeline_num_virtual_layers_post : int = 0 d9d.module.parallelism.model.qwen3_moe parallelize_qwen3_moe_for_causal_lm ( dist_context , model , stage ) Parallelizes the Qwen3 MoE Causal LM model. This function delegates backbone parallelization to parallelize_qwen3_moe_model and additionally configures the language model head with Hybrid Sharded Data Parallelism (HSDP). Parameters: Name Type Description Default dist_context DistributedContext The distributed context containing device meshes and topology info. required model Qwen3MoEForCausalLM The Qwen3 MoE Causal LM model to parallelize. required stage PipelineStageInfo Information about the current pipeline stage. required Source code in d9d/module/parallelism/model/qwen3_moe.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def parallelize_qwen3_moe_for_causal_lm ( dist_context : DistributedContext , model : Qwen3MoEForCausalLM , stage : PipelineStageInfo ): \"\"\" Parallelizes the Qwen3 MoE Causal LM model. This function delegates backbone parallelization to ``parallelize_qwen3_moe_model`` and additionally configures the language model head with Hybrid Sharded Data Parallelism (HSDP). Args: dist_context: The distributed context containing device meshes and topology info. model: The Qwen3 MoE Causal LM model to parallelize. stage: Information about the current pipeline stage. \"\"\" dense_mesh = dist_context . mesh_for ( DENSE_DOMAIN ) parallelize_qwen3_moe_model ( dist_context , model . model , stage ) if stage . is_current_stage_last : parallelize_hsdp ( model . lm_head , mesh = dense_mesh [ \"dp_replicate\" , \"dp_cp_shard\" , \"cp_replicate\" ], ) parallelize_qwen3_moe_for_classification ( dist_context , model , stage ) Parallelizes the Qwen3 MoE classification model. This function delegates backbone parallelization to parallelize_qwen3_moe_model and additionally configures the classification head with Hybrid Sharded Data Parallelism (HSDP). Parameters: Name Type Description Default dist_context DistributedContext The distributed context containing device meshes and topology info. required model Qwen3MoEForClassification The Qwen3 MoE classification model to parallelize. required stage PipelineStageInfo Information about the current pipeline stage. required Source code in d9d/module/parallelism/model/qwen3_moe.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def parallelize_qwen3_moe_for_classification ( dist_context : DistributedContext , model : Qwen3MoEForClassification , stage : PipelineStageInfo ): \"\"\" Parallelizes the Qwen3 MoE classification model. This function delegates backbone parallelization to ``parallelize_qwen3_moe_model`` and additionally configures the classification head with Hybrid Sharded Data Parallelism (HSDP). Args: dist_context: The distributed context containing device meshes and topology info. model: The Qwen3 MoE classification model to parallelize. stage: Information about the current pipeline stage. \"\"\" dense_mesh = dist_context . mesh_for ( DENSE_DOMAIN ) parallelize_qwen3_moe_model ( dist_context , model . model , stage ) if stage . is_current_stage_last : parallelize_hsdp ( model . cls_head , mesh = dense_mesh [ \"dp_replicate\" , \"dp_cp_shard\" , \"cp_replicate\" ], ) parallelize_qwen3_moe_model ( dist_context , model , stage ) Parallelizes the base Qwen3 MoE model components. This function configures the model layers for distributed execution within a pipeline stage. It applies Hybrid Sharded Data Parallelism (HSDP) to dense components (embeddings, norms, attention) and Expert Parallelism (EP) to the Mixture-of-Experts (MLP) layers. Current usage constraints: * Tensor Parallelism is not supported (we may implement it later). * Context Parallelism is not supported (we will implement it later). Parameters: Name Type Description Default dist_context DistributedContext The distributed context. required model Qwen3MoEModel The Qwen3 MoE base model to parallelize. required stage PipelineStageInfo Information about the current pipeline stage. required Raises: Type Description ValueError If Tensor Parallel or Context Parallel is enabled in the context. Source code in d9d/module/parallelism/model/qwen3_moe.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def parallelize_qwen3_moe_model ( dist_context : DistributedContext , model : Qwen3MoEModel , stage : PipelineStageInfo ): \"\"\" Parallelizes the base Qwen3 MoE model components. This function configures the model layers for distributed execution within a pipeline stage. It applies Hybrid Sharded Data Parallelism (HSDP) to dense components (embeddings, norms, attention) and Expert Parallelism (EP) to the Mixture-of-Experts (MLP) layers. Current usage constraints: * Tensor Parallelism is not supported (we may implement it later). * Context Parallelism is not supported (we will implement it later). Args: dist_context: The distributed context. model: The Qwen3 MoE base model to parallelize. stage: Information about the current pipeline stage. Raises: ValueError: If Tensor Parallel or Context Parallel is enabled in the context. \"\"\" dims = dist_context . mesh_params dense_mesh = dist_context . mesh_for ( DENSE_DOMAIN ) expert_mesh = dist_context . mesh_for ( EXPERT_DOMAIN ) if dims . has_tensor_parallel : raise ValueError ( \"Tensor Parallel currently is not supported for this model.\" ) if dims . has_context_parallel_replicate or dims . has_context_parallel_shard : raise ValueError ( \"Context Parallel currently is not supported for this model.\" ) if stage . is_current_stage_first : parallelize_hsdp ( model . embed_tokens , mesh = dense_mesh [ \"dp_replicate\" , \"dp_cp_shard\" , \"cp_replicate\" ]) if stage . is_current_stage_last : parallelize_hsdp ( model . norm , mesh = dense_mesh [ \"dp_replicate\" , \"dp_cp_shard\" , \"cp_replicate\" ], ) for layer in model . layers . values (): parallelize_expert_parallel ( layer . mlp , mesh_experts = expert_mesh [ \"ep_replicate\" , \"ep_shard\" ]) parallelize_hsdp ( layer . self_attn , mesh = dense_mesh [ \"dp_replicate\" , \"dp_cp_shard\" , \"cp_replicate\" ], ) parallelize_hsdp ( layer . input_layernorm , mesh = dense_mesh [ \"dp_replicate\" , \"dp_cp_shard\" , \"cp_replicate\" ], ) parallelize_hsdp ( layer . post_attention_layernorm , mesh = dense_mesh [ \"dp_replicate\" , \"dp_cp_shard\" , \"cp_replicate\" ], )","title":"Qwen3 MoE"},{"location":"models/qwen3_moe/#about","text":"The d9d.module.model.qwen3_moe package implements the Qwen3 Mixture-of-Experts model architecture. The d9d.module.parallelism.model.qwen3_moe package implements default horizontal parallelism strategy for this model.","title":"About"},{"location":"models/qwen3_moe/#d9d.module.model.qwen3_moe","text":"","title":"qwen3_moe"},{"location":"models/qwen3_moe/#d9d.module.model.qwen3_moe.Qwen3MoEForCausalLM","text":"Bases: Module , ModuleLateInit , ModuleSupportsPipelining A Qwen3 MoE model wrapped with a Causal Language Modeling head. It is designed to be split across multiple pipeline stages. Source code in d9d/module/model/qwen3_moe/model.py 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 class Qwen3MoEForCausalLM ( nn . Module , ModuleLateInit , ModuleSupportsPipelining ): \"\"\" A Qwen3 MoE model wrapped with a Causal Language Modeling head. It is designed to be split across multiple pipeline stages. \"\"\" def __init__ ( self , params : Qwen3MoEForCausalLMParameters , stage : PipelineStageInfo , hidden_states_snapshot_mode : HiddenStatesAggregationMode , enable_checkpointing : bool , ): \"\"\" Constructs the Qwen3MoEForCausalLM object. Args: params: Full model configuration parameters. stage: Pipeline stage information for this instance. hidden_states_snapshot_mode: Configures intermediate hidden state aggregation & snapshotting mode. enable_checkpointing: Whether to enable activation checkpointing. \"\"\" super () . __init__ () self . model = Qwen3MoEModel ( params . model , stage , hidden_states_snapshot_mode = hidden_states_snapshot_mode , enable_checkpointing = enable_checkpointing , ) if stage . is_current_stage_last : self . lm_head = SplitLanguageModellingHead ( split_vocab_size = params . model . split_vocab_size , split_order = params . model . split_vocab_order , hidden_size = params . model . layer . hidden_size , ) self . _stage = stage self . _hidden_size = params . model . layer . hidden_size def forward ( self , input_ids : torch . Tensor | None = None , hidden_states : torch . Tensor | None = None , position_ids : torch . Tensor | None = None , hidden_states_snapshot : torch . Tensor | None = None , hidden_states_agg_mask : torch . Tensor | None = None , labels : torch . Tensor | None = None , ) -> dict [ str , torch . Tensor ]: \"\"\" Executes the model forward pass. If this is the last stage, it expects `labels` to be provided and computes the cross-entropy loss (returned as 'logps' typically representing per-token loss). Args: input_ids: Input token IDS (for Stage 0). hidden_states: Hidden states from previous stage (for Stage > 0). position_ids: Positional indices for RoPE. hidden_states_snapshot: Intermediate state collector. hidden_states_agg_mask: Mask for state aggregation. labels: Target tokens for loss computation (Last Stage). Returns: Dictionary containing 'hidden_states', optionally 'hidden_states_snapshot', and per-token 'logps' if on the last stage. \"\"\" model_outputs = self . model ( input_ids = input_ids , hidden_states = hidden_states , position_ids = position_ids , hidden_states_snapshot = hidden_states_snapshot , hidden_states_agg_mask = hidden_states_agg_mask , ) if self . _stage . is_current_stage_last : lm_out = self . lm_head ( hidden_states = model_outputs [ \"hidden_states\" ], labels = labels ) model_outputs [ \"logps\" ] = lm_out return model_outputs def reset_parameters ( self ): \"\"\" Resets module parameters. \"\"\" self . model . reset_parameters () if self . _stage . is_current_stage_last : self . lm_head . reset_parameters () def reset_moe_stats ( self ): \"\"\" Resets MoE routing statistics in the backbone. \"\"\" self . model . reset_moe_stats () @property def moe_tokens_per_expert ( self ) -> torch . Tensor : \"\"\" Accesses MoE routing statistics from the backbone. \"\"\" return self . model . moe_tokens_per_expert def infer_stage_inputs_from_pipeline_inputs ( self , inputs : dict [ str , torch . Tensor ], n_microbatches : int ) -> dict [ str , torch . Tensor ]: return self . model . infer_stage_inputs_from_pipeline_inputs ( inputs , n_microbatches ) def infer_stage_outputs_from_pipeline_inputs ( self , inputs : dict [ str , torch . Tensor ], n_microbatches : int ) -> dict [ str , torch . Tensor ]: pp_outputs = self . model . infer_stage_outputs_from_pipeline_inputs ( inputs , n_microbatches ) if self . _stage . is_current_stage_last : pp_outputs [ \"logps\" ] = torch . empty ( inputs [ \"input_ids\" ] . shape , dtype = torch . float32 ) return pp_outputs","title":"Qwen3MoEForCausalLM"},{"location":"models/qwen3_moe/#d9d.module.model.qwen3_moe.Qwen3MoEForCausalLM.moe_tokens_per_expert","text":"Accesses MoE routing statistics from the backbone.","title":"moe_tokens_per_expert"},{"location":"models/qwen3_moe/#d9d.module.model.qwen3_moe.Qwen3MoEForCausalLM.__init__","text":"Constructs the Qwen3MoEForCausalLM object. Parameters: Name Type Description Default params Qwen3MoEForCausalLMParameters Full model configuration parameters. required stage PipelineStageInfo Pipeline stage information for this instance. required hidden_states_snapshot_mode HiddenStatesAggregationMode Configures intermediate hidden state aggregation & snapshotting mode. required enable_checkpointing bool Whether to enable activation checkpointing. required Source code in d9d/module/model/qwen3_moe/model.py 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 def __init__ ( self , params : Qwen3MoEForCausalLMParameters , stage : PipelineStageInfo , hidden_states_snapshot_mode : HiddenStatesAggregationMode , enable_checkpointing : bool , ): \"\"\" Constructs the Qwen3MoEForCausalLM object. Args: params: Full model configuration parameters. stage: Pipeline stage information for this instance. hidden_states_snapshot_mode: Configures intermediate hidden state aggregation & snapshotting mode. enable_checkpointing: Whether to enable activation checkpointing. \"\"\" super () . __init__ () self . model = Qwen3MoEModel ( params . model , stage , hidden_states_snapshot_mode = hidden_states_snapshot_mode , enable_checkpointing = enable_checkpointing , ) if stage . is_current_stage_last : self . lm_head = SplitLanguageModellingHead ( split_vocab_size = params . model . split_vocab_size , split_order = params . model . split_vocab_order , hidden_size = params . model . layer . hidden_size , ) self . _stage = stage self . _hidden_size = params . model . layer . hidden_size","title":"__init__"},{"location":"models/qwen3_moe/#d9d.module.model.qwen3_moe.Qwen3MoEForCausalLM.forward","text":"Executes the model forward pass. If this is the last stage, it expects labels to be provided and computes the cross-entropy loss (returned as 'logps' typically representing per-token loss). Parameters: Name Type Description Default input_ids Tensor | None Input token IDS (for Stage 0). None hidden_states Tensor | None Hidden states from previous stage (for Stage > 0). None position_ids Tensor | None Positional indices for RoPE. None hidden_states_snapshot Tensor | None Intermediate state collector. None hidden_states_agg_mask Tensor | None Mask for state aggregation. None labels Tensor | None Target tokens for loss computation (Last Stage). None Returns: Type Description dict [ str , Tensor ] Dictionary containing 'hidden_states', optionally 'hidden_states_snapshot', dict [ str , Tensor ] and per-token 'logps' if on the last stage. Source code in d9d/module/model/qwen3_moe/model.py 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 def forward ( self , input_ids : torch . Tensor | None = None , hidden_states : torch . Tensor | None = None , position_ids : torch . Tensor | None = None , hidden_states_snapshot : torch . Tensor | None = None , hidden_states_agg_mask : torch . Tensor | None = None , labels : torch . Tensor | None = None , ) -> dict [ str , torch . Tensor ]: \"\"\" Executes the model forward pass. If this is the last stage, it expects `labels` to be provided and computes the cross-entropy loss (returned as 'logps' typically representing per-token loss). Args: input_ids: Input token IDS (for Stage 0). hidden_states: Hidden states from previous stage (for Stage > 0). position_ids: Positional indices for RoPE. hidden_states_snapshot: Intermediate state collector. hidden_states_agg_mask: Mask for state aggregation. labels: Target tokens for loss computation (Last Stage). Returns: Dictionary containing 'hidden_states', optionally 'hidden_states_snapshot', and per-token 'logps' if on the last stage. \"\"\" model_outputs = self . model ( input_ids = input_ids , hidden_states = hidden_states , position_ids = position_ids , hidden_states_snapshot = hidden_states_snapshot , hidden_states_agg_mask = hidden_states_agg_mask , ) if self . _stage . is_current_stage_last : lm_out = self . lm_head ( hidden_states = model_outputs [ \"hidden_states\" ], labels = labels ) model_outputs [ \"logps\" ] = lm_out return model_outputs","title":"forward"},{"location":"models/qwen3_moe/#d9d.module.model.qwen3_moe.Qwen3MoEForCausalLM.reset_moe_stats","text":"Resets MoE routing statistics in the backbone. Source code in d9d/module/model/qwen3_moe/model.py 327 328 329 330 331 332 def reset_moe_stats ( self ): \"\"\" Resets MoE routing statistics in the backbone. \"\"\" self . model . reset_moe_stats ()","title":"reset_moe_stats"},{"location":"models/qwen3_moe/#d9d.module.model.qwen3_moe.Qwen3MoEForCausalLM.reset_parameters","text":"Resets module parameters. Source code in d9d/module/model/qwen3_moe/model.py 317 318 319 320 321 322 323 324 325 def reset_parameters ( self ): \"\"\" Resets module parameters. \"\"\" self . model . reset_parameters () if self . _stage . is_current_stage_last : self . lm_head . reset_parameters ()","title":"reset_parameters"},{"location":"models/qwen3_moe/#d9d.module.model.qwen3_moe.Qwen3MoEForCausalLMParameters","text":"Bases: BaseModel Configuration parameters for Qwen3 Mixture-of-Experts model with a Causal Language Modeling head. Attributes: Name Type Description model Qwen3MoEParameters The configuration for the underlying Qwen3 MoE model. Source code in d9d/module/model/qwen3_moe/params.py 61 62 63 64 65 66 67 68 69 class Qwen3MoEForCausalLMParameters ( BaseModel ): \"\"\" Configuration parameters for Qwen3 Mixture-of-Experts model with a Causal Language Modeling head. Attributes: model: The configuration for the underlying Qwen3 MoE model. \"\"\" model : Qwen3MoEParameters","title":"Qwen3MoEForCausalLMParameters"},{"location":"models/qwen3_moe/#d9d.module.model.qwen3_moe.Qwen3MoEForClassification","text":"Bases: Module , ModuleLateInit , ModuleSupportsPipelining A Qwen3 MoE model wrapped with a Sequence/Token Classification head. It is designed to be split across multiple pipeline stages. Source code in d9d/module/model/qwen3_moe/model.py 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 class Qwen3MoEForClassification ( nn . Module , ModuleLateInit , ModuleSupportsPipelining ): \"\"\" A Qwen3 MoE model wrapped with a Sequence/Token Classification head. It is designed to be split across multiple pipeline stages. \"\"\" def __init__ ( self , params : Qwen3MoEForClassificationParameters , stage : PipelineStageInfo , hidden_states_snapshot_mode : HiddenStatesAggregationMode , enable_checkpointing : bool , ): \"\"\" Constructs the Qwen3MoEForClassification object. Args: params: Full model configuration parameters. stage: Pipeline stage information for this instance. hidden_states_snapshot_mode: Configures intermediate hidden state aggregation & snapshotting mode. enable_checkpointing: Whether to enable activation checkpointing. \"\"\" super () . __init__ () self . model = Qwen3MoEModel ( params . model , stage , hidden_states_snapshot_mode = hidden_states_snapshot_mode , enable_checkpointing = enable_checkpointing , ) if stage . is_current_stage_last : self . cls_head = ClassificationHead ( hidden_size = params . model . layer . hidden_size , num_labels = params . num_labels , dropout = params . classifier_dropout , ) self . _stage = stage self . _hidden_size = params . model . layer . hidden_size self . _num_labels = params . num_labels def forward ( self , input_ids : torch . Tensor | None = None , hidden_states : torch . Tensor | None = None , position_ids : torch . Tensor | None = None , hidden_states_snapshot : torch . Tensor | None = None , hidden_states_agg_mask : torch . Tensor | None = None , pooling_mask : torch . Tensor | None = None , ) -> dict [ str , torch . Tensor ]: \"\"\" Executes the classification model forward pass. Args: input_ids: Input token IDS (for Stage 0). hidden_states: Hidden states from previous stage (for Stage > 0). position_ids: Positional indices for RoPE. hidden_states_snapshot: Intermediate state collector. hidden_states_agg_mask: Mask for state aggregation. pooling_mask: Binary mask indicating which token(s) to pool for classification. Note: you can use `d9d.dataset.token_pooling_mask_from_attention_mask` in your Dataset to preallocate the pooling mask from attention mask. Returns: Dictionary containing 'hidden_states', optionally 'hidden_states_snapshot'. If on the last stage, also contains 'scores' (logits) of shape [batch, num_labels]. \"\"\" model_outputs = self . model ( input_ids = input_ids , hidden_states = hidden_states , position_ids = position_ids , hidden_states_snapshot = hidden_states_snapshot , hidden_states_agg_mask = hidden_states_agg_mask , ) if self . _stage . is_current_stage_last : model_outputs [ \"scores\" ] = self . cls_head ( hidden_states = model_outputs [ \"hidden_states\" ], pooling_mask = pooling_mask ) return model_outputs def reset_parameters ( self ): \"\"\" Resets module parameters. \"\"\" self . model . reset_parameters () if self . _stage . is_current_stage_last : self . cls_head . reset_parameters () def reset_moe_stats ( self ): \"\"\" Resets MoE routing statistics in the backbone. \"\"\" self . model . reset_moe_stats () @property def moe_tokens_per_expert ( self ) -> torch . Tensor : \"\"\" Accesses MoE routing statistics from the backbone. \"\"\" return self . model . moe_tokens_per_expert def infer_stage_inputs_from_pipeline_inputs ( self , inputs : dict [ str , torch . Tensor ], n_microbatches : int ) -> dict [ str , torch . Tensor ]: return self . model . infer_stage_inputs_from_pipeline_inputs ( inputs , n_microbatches ) def infer_stage_outputs_from_pipeline_inputs ( self , inputs : dict [ str , torch . Tensor ], n_microbatches : int ) -> dict [ str , torch . Tensor ]: pp_outputs = self . model . infer_stage_outputs_from_pipeline_inputs ( inputs , n_microbatches ) if self . _stage . is_current_stage_last : batch_size = inputs [ \"input_ids\" ] . shape [ 0 ] // n_microbatches pp_outputs [ \"scores\" ] = torch . empty (( batch_size , self . _num_labels ), dtype = torch . float32 ) return pp_outputs","title":"Qwen3MoEForClassification"},{"location":"models/qwen3_moe/#d9d.module.model.qwen3_moe.Qwen3MoEForClassification.moe_tokens_per_expert","text":"Accesses MoE routing statistics from the backbone.","title":"moe_tokens_per_expert"},{"location":"models/qwen3_moe/#d9d.module.model.qwen3_moe.Qwen3MoEForClassification.__init__","text":"Constructs the Qwen3MoEForClassification object. Parameters: Name Type Description Default params Qwen3MoEForClassificationParameters Full model configuration parameters. required stage PipelineStageInfo Pipeline stage information for this instance. required hidden_states_snapshot_mode HiddenStatesAggregationMode Configures intermediate hidden state aggregation & snapshotting mode. required enable_checkpointing bool Whether to enable activation checkpointing. required Source code in d9d/module/model/qwen3_moe/model.py 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 def __init__ ( self , params : Qwen3MoEForClassificationParameters , stage : PipelineStageInfo , hidden_states_snapshot_mode : HiddenStatesAggregationMode , enable_checkpointing : bool , ): \"\"\" Constructs the Qwen3MoEForClassification object. Args: params: Full model configuration parameters. stage: Pipeline stage information for this instance. hidden_states_snapshot_mode: Configures intermediate hidden state aggregation & snapshotting mode. enable_checkpointing: Whether to enable activation checkpointing. \"\"\" super () . __init__ () self . model = Qwen3MoEModel ( params . model , stage , hidden_states_snapshot_mode = hidden_states_snapshot_mode , enable_checkpointing = enable_checkpointing , ) if stage . is_current_stage_last : self . cls_head = ClassificationHead ( hidden_size = params . model . layer . hidden_size , num_labels = params . num_labels , dropout = params . classifier_dropout , ) self . _stage = stage self . _hidden_size = params . model . layer . hidden_size self . _num_labels = params . num_labels","title":"__init__"},{"location":"models/qwen3_moe/#d9d.module.model.qwen3_moe.Qwen3MoEForClassification.forward","text":"Executes the classification model forward pass. Parameters: Name Type Description Default input_ids Tensor | None Input token IDS (for Stage 0). None hidden_states Tensor | None Hidden states from previous stage (for Stage > 0). None position_ids Tensor | None Positional indices for RoPE. None hidden_states_snapshot Tensor | None Intermediate state collector. None hidden_states_agg_mask Tensor | None Mask for state aggregation. None pooling_mask Tensor | None Binary mask indicating which token(s) to pool for classification. Note: you can use d9d.dataset.token_pooling_mask_from_attention_mask in your Dataset to preallocate the pooling mask from attention mask. None Returns: Type Description dict [ str , Tensor ] Dictionary containing 'hidden_states', optionally 'hidden_states_snapshot'. If on the last stage, also contains 'scores' (logits) of shape [batch, num_labels]. Source code in d9d/module/model/qwen3_moe/model.py 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 def forward ( self , input_ids : torch . Tensor | None = None , hidden_states : torch . Tensor | None = None , position_ids : torch . Tensor | None = None , hidden_states_snapshot : torch . Tensor | None = None , hidden_states_agg_mask : torch . Tensor | None = None , pooling_mask : torch . Tensor | None = None , ) -> dict [ str , torch . Tensor ]: \"\"\" Executes the classification model forward pass. Args: input_ids: Input token IDS (for Stage 0). hidden_states: Hidden states from previous stage (for Stage > 0). position_ids: Positional indices for RoPE. hidden_states_snapshot: Intermediate state collector. hidden_states_agg_mask: Mask for state aggregation. pooling_mask: Binary mask indicating which token(s) to pool for classification. Note: you can use `d9d.dataset.token_pooling_mask_from_attention_mask` in your Dataset to preallocate the pooling mask from attention mask. Returns: Dictionary containing 'hidden_states', optionally 'hidden_states_snapshot'. If on the last stage, also contains 'scores' (logits) of shape [batch, num_labels]. \"\"\" model_outputs = self . model ( input_ids = input_ids , hidden_states = hidden_states , position_ids = position_ids , hidden_states_snapshot = hidden_states_snapshot , hidden_states_agg_mask = hidden_states_agg_mask , ) if self . _stage . is_current_stage_last : model_outputs [ \"scores\" ] = self . cls_head ( hidden_states = model_outputs [ \"hidden_states\" ], pooling_mask = pooling_mask ) return model_outputs","title":"forward"},{"location":"models/qwen3_moe/#d9d.module.model.qwen3_moe.Qwen3MoEForClassification.reset_moe_stats","text":"Resets MoE routing statistics in the backbone. Source code in d9d/module/model/qwen3_moe/model.py 452 453 454 455 456 457 def reset_moe_stats ( self ): \"\"\" Resets MoE routing statistics in the backbone. \"\"\" self . model . reset_moe_stats ()","title":"reset_moe_stats"},{"location":"models/qwen3_moe/#d9d.module.model.qwen3_moe.Qwen3MoEForClassification.reset_parameters","text":"Resets module parameters. Source code in d9d/module/model/qwen3_moe/model.py 442 443 444 445 446 447 448 449 450 def reset_parameters ( self ): \"\"\" Resets module parameters. \"\"\" self . model . reset_parameters () if self . _stage . is_current_stage_last : self . cls_head . reset_parameters ()","title":"reset_parameters"},{"location":"models/qwen3_moe/#d9d.module.model.qwen3_moe.Qwen3MoEForClassificationParameters","text":"Bases: BaseModel Configuration parameters for Qwen3 Mixture-of-Experts model with a token/sequnce classification head. Attributes: Name Type Description model Qwen3MoEParameters The configuration for the underlying Qwen3 MoE model. num_labels int The number of output labels for classification. classifier_dropout float The dropout probability for the classification head. Source code in d9d/module/model/qwen3_moe/params.py 72 73 74 75 76 77 78 79 80 81 82 83 84 class Qwen3MoEForClassificationParameters ( BaseModel ): \"\"\" Configuration parameters for Qwen3 Mixture-of-Experts model with a token/sequnce classification head. Attributes: model: The configuration for the underlying Qwen3 MoE model. num_labels: The number of output labels for classification. classifier_dropout: The dropout probability for the classification head. \"\"\" model : Qwen3MoEParameters num_labels : int classifier_dropout : float","title":"Qwen3MoEForClassificationParameters"},{"location":"models/qwen3_moe/#d9d.module.model.qwen3_moe.Qwen3MoELayer","text":"Bases: Module , ModuleLateInit Implements a single Qwen3 Mixture-of-Experts (MoE) transformer layer. This layer consists of a Grouped Query Attention mechanism followed by an MoE MLP block, with pre-RMSNorm applied before each sub-layer. Source code in d9d/module/model/qwen3_moe/decoder_layer.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 class Qwen3MoELayer ( nn . Module , ModuleLateInit ): \"\"\" Implements a single Qwen3 Mixture-of-Experts (MoE) transformer layer. This layer consists of a Grouped Query Attention mechanism followed by an MoE MLP block, with pre-RMSNorm applied before each sub-layer. \"\"\" def __init__ ( self , params : Qwen3MoELayerParameters ): \"\"\" Constructs a Qwen3MoELayer object. Args: params: Configuration parameters for the layer. \"\"\" super () . __init__ () self . self_attn = GroupedQueryAttention ( hidden_size = params . hidden_size , num_attention_heads = params . num_attention_heads , num_key_value_heads = params . num_key_value_heads , is_causal = True , qk_norm_eps = params . rms_norm_eps , head_dim = params . head_dim , ) self . mlp = MoELayer ( hidden_dim = params . hidden_size , num_grouped_experts = params . num_experts , intermediate_dim_grouped = params . intermediate_size , top_k = params . experts_top_k , router_renormalize_probabilities = True , ) self . input_layernorm = nn . RMSNorm ( params . hidden_size , eps = params . rms_norm_eps ) self . post_attention_layernorm = nn . RMSNorm ( params . hidden_size , eps = params . rms_norm_eps ) def forward ( self , hidden_states : torch . Tensor , position_embeddings : tuple [ torch . Tensor , torch . Tensor ] ) -> torch . Tensor : \"\"\" Performs the forward pass of the MoE layer. Args: hidden_states: Input tensor of shape `(batch, seq_len, hidden_dim)`. position_embeddings: Tuple containing RoPE precomputed embeddings (cos, sin). Returns: Output tensor after attention and MoE blocks, shape `(batch, seq_len, hidden_dim)`. \"\"\" residual = hidden_states hidden_states = self . input_layernorm ( hidden_states ) hidden_states = self . self_attn ( hidden_states = hidden_states , position_embeddings = position_embeddings , attention_mask = None , # no mask for moe decoder ) hidden_states = residual + hidden_states residual = hidden_states hidden_states = self . post_attention_layernorm ( hidden_states ) hidden_states = self . mlp ( hidden_states ) hidden_states = residual + hidden_states return hidden_states def reset_moe_stats ( self ): \"\"\" Resets statistical counters inside the MoE router (e.g., token counts per expert). \"\"\" self . mlp . reset_stats () @property def moe_tokens_per_expert ( self ) -> torch . Tensor : \"\"\" Returns the number of tokens routed to each expert. \"\"\" return self . mlp . tokens_per_expert def reset_parameters ( self ): \"\"\" Resets module parameters. \"\"\" self . self_attn . reset_parameters () self . mlp . reset_parameters () self . input_layernorm . reset_parameters () self . post_attention_layernorm . reset_parameters ()","title":"Qwen3MoELayer"},{"location":"models/qwen3_moe/#d9d.module.model.qwen3_moe.Qwen3MoELayer.moe_tokens_per_expert","text":"Returns the number of tokens routed to each expert.","title":"moe_tokens_per_expert"},{"location":"models/qwen3_moe/#d9d.module.model.qwen3_moe.Qwen3MoELayer.__init__","text":"Constructs a Qwen3MoELayer object. Parameters: Name Type Description Default params Qwen3MoELayerParameters Configuration parameters for the layer. required Source code in d9d/module/model/qwen3_moe/decoder_layer.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def __init__ ( self , params : Qwen3MoELayerParameters ): \"\"\" Constructs a Qwen3MoELayer object. Args: params: Configuration parameters for the layer. \"\"\" super () . __init__ () self . self_attn = GroupedQueryAttention ( hidden_size = params . hidden_size , num_attention_heads = params . num_attention_heads , num_key_value_heads = params . num_key_value_heads , is_causal = True , qk_norm_eps = params . rms_norm_eps , head_dim = params . head_dim , ) self . mlp = MoELayer ( hidden_dim = params . hidden_size , num_grouped_experts = params . num_experts , intermediate_dim_grouped = params . intermediate_size , top_k = params . experts_top_k , router_renormalize_probabilities = True , ) self . input_layernorm = nn . RMSNorm ( params . hidden_size , eps = params . rms_norm_eps ) self . post_attention_layernorm = nn . RMSNorm ( params . hidden_size , eps = params . rms_norm_eps )","title":"__init__"},{"location":"models/qwen3_moe/#d9d.module.model.qwen3_moe.Qwen3MoELayer.forward","text":"Performs the forward pass of the MoE layer. Parameters: Name Type Description Default hidden_states Tensor Input tensor of shape (batch, seq_len, hidden_dim) . required position_embeddings tuple [ Tensor , Tensor ] Tuple containing RoPE precomputed embeddings (cos, sin). required Returns: Type Description Tensor Output tensor after attention and MoE blocks, shape (batch, seq_len, hidden_dim) . Source code in d9d/module/model/qwen3_moe/decoder_layer.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def forward ( self , hidden_states : torch . Tensor , position_embeddings : tuple [ torch . Tensor , torch . Tensor ] ) -> torch . Tensor : \"\"\" Performs the forward pass of the MoE layer. Args: hidden_states: Input tensor of shape `(batch, seq_len, hidden_dim)`. position_embeddings: Tuple containing RoPE precomputed embeddings (cos, sin). Returns: Output tensor after attention and MoE blocks, shape `(batch, seq_len, hidden_dim)`. \"\"\" residual = hidden_states hidden_states = self . input_layernorm ( hidden_states ) hidden_states = self . self_attn ( hidden_states = hidden_states , position_embeddings = position_embeddings , attention_mask = None , # no mask for moe decoder ) hidden_states = residual + hidden_states residual = hidden_states hidden_states = self . post_attention_layernorm ( hidden_states ) hidden_states = self . mlp ( hidden_states ) hidden_states = residual + hidden_states return hidden_states","title":"forward"},{"location":"models/qwen3_moe/#d9d.module.model.qwen3_moe.Qwen3MoELayer.reset_moe_stats","text":"Resets statistical counters inside the MoE router (e.g., token counts per expert). Source code in d9d/module/model/qwen3_moe/decoder_layer.py 82 83 84 85 86 87 def reset_moe_stats ( self ): \"\"\" Resets statistical counters inside the MoE router (e.g., token counts per expert). \"\"\" self . mlp . reset_stats ()","title":"reset_moe_stats"},{"location":"models/qwen3_moe/#d9d.module.model.qwen3_moe.Qwen3MoELayer.reset_parameters","text":"Resets module parameters. Source code in d9d/module/model/qwen3_moe/decoder_layer.py 97 98 99 100 101 102 103 104 105 def reset_parameters ( self ): \"\"\" Resets module parameters. \"\"\" self . self_attn . reset_parameters () self . mlp . reset_parameters () self . input_layernorm . reset_parameters () self . post_attention_layernorm . reset_parameters ()","title":"reset_parameters"},{"location":"models/qwen3_moe/#d9d.module.model.qwen3_moe.Qwen3MoELayerParameters","text":"Bases: BaseModel Configuration parameters for a single Qwen3 MoE layer. Attributes: Name Type Description hidden_size int Dimension of the model's hidden states. intermediate_size int Dimension of the feed-forward hidden state. num_experts int Total number of experts in the MoE layer. experts_top_k int Number of experts to route tokens to. num_attention_heads int Number of attention heads for the query. num_key_value_heads int Number of attention heads for key and value. rms_norm_eps float Epsilon value found in the RMSNorm layers. head_dim int Dimension of a single attention head. Source code in d9d/module/model/qwen3_moe/params.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 class Qwen3MoELayerParameters ( BaseModel ): \"\"\" Configuration parameters for a single Qwen3 MoE layer. Attributes: hidden_size: Dimension of the model's hidden states. intermediate_size: Dimension of the feed-forward hidden state. num_experts: Total number of experts in the MoE layer. experts_top_k: Number of experts to route tokens to. num_attention_heads: Number of attention heads for the query. num_key_value_heads: Number of attention heads for key and value. rms_norm_eps: Epsilon value found in the RMSNorm layers. head_dim: Dimension of a single attention head. \"\"\" hidden_size : int intermediate_size : int num_experts : int experts_top_k : int num_attention_heads : int num_key_value_heads : int rms_norm_eps : float head_dim : int","title":"Qwen3MoELayerParameters"},{"location":"models/qwen3_moe/#d9d.module.model.qwen3_moe.Qwen3MoEModel","text":"Bases: Module , ModuleLateInit , ModuleSupportsPipelining The Qwen3 Mixture-of-Experts (MoE) Transformer Decoder backbone. It is designed to be split across multiple pipeline stages. Source code in d9d/module/model/qwen3_moe/model.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 class Qwen3MoEModel ( nn . Module , ModuleLateInit , ModuleSupportsPipelining ): \"\"\" The Qwen3 Mixture-of-Experts (MoE) Transformer Decoder backbone. It is designed to be split across multiple pipeline stages. \"\"\" def __init__ ( self , params : Qwen3MoEParameters , stage : PipelineStageInfo , hidden_states_snapshot_mode : HiddenStatesAggregationMode , enable_checkpointing : bool , ): \"\"\" Constructs the Qwen3MoEModel object. Args: params: Configuration parameters for the full model. stage: Information about the pipeline stage this instance belongs to. hidden_states_snapshot_mode: Configures intermediate hidden state aggregation & snapshotting mode enable_checkpointing: If True, enables activation checkpointing for transformer layers to save memory. \"\"\" super () . __init__ () if stage . is_current_stage_first : self . embed_tokens = SplitTokenEmbeddings ( hidden_size = params . layer . hidden_size , split_vocab_size = params . split_vocab_size , split_order = params . split_vocab_order , ) # we use ModuleDict here to properly handle pipelining and loading weights after the model # was pipelined layer_start , layer_end = distribute_layers_for_pipeline_stage ( num_layers = params . num_hidden_layers , num_virtual_layers_pre = params . pipeline_num_virtual_layers_pre , # embeddings num_virtual_layers_post = params . pipeline_num_virtual_layers_post , # LM head stage = stage , ) self . _num_layers_before = layer_start self . _layers_iter = list ( map ( str , range ( layer_start , layer_end ))) layers = nn . ModuleDict ({ str ( layer_idx ): Qwen3MoELayer ( params = params . layer ) for layer_idx in self . _layers_iter }) self . layers : Mapping [ str , Qwen3MoELayer ] = cast ( Mapping [ str , Qwen3MoELayer ], layers ) self . rope_provider = RotaryEmbeddingProvider ( max_position_ids = params . max_position_ids , rope_base = params . rope_base , head_dim = params . layer . head_dim ) if stage . is_current_stage_last : self . norm = nn . RMSNorm ( normalized_shape = params . layer . hidden_size , eps = params . layer . rms_norm_eps ) self . _stage = stage self . _hidden_states_snapshot_mode = hidden_states_snapshot_mode self . _hidden_size = params . layer . hidden_size self . _enable_checkpointing = enable_checkpointing def output_dtype ( self ) -> torch . dtype : \"\"\" Returns the data type of the model output hidden states. \"\"\" return self . layers [ self . _layers_iter [ 0 ]] . input_layernorm . weight . dtype def forward ( self , input_ids : torch . Tensor | None = None , hidden_states : torch . Tensor | None = None , position_ids : torch . Tensor | None = None , hidden_states_snapshot : torch . Tensor | None = None , hidden_states_agg_mask : torch . Tensor | None = None , ) -> dict [ str , torch . Tensor | None ]: \"\"\" Executes the forward pass for the current pipeline stage. Args: input_ids: Indices of input sequence tokens. Required if this is the first pipeline stage. hidden_states: Hidden states from the previous pipeline stage. Required if this is not the first pipeline stage. position_ids: Indices of positions of each input sequence tokens in the position embeddings. hidden_states_snapshot: Accumulated tensor of aggregated hidden states from previous stages. Used if snapshotting is enabled. hidden_states_agg_mask: Mask used to aggregate hidden states for snapshots. Returns: A dictionary containing: * 'hidden_states': The output of the last layer in this stage. * 'hidden_states_snapshot': (Optional) The updated snapshot tensor. \"\"\" state_aggregator = create_hidden_states_aggregator ( self . _hidden_states_snapshot_mode , hidden_states_agg_mask ) if input_ids is not None : last_hidden_states = self . embed_tokens ( input_ids ) state_aggregator . add_hidden_states ( last_hidden_states ) else : last_hidden_states = hidden_states rope_params = self . rope_provider ( position_ids ) for decoder_layer_name in self . _layers_iter : decoder_layer = self . layers [ decoder_layer_name ] if self . _enable_checkpointing : last_hidden_states = checkpoint ( decoder_layer , last_hidden_states , rope_params , use_reentrant = False ) else : last_hidden_states = decoder_layer ( last_hidden_states , rope_params ) state_aggregator . add_hidden_states ( last_hidden_states ) if self . _stage . is_current_stage_last : last_hidden_states = self . norm ( last_hidden_states ) return { \"hidden_states\" : last_hidden_states , \"hidden_states_snapshot\" : state_aggregator . pack_with_snapshot ( hidden_states_snapshot ), } def reset_moe_stats ( self ): \"\"\" Resets routing statistics for all MoE layers in this stage. \"\"\" for layer_name in self . _layers_iter : self . layers [ layer_name ] . reset_moe_stats () @property def moe_tokens_per_expert ( self ) -> torch . Tensor : \"\"\" Retrieves the number of tokens routed to each expert across all layers. Returns: A tensor of shape (num_local_layers, num_experts) containing counts. \"\"\" return torch . stack ([ self . layers [ layer_name ] . moe_tokens_per_expert for layer_name in self . _layers_iter ], dim = 0 ) def reset_parameters ( self ): \"\"\"Resets module parameters\"\"\" if self . _stage . is_current_stage_first : self . embed_tokens . reset_parameters () self . rope_provider . reset_parameters () for decoder_layer_name in self . _layers_iter : decoder_layer = self . layers [ decoder_layer_name ] decoder_layer . reset_parameters () if self . _stage . is_current_stage_last : self . norm . reset_parameters () def infer_stage_inputs_from_pipeline_inputs ( self , inputs : dict [ str , torch . Tensor ], n_microbatches : int ) -> dict [ str , torch . Tensor ]: input_ids = inputs [ \"input_ids\" ] pp_inputs = {} # for calculation - input ids or prev hidden state if self . _stage . is_current_stage_first : pp_inputs [ \"input_ids\" ] = torch . empty ( ( input_ids . shape [ 0 ] // n_microbatches , input_ids . shape [ 1 ]), dtype = torch . long , device = input_ids . device ) else : pp_inputs [ \"hidden_states\" ] = torch . empty ( ( input_ids . shape [ 0 ] // n_microbatches , input_ids . shape [ 1 ], self . _hidden_size ), dtype = self . output_dtype (), device = input_ids . device , ) if self . _hidden_states_snapshot_mode != HiddenStatesAggregationMode . no : num_layers_before = self . _num_layers_before + 1 # 1 for embedding pp_inputs [ \"hidden_states_snapshot\" ] = torch . empty ( ( num_layers_before , input_ids . shape [ 0 ] // n_microbatches , self . _hidden_size ), dtype = self . output_dtype (), device = input_ids . device , ) return pp_inputs def infer_stage_outputs_from_pipeline_inputs ( self , inputs : dict [ str , torch . Tensor ], n_microbatches : int ) -> dict [ str , torch . Tensor ]: input_ids = inputs [ \"input_ids\" ] # for calculation - last hidden state pp_outputs = { \"hidden_states\" : torch . empty ( ( input_ids . shape [ 0 ] // n_microbatches , input_ids . shape [ 1 ], self . _hidden_size ), dtype = self . output_dtype (), device = input_ids . device , ) } # for state caching if self . _hidden_states_snapshot_mode != HiddenStatesAggregationMode . no : num_layers_before = self . _num_layers_before + 1 num_layers_current = len ( self . layers ) num_layers_after = num_layers_before + num_layers_current pp_outputs [ \"hidden_states_snapshot\" ] = torch . empty ( ( num_layers_after , input_ids . shape [ 0 ] // n_microbatches , self . _hidden_size ), dtype = self . output_dtype (), device = input_ids . device , ) return pp_outputs","title":"Qwen3MoEModel"},{"location":"models/qwen3_moe/#d9d.module.model.qwen3_moe.Qwen3MoEModel.moe_tokens_per_expert","text":"Retrieves the number of tokens routed to each expert across all layers. Returns: Type Description Tensor A tensor of shape (num_local_layers, num_experts) containing counts.","title":"moe_tokens_per_expert"},{"location":"models/qwen3_moe/#d9d.module.model.qwen3_moe.Qwen3MoEModel.__init__","text":"Constructs the Qwen3MoEModel object. Parameters: Name Type Description Default params Qwen3MoEParameters Configuration parameters for the full model. required stage PipelineStageInfo Information about the pipeline stage this instance belongs to. required hidden_states_snapshot_mode HiddenStatesAggregationMode Configures intermediate hidden state aggregation & snapshotting mode required enable_checkpointing bool If True, enables activation checkpointing for transformer layers to save memory. required Source code in d9d/module/model/qwen3_moe/model.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def __init__ ( self , params : Qwen3MoEParameters , stage : PipelineStageInfo , hidden_states_snapshot_mode : HiddenStatesAggregationMode , enable_checkpointing : bool , ): \"\"\" Constructs the Qwen3MoEModel object. Args: params: Configuration parameters for the full model. stage: Information about the pipeline stage this instance belongs to. hidden_states_snapshot_mode: Configures intermediate hidden state aggregation & snapshotting mode enable_checkpointing: If True, enables activation checkpointing for transformer layers to save memory. \"\"\" super () . __init__ () if stage . is_current_stage_first : self . embed_tokens = SplitTokenEmbeddings ( hidden_size = params . layer . hidden_size , split_vocab_size = params . split_vocab_size , split_order = params . split_vocab_order , ) # we use ModuleDict here to properly handle pipelining and loading weights after the model # was pipelined layer_start , layer_end = distribute_layers_for_pipeline_stage ( num_layers = params . num_hidden_layers , num_virtual_layers_pre = params . pipeline_num_virtual_layers_pre , # embeddings num_virtual_layers_post = params . pipeline_num_virtual_layers_post , # LM head stage = stage , ) self . _num_layers_before = layer_start self . _layers_iter = list ( map ( str , range ( layer_start , layer_end ))) layers = nn . ModuleDict ({ str ( layer_idx ): Qwen3MoELayer ( params = params . layer ) for layer_idx in self . _layers_iter }) self . layers : Mapping [ str , Qwen3MoELayer ] = cast ( Mapping [ str , Qwen3MoELayer ], layers ) self . rope_provider = RotaryEmbeddingProvider ( max_position_ids = params . max_position_ids , rope_base = params . rope_base , head_dim = params . layer . head_dim ) if stage . is_current_stage_last : self . norm = nn . RMSNorm ( normalized_shape = params . layer . hidden_size , eps = params . layer . rms_norm_eps ) self . _stage = stage self . _hidden_states_snapshot_mode = hidden_states_snapshot_mode self . _hidden_size = params . layer . hidden_size self . _enable_checkpointing = enable_checkpointing","title":"__init__"},{"location":"models/qwen3_moe/#d9d.module.model.qwen3_moe.Qwen3MoEModel.forward","text":"Executes the forward pass for the current pipeline stage. Parameters: Name Type Description Default input_ids Tensor | None Indices of input sequence tokens. Required if this is the first pipeline stage. None hidden_states Tensor | None Hidden states from the previous pipeline stage. Required if this is not the first pipeline stage. None position_ids Tensor | None Indices of positions of each input sequence tokens in the position embeddings. None hidden_states_snapshot Tensor | None Accumulated tensor of aggregated hidden states from previous stages. Used if snapshotting is enabled. None hidden_states_agg_mask Tensor | None Mask used to aggregate hidden states for snapshots. None Returns: Type Description dict [ str , Tensor | None] A dictionary containing: * 'hidden_states': The output of the last layer in this stage. * 'hidden_states_snapshot': (Optional) The updated snapshot tensor. Source code in d9d/module/model/qwen3_moe/model.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 def forward ( self , input_ids : torch . Tensor | None = None , hidden_states : torch . Tensor | None = None , position_ids : torch . Tensor | None = None , hidden_states_snapshot : torch . Tensor | None = None , hidden_states_agg_mask : torch . Tensor | None = None , ) -> dict [ str , torch . Tensor | None ]: \"\"\" Executes the forward pass for the current pipeline stage. Args: input_ids: Indices of input sequence tokens. Required if this is the first pipeline stage. hidden_states: Hidden states from the previous pipeline stage. Required if this is not the first pipeline stage. position_ids: Indices of positions of each input sequence tokens in the position embeddings. hidden_states_snapshot: Accumulated tensor of aggregated hidden states from previous stages. Used if snapshotting is enabled. hidden_states_agg_mask: Mask used to aggregate hidden states for snapshots. Returns: A dictionary containing: * 'hidden_states': The output of the last layer in this stage. * 'hidden_states_snapshot': (Optional) The updated snapshot tensor. \"\"\" state_aggregator = create_hidden_states_aggregator ( self . _hidden_states_snapshot_mode , hidden_states_agg_mask ) if input_ids is not None : last_hidden_states = self . embed_tokens ( input_ids ) state_aggregator . add_hidden_states ( last_hidden_states ) else : last_hidden_states = hidden_states rope_params = self . rope_provider ( position_ids ) for decoder_layer_name in self . _layers_iter : decoder_layer = self . layers [ decoder_layer_name ] if self . _enable_checkpointing : last_hidden_states = checkpoint ( decoder_layer , last_hidden_states , rope_params , use_reentrant = False ) else : last_hidden_states = decoder_layer ( last_hidden_states , rope_params ) state_aggregator . add_hidden_states ( last_hidden_states ) if self . _stage . is_current_stage_last : last_hidden_states = self . norm ( last_hidden_states ) return { \"hidden_states\" : last_hidden_states , \"hidden_states_snapshot\" : state_aggregator . pack_with_snapshot ( hidden_states_snapshot ), }","title":"forward"},{"location":"models/qwen3_moe/#d9d.module.model.qwen3_moe.Qwen3MoEModel.output_dtype","text":"Returns the data type of the model output hidden states. Source code in d9d/module/model/qwen3_moe/model.py 82 83 84 85 86 def output_dtype ( self ) -> torch . dtype : \"\"\" Returns the data type of the model output hidden states. \"\"\" return self . layers [ self . _layers_iter [ 0 ]] . input_layernorm . weight . dtype","title":"output_dtype"},{"location":"models/qwen3_moe/#d9d.module.model.qwen3_moe.Qwen3MoEModel.reset_moe_stats","text":"Resets routing statistics for all MoE layers in this stage. Source code in d9d/module/model/qwen3_moe/model.py 144 145 146 147 148 149 150 def reset_moe_stats ( self ): \"\"\" Resets routing statistics for all MoE layers in this stage. \"\"\" for layer_name in self . _layers_iter : self . layers [ layer_name ] . reset_moe_stats ()","title":"reset_moe_stats"},{"location":"models/qwen3_moe/#d9d.module.model.qwen3_moe.Qwen3MoEModel.reset_parameters","text":"Resets module parameters Source code in d9d/module/model/qwen3_moe/model.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 def reset_parameters ( self ): \"\"\"Resets module parameters\"\"\" if self . _stage . is_current_stage_first : self . embed_tokens . reset_parameters () self . rope_provider . reset_parameters () for decoder_layer_name in self . _layers_iter : decoder_layer = self . layers [ decoder_layer_name ] decoder_layer . reset_parameters () if self . _stage . is_current_stage_last : self . norm . reset_parameters ()","title":"reset_parameters"},{"location":"models/qwen3_moe/#d9d.module.model.qwen3_moe.Qwen3MoEParameters","text":"Bases: BaseModel Configuration parameters for the Qwen3 Mixture-of-Experts model backbone. Attributes: Name Type Description layer Qwen3MoELayerParameters Configuration shared across all transformer layers. num_hidden_layers int The total number of transformer layers. rope_base int Base value for RoPE frequency calculation. max_position_ids int Maximum sequence length. split_vocab_size dict [ str , int ] A dictionary mapping vocabulary segment names to their sizes. split_vocab_order list [ str ] The sequence in which vocabulary splits are correctly ordered. pipeline_num_virtual_layers_pre int The number of 'virtual' layers representing the computational cost of modules on the first stage, before the main layers (e.g., token and positional embeddings). pipeline_num_virtual_layers_post int The number of 'virtual' layers representing the computational cost of modules on the last stage, after the main layers (e.g., the final layer normalization and LM head). Source code in d9d/module/model/qwen3_moe/params.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 class Qwen3MoEParameters ( BaseModel ): \"\"\" Configuration parameters for the Qwen3 Mixture-of-Experts model backbone. Attributes: layer: Configuration shared across all transformer layers. num_hidden_layers: The total number of transformer layers. rope_base: Base value for RoPE frequency calculation. max_position_ids: Maximum sequence length. split_vocab_size: A dictionary mapping vocabulary segment names to their sizes. split_vocab_order: The sequence in which vocabulary splits are correctly ordered. pipeline_num_virtual_layers_pre: The number of 'virtual' layers representing the computational cost of modules on the *first* stage, before the main layers (e.g., token and positional embeddings). pipeline_num_virtual_layers_post: The number of 'virtual' layers representing the computational cost of modules on the *last* stage, after the main layers (e.g., the final layer normalization and LM head). \"\"\" layer : Qwen3MoELayerParameters num_hidden_layers : int rope_base : int max_position_ids : int split_vocab_size : dict [ str , int ] split_vocab_order : list [ str ] pipeline_num_virtual_layers_pre : int = 0 pipeline_num_virtual_layers_post : int = 0","title":"Qwen3MoEParameters"},{"location":"models/qwen3_moe/#d9d.module.parallelism.model.qwen3_moe","text":"","title":"qwen3_moe"},{"location":"models/qwen3_moe/#d9d.module.parallelism.model.qwen3_moe.parallelize_qwen3_moe_for_causal_lm","text":"Parallelizes the Qwen3 MoE Causal LM model. This function delegates backbone parallelization to parallelize_qwen3_moe_model and additionally configures the language model head with Hybrid Sharded Data Parallelism (HSDP). Parameters: Name Type Description Default dist_context DistributedContext The distributed context containing device meshes and topology info. required model Qwen3MoEForCausalLM The Qwen3 MoE Causal LM model to parallelize. required stage PipelineStageInfo Information about the current pipeline stage. required Source code in d9d/module/parallelism/model/qwen3_moe.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def parallelize_qwen3_moe_for_causal_lm ( dist_context : DistributedContext , model : Qwen3MoEForCausalLM , stage : PipelineStageInfo ): \"\"\" Parallelizes the Qwen3 MoE Causal LM model. This function delegates backbone parallelization to ``parallelize_qwen3_moe_model`` and additionally configures the language model head with Hybrid Sharded Data Parallelism (HSDP). Args: dist_context: The distributed context containing device meshes and topology info. model: The Qwen3 MoE Causal LM model to parallelize. stage: Information about the current pipeline stage. \"\"\" dense_mesh = dist_context . mesh_for ( DENSE_DOMAIN ) parallelize_qwen3_moe_model ( dist_context , model . model , stage ) if stage . is_current_stage_last : parallelize_hsdp ( model . lm_head , mesh = dense_mesh [ \"dp_replicate\" , \"dp_cp_shard\" , \"cp_replicate\" ], )","title":"parallelize_qwen3_moe_for_causal_lm"},{"location":"models/qwen3_moe/#d9d.module.parallelism.model.qwen3_moe.parallelize_qwen3_moe_for_classification","text":"Parallelizes the Qwen3 MoE classification model. This function delegates backbone parallelization to parallelize_qwen3_moe_model and additionally configures the classification head with Hybrid Sharded Data Parallelism (HSDP). Parameters: Name Type Description Default dist_context DistributedContext The distributed context containing device meshes and topology info. required model Qwen3MoEForClassification The Qwen3 MoE classification model to parallelize. required stage PipelineStageInfo Information about the current pipeline stage. required Source code in d9d/module/parallelism/model/qwen3_moe.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def parallelize_qwen3_moe_for_classification ( dist_context : DistributedContext , model : Qwen3MoEForClassification , stage : PipelineStageInfo ): \"\"\" Parallelizes the Qwen3 MoE classification model. This function delegates backbone parallelization to ``parallelize_qwen3_moe_model`` and additionally configures the classification head with Hybrid Sharded Data Parallelism (HSDP). Args: dist_context: The distributed context containing device meshes and topology info. model: The Qwen3 MoE classification model to parallelize. stage: Information about the current pipeline stage. \"\"\" dense_mesh = dist_context . mesh_for ( DENSE_DOMAIN ) parallelize_qwen3_moe_model ( dist_context , model . model , stage ) if stage . is_current_stage_last : parallelize_hsdp ( model . cls_head , mesh = dense_mesh [ \"dp_replicate\" , \"dp_cp_shard\" , \"cp_replicate\" ], )","title":"parallelize_qwen3_moe_for_classification"},{"location":"models/qwen3_moe/#d9d.module.parallelism.model.qwen3_moe.parallelize_qwen3_moe_model","text":"Parallelizes the base Qwen3 MoE model components. This function configures the model layers for distributed execution within a pipeline stage. It applies Hybrid Sharded Data Parallelism (HSDP) to dense components (embeddings, norms, attention) and Expert Parallelism (EP) to the Mixture-of-Experts (MLP) layers. Current usage constraints: * Tensor Parallelism is not supported (we may implement it later). * Context Parallelism is not supported (we will implement it later). Parameters: Name Type Description Default dist_context DistributedContext The distributed context. required model Qwen3MoEModel The Qwen3 MoE base model to parallelize. required stage PipelineStageInfo Information about the current pipeline stage. required Raises: Type Description ValueError If Tensor Parallel or Context Parallel is enabled in the context. Source code in d9d/module/parallelism/model/qwen3_moe.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def parallelize_qwen3_moe_model ( dist_context : DistributedContext , model : Qwen3MoEModel , stage : PipelineStageInfo ): \"\"\" Parallelizes the base Qwen3 MoE model components. This function configures the model layers for distributed execution within a pipeline stage. It applies Hybrid Sharded Data Parallelism (HSDP) to dense components (embeddings, norms, attention) and Expert Parallelism (EP) to the Mixture-of-Experts (MLP) layers. Current usage constraints: * Tensor Parallelism is not supported (we may implement it later). * Context Parallelism is not supported (we will implement it later). Args: dist_context: The distributed context. model: The Qwen3 MoE base model to parallelize. stage: Information about the current pipeline stage. Raises: ValueError: If Tensor Parallel or Context Parallel is enabled in the context. \"\"\" dims = dist_context . mesh_params dense_mesh = dist_context . mesh_for ( DENSE_DOMAIN ) expert_mesh = dist_context . mesh_for ( EXPERT_DOMAIN ) if dims . has_tensor_parallel : raise ValueError ( \"Tensor Parallel currently is not supported for this model.\" ) if dims . has_context_parallel_replicate or dims . has_context_parallel_shard : raise ValueError ( \"Context Parallel currently is not supported for this model.\" ) if stage . is_current_stage_first : parallelize_hsdp ( model . embed_tokens , mesh = dense_mesh [ \"dp_replicate\" , \"dp_cp_shard\" , \"cp_replicate\" ]) if stage . is_current_stage_last : parallelize_hsdp ( model . norm , mesh = dense_mesh [ \"dp_replicate\" , \"dp_cp_shard\" , \"cp_replicate\" ], ) for layer in model . layers . values (): parallelize_expert_parallel ( layer . mlp , mesh_experts = expert_mesh [ \"ep_replicate\" , \"ep_shard\" ]) parallelize_hsdp ( layer . self_attn , mesh = dense_mesh [ \"dp_replicate\" , \"dp_cp_shard\" , \"cp_replicate\" ], ) parallelize_hsdp ( layer . input_layernorm , mesh = dense_mesh [ \"dp_replicate\" , \"dp_cp_shard\" , \"cp_replicate\" ], ) parallelize_hsdp ( layer . post_attention_layernorm , mesh = dense_mesh [ \"dp_replicate\" , \"dp_cp_shard\" , \"cp_replicate\" ], )","title":"parallelize_qwen3_moe_model"},{"location":"modules/attention/","text":"About The d9d.module.block.attention package provides optimized attention mechanism implementations. Features Scaled Dot-Product Attention Kernels FlashSdpa - FlashAttention 2 (using new Torch SDPA API) Grouped-Query Attention GroupedQueryAttention is a Grouped-Query Attention implementation. Due to its abstract nature it is also can be used as Multi-Head Attention and Multi-Query Attention module. Uses FlashSDPA kernel. Uses Rotary Positional Encoding Supports optional QK Normalization . d9d.module.block.attention Provides attention layer implementations. GroupedQueryAttention Bases: Module , ModuleLateInit Implements Grouped Query Attention (GQA) with RoPE and optional QK Normalization. This module performs the full attention mechanism pipeline: 1. Linear projection to Q, K, V. 2. Optional RMS Normalization on Q and K. 3. Rotary Positional Embedding (RoPE) application. 4. Scaled Dot Product Attention (via FlashAttention). 5. Output projection. Source code in d9d/module/block/attention/grouped_query.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 class GroupedQueryAttention ( nn . Module , ModuleLateInit ): \"\"\" Implements Grouped Query Attention (GQA) with RoPE and optional QK Normalization. This module performs the full attention mechanism pipeline: 1. Linear projection to Q, K, V. 2. Optional RMS Normalization on Q and K. 3. Rotary Positional Embedding (RoPE) application. 4. Scaled Dot Product Attention (via FlashAttention). 5. Output projection. \"\"\" def __init__ ( self , hidden_size : int , num_attention_heads : int , num_key_value_heads : int , head_dim : int , qk_norm_eps : float | None , is_causal : bool , ): \"\"\" Constructs the GroupedQueryAttention layer. Args: hidden_size: Hidden size. num_attention_heads: Number of Query heads. num_key_value_heads: Number of Key/Value heads. If less than `num_attention_heads`, GQA/MQA is enabled. head_dim: Dimensionality of a single attention head. qk_norm_eps: Epsilon for LayerNorm/RMSNorm applied to Q and K. If None, normalization is disabled. is_causal: Whether to apply a causal mask (auto-regressive constraint). \"\"\" super () . __init__ () self . _head_dim = head_dim self . _num_key_value_groups = num_attention_heads // num_key_value_heads self . _scaling = head_dim **- 0.5 self . q_proj = nn . Linear ( hidden_size , num_attention_heads * head_dim , bias = False ) self . k_proj = nn . Linear ( hidden_size , num_key_value_heads * head_dim , bias = False ) self . v_proj = nn . Linear ( hidden_size , num_key_value_heads * head_dim , bias = False ) self . o_proj = nn . Linear ( num_attention_heads * head_dim , hidden_size , bias = False ) self . q_norm : nn . RMSNorm | None self . k_norm : nn . RMSNorm | None if qk_norm_eps is not None : self . q_norm = nn . RMSNorm ( normalized_shape = head_dim , eps = qk_norm_eps ) self . k_norm = nn . RMSNorm ( normalized_shape = head_dim , eps = qk_norm_eps ) else : self . q_norm = None self . k_norm = None self . rope = RotaryEmbeddingApplicator () self . kernel = FlashSdpa () self . _is_causal = is_causal def forward ( self , hidden_states : torch . Tensor , attention_mask : torch . Tensor | None , position_embeddings : tuple [ torch . Tensor , torch . Tensor ], ) -> torch . Tensor : \"\"\" Computes the attention operation. Args: hidden_states: Input tensor. Shape: `(batch, seq_len, hidden_size)`. attention_mask: Optional mask associated with the inputs. position_embeddings: Tuple of `(cos, sin)` tensors for RoPE application. Each tensor should be of shape `(batch, seq_len, head_dim)` Returns: The attention output tensor. Shape: `(batch, seq_len, hidden_size)`. \"\"\" input_shape = hidden_states . shape [: - 1 ] hidden_shape = ( * input_shape , - 1 , self . _head_dim ) query_states = self . q_proj ( hidden_states ) . view ( hidden_shape ) if self . q_norm is not None : query_states = self . q_norm ( query_states ) query_states = query_states . transpose ( 1 , 2 ) key_states = self . k_proj ( hidden_states ) . view ( hidden_shape ) if self . k_norm is not None : key_states = self . k_norm ( key_states ) key_states = key_states . transpose ( 1 , 2 ) value_states = self . v_proj ( hidden_states ) . view ( hidden_shape ) . transpose ( 1 , 2 ) query_states , key_states = self . rope ( query_states , key_states , position_embeddings [ 0 ], position_embeddings [ 1 ]) outputs = self . kernel ( query_states , key_states , value_states , attention_mask = attention_mask , is_causal = self . _is_causal , scale = self . _scaling , ) outputs = outputs . reshape ( * input_shape , - 1 ) . contiguous () outputs = self . o_proj ( outputs ) return outputs def reset_parameters ( self ): \"\"\"Resets module parameters.\"\"\" self . q_proj . reset_parameters () self . k_proj . reset_parameters () self . v_proj . reset_parameters () self . o_proj . reset_parameters () if self . q_norm is not None : self . q_norm . reset_parameters () if self . k_norm is not None : self . k_norm . reset_parameters () __init__ ( hidden_size , num_attention_heads , num_key_value_heads , head_dim , qk_norm_eps , is_causal ) Constructs the GroupedQueryAttention layer. Parameters: Name Type Description Default hidden_size int Hidden size. required num_attention_heads int Number of Query heads. required num_key_value_heads int Number of Key/Value heads. If less than num_attention_heads , GQA/MQA is enabled. required head_dim int Dimensionality of a single attention head. required qk_norm_eps float | None Epsilon for LayerNorm/RMSNorm applied to Q and K. If None, normalization is disabled. required is_causal bool Whether to apply a causal mask (auto-regressive constraint). required Source code in d9d/module/block/attention/grouped_query.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def __init__ ( self , hidden_size : int , num_attention_heads : int , num_key_value_heads : int , head_dim : int , qk_norm_eps : float | None , is_causal : bool , ): \"\"\" Constructs the GroupedQueryAttention layer. Args: hidden_size: Hidden size. num_attention_heads: Number of Query heads. num_key_value_heads: Number of Key/Value heads. If less than `num_attention_heads`, GQA/MQA is enabled. head_dim: Dimensionality of a single attention head. qk_norm_eps: Epsilon for LayerNorm/RMSNorm applied to Q and K. If None, normalization is disabled. is_causal: Whether to apply a causal mask (auto-regressive constraint). \"\"\" super () . __init__ () self . _head_dim = head_dim self . _num_key_value_groups = num_attention_heads // num_key_value_heads self . _scaling = head_dim **- 0.5 self . q_proj = nn . Linear ( hidden_size , num_attention_heads * head_dim , bias = False ) self . k_proj = nn . Linear ( hidden_size , num_key_value_heads * head_dim , bias = False ) self . v_proj = nn . Linear ( hidden_size , num_key_value_heads * head_dim , bias = False ) self . o_proj = nn . Linear ( num_attention_heads * head_dim , hidden_size , bias = False ) self . q_norm : nn . RMSNorm | None self . k_norm : nn . RMSNorm | None if qk_norm_eps is not None : self . q_norm = nn . RMSNorm ( normalized_shape = head_dim , eps = qk_norm_eps ) self . k_norm = nn . RMSNorm ( normalized_shape = head_dim , eps = qk_norm_eps ) else : self . q_norm = None self . k_norm = None self . rope = RotaryEmbeddingApplicator () self . kernel = FlashSdpa () self . _is_causal = is_causal forward ( hidden_states , attention_mask , position_embeddings ) Computes the attention operation. Parameters: Name Type Description Default hidden_states Tensor Input tensor. Shape: (batch, seq_len, hidden_size) . required attention_mask Tensor | None Optional mask associated with the inputs. required position_embeddings tuple [ Tensor , Tensor ] Tuple of (cos, sin) tensors for RoPE application. Each tensor should be of shape (batch, seq_len, head_dim) required Returns: Type Description Tensor The attention output tensor. Shape: (batch, seq_len, hidden_size) . Source code in d9d/module/block/attention/grouped_query.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 def forward ( self , hidden_states : torch . Tensor , attention_mask : torch . Tensor | None , position_embeddings : tuple [ torch . Tensor , torch . Tensor ], ) -> torch . Tensor : \"\"\" Computes the attention operation. Args: hidden_states: Input tensor. Shape: `(batch, seq_len, hidden_size)`. attention_mask: Optional mask associated with the inputs. position_embeddings: Tuple of `(cos, sin)` tensors for RoPE application. Each tensor should be of shape `(batch, seq_len, head_dim)` Returns: The attention output tensor. Shape: `(batch, seq_len, hidden_size)`. \"\"\" input_shape = hidden_states . shape [: - 1 ] hidden_shape = ( * input_shape , - 1 , self . _head_dim ) query_states = self . q_proj ( hidden_states ) . view ( hidden_shape ) if self . q_norm is not None : query_states = self . q_norm ( query_states ) query_states = query_states . transpose ( 1 , 2 ) key_states = self . k_proj ( hidden_states ) . view ( hidden_shape ) if self . k_norm is not None : key_states = self . k_norm ( key_states ) key_states = key_states . transpose ( 1 , 2 ) value_states = self . v_proj ( hidden_states ) . view ( hidden_shape ) . transpose ( 1 , 2 ) query_states , key_states = self . rope ( query_states , key_states , position_embeddings [ 0 ], position_embeddings [ 1 ]) outputs = self . kernel ( query_states , key_states , value_states , attention_mask = attention_mask , is_causal = self . _is_causal , scale = self . _scaling , ) outputs = outputs . reshape ( * input_shape , - 1 ) . contiguous () outputs = self . o_proj ( outputs ) return outputs reset_parameters () Resets module parameters. Source code in d9d/module/block/attention/grouped_query.py 119 120 121 122 123 124 125 126 127 128 129 def reset_parameters ( self ): \"\"\"Resets module parameters.\"\"\" self . q_proj . reset_parameters () self . k_proj . reset_parameters () self . v_proj . reset_parameters () self . o_proj . reset_parameters () if self . q_norm is not None : self . q_norm . reset_parameters () if self . k_norm is not None : self . k_norm . reset_parameters () d9d.module.block.attention.sdpa FlashSdpa Bases: Module Executes Scaled Dot Product Attention (SDPA) enforcing the FlashAttention backend. Source code in d9d/module/block/attention/sdpa/flash.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 class FlashSdpa ( nn . Module ): \"\"\"Executes Scaled Dot Product Attention (SDPA) enforcing the FlashAttention backend.\"\"\" def __init__ ( self ): \"\"\" Constructs the FlashSdpa object. \"\"\" super () . __init__ () def forward ( self , query_states : torch . Tensor , key_states : torch . Tensor , value_states : torch . Tensor , attention_mask : torch . Tensor | None , is_causal : bool , scale : float , ) -> torch . Tensor : \"\"\" Computes Scaled Dot-Product Attention using FlashAttention. Args: query_states: Query tensor. Shape: `(batch, n_q_heads, seq_len, head_dim)`. key_states: Key tensor. Shape: `(batch, n_kv_heads, seq_len, head_dim)`. value_states: Value tensor. Shape: `(batch, n_kv_heads, seq_len, head_dim)`. attention_mask: Optional attention mask (usually not needed for FlashAttn with causal=True). is_causal: If True, applies a causal mask (upper triangular masking). scale: Scaling factor applied to the dot products (usually `1 / sqrt(head_dim)`). Returns: The attention output tensor, permuted to channel-last format. Shape: `(batch, seq_len, n_q_heads, head_dim)`. \"\"\" with sdpa_kernel ( SDPBackend . FLASH_ATTENTION ): results = F . scaled_dot_product_attention ( query_states , key_states , value_states , attn_mask = attention_mask , dropout_p = 0.0 , is_causal = is_causal , scale = scale , enable_gqa = query_states . shape [ 1 ] != key_states . shape [ 1 ], ) return results . transpose ( 1 , 2 ) . contiguous () __init__ () Constructs the FlashSdpa object. Source code in d9d/module/block/attention/sdpa/flash.py 10 11 12 13 14 def __init__ ( self ): \"\"\" Constructs the FlashSdpa object. \"\"\" super () . __init__ () forward ( query_states , key_states , value_states , attention_mask , is_causal , scale ) Computes Scaled Dot-Product Attention using FlashAttention. Parameters: Name Type Description Default query_states Tensor Query tensor. Shape: (batch, n_q_heads, seq_len, head_dim) . required key_states Tensor Key tensor. Shape: (batch, n_kv_heads, seq_len, head_dim) . required value_states Tensor Value tensor. Shape: (batch, n_kv_heads, seq_len, head_dim) . required attention_mask Tensor | None Optional attention mask (usually not needed for FlashAttn with causal=True). required is_causal bool If True, applies a causal mask (upper triangular masking). required scale float Scaling factor applied to the dot products (usually 1 / sqrt(head_dim) ). required Returns: Type Description Tensor The attention output tensor, permuted to channel-last format. Shape: (batch, seq_len, n_q_heads, head_dim) . Source code in d9d/module/block/attention/sdpa/flash.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def forward ( self , query_states : torch . Tensor , key_states : torch . Tensor , value_states : torch . Tensor , attention_mask : torch . Tensor | None , is_causal : bool , scale : float , ) -> torch . Tensor : \"\"\" Computes Scaled Dot-Product Attention using FlashAttention. Args: query_states: Query tensor. Shape: `(batch, n_q_heads, seq_len, head_dim)`. key_states: Key tensor. Shape: `(batch, n_kv_heads, seq_len, head_dim)`. value_states: Value tensor. Shape: `(batch, n_kv_heads, seq_len, head_dim)`. attention_mask: Optional attention mask (usually not needed for FlashAttn with causal=True). is_causal: If True, applies a causal mask (upper triangular masking). scale: Scaling factor applied to the dot products (usually `1 / sqrt(head_dim)`). Returns: The attention output tensor, permuted to channel-last format. Shape: `(batch, seq_len, n_q_heads, head_dim)`. \"\"\" with sdpa_kernel ( SDPBackend . FLASH_ATTENTION ): results = F . scaled_dot_product_attention ( query_states , key_states , value_states , attn_mask = attention_mask , dropout_p = 0.0 , is_causal = is_causal , scale = scale , enable_gqa = query_states . shape [ 1 ] != key_states . shape [ 1 ], ) return results . transpose ( 1 , 2 ) . contiguous ()","title":"Attention Layers"},{"location":"modules/attention/#about","text":"The d9d.module.block.attention package provides optimized attention mechanism implementations.","title":"About"},{"location":"modules/attention/#features","text":"","title":"Features"},{"location":"modules/attention/#scaled-dot-product-attention-kernels","text":"FlashSdpa - FlashAttention 2 (using new Torch SDPA API)","title":"Scaled Dot-Product Attention Kernels"},{"location":"modules/attention/#grouped-query-attention","text":"GroupedQueryAttention is a Grouped-Query Attention implementation. Due to its abstract nature it is also can be used as Multi-Head Attention and Multi-Query Attention module. Uses FlashSDPA kernel. Uses Rotary Positional Encoding Supports optional QK Normalization .","title":"Grouped-Query Attention"},{"location":"modules/attention/#d9d.module.block.attention","text":"Provides attention layer implementations.","title":"attention"},{"location":"modules/attention/#d9d.module.block.attention.GroupedQueryAttention","text":"Bases: Module , ModuleLateInit Implements Grouped Query Attention (GQA) with RoPE and optional QK Normalization. This module performs the full attention mechanism pipeline: 1. Linear projection to Q, K, V. 2. Optional RMS Normalization on Q and K. 3. Rotary Positional Embedding (RoPE) application. 4. Scaled Dot Product Attention (via FlashAttention). 5. Output projection. Source code in d9d/module/block/attention/grouped_query.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 class GroupedQueryAttention ( nn . Module , ModuleLateInit ): \"\"\" Implements Grouped Query Attention (GQA) with RoPE and optional QK Normalization. This module performs the full attention mechanism pipeline: 1. Linear projection to Q, K, V. 2. Optional RMS Normalization on Q and K. 3. Rotary Positional Embedding (RoPE) application. 4. Scaled Dot Product Attention (via FlashAttention). 5. Output projection. \"\"\" def __init__ ( self , hidden_size : int , num_attention_heads : int , num_key_value_heads : int , head_dim : int , qk_norm_eps : float | None , is_causal : bool , ): \"\"\" Constructs the GroupedQueryAttention layer. Args: hidden_size: Hidden size. num_attention_heads: Number of Query heads. num_key_value_heads: Number of Key/Value heads. If less than `num_attention_heads`, GQA/MQA is enabled. head_dim: Dimensionality of a single attention head. qk_norm_eps: Epsilon for LayerNorm/RMSNorm applied to Q and K. If None, normalization is disabled. is_causal: Whether to apply a causal mask (auto-regressive constraint). \"\"\" super () . __init__ () self . _head_dim = head_dim self . _num_key_value_groups = num_attention_heads // num_key_value_heads self . _scaling = head_dim **- 0.5 self . q_proj = nn . Linear ( hidden_size , num_attention_heads * head_dim , bias = False ) self . k_proj = nn . Linear ( hidden_size , num_key_value_heads * head_dim , bias = False ) self . v_proj = nn . Linear ( hidden_size , num_key_value_heads * head_dim , bias = False ) self . o_proj = nn . Linear ( num_attention_heads * head_dim , hidden_size , bias = False ) self . q_norm : nn . RMSNorm | None self . k_norm : nn . RMSNorm | None if qk_norm_eps is not None : self . q_norm = nn . RMSNorm ( normalized_shape = head_dim , eps = qk_norm_eps ) self . k_norm = nn . RMSNorm ( normalized_shape = head_dim , eps = qk_norm_eps ) else : self . q_norm = None self . k_norm = None self . rope = RotaryEmbeddingApplicator () self . kernel = FlashSdpa () self . _is_causal = is_causal def forward ( self , hidden_states : torch . Tensor , attention_mask : torch . Tensor | None , position_embeddings : tuple [ torch . Tensor , torch . Tensor ], ) -> torch . Tensor : \"\"\" Computes the attention operation. Args: hidden_states: Input tensor. Shape: `(batch, seq_len, hidden_size)`. attention_mask: Optional mask associated with the inputs. position_embeddings: Tuple of `(cos, sin)` tensors for RoPE application. Each tensor should be of shape `(batch, seq_len, head_dim)` Returns: The attention output tensor. Shape: `(batch, seq_len, hidden_size)`. \"\"\" input_shape = hidden_states . shape [: - 1 ] hidden_shape = ( * input_shape , - 1 , self . _head_dim ) query_states = self . q_proj ( hidden_states ) . view ( hidden_shape ) if self . q_norm is not None : query_states = self . q_norm ( query_states ) query_states = query_states . transpose ( 1 , 2 ) key_states = self . k_proj ( hidden_states ) . view ( hidden_shape ) if self . k_norm is not None : key_states = self . k_norm ( key_states ) key_states = key_states . transpose ( 1 , 2 ) value_states = self . v_proj ( hidden_states ) . view ( hidden_shape ) . transpose ( 1 , 2 ) query_states , key_states = self . rope ( query_states , key_states , position_embeddings [ 0 ], position_embeddings [ 1 ]) outputs = self . kernel ( query_states , key_states , value_states , attention_mask = attention_mask , is_causal = self . _is_causal , scale = self . _scaling , ) outputs = outputs . reshape ( * input_shape , - 1 ) . contiguous () outputs = self . o_proj ( outputs ) return outputs def reset_parameters ( self ): \"\"\"Resets module parameters.\"\"\" self . q_proj . reset_parameters () self . k_proj . reset_parameters () self . v_proj . reset_parameters () self . o_proj . reset_parameters () if self . q_norm is not None : self . q_norm . reset_parameters () if self . k_norm is not None : self . k_norm . reset_parameters ()","title":"GroupedQueryAttention"},{"location":"modules/attention/#d9d.module.block.attention.GroupedQueryAttention.__init__","text":"Constructs the GroupedQueryAttention layer. Parameters: Name Type Description Default hidden_size int Hidden size. required num_attention_heads int Number of Query heads. required num_key_value_heads int Number of Key/Value heads. If less than num_attention_heads , GQA/MQA is enabled. required head_dim int Dimensionality of a single attention head. required qk_norm_eps float | None Epsilon for LayerNorm/RMSNorm applied to Q and K. If None, normalization is disabled. required is_causal bool Whether to apply a causal mask (auto-regressive constraint). required Source code in d9d/module/block/attention/grouped_query.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def __init__ ( self , hidden_size : int , num_attention_heads : int , num_key_value_heads : int , head_dim : int , qk_norm_eps : float | None , is_causal : bool , ): \"\"\" Constructs the GroupedQueryAttention layer. Args: hidden_size: Hidden size. num_attention_heads: Number of Query heads. num_key_value_heads: Number of Key/Value heads. If less than `num_attention_heads`, GQA/MQA is enabled. head_dim: Dimensionality of a single attention head. qk_norm_eps: Epsilon for LayerNorm/RMSNorm applied to Q and K. If None, normalization is disabled. is_causal: Whether to apply a causal mask (auto-regressive constraint). \"\"\" super () . __init__ () self . _head_dim = head_dim self . _num_key_value_groups = num_attention_heads // num_key_value_heads self . _scaling = head_dim **- 0.5 self . q_proj = nn . Linear ( hidden_size , num_attention_heads * head_dim , bias = False ) self . k_proj = nn . Linear ( hidden_size , num_key_value_heads * head_dim , bias = False ) self . v_proj = nn . Linear ( hidden_size , num_key_value_heads * head_dim , bias = False ) self . o_proj = nn . Linear ( num_attention_heads * head_dim , hidden_size , bias = False ) self . q_norm : nn . RMSNorm | None self . k_norm : nn . RMSNorm | None if qk_norm_eps is not None : self . q_norm = nn . RMSNorm ( normalized_shape = head_dim , eps = qk_norm_eps ) self . k_norm = nn . RMSNorm ( normalized_shape = head_dim , eps = qk_norm_eps ) else : self . q_norm = None self . k_norm = None self . rope = RotaryEmbeddingApplicator () self . kernel = FlashSdpa () self . _is_causal = is_causal","title":"__init__"},{"location":"modules/attention/#d9d.module.block.attention.GroupedQueryAttention.forward","text":"Computes the attention operation. Parameters: Name Type Description Default hidden_states Tensor Input tensor. Shape: (batch, seq_len, hidden_size) . required attention_mask Tensor | None Optional mask associated with the inputs. required position_embeddings tuple [ Tensor , Tensor ] Tuple of (cos, sin) tensors for RoPE application. Each tensor should be of shape (batch, seq_len, head_dim) required Returns: Type Description Tensor The attention output tensor. Shape: (batch, seq_len, hidden_size) . Source code in d9d/module/block/attention/grouped_query.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 def forward ( self , hidden_states : torch . Tensor , attention_mask : torch . Tensor | None , position_embeddings : tuple [ torch . Tensor , torch . Tensor ], ) -> torch . Tensor : \"\"\" Computes the attention operation. Args: hidden_states: Input tensor. Shape: `(batch, seq_len, hidden_size)`. attention_mask: Optional mask associated with the inputs. position_embeddings: Tuple of `(cos, sin)` tensors for RoPE application. Each tensor should be of shape `(batch, seq_len, head_dim)` Returns: The attention output tensor. Shape: `(batch, seq_len, hidden_size)`. \"\"\" input_shape = hidden_states . shape [: - 1 ] hidden_shape = ( * input_shape , - 1 , self . _head_dim ) query_states = self . q_proj ( hidden_states ) . view ( hidden_shape ) if self . q_norm is not None : query_states = self . q_norm ( query_states ) query_states = query_states . transpose ( 1 , 2 ) key_states = self . k_proj ( hidden_states ) . view ( hidden_shape ) if self . k_norm is not None : key_states = self . k_norm ( key_states ) key_states = key_states . transpose ( 1 , 2 ) value_states = self . v_proj ( hidden_states ) . view ( hidden_shape ) . transpose ( 1 , 2 ) query_states , key_states = self . rope ( query_states , key_states , position_embeddings [ 0 ], position_embeddings [ 1 ]) outputs = self . kernel ( query_states , key_states , value_states , attention_mask = attention_mask , is_causal = self . _is_causal , scale = self . _scaling , ) outputs = outputs . reshape ( * input_shape , - 1 ) . contiguous () outputs = self . o_proj ( outputs ) return outputs","title":"forward"},{"location":"modules/attention/#d9d.module.block.attention.GroupedQueryAttention.reset_parameters","text":"Resets module parameters. Source code in d9d/module/block/attention/grouped_query.py 119 120 121 122 123 124 125 126 127 128 129 def reset_parameters ( self ): \"\"\"Resets module parameters.\"\"\" self . q_proj . reset_parameters () self . k_proj . reset_parameters () self . v_proj . reset_parameters () self . o_proj . reset_parameters () if self . q_norm is not None : self . q_norm . reset_parameters () if self . k_norm is not None : self . k_norm . reset_parameters ()","title":"reset_parameters"},{"location":"modules/attention/#d9d.module.block.attention.sdpa","text":"","title":"sdpa"},{"location":"modules/attention/#d9d.module.block.attention.sdpa.FlashSdpa","text":"Bases: Module Executes Scaled Dot Product Attention (SDPA) enforcing the FlashAttention backend. Source code in d9d/module/block/attention/sdpa/flash.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 class FlashSdpa ( nn . Module ): \"\"\"Executes Scaled Dot Product Attention (SDPA) enforcing the FlashAttention backend.\"\"\" def __init__ ( self ): \"\"\" Constructs the FlashSdpa object. \"\"\" super () . __init__ () def forward ( self , query_states : torch . Tensor , key_states : torch . Tensor , value_states : torch . Tensor , attention_mask : torch . Tensor | None , is_causal : bool , scale : float , ) -> torch . Tensor : \"\"\" Computes Scaled Dot-Product Attention using FlashAttention. Args: query_states: Query tensor. Shape: `(batch, n_q_heads, seq_len, head_dim)`. key_states: Key tensor. Shape: `(batch, n_kv_heads, seq_len, head_dim)`. value_states: Value tensor. Shape: `(batch, n_kv_heads, seq_len, head_dim)`. attention_mask: Optional attention mask (usually not needed for FlashAttn with causal=True). is_causal: If True, applies a causal mask (upper triangular masking). scale: Scaling factor applied to the dot products (usually `1 / sqrt(head_dim)`). Returns: The attention output tensor, permuted to channel-last format. Shape: `(batch, seq_len, n_q_heads, head_dim)`. \"\"\" with sdpa_kernel ( SDPBackend . FLASH_ATTENTION ): results = F . scaled_dot_product_attention ( query_states , key_states , value_states , attn_mask = attention_mask , dropout_p = 0.0 , is_causal = is_causal , scale = scale , enable_gqa = query_states . shape [ 1 ] != key_states . shape [ 1 ], ) return results . transpose ( 1 , 2 ) . contiguous ()","title":"FlashSdpa"},{"location":"modules/attention/#d9d.module.block.attention.sdpa.FlashSdpa.__init__","text":"Constructs the FlashSdpa object. Source code in d9d/module/block/attention/sdpa/flash.py 10 11 12 13 14 def __init__ ( self ): \"\"\" Constructs the FlashSdpa object. \"\"\" super () . __init__ ()","title":"__init__"},{"location":"modules/attention/#d9d.module.block.attention.sdpa.FlashSdpa.forward","text":"Computes Scaled Dot-Product Attention using FlashAttention. Parameters: Name Type Description Default query_states Tensor Query tensor. Shape: (batch, n_q_heads, seq_len, head_dim) . required key_states Tensor Key tensor. Shape: (batch, n_kv_heads, seq_len, head_dim) . required value_states Tensor Value tensor. Shape: (batch, n_kv_heads, seq_len, head_dim) . required attention_mask Tensor | None Optional attention mask (usually not needed for FlashAttn with causal=True). required is_causal bool If True, applies a causal mask (upper triangular masking). required scale float Scaling factor applied to the dot products (usually 1 / sqrt(head_dim) ). required Returns: Type Description Tensor The attention output tensor, permuted to channel-last format. Shape: (batch, seq_len, n_q_heads, head_dim) . Source code in d9d/module/block/attention/sdpa/flash.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def forward ( self , query_states : torch . Tensor , key_states : torch . Tensor , value_states : torch . Tensor , attention_mask : torch . Tensor | None , is_causal : bool , scale : float , ) -> torch . Tensor : \"\"\" Computes Scaled Dot-Product Attention using FlashAttention. Args: query_states: Query tensor. Shape: `(batch, n_q_heads, seq_len, head_dim)`. key_states: Key tensor. Shape: `(batch, n_kv_heads, seq_len, head_dim)`. value_states: Value tensor. Shape: `(batch, n_kv_heads, seq_len, head_dim)`. attention_mask: Optional attention mask (usually not needed for FlashAttn with causal=True). is_causal: If True, applies a causal mask (upper triangular masking). scale: Scaling factor applied to the dot products (usually `1 / sqrt(head_dim)`). Returns: The attention output tensor, permuted to channel-last format. Shape: `(batch, seq_len, n_q_heads, head_dim)`. \"\"\" with sdpa_kernel ( SDPBackend . FLASH_ATTENTION ): results = F . scaled_dot_product_attention ( query_states , key_states , value_states , attn_mask = attention_mask , dropout_p = 0.0 , is_causal = is_causal , scale = scale , enable_gqa = query_states . shape [ 1 ] != key_states . shape [ 1 ], ) return results . transpose ( 1 , 2 ) . contiguous ()","title":"forward"},{"location":"modules/embedding/","text":"About The d9d.module.block.embedding package provides enhanced embedding layers. Features Currently, this package provides only SplitTokenEmbeddings module. You can use this module: Regular Token Embedding Layer : Specify a single split with global vocab size. For Prompt Tuning : Add additional tokens to your Tokenizer and specify two splits - first one will be original token embeddings, second one will be newly added learnable prompt tokens. Unfreeze only nn.Embedding module that is related to the second split. d9d.module.block.embedding Package providing various embedding layer implementations SplitTokenEmbeddings Bases: Module , ModuleLateInit A token embedding layer composed of multiple named, independent embedding tables. This class maintains a dictionary of embedding layers, mapping contiguous ranges of global vocabulary indices to specific named splits (e.g., 'orig', 'special', 'prompt_prefix'). This is useful for model adaptation strategies where different sets of tokens require different initialization training behaviors. Source code in d9d/module/block/embedding/shard_token_embedding.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 class SplitTokenEmbeddings ( nn . Module , ModuleLateInit ): \"\"\" A token embedding layer composed of multiple named, independent embedding tables. This class maintains a dictionary of embedding layers, mapping contiguous ranges of global vocabulary indices to specific named splits (e.g., 'orig', 'special', 'prompt_prefix'). This is useful for model adaptation strategies where different sets of tokens require different initialization training behaviors. \"\"\" def __init__ ( self , split_vocab_size : dict [ str , int ], split_order : Sequence [ str ], hidden_size : int ): \"\"\" Constructs the SplitTokenEmbeddings object. Args: split_vocab_size: A dictionary mapping split names to their vocabulary sizes. split_order: A sequence defining the order in which splits are concatenated to form the global vocabulary. Keys provided here must exist in split_vocab_size. hidden_size: The dimensionality of the embedding vectors. \"\"\" super () . __init__ () token_embedding = nn . ModuleDict ( { split_name : nn . Embedding ( vocab_size , hidden_size ) for split_name , vocab_size in split_vocab_size . items ()} ) self . token_embedding : Mapping [ str , nn . Embedding ] = cast ( Mapping [ str , nn . Embedding ], token_embedding ) self . _id_start , self . _id_end = _build_token_start_end_indices ( split_vocab_size , split_order ) self . _hidden_size = hidden_size self . _split_order = split_order def forward ( self , input_ids : torch . Tensor ) -> torch . Tensor : \"\"\" Retrieves embeddings for the input indices by routing them to appropriate internal layers. Args: input_ids: Tensor of arbitrary shape containing global vocabulary indices. Returns: Tensor of same shape as input_ids plus a last dimension of hidden_size. \"\"\" output_embeds : torch . Tensor | None = None for split_name in self . _split_order : start_idx = self . _id_start [ split_name ] end_idx = self . _id_end [ split_name ] layer = self . token_embedding [ split_name ] mask = ( input_ids >= start_idx ) & ( input_ids < end_idx ) safe_ids = torch . where ( mask , input_ids - start_idx , 0 ) masked_embed = layer ( safe_ids ) * mask [ ... , None ] if output_embeds is None : output_embeds = masked_embed else : output_embeds = output_embeds + masked_embed if output_embeds is None : raise ValueError ( \"Embeddings are empty - perhaps no splits were configured\" ) return output_embeds def reset_parameters ( self ): \"\"\" Resets parameters for all registered embedding splits. \"\"\" for layer in self . token_embedding . values (): layer . reset_parameters () __init__ ( split_vocab_size , split_order , hidden_size ) Constructs the SplitTokenEmbeddings object. Parameters: Name Type Description Default split_vocab_size dict [ str , int ] A dictionary mapping split names to their vocabulary sizes. required split_order Sequence [ str ] A sequence defining the order in which splits are concatenated to form the global vocabulary. Keys provided here must exist in split_vocab_size. required hidden_size int The dimensionality of the embedding vectors. required Source code in d9d/module/block/embedding/shard_token_embedding.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def __init__ ( self , split_vocab_size : dict [ str , int ], split_order : Sequence [ str ], hidden_size : int ): \"\"\" Constructs the SplitTokenEmbeddings object. Args: split_vocab_size: A dictionary mapping split names to their vocabulary sizes. split_order: A sequence defining the order in which splits are concatenated to form the global vocabulary. Keys provided here must exist in split_vocab_size. hidden_size: The dimensionality of the embedding vectors. \"\"\" super () . __init__ () token_embedding = nn . ModuleDict ( { split_name : nn . Embedding ( vocab_size , hidden_size ) for split_name , vocab_size in split_vocab_size . items ()} ) self . token_embedding : Mapping [ str , nn . Embedding ] = cast ( Mapping [ str , nn . Embedding ], token_embedding ) self . _id_start , self . _id_end = _build_token_start_end_indices ( split_vocab_size , split_order ) self . _hidden_size = hidden_size self . _split_order = split_order forward ( input_ids ) Retrieves embeddings for the input indices by routing them to appropriate internal layers. Parameters: Name Type Description Default input_ids Tensor Tensor of arbitrary shape containing global vocabulary indices. required Returns: Type Description Tensor Tensor of same shape as input_ids plus a last dimension of hidden_size. Source code in d9d/module/block/embedding/shard_token_embedding.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def forward ( self , input_ids : torch . Tensor ) -> torch . Tensor : \"\"\" Retrieves embeddings for the input indices by routing them to appropriate internal layers. Args: input_ids: Tensor of arbitrary shape containing global vocabulary indices. Returns: Tensor of same shape as input_ids plus a last dimension of hidden_size. \"\"\" output_embeds : torch . Tensor | None = None for split_name in self . _split_order : start_idx = self . _id_start [ split_name ] end_idx = self . _id_end [ split_name ] layer = self . token_embedding [ split_name ] mask = ( input_ids >= start_idx ) & ( input_ids < end_idx ) safe_ids = torch . where ( mask , input_ids - start_idx , 0 ) masked_embed = layer ( safe_ids ) * mask [ ... , None ] if output_embeds is None : output_embeds = masked_embed else : output_embeds = output_embeds + masked_embed if output_embeds is None : raise ValueError ( \"Embeddings are empty - perhaps no splits were configured\" ) return output_embeds reset_parameters () Resets parameters for all registered embedding splits. Source code in d9d/module/block/embedding/shard_token_embedding.py 91 92 93 94 95 96 97 def reset_parameters ( self ): \"\"\" Resets parameters for all registered embedding splits. \"\"\" for layer in self . token_embedding . values (): layer . reset_parameters ()","title":"Embeddings"},{"location":"modules/embedding/#about","text":"The d9d.module.block.embedding package provides enhanced embedding layers.","title":"About"},{"location":"modules/embedding/#features","text":"Currently, this package provides only SplitTokenEmbeddings module. You can use this module: Regular Token Embedding Layer : Specify a single split with global vocab size. For Prompt Tuning : Add additional tokens to your Tokenizer and specify two splits - first one will be original token embeddings, second one will be newly added learnable prompt tokens. Unfreeze only nn.Embedding module that is related to the second split.","title":"Features"},{"location":"modules/embedding/#d9d.module.block.embedding","text":"Package providing various embedding layer implementations","title":"embedding"},{"location":"modules/embedding/#d9d.module.block.embedding.SplitTokenEmbeddings","text":"Bases: Module , ModuleLateInit A token embedding layer composed of multiple named, independent embedding tables. This class maintains a dictionary of embedding layers, mapping contiguous ranges of global vocabulary indices to specific named splits (e.g., 'orig', 'special', 'prompt_prefix'). This is useful for model adaptation strategies where different sets of tokens require different initialization training behaviors. Source code in d9d/module/block/embedding/shard_token_embedding.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 class SplitTokenEmbeddings ( nn . Module , ModuleLateInit ): \"\"\" A token embedding layer composed of multiple named, independent embedding tables. This class maintains a dictionary of embedding layers, mapping contiguous ranges of global vocabulary indices to specific named splits (e.g., 'orig', 'special', 'prompt_prefix'). This is useful for model adaptation strategies where different sets of tokens require different initialization training behaviors. \"\"\" def __init__ ( self , split_vocab_size : dict [ str , int ], split_order : Sequence [ str ], hidden_size : int ): \"\"\" Constructs the SplitTokenEmbeddings object. Args: split_vocab_size: A dictionary mapping split names to their vocabulary sizes. split_order: A sequence defining the order in which splits are concatenated to form the global vocabulary. Keys provided here must exist in split_vocab_size. hidden_size: The dimensionality of the embedding vectors. \"\"\" super () . __init__ () token_embedding = nn . ModuleDict ( { split_name : nn . Embedding ( vocab_size , hidden_size ) for split_name , vocab_size in split_vocab_size . items ()} ) self . token_embedding : Mapping [ str , nn . Embedding ] = cast ( Mapping [ str , nn . Embedding ], token_embedding ) self . _id_start , self . _id_end = _build_token_start_end_indices ( split_vocab_size , split_order ) self . _hidden_size = hidden_size self . _split_order = split_order def forward ( self , input_ids : torch . Tensor ) -> torch . Tensor : \"\"\" Retrieves embeddings for the input indices by routing them to appropriate internal layers. Args: input_ids: Tensor of arbitrary shape containing global vocabulary indices. Returns: Tensor of same shape as input_ids plus a last dimension of hidden_size. \"\"\" output_embeds : torch . Tensor | None = None for split_name in self . _split_order : start_idx = self . _id_start [ split_name ] end_idx = self . _id_end [ split_name ] layer = self . token_embedding [ split_name ] mask = ( input_ids >= start_idx ) & ( input_ids < end_idx ) safe_ids = torch . where ( mask , input_ids - start_idx , 0 ) masked_embed = layer ( safe_ids ) * mask [ ... , None ] if output_embeds is None : output_embeds = masked_embed else : output_embeds = output_embeds + masked_embed if output_embeds is None : raise ValueError ( \"Embeddings are empty - perhaps no splits were configured\" ) return output_embeds def reset_parameters ( self ): \"\"\" Resets parameters for all registered embedding splits. \"\"\" for layer in self . token_embedding . values (): layer . reset_parameters ()","title":"SplitTokenEmbeddings"},{"location":"modules/embedding/#d9d.module.block.embedding.SplitTokenEmbeddings.__init__","text":"Constructs the SplitTokenEmbeddings object. Parameters: Name Type Description Default split_vocab_size dict [ str , int ] A dictionary mapping split names to their vocabulary sizes. required split_order Sequence [ str ] A sequence defining the order in which splits are concatenated to form the global vocabulary. Keys provided here must exist in split_vocab_size. required hidden_size int The dimensionality of the embedding vectors. required Source code in d9d/module/block/embedding/shard_token_embedding.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def __init__ ( self , split_vocab_size : dict [ str , int ], split_order : Sequence [ str ], hidden_size : int ): \"\"\" Constructs the SplitTokenEmbeddings object. Args: split_vocab_size: A dictionary mapping split names to their vocabulary sizes. split_order: A sequence defining the order in which splits are concatenated to form the global vocabulary. Keys provided here must exist in split_vocab_size. hidden_size: The dimensionality of the embedding vectors. \"\"\" super () . __init__ () token_embedding = nn . ModuleDict ( { split_name : nn . Embedding ( vocab_size , hidden_size ) for split_name , vocab_size in split_vocab_size . items ()} ) self . token_embedding : Mapping [ str , nn . Embedding ] = cast ( Mapping [ str , nn . Embedding ], token_embedding ) self . _id_start , self . _id_end = _build_token_start_end_indices ( split_vocab_size , split_order ) self . _hidden_size = hidden_size self . _split_order = split_order","title":"__init__"},{"location":"modules/embedding/#d9d.module.block.embedding.SplitTokenEmbeddings.forward","text":"Retrieves embeddings for the input indices by routing them to appropriate internal layers. Parameters: Name Type Description Default input_ids Tensor Tensor of arbitrary shape containing global vocabulary indices. required Returns: Type Description Tensor Tensor of same shape as input_ids plus a last dimension of hidden_size. Source code in d9d/module/block/embedding/shard_token_embedding.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def forward ( self , input_ids : torch . Tensor ) -> torch . Tensor : \"\"\" Retrieves embeddings for the input indices by routing them to appropriate internal layers. Args: input_ids: Tensor of arbitrary shape containing global vocabulary indices. Returns: Tensor of same shape as input_ids plus a last dimension of hidden_size. \"\"\" output_embeds : torch . Tensor | None = None for split_name in self . _split_order : start_idx = self . _id_start [ split_name ] end_idx = self . _id_end [ split_name ] layer = self . token_embedding [ split_name ] mask = ( input_ids >= start_idx ) & ( input_ids < end_idx ) safe_ids = torch . where ( mask , input_ids - start_idx , 0 ) masked_embed = layer ( safe_ids ) * mask [ ... , None ] if output_embeds is None : output_embeds = masked_embed else : output_embeds = output_embeds + masked_embed if output_embeds is None : raise ValueError ( \"Embeddings are empty - perhaps no splits were configured\" ) return output_embeds","title":"forward"},{"location":"modules/embedding/#d9d.module.block.embedding.SplitTokenEmbeddings.reset_parameters","text":"Resets parameters for all registered embedding splits. Source code in d9d/module/block/embedding/shard_token_embedding.py 91 92 93 94 95 96 97 def reset_parameters ( self ): \"\"\" Resets parameters for all registered embedding splits. \"\"\" for layer in self . token_embedding . values (): layer . reset_parameters ()","title":"reset_parameters"},{"location":"modules/ffn/","text":"About The d9d.module.block.ffn package implements standard dense Feed-Forward networks used in Transformer blocks. Features SwiGLU SwiGLU is a SwiGLU layer . Uses efficient SiLU-Mul kernel. Kernel Benchmarks (BF16, H100) d9d.module.block.ffn SwiGLU Bases: Module , ModuleLateInit Implements the SwiGLU Feed-Forward Network (FFN). This module applies the gated activation function: down(SiLU(gate(x)) * up(x)) . It corresponds to the standard MLP block used in architectures like LLaMA. Source code in d9d/module/block/ffn/swiglu.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 class SwiGLU ( nn . Module , ModuleLateInit ): \"\"\" Implements the SwiGLU Feed-Forward Network (FFN). This module applies the gated activation function: `down(SiLU(gate(x)) * up(x))`. It corresponds to the standard MLP block used in architectures like LLaMA. \"\"\" def __init__ ( self , hidden_size : int , intermediate_size : int ): \"\"\" Constructs a SwiGLU object. Args: hidden_size: The hidden dim size. intermediate_size: The intermediate dim size of the FFN. \"\"\" super () . __init__ () self . gate_proj = nn . Linear ( hidden_size , intermediate_size ) self . up_proj = nn . Linear ( hidden_size , intermediate_size ) self . down_proj = nn . Linear ( intermediate_size , hidden_size ) def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Applies the SwiGLU FFN to the input. Args: x: Input tensor. Shape: `(batch_size, seq_len, hidden_dim)`. Returns: Output tensor. Shape: `(batch_size, seq_len, hidden_dim)`. \"\"\" return self . down_proj ( silu_mul ( self . gate_proj ( x ), self . up_proj ( x ))) def reset_parameters ( self ): \"\"\"Resets module parameters.\"\"\" self . gate_proj . reset_parameters () self . up_proj . reset_parameters () self . down_proj . reset_parameters () __init__ ( hidden_size , intermediate_size ) Constructs a SwiGLU object. Parameters: Name Type Description Default hidden_size int The hidden dim size. required intermediate_size int The intermediate dim size of the FFN. required Source code in d9d/module/block/ffn/swiglu.py 16 17 18 19 20 21 22 23 24 25 26 27 28 def __init__ ( self , hidden_size : int , intermediate_size : int ): \"\"\" Constructs a SwiGLU object. Args: hidden_size: The hidden dim size. intermediate_size: The intermediate dim size of the FFN. \"\"\" super () . __init__ () self . gate_proj = nn . Linear ( hidden_size , intermediate_size ) self . up_proj = nn . Linear ( hidden_size , intermediate_size ) self . down_proj = nn . Linear ( intermediate_size , hidden_size ) forward ( x ) Applies the SwiGLU FFN to the input. Parameters: Name Type Description Default x Tensor Input tensor. Shape: (batch_size, seq_len, hidden_dim) . required Returns: Type Description Tensor Output tensor. Shape: (batch_size, seq_len, hidden_dim) . Source code in d9d/module/block/ffn/swiglu.py 30 31 32 33 34 35 36 37 38 39 40 41 def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Applies the SwiGLU FFN to the input. Args: x: Input tensor. Shape: `(batch_size, seq_len, hidden_dim)`. Returns: Output tensor. Shape: `(batch_size, seq_len, hidden_dim)`. \"\"\" return self . down_proj ( silu_mul ( self . gate_proj ( x ), self . up_proj ( x ))) reset_parameters () Resets module parameters. Source code in d9d/module/block/ffn/swiglu.py 43 44 45 46 47 48 def reset_parameters ( self ): \"\"\"Resets module parameters.\"\"\" self . gate_proj . reset_parameters () self . up_proj . reset_parameters () self . down_proj . reset_parameters ()","title":"Feed Forward Networks (FFN)"},{"location":"modules/ffn/#about","text":"The d9d.module.block.ffn package implements standard dense Feed-Forward networks used in Transformer blocks.","title":"About"},{"location":"modules/ffn/#features","text":"","title":"Features"},{"location":"modules/ffn/#swiglu","text":"SwiGLU is a SwiGLU layer . Uses efficient SiLU-Mul kernel.","title":"SwiGLU"},{"location":"modules/ffn/#kernel-benchmarks-bf16-h100","text":"","title":"Kernel Benchmarks (BF16, H100)"},{"location":"modules/ffn/#d9d.module.block.ffn","text":"","title":"ffn"},{"location":"modules/ffn/#d9d.module.block.ffn.SwiGLU","text":"Bases: Module , ModuleLateInit Implements the SwiGLU Feed-Forward Network (FFN). This module applies the gated activation function: down(SiLU(gate(x)) * up(x)) . It corresponds to the standard MLP block used in architectures like LLaMA. Source code in d9d/module/block/ffn/swiglu.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 class SwiGLU ( nn . Module , ModuleLateInit ): \"\"\" Implements the SwiGLU Feed-Forward Network (FFN). This module applies the gated activation function: `down(SiLU(gate(x)) * up(x))`. It corresponds to the standard MLP block used in architectures like LLaMA. \"\"\" def __init__ ( self , hidden_size : int , intermediate_size : int ): \"\"\" Constructs a SwiGLU object. Args: hidden_size: The hidden dim size. intermediate_size: The intermediate dim size of the FFN. \"\"\" super () . __init__ () self . gate_proj = nn . Linear ( hidden_size , intermediate_size ) self . up_proj = nn . Linear ( hidden_size , intermediate_size ) self . down_proj = nn . Linear ( intermediate_size , hidden_size ) def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Applies the SwiGLU FFN to the input. Args: x: Input tensor. Shape: `(batch_size, seq_len, hidden_dim)`. Returns: Output tensor. Shape: `(batch_size, seq_len, hidden_dim)`. \"\"\" return self . down_proj ( silu_mul ( self . gate_proj ( x ), self . up_proj ( x ))) def reset_parameters ( self ): \"\"\"Resets module parameters.\"\"\" self . gate_proj . reset_parameters () self . up_proj . reset_parameters () self . down_proj . reset_parameters ()","title":"SwiGLU"},{"location":"modules/ffn/#d9d.module.block.ffn.SwiGLU.__init__","text":"Constructs a SwiGLU object. Parameters: Name Type Description Default hidden_size int The hidden dim size. required intermediate_size int The intermediate dim size of the FFN. required Source code in d9d/module/block/ffn/swiglu.py 16 17 18 19 20 21 22 23 24 25 26 27 28 def __init__ ( self , hidden_size : int , intermediate_size : int ): \"\"\" Constructs a SwiGLU object. Args: hidden_size: The hidden dim size. intermediate_size: The intermediate dim size of the FFN. \"\"\" super () . __init__ () self . gate_proj = nn . Linear ( hidden_size , intermediate_size ) self . up_proj = nn . Linear ( hidden_size , intermediate_size ) self . down_proj = nn . Linear ( intermediate_size , hidden_size )","title":"__init__"},{"location":"modules/ffn/#d9d.module.block.ffn.SwiGLU.forward","text":"Applies the SwiGLU FFN to the input. Parameters: Name Type Description Default x Tensor Input tensor. Shape: (batch_size, seq_len, hidden_dim) . required Returns: Type Description Tensor Output tensor. Shape: (batch_size, seq_len, hidden_dim) . Source code in d9d/module/block/ffn/swiglu.py 30 31 32 33 34 35 36 37 38 39 40 41 def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Applies the SwiGLU FFN to the input. Args: x: Input tensor. Shape: `(batch_size, seq_len, hidden_dim)`. Returns: Output tensor. Shape: `(batch_size, seq_len, hidden_dim)`. \"\"\" return self . down_proj ( silu_mul ( self . gate_proj ( x ), self . up_proj ( x )))","title":"forward"},{"location":"modules/ffn/#d9d.module.block.ffn.SwiGLU.reset_parameters","text":"Resets module parameters. Source code in d9d/module/block/ffn/swiglu.py 43 44 45 46 47 48 def reset_parameters ( self ): \"\"\"Resets module parameters.\"\"\" self . gate_proj . reset_parameters () self . up_proj . reset_parameters () self . down_proj . reset_parameters ()","title":"reset_parameters"},{"location":"modules/head/","text":"About The d9d.module.block.head package handles the model heads. Features Causal Language Modelling SplitLanguageModellingHead provides a causal language modelling head that computes per-token logprobs. It uses efficient fused Linear-Cross-Entropy kernel from the Cut-Cross-Entropy project and avoids full logit tensor materialization. Supports vocab split to multiple independent splits following the SplitTokenEmbeddings embedding implementation. d9d.module.block.head LM_IGNORE_INDEX = - 100 module-attribute Index ignored by LM head while calculating logps ClassificationHead Bases: Module , ModuleLateInit A classification head module that is typically used on top of model hidden states. It applies dropout followed by a linear projection to produce logits for a specified number of classes. It supports optional pooling via a mask, allowing for selection of specific tokens (e.g., [CLS] tokens or specific sequence positions) before projection. Source code in d9d/module/block/head/classification.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 class ClassificationHead ( nn . Module , ModuleLateInit ): \"\"\" A classification head module that is typically used on top of model hidden states. It applies dropout followed by a linear projection to produce logits for a specified number of classes. It supports optional pooling via a mask, allowing for selection of specific tokens (e.g., [CLS] tokens or specific sequence positions) before projection. \"\"\" def __init__ ( self , hidden_size : int , num_labels : int , dropout : float ): \"\"\" Constructs the ClassificationHead object. Args: hidden_size: The input dimensionality (hidden state size). num_labels: The number of output classes. dropout: The dropout probability. \"\"\" super () . __init__ () self . dropout = nn . Dropout ( dropout ) self . score = nn . Linear ( hidden_size , num_labels , bias = False ) def forward ( self , hidden_states : torch . Tensor , pooling_mask : torch . Tensor | None ) -> torch . Tensor : \"\"\" Computes class logits from hidden states. Args: hidden_states: Input tensor of hidden states. pooling_mask: Optional mask to select specific hidden states. If provided, the input is indexed as `hidden_states[pooling_mask == 1]`, flattening the batch and sequence dimensions into a single dimension of selected tokens. Returns: A tensor containing the unnormalized logits. \"\"\" if pooling_mask is not None : hidden_states = hidden_states [ pooling_mask == 1 ] logits = self . score ( self . dropout ( hidden_states )) logits = logits . float () # force convert to FP32 to make sure loss is calculated properly return logits def reset_parameters ( self ): \"\"\"Resets module parameters.\"\"\" self . score . reset_parameters () __init__ ( hidden_size , num_labels , dropout ) Constructs the ClassificationHead object. Parameters: Name Type Description Default hidden_size int The input dimensionality (hidden state size). required num_labels int The number of output classes. required dropout float The dropout probability. required Source code in d9d/module/block/head/classification.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def __init__ ( self , hidden_size : int , num_labels : int , dropout : float ): \"\"\" Constructs the ClassificationHead object. Args: hidden_size: The input dimensionality (hidden state size). num_labels: The number of output classes. dropout: The dropout probability. \"\"\" super () . __init__ () self . dropout = nn . Dropout ( dropout ) self . score = nn . Linear ( hidden_size , num_labels , bias = False ) forward ( hidden_states , pooling_mask ) Computes class logits from hidden states. Parameters: Name Type Description Default hidden_states Tensor Input tensor of hidden states. required pooling_mask Tensor | None Optional mask to select specific hidden states. If provided, the input is indexed as hidden_states[pooling_mask == 1] , flattening the batch and sequence dimensions into a single dimension of selected tokens. required Returns: Type Description Tensor A tensor containing the unnormalized logits. Source code in d9d/module/block/head/classification.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def forward ( self , hidden_states : torch . Tensor , pooling_mask : torch . Tensor | None ) -> torch . Tensor : \"\"\" Computes class logits from hidden states. Args: hidden_states: Input tensor of hidden states. pooling_mask: Optional mask to select specific hidden states. If provided, the input is indexed as `hidden_states[pooling_mask == 1]`, flattening the batch and sequence dimensions into a single dimension of selected tokens. Returns: A tensor containing the unnormalized logits. \"\"\" if pooling_mask is not None : hidden_states = hidden_states [ pooling_mask == 1 ] logits = self . score ( self . dropout ( hidden_states )) logits = logits . float () # force convert to FP32 to make sure loss is calculated properly return logits reset_parameters () Resets module parameters. Source code in d9d/module/block/head/classification.py 53 54 55 def reset_parameters ( self ): \"\"\"Resets module parameters.\"\"\" self . score . reset_parameters () SplitLanguageModellingHead Bases: Module , ModuleLateInit A segmented language modeling head that computes per-token cross-entropy loss values using a composed weight matrix. This class maintains separate linear layers for different segments of the vocabulary (e.g., regular vs. special tokens). During the forward pass, it concatenates the weights to form a unified projection matrix and computes the cross-entropy loss efficiently, typically using a fused kernel to avoid materializing full logits. The concatenation order of the weights is determined by split_order , which ensures consistency with the global vocabulary indices. Source code in d9d/module/block/head/language_modelling.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 class SplitLanguageModellingHead ( nn . Module , ModuleLateInit ): \"\"\" A segmented language modeling head that computes per-token cross-entropy loss values using a composed weight matrix. This class maintains separate linear layers for different segments of the vocabulary (e.g., regular vs. special tokens). During the forward pass, it concatenates the weights to form a unified projection matrix and computes the cross-entropy loss efficiently, typically using a fused kernel to avoid materializing full logits. The concatenation order of the weights is determined by `split_order`, which ensures consistency with the global vocabulary indices. \"\"\" def __init__ ( self , split_vocab_size : dict [ str , int ], split_order : Sequence [ str ], hidden_size : int ): \"\"\" Constructs the SplitLanguageModellingHead object. Args: split_vocab_size: A dictionary mapping split names to their output vocabulary sizes. split_order: A sequence defining the order in which vocabulary segments should be concatenated. This determines the mapping of global indices to specific heads. hidden_size: The input dimensionality (hidden state size). \"\"\" super () . __init__ () lm_head = nn . ModuleDict ( { split_name : nn . Linear ( hidden_size , vocab_size , bias = False ) for split_name , vocab_size in split_vocab_size . items () } ) self . lm_head : Mapping [ str , nn . Linear ] = cast ( Mapping [ str , nn . Linear ], lm_head ) self . _split_order = split_order self . _hidden_size = hidden_size def forward ( self , hidden_states : torch . Tensor , labels : torch . Tensor ) -> torch . Tensor : \"\"\" Computes the cross-entropy loss for the given hidden states and labels. Args: hidden_states: Input tensor of shape `(B, S, H)`. labels: Target label tensor of shape `(B, S)`. Indices must correspond to the global vocabulary formed by concatenating splits in `split_order`. Returns: A tensor containing per-token loss values (reduction='none'), matching the shape of the labels tensor. \"\"\" lm_head_weight = torch . cat ([ self . lm_head [ split_name ] . weight for split_name in self . _split_order ], dim = 0 ) losses = linear_cross_entropy ( hidden_states , lm_head_weight , labels , ignore_index = LM_IGNORE_INDEX , reduction = \"none\" ) return losses def reset_parameters ( self ): \"\"\"Resets module parameters.\"\"\" for head in self . lm_head . values (): head . reset_parameters () __init__ ( split_vocab_size , split_order , hidden_size ) Constructs the SplitLanguageModellingHead object. Parameters: Name Type Description Default split_vocab_size dict [ str , int ] A dictionary mapping split names to their output vocabulary sizes. required split_order Sequence [ str ] A sequence defining the order in which vocabulary segments should be concatenated. This determines the mapping of global indices to specific heads. required hidden_size int The input dimensionality (hidden state size). required Source code in d9d/module/block/head/language_modelling.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def __init__ ( self , split_vocab_size : dict [ str , int ], split_order : Sequence [ str ], hidden_size : int ): \"\"\" Constructs the SplitLanguageModellingHead object. Args: split_vocab_size: A dictionary mapping split names to their output vocabulary sizes. split_order: A sequence defining the order in which vocabulary segments should be concatenated. This determines the mapping of global indices to specific heads. hidden_size: The input dimensionality (hidden state size). \"\"\" super () . __init__ () lm_head = nn . ModuleDict ( { split_name : nn . Linear ( hidden_size , vocab_size , bias = False ) for split_name , vocab_size in split_vocab_size . items () } ) self . lm_head : Mapping [ str , nn . Linear ] = cast ( Mapping [ str , nn . Linear ], lm_head ) self . _split_order = split_order self . _hidden_size = hidden_size forward ( hidden_states , labels ) Computes the cross-entropy loss for the given hidden states and labels. Parameters: Name Type Description Default hidden_states Tensor Input tensor of shape (B, S, H) . required labels Tensor Target label tensor of shape (B, S) . Indices must correspond to the global vocabulary formed by concatenating splits in split_order . required Returns: Type Description Tensor A tensor containing per-token loss values (reduction='none'), matching the Tensor shape of the labels tensor. Source code in d9d/module/block/head/language_modelling.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def forward ( self , hidden_states : torch . Tensor , labels : torch . Tensor ) -> torch . Tensor : \"\"\" Computes the cross-entropy loss for the given hidden states and labels. Args: hidden_states: Input tensor of shape `(B, S, H)`. labels: Target label tensor of shape `(B, S)`. Indices must correspond to the global vocabulary formed by concatenating splits in `split_order`. Returns: A tensor containing per-token loss values (reduction='none'), matching the shape of the labels tensor. \"\"\" lm_head_weight = torch . cat ([ self . lm_head [ split_name ] . weight for split_name in self . _split_order ], dim = 0 ) losses = linear_cross_entropy ( hidden_states , lm_head_weight , labels , ignore_index = LM_IGNORE_INDEX , reduction = \"none\" ) return losses reset_parameters () Resets module parameters. Source code in d9d/module/block/head/language_modelling.py 72 73 74 75 76 def reset_parameters ( self ): \"\"\"Resets module parameters.\"\"\" for head in self . lm_head . values (): head . reset_parameters ()","title":"Model Heads"},{"location":"modules/head/#about","text":"The d9d.module.block.head package handles the model heads.","title":"About"},{"location":"modules/head/#features","text":"","title":"Features"},{"location":"modules/head/#causal-language-modelling","text":"SplitLanguageModellingHead provides a causal language modelling head that computes per-token logprobs. It uses efficient fused Linear-Cross-Entropy kernel from the Cut-Cross-Entropy project and avoids full logit tensor materialization. Supports vocab split to multiple independent splits following the SplitTokenEmbeddings embedding implementation.","title":"Causal Language Modelling"},{"location":"modules/head/#d9d.module.block.head","text":"","title":"head"},{"location":"modules/head/#d9d.module.block.head.LM_IGNORE_INDEX","text":"Index ignored by LM head while calculating logps","title":"LM_IGNORE_INDEX"},{"location":"modules/head/#d9d.module.block.head.ClassificationHead","text":"Bases: Module , ModuleLateInit A classification head module that is typically used on top of model hidden states. It applies dropout followed by a linear projection to produce logits for a specified number of classes. It supports optional pooling via a mask, allowing for selection of specific tokens (e.g., [CLS] tokens or specific sequence positions) before projection. Source code in d9d/module/block/head/classification.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 class ClassificationHead ( nn . Module , ModuleLateInit ): \"\"\" A classification head module that is typically used on top of model hidden states. It applies dropout followed by a linear projection to produce logits for a specified number of classes. It supports optional pooling via a mask, allowing for selection of specific tokens (e.g., [CLS] tokens or specific sequence positions) before projection. \"\"\" def __init__ ( self , hidden_size : int , num_labels : int , dropout : float ): \"\"\" Constructs the ClassificationHead object. Args: hidden_size: The input dimensionality (hidden state size). num_labels: The number of output classes. dropout: The dropout probability. \"\"\" super () . __init__ () self . dropout = nn . Dropout ( dropout ) self . score = nn . Linear ( hidden_size , num_labels , bias = False ) def forward ( self , hidden_states : torch . Tensor , pooling_mask : torch . Tensor | None ) -> torch . Tensor : \"\"\" Computes class logits from hidden states. Args: hidden_states: Input tensor of hidden states. pooling_mask: Optional mask to select specific hidden states. If provided, the input is indexed as `hidden_states[pooling_mask == 1]`, flattening the batch and sequence dimensions into a single dimension of selected tokens. Returns: A tensor containing the unnormalized logits. \"\"\" if pooling_mask is not None : hidden_states = hidden_states [ pooling_mask == 1 ] logits = self . score ( self . dropout ( hidden_states )) logits = logits . float () # force convert to FP32 to make sure loss is calculated properly return logits def reset_parameters ( self ): \"\"\"Resets module parameters.\"\"\" self . score . reset_parameters ()","title":"ClassificationHead"},{"location":"modules/head/#d9d.module.block.head.ClassificationHead.__init__","text":"Constructs the ClassificationHead object. Parameters: Name Type Description Default hidden_size int The input dimensionality (hidden state size). required num_labels int The number of output classes. required dropout float The dropout probability. required Source code in d9d/module/block/head/classification.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def __init__ ( self , hidden_size : int , num_labels : int , dropout : float ): \"\"\" Constructs the ClassificationHead object. Args: hidden_size: The input dimensionality (hidden state size). num_labels: The number of output classes. dropout: The dropout probability. \"\"\" super () . __init__ () self . dropout = nn . Dropout ( dropout ) self . score = nn . Linear ( hidden_size , num_labels , bias = False )","title":"__init__"},{"location":"modules/head/#d9d.module.block.head.ClassificationHead.forward","text":"Computes class logits from hidden states. Parameters: Name Type Description Default hidden_states Tensor Input tensor of hidden states. required pooling_mask Tensor | None Optional mask to select specific hidden states. If provided, the input is indexed as hidden_states[pooling_mask == 1] , flattening the batch and sequence dimensions into a single dimension of selected tokens. required Returns: Type Description Tensor A tensor containing the unnormalized logits. Source code in d9d/module/block/head/classification.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def forward ( self , hidden_states : torch . Tensor , pooling_mask : torch . Tensor | None ) -> torch . Tensor : \"\"\" Computes class logits from hidden states. Args: hidden_states: Input tensor of hidden states. pooling_mask: Optional mask to select specific hidden states. If provided, the input is indexed as `hidden_states[pooling_mask == 1]`, flattening the batch and sequence dimensions into a single dimension of selected tokens. Returns: A tensor containing the unnormalized logits. \"\"\" if pooling_mask is not None : hidden_states = hidden_states [ pooling_mask == 1 ] logits = self . score ( self . dropout ( hidden_states )) logits = logits . float () # force convert to FP32 to make sure loss is calculated properly return logits","title":"forward"},{"location":"modules/head/#d9d.module.block.head.ClassificationHead.reset_parameters","text":"Resets module parameters. Source code in d9d/module/block/head/classification.py 53 54 55 def reset_parameters ( self ): \"\"\"Resets module parameters.\"\"\" self . score . reset_parameters ()","title":"reset_parameters"},{"location":"modules/head/#d9d.module.block.head.SplitLanguageModellingHead","text":"Bases: Module , ModuleLateInit A segmented language modeling head that computes per-token cross-entropy loss values using a composed weight matrix. This class maintains separate linear layers for different segments of the vocabulary (e.g., regular vs. special tokens). During the forward pass, it concatenates the weights to form a unified projection matrix and computes the cross-entropy loss efficiently, typically using a fused kernel to avoid materializing full logits. The concatenation order of the weights is determined by split_order , which ensures consistency with the global vocabulary indices. Source code in d9d/module/block/head/language_modelling.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 class SplitLanguageModellingHead ( nn . Module , ModuleLateInit ): \"\"\" A segmented language modeling head that computes per-token cross-entropy loss values using a composed weight matrix. This class maintains separate linear layers for different segments of the vocabulary (e.g., regular vs. special tokens). During the forward pass, it concatenates the weights to form a unified projection matrix and computes the cross-entropy loss efficiently, typically using a fused kernel to avoid materializing full logits. The concatenation order of the weights is determined by `split_order`, which ensures consistency with the global vocabulary indices. \"\"\" def __init__ ( self , split_vocab_size : dict [ str , int ], split_order : Sequence [ str ], hidden_size : int ): \"\"\" Constructs the SplitLanguageModellingHead object. Args: split_vocab_size: A dictionary mapping split names to their output vocabulary sizes. split_order: A sequence defining the order in which vocabulary segments should be concatenated. This determines the mapping of global indices to specific heads. hidden_size: The input dimensionality (hidden state size). \"\"\" super () . __init__ () lm_head = nn . ModuleDict ( { split_name : nn . Linear ( hidden_size , vocab_size , bias = False ) for split_name , vocab_size in split_vocab_size . items () } ) self . lm_head : Mapping [ str , nn . Linear ] = cast ( Mapping [ str , nn . Linear ], lm_head ) self . _split_order = split_order self . _hidden_size = hidden_size def forward ( self , hidden_states : torch . Tensor , labels : torch . Tensor ) -> torch . Tensor : \"\"\" Computes the cross-entropy loss for the given hidden states and labels. Args: hidden_states: Input tensor of shape `(B, S, H)`. labels: Target label tensor of shape `(B, S)`. Indices must correspond to the global vocabulary formed by concatenating splits in `split_order`. Returns: A tensor containing per-token loss values (reduction='none'), matching the shape of the labels tensor. \"\"\" lm_head_weight = torch . cat ([ self . lm_head [ split_name ] . weight for split_name in self . _split_order ], dim = 0 ) losses = linear_cross_entropy ( hidden_states , lm_head_weight , labels , ignore_index = LM_IGNORE_INDEX , reduction = \"none\" ) return losses def reset_parameters ( self ): \"\"\"Resets module parameters.\"\"\" for head in self . lm_head . values (): head . reset_parameters ()","title":"SplitLanguageModellingHead"},{"location":"modules/head/#d9d.module.block.head.SplitLanguageModellingHead.__init__","text":"Constructs the SplitLanguageModellingHead object. Parameters: Name Type Description Default split_vocab_size dict [ str , int ] A dictionary mapping split names to their output vocabulary sizes. required split_order Sequence [ str ] A sequence defining the order in which vocabulary segments should be concatenated. This determines the mapping of global indices to specific heads. required hidden_size int The input dimensionality (hidden state size). required Source code in d9d/module/block/head/language_modelling.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def __init__ ( self , split_vocab_size : dict [ str , int ], split_order : Sequence [ str ], hidden_size : int ): \"\"\" Constructs the SplitLanguageModellingHead object. Args: split_vocab_size: A dictionary mapping split names to their output vocabulary sizes. split_order: A sequence defining the order in which vocabulary segments should be concatenated. This determines the mapping of global indices to specific heads. hidden_size: The input dimensionality (hidden state size). \"\"\" super () . __init__ () lm_head = nn . ModuleDict ( { split_name : nn . Linear ( hidden_size , vocab_size , bias = False ) for split_name , vocab_size in split_vocab_size . items () } ) self . lm_head : Mapping [ str , nn . Linear ] = cast ( Mapping [ str , nn . Linear ], lm_head ) self . _split_order = split_order self . _hidden_size = hidden_size","title":"__init__"},{"location":"modules/head/#d9d.module.block.head.SplitLanguageModellingHead.forward","text":"Computes the cross-entropy loss for the given hidden states and labels. Parameters: Name Type Description Default hidden_states Tensor Input tensor of shape (B, S, H) . required labels Tensor Target label tensor of shape (B, S) . Indices must correspond to the global vocabulary formed by concatenating splits in split_order . required Returns: Type Description Tensor A tensor containing per-token loss values (reduction='none'), matching the Tensor shape of the labels tensor. Source code in d9d/module/block/head/language_modelling.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def forward ( self , hidden_states : torch . Tensor , labels : torch . Tensor ) -> torch . Tensor : \"\"\" Computes the cross-entropy loss for the given hidden states and labels. Args: hidden_states: Input tensor of shape `(B, S, H)`. labels: Target label tensor of shape `(B, S)`. Indices must correspond to the global vocabulary formed by concatenating splits in `split_order`. Returns: A tensor containing per-token loss values (reduction='none'), matching the shape of the labels tensor. \"\"\" lm_head_weight = torch . cat ([ self . lm_head [ split_name ] . weight for split_name in self . _split_order ], dim = 0 ) losses = linear_cross_entropy ( hidden_states , lm_head_weight , labels , ignore_index = LM_IGNORE_INDEX , reduction = \"none\" ) return losses","title":"forward"},{"location":"modules/head/#d9d.module.block.head.SplitLanguageModellingHead.reset_parameters","text":"Resets module parameters. Source code in d9d/module/block/head/language_modelling.py 72 73 74 75 76 def reset_parameters ( self ): \"\"\"Resets module parameters.\"\"\" for head in self . lm_head . values (): head . reset_parameters ()","title":"reset_parameters"},{"location":"modules/hidden_states_aggregator/","text":"About The d9d.module.block.hidden_states_aggregator package provides interfaces and implementations for collecting, reducing, and managing model hidden states during execution. This is particularly useful in pipelines where intermediate activations need to be analyzed or stored (e.g., for reward modeling, custom distillation objectives, or analysis) without keeping the entire raw tensor history in memory. As an end user, you typically will instantiate an aggregator with a factory method create_hidden_states_aggregator . Aggregators support a pack_with_snapshot mechanism. This allows combining currently collected states with a pre-existing \"snapshot\" tensor (historical data or from previous pipeline stages), facilitating state management in stateful or iterative loops. Modes HiddenStatesAggregationMode.noop Acts as a \"null\"-aggregator. HiddenStatesAggregationMode.mean The Mean mode ( HiddenStatesAggregationMode.mean ) performs \"eager\" reduction. Instead of storing the full [Batch, Seq_Len, Hidden_Dim] tensors for every step, it: Takes an aggregation mask. Computes the masked average immediately upon receiving the hidden states. Stores only the reduced [Batch, Hidden_Dim] vectors. This significantly reduces memory footprint when accumulating states over many iterations. d9d.module.block.hidden_states_aggregator Aggregation utilities for model hidden states. BaseHiddenStatesAggregator Bases: ABC Abstract base class for hidden states aggregation strategies. This interface defines how hidden states should be collected (added) and how they should be finalized (packed) combined with optional historical snapshots. Source code in d9d/module/block/hidden_states_aggregator/base.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class BaseHiddenStatesAggregator ( abc . ABC ): \"\"\"Abstract base class for hidden states aggregation strategies. This interface defines how hidden states should be collected (added) and how they should be finalized (packed) combined with optional historical snapshots. \"\"\" @abc . abstractmethod def add_hidden_states ( self , hidden_states : torch . Tensor ) -> None : \"\"\"Accumulates a batch of hidden states into the aggregator. Args: hidden_states: The tensor containing the hidden states to process. \"\"\" @abc . abstractmethod def pack_with_snapshot ( self , snapshot : torch . Tensor | None ) -> torch . Tensor | None : \"\"\"Finalizes the aggregation and combines it with an optional previous snapshot. This method typically retrieves the accumulated states, processes them (if not done during addition), and concatenates them with the snapshot. Args: snapshot: An optional tensor representing previously aggregated states to be prepended to the current collection. Returns: The combined result of the snapshot and the newly aggregated states, or None if no states were collected. \"\"\" add_hidden_states ( hidden_states ) abstractmethod Accumulates a batch of hidden states into the aggregator. Parameters: Name Type Description Default hidden_states Tensor The tensor containing the hidden states to process. required Source code in d9d/module/block/hidden_states_aggregator/base.py 13 14 15 16 17 18 19 @abc . abstractmethod def add_hidden_states ( self , hidden_states : torch . Tensor ) -> None : \"\"\"Accumulates a batch of hidden states into the aggregator. Args: hidden_states: The tensor containing the hidden states to process. \"\"\" pack_with_snapshot ( snapshot ) abstractmethod Finalizes the aggregation and combines it with an optional previous snapshot. This method typically retrieves the accumulated states, processes them (if not done during addition), and concatenates them with the snapshot. Parameters: Name Type Description Default snapshot Tensor | None An optional tensor representing previously aggregated states to be prepended to the current collection. required Returns: Type Description Tensor | None The combined result of the snapshot and the newly aggregated states, Tensor | None or None if no states were collected. Source code in d9d/module/block/hidden_states_aggregator/base.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 @abc . abstractmethod def pack_with_snapshot ( self , snapshot : torch . Tensor | None ) -> torch . Tensor | None : \"\"\"Finalizes the aggregation and combines it with an optional previous snapshot. This method typically retrieves the accumulated states, processes them (if not done during addition), and concatenates them with the snapshot. Args: snapshot: An optional tensor representing previously aggregated states to be prepended to the current collection. Returns: The combined result of the snapshot and the newly aggregated states, or None if no states were collected. \"\"\" HiddenStatesAggregationMode Bases: StrEnum Enumeration of available hidden state aggregation strategies. Attributes: Name Type Description no Performs no aggregation (No-Op). mean Computes the mean of hidden states, taking a mask into account. Source code in d9d/module/block/hidden_states_aggregator/factory.py 10 11 12 13 14 15 16 17 18 19 class HiddenStatesAggregationMode ( StrEnum ): \"\"\"Enumeration of available hidden state aggregation strategies. Attributes: no: Performs no aggregation (No-Op). mean: Computes the mean of hidden states, taking a mask into account. \"\"\" no = \"no\" mean = \"mean\" create_hidden_states_aggregator ( mode , agg_mask ) Factory function to create a hidden states aggregator. Parameters: Name Type Description Default mode HiddenStatesAggregationMode The specific aggregation mode to instantiate. required agg_mask Tensor | None A tensor mask required for specific modes. Can be None if the selected mode does not require masking. required Returns: Type Description BaseHiddenStatesAggregator An instance of a concrete BaseHiddenStatesAggregator subclass. Raises: Type Description ValueError If 'mean' mode is selected but 'agg_mask' is None, or if an unknown mode is provided. Source code in d9d/module/block/hidden_states_aggregator/factory.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def create_hidden_states_aggregator ( mode : HiddenStatesAggregationMode , agg_mask : torch . Tensor | None ) -> BaseHiddenStatesAggregator : \"\"\"Factory function to create a hidden states aggregator. Args: mode: The specific aggregation mode to instantiate. agg_mask: A tensor mask required for specific modes. Can be None if the selected mode does not require masking. Returns: An instance of a concrete BaseHiddenStatesAggregator subclass. Raises: ValueError: If 'mean' mode is selected but 'agg_mask' is None, or if an unknown mode is provided. \"\"\" match mode : case HiddenStatesAggregationMode . no : return HiddenStatesAggregatorNoOp () case HiddenStatesAggregationMode . mean : if agg_mask is None : raise ValueError ( \"You have to specify aggregation mask\" ) return HiddenStatesAggregatorMean ( agg_mask ) case _ : raise ValueError ( \"Unknown hidden states aggregation mode\" )","title":"Hidden States Aggregation"},{"location":"modules/hidden_states_aggregator/#about","text":"The d9d.module.block.hidden_states_aggregator package provides interfaces and implementations for collecting, reducing, and managing model hidden states during execution. This is particularly useful in pipelines where intermediate activations need to be analyzed or stored (e.g., for reward modeling, custom distillation objectives, or analysis) without keeping the entire raw tensor history in memory. As an end user, you typically will instantiate an aggregator with a factory method create_hidden_states_aggregator . Aggregators support a pack_with_snapshot mechanism. This allows combining currently collected states with a pre-existing \"snapshot\" tensor (historical data or from previous pipeline stages), facilitating state management in stateful or iterative loops.","title":"About"},{"location":"modules/hidden_states_aggregator/#modes","text":"","title":"Modes"},{"location":"modules/hidden_states_aggregator/#hiddenstatesaggregationmodenoop","text":"Acts as a \"null\"-aggregator.","title":"HiddenStatesAggregationMode.noop"},{"location":"modules/hidden_states_aggregator/#hiddenstatesaggregationmodemean","text":"The Mean mode ( HiddenStatesAggregationMode.mean ) performs \"eager\" reduction. Instead of storing the full [Batch, Seq_Len, Hidden_Dim] tensors for every step, it: Takes an aggregation mask. Computes the masked average immediately upon receiving the hidden states. Stores only the reduced [Batch, Hidden_Dim] vectors. This significantly reduces memory footprint when accumulating states over many iterations.","title":"HiddenStatesAggregationMode.mean"},{"location":"modules/hidden_states_aggregator/#d9d.module.block.hidden_states_aggregator","text":"Aggregation utilities for model hidden states.","title":"hidden_states_aggregator"},{"location":"modules/hidden_states_aggregator/#d9d.module.block.hidden_states_aggregator.BaseHiddenStatesAggregator","text":"Bases: ABC Abstract base class for hidden states aggregation strategies. This interface defines how hidden states should be collected (added) and how they should be finalized (packed) combined with optional historical snapshots. Source code in d9d/module/block/hidden_states_aggregator/base.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class BaseHiddenStatesAggregator ( abc . ABC ): \"\"\"Abstract base class for hidden states aggregation strategies. This interface defines how hidden states should be collected (added) and how they should be finalized (packed) combined with optional historical snapshots. \"\"\" @abc . abstractmethod def add_hidden_states ( self , hidden_states : torch . Tensor ) -> None : \"\"\"Accumulates a batch of hidden states into the aggregator. Args: hidden_states: The tensor containing the hidden states to process. \"\"\" @abc . abstractmethod def pack_with_snapshot ( self , snapshot : torch . Tensor | None ) -> torch . Tensor | None : \"\"\"Finalizes the aggregation and combines it with an optional previous snapshot. This method typically retrieves the accumulated states, processes them (if not done during addition), and concatenates them with the snapshot. Args: snapshot: An optional tensor representing previously aggregated states to be prepended to the current collection. Returns: The combined result of the snapshot and the newly aggregated states, or None if no states were collected. \"\"\"","title":"BaseHiddenStatesAggregator"},{"location":"modules/hidden_states_aggregator/#d9d.module.block.hidden_states_aggregator.BaseHiddenStatesAggregator.add_hidden_states","text":"Accumulates a batch of hidden states into the aggregator. Parameters: Name Type Description Default hidden_states Tensor The tensor containing the hidden states to process. required Source code in d9d/module/block/hidden_states_aggregator/base.py 13 14 15 16 17 18 19 @abc . abstractmethod def add_hidden_states ( self , hidden_states : torch . Tensor ) -> None : \"\"\"Accumulates a batch of hidden states into the aggregator. Args: hidden_states: The tensor containing the hidden states to process. \"\"\"","title":"add_hidden_states"},{"location":"modules/hidden_states_aggregator/#d9d.module.block.hidden_states_aggregator.BaseHiddenStatesAggregator.pack_with_snapshot","text":"Finalizes the aggregation and combines it with an optional previous snapshot. This method typically retrieves the accumulated states, processes them (if not done during addition), and concatenates them with the snapshot. Parameters: Name Type Description Default snapshot Tensor | None An optional tensor representing previously aggregated states to be prepended to the current collection. required Returns: Type Description Tensor | None The combined result of the snapshot and the newly aggregated states, Tensor | None or None if no states were collected. Source code in d9d/module/block/hidden_states_aggregator/base.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 @abc . abstractmethod def pack_with_snapshot ( self , snapshot : torch . Tensor | None ) -> torch . Tensor | None : \"\"\"Finalizes the aggregation and combines it with an optional previous snapshot. This method typically retrieves the accumulated states, processes them (if not done during addition), and concatenates them with the snapshot. Args: snapshot: An optional tensor representing previously aggregated states to be prepended to the current collection. Returns: The combined result of the snapshot and the newly aggregated states, or None if no states were collected. \"\"\"","title":"pack_with_snapshot"},{"location":"modules/hidden_states_aggregator/#d9d.module.block.hidden_states_aggregator.HiddenStatesAggregationMode","text":"Bases: StrEnum Enumeration of available hidden state aggregation strategies. Attributes: Name Type Description no Performs no aggregation (No-Op). mean Computes the mean of hidden states, taking a mask into account. Source code in d9d/module/block/hidden_states_aggregator/factory.py 10 11 12 13 14 15 16 17 18 19 class HiddenStatesAggregationMode ( StrEnum ): \"\"\"Enumeration of available hidden state aggregation strategies. Attributes: no: Performs no aggregation (No-Op). mean: Computes the mean of hidden states, taking a mask into account. \"\"\" no = \"no\" mean = \"mean\"","title":"HiddenStatesAggregationMode"},{"location":"modules/hidden_states_aggregator/#d9d.module.block.hidden_states_aggregator.create_hidden_states_aggregator","text":"Factory function to create a hidden states aggregator. Parameters: Name Type Description Default mode HiddenStatesAggregationMode The specific aggregation mode to instantiate. required agg_mask Tensor | None A tensor mask required for specific modes. Can be None if the selected mode does not require masking. required Returns: Type Description BaseHiddenStatesAggregator An instance of a concrete BaseHiddenStatesAggregator subclass. Raises: Type Description ValueError If 'mean' mode is selected but 'agg_mask' is None, or if an unknown mode is provided. Source code in d9d/module/block/hidden_states_aggregator/factory.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def create_hidden_states_aggregator ( mode : HiddenStatesAggregationMode , agg_mask : torch . Tensor | None ) -> BaseHiddenStatesAggregator : \"\"\"Factory function to create a hidden states aggregator. Args: mode: The specific aggregation mode to instantiate. agg_mask: A tensor mask required for specific modes. Can be None if the selected mode does not require masking. Returns: An instance of a concrete BaseHiddenStatesAggregator subclass. Raises: ValueError: If 'mean' mode is selected but 'agg_mask' is None, or if an unknown mode is provided. \"\"\" match mode : case HiddenStatesAggregationMode . no : return HiddenStatesAggregatorNoOp () case HiddenStatesAggregationMode . mean : if agg_mask is None : raise ValueError ( \"You have to specify aggregation mask\" ) return HiddenStatesAggregatorMean ( agg_mask ) case _ : raise ValueError ( \"Unknown hidden states aggregation mode\" )","title":"create_hidden_states_aggregator"},{"location":"modules/moe/","text":"About The d9d.module.block.moe package provides a complete, high-performance implementation of Sparse Mixture-of-Experts layers. Expert Parallelism For information on setting up Expert Parallelism, see this page . Features Sparse Expert Router TopKRouter is a learnable router implementation. It computes routing probabilities in FP32 to ensure numeric stability. Sparse Expert Token Dispatcher ExpertCommunicationHandler is the messaging layer. NoCommunicationHandler is used by default for single-GPU or Tensor Parallel setups where no token movement is needed. DeepEpCommunicationHandler is enabled if using Expert Parallelism. It uses the DeepEP library for highly optimized all-to-all communication over NVLink/RDMA, enabling scaling to thousands of experts. Sparse Experts GroupedSwiGLU provides a sparse SwiGLU experts module implementation. Instead of looping over experts, it uses Grouped GEMM kernels to execute all experts in parallel, regardless of how many tokens each expert received. Uses efficient fused SiLU-Mul kernel. Kernel Benchmarks (BF16, H100) Shared Experts Currently not supported, feel free to contribute :) d9d.module.block.moe Provides building blocks for Mixture-of-Experts (MoE) architectures. GroupedLinear Bases: Module , ModuleLateInit Applies a linear transformation using Grouped GEMM (Generalized Matrix Multiplication). This module allows efficient execution of multiple linear layers (experts) in parallel, where each expert processes a variable number of tokens. It is the computational core of the Mixture-of-Experts layer. Source code in d9d/module/block/moe/grouped_linear.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 class GroupedLinear ( nn . Module , ModuleLateInit ): \"\"\" Applies a linear transformation using Grouped GEMM (Generalized Matrix Multiplication). This module allows efficient execution of multiple linear layers (experts) in parallel, where each expert processes a variable number of tokens. It is the computational core of the Mixture-of-Experts layer. \"\"\" def __init__ ( self , n_groups : int , in_features : int , out_features : int , device : torch . device | str | None = None , dtype : torch . dtype | None = None , ): \"\"\" Constructs the GroupedLinear layer. Args: n_groups: Number of groups (experts). in_features: Input hidden size. out_features: Output hidden size. device: Target device. dtype: Target data type. \"\"\" super () . __init__ () self . weight = nn . Parameter ( torch . empty ( n_groups , in_features , out_features , device = device , dtype = dtype )) self . n_groups = n_groups self . in_features = in_features self . out_features = out_features self . reset_parameters () def forward ( self , x : torch . Tensor , x_groups : torch . Tensor ) -> torch . Tensor : \"\"\" Performs the grouped matrix multiplication. Args: x: Flattened input tensor containing tokens for all groups. Shape: `(total_tokens, in_features)`. x_groups: CPU Tensor indicating the number of tokens assigned to each group. Must sum to `total_tokens`. Shape: `(n_groups,)`. Returns: The output tensor. Shape: `(total_tokens, out_features)`. \"\"\" weight : torch . Tensor = self . weight if isinstance ( weight , DTensor ): weight = weight . to_local () return gmm ( x , weight , x_groups , a_grad_direction = GradDirection . inputs , b_grad_direction = GradDirection . weight ) def reset_parameters ( self ): \"\"\"Initializes weights using a uniform distribution based on input features.\"\"\" nn . init . uniform_ ( self . weight , - 1 / math . sqrt ( self . in_features ), 1 / math . sqrt ( self . in_features )) __init__ ( n_groups , in_features , out_features , device = None , dtype = None ) Constructs the GroupedLinear layer. Parameters: Name Type Description Default n_groups int Number of groups (experts). required in_features int Input hidden size. required out_features int Output hidden size. required device device | str | None Target device. None dtype dtype | None Target data type. None Source code in d9d/module/block/moe/grouped_linear.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def __init__ ( self , n_groups : int , in_features : int , out_features : int , device : torch . device | str | None = None , dtype : torch . dtype | None = None , ): \"\"\" Constructs the GroupedLinear layer. Args: n_groups: Number of groups (experts). in_features: Input hidden size. out_features: Output hidden size. device: Target device. dtype: Target data type. \"\"\" super () . __init__ () self . weight = nn . Parameter ( torch . empty ( n_groups , in_features , out_features , device = device , dtype = dtype )) self . n_groups = n_groups self . in_features = in_features self . out_features = out_features self . reset_parameters () forward ( x , x_groups ) Performs the grouped matrix multiplication. Parameters: Name Type Description Default x Tensor Flattened input tensor containing tokens for all groups. Shape: (total_tokens, in_features) . required x_groups Tensor CPU Tensor indicating the number of tokens assigned to each group. Must sum to total_tokens . Shape: (n_groups,) . required Returns: Type Description Tensor The output tensor. Shape: (total_tokens, out_features) . Source code in d9d/module/block/moe/grouped_linear.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def forward ( self , x : torch . Tensor , x_groups : torch . Tensor ) -> torch . Tensor : \"\"\" Performs the grouped matrix multiplication. Args: x: Flattened input tensor containing tokens for all groups. Shape: `(total_tokens, in_features)`. x_groups: CPU Tensor indicating the number of tokens assigned to each group. Must sum to `total_tokens`. Shape: `(n_groups,)`. Returns: The output tensor. Shape: `(total_tokens, out_features)`. \"\"\" weight : torch . Tensor = self . weight if isinstance ( weight , DTensor ): weight = weight . to_local () return gmm ( x , weight , x_groups , a_grad_direction = GradDirection . inputs , b_grad_direction = GradDirection . weight ) reset_parameters () Initializes weights using a uniform distribution based on input features. Source code in d9d/module/block/moe/grouped_linear.py 69 70 71 def reset_parameters ( self ): \"\"\"Initializes weights using a uniform distribution based on input features.\"\"\" nn . init . uniform_ ( self . weight , - 1 / math . sqrt ( self . in_features ), 1 / math . sqrt ( self . in_features )) GroupedSwiGLU Bases: Module , ModuleLateInit Executes a collection of SwiGLU experts efficiently using Grouped GEMM. This module implements the architectural pattern: down_proj(SiLU(gate_proj(x)) * up_proj(x)) . It applies this operation across multiple discrete experts in parallel without padding or masking. Source code in d9d/module/block/moe/grouped_experts.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 class GroupedSwiGLU ( nn . Module , ModuleLateInit ): \"\"\" Executes a collection of SwiGLU experts efficiently using Grouped GEMM. This module implements the architectural pattern: `down_proj(SiLU(gate_proj(x)) * up_proj(x))`. It applies this operation across multiple discrete experts in parallel without padding or masking. \"\"\" def __init__ ( self , hidden_dim : int , intermediate_dim : int , num_experts : int ): \"\"\" Constructs the GroupedSwiGLU module. Args: hidden_dim: Dimensionality of the input and output hidden states. intermediate_dim: Dimensionality of the intermediate projection. num_experts: Total number of experts managed by this local instance. \"\"\" super () . __init__ () self . _num_experts = num_experts self . gate_proj = GroupedLinear ( num_experts , hidden_dim , intermediate_dim ) self . up_proj = GroupedLinear ( num_experts , hidden_dim , intermediate_dim ) self . down_proj = GroupedLinear ( num_experts , intermediate_dim , hidden_dim ) def forward ( self , permuted_x : torch . Tensor , permuted_probs : torch . Tensor , tokens_per_expert : torch . Tensor , ) -> torch . Tensor : \"\"\" Computes expert outputs for sorted input tokens. Args: permuted_x: Input tokens sorted by their assigned expert. Shape: `(total_tokens, hidden_dim)`. permuted_probs: Routing weights/probabilities corresponding to the sorted tokens. Shape: `(total_tokens)`. tokens_per_expert: Number of tokens assigned to each consecutive expert. It is a CPU tensor. Shape: `(num_experts)`. Returns: The computed and weighted output tokens (still permuted). Shape: `(total_tokens, hidden_dim)`. \"\"\" if permuted_x . numel () == 0 : # handle cases when there are no routed experts to this instance return permuted_x probs = permuted_probs [:, None ] . to ( permuted_x . dtype ) values = self . down_proj ( silu_mul ( self . gate_proj ( permuted_x , tokens_per_expert ), self . up_proj ( permuted_x , tokens_per_expert )), tokens_per_expert , ) return probs * values def reset_parameters ( self ): \"\"\"Resets parameters for all internal linear projections.\"\"\" self . gate_proj . reset_parameters () self . up_proj . reset_parameters () self . down_proj . reset_parameters () __init__ ( hidden_dim , intermediate_dim , num_experts ) Constructs the GroupedSwiGLU module. Parameters: Name Type Description Default hidden_dim int Dimensionality of the input and output hidden states. required intermediate_dim int Dimensionality of the intermediate projection. required num_experts int Total number of experts managed by this local instance. required Source code in d9d/module/block/moe/grouped_experts.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def __init__ ( self , hidden_dim : int , intermediate_dim : int , num_experts : int ): \"\"\" Constructs the GroupedSwiGLU module. Args: hidden_dim: Dimensionality of the input and output hidden states. intermediate_dim: Dimensionality of the intermediate projection. num_experts: Total number of experts managed by this local instance. \"\"\" super () . __init__ () self . _num_experts = num_experts self . gate_proj = GroupedLinear ( num_experts , hidden_dim , intermediate_dim ) self . up_proj = GroupedLinear ( num_experts , hidden_dim , intermediate_dim ) self . down_proj = GroupedLinear ( num_experts , intermediate_dim , hidden_dim ) forward ( permuted_x , permuted_probs , tokens_per_expert ) Computes expert outputs for sorted input tokens. Parameters: Name Type Description Default permuted_x Tensor Input tokens sorted by their assigned expert. Shape: (total_tokens, hidden_dim) . required permuted_probs Tensor Routing weights/probabilities corresponding to the sorted tokens. Shape: (total_tokens) . required tokens_per_expert Tensor Number of tokens assigned to each consecutive expert. It is a CPU tensor. Shape: (num_experts) . required Returns: Name Type Description Tensor The computed and weighted output tokens (still permuted). Shape Tensor (total_tokens, hidden_dim) . Source code in d9d/module/block/moe/grouped_experts.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def forward ( self , permuted_x : torch . Tensor , permuted_probs : torch . Tensor , tokens_per_expert : torch . Tensor , ) -> torch . Tensor : \"\"\" Computes expert outputs for sorted input tokens. Args: permuted_x: Input tokens sorted by their assigned expert. Shape: `(total_tokens, hidden_dim)`. permuted_probs: Routing weights/probabilities corresponding to the sorted tokens. Shape: `(total_tokens)`. tokens_per_expert: Number of tokens assigned to each consecutive expert. It is a CPU tensor. Shape: `(num_experts)`. Returns: The computed and weighted output tokens (still permuted). Shape: `(total_tokens, hidden_dim)`. \"\"\" if permuted_x . numel () == 0 : # handle cases when there are no routed experts to this instance return permuted_x probs = permuted_probs [:, None ] . to ( permuted_x . dtype ) values = self . down_proj ( silu_mul ( self . gate_proj ( permuted_x , tokens_per_expert ), self . up_proj ( permuted_x , tokens_per_expert )), tokens_per_expert , ) return probs * values reset_parameters () Resets parameters for all internal linear projections. Source code in d9d/module/block/moe/grouped_experts.py 68 69 70 71 72 73 def reset_parameters ( self ): \"\"\"Resets parameters for all internal linear projections.\"\"\" self . gate_proj . reset_parameters () self . up_proj . reset_parameters () self . down_proj . reset_parameters () MoELayer Bases: Module , ModuleLateInit A complete Mixture-of-Experts (MoE) block comprising routing, communication, and computation. This layer integrates: Router : Selects experts for each token. Communicator : Handles token dispatch to local or remote experts (EP). Experts : Performs parallelized computation (Grouped SwiGLU). Source code in d9d/module/block/moe/layer.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 class MoELayer ( nn . Module , ModuleLateInit ): \"\"\" A complete Mixture-of-Experts (MoE) block comprising routing, communication, and computation. This layer integrates: 1. **Router**: Selects experts for each token. 2. **Communicator**: Handles token dispatch to local or remote experts (EP). 3. **Experts**: Performs parallelized computation (Grouped SwiGLU). \"\"\" def __init__ ( self , hidden_dim : int , intermediate_dim_grouped : int , num_grouped_experts : int , top_k : int , router_renormalize_probabilities : bool , ): \"\"\" Constructs the MoELayer. Args: hidden_dim: Hidden size. intermediate_dim_grouped: Intermediate dimension for the Expert FFNs. num_grouped_experts: Total number of experts. top_k: Number of experts to route each token to. router_renormalize_probabilities: Configures router probability normalization behavior. \"\"\" super () . __init__ () self . router = TopKRouter ( dim = hidden_dim , num_experts = num_grouped_experts , top_k = top_k , renormalize_probabilities = router_renormalize_probabilities , ) self . grouped_experts = GroupedSwiGLU ( hidden_dim = hidden_dim , intermediate_dim = intermediate_dim_grouped , num_experts = num_grouped_experts ) self . _communicator : ExpertCommunicationHandler = NoCommunicationHandler ( num_grouped_experts ) self . _num_grouped_experts = num_grouped_experts self . _hidden_dim = hidden_dim self . tokens_per_expert = nn . Buffer ( torch . empty (( num_grouped_experts ,), dtype = torch . int64 ), persistent = False ) def enable_distributed_communicator ( self , group : ProcessGroup ): \"\"\" Switches from local no-op communication to distributed DeepEP communication. This should be called during model initialization if the model is running in a distributed Expert Parallel environment. Args: group: The PyTorch process group spanning the expert parallel ranks. \"\"\" communicator = DeepEpCommunicationHandler ( num_experts = self . _num_grouped_experts ) communicator . setup ( group , self . _hidden_dim , self . router . gate . weight . dtype ) self . _communicator = communicator @torch . no_grad () def _update_tokens_per_expert ( self , expert_indices : torch . Tensor ): self . tokens_per_expert . add_ ( expert_indices . view ( - 1 ) . bincount ( minlength = self . _num_grouped_experts )) @torch . no_grad () def reset_stats ( self ): \"\"\"Resets the expert load balancing counters.\"\"\" self . tokens_per_expert . zero_ () def forward ( self , hidden_states : torch . Tensor ) -> torch . Tensor : \"\"\" Routes tokens to experts, computes, and combines results. Args: hidden_states: Input tensor. Shape: `(batch_size, seq_len, hidden_dim)`. Returns: Output tensor combined from experts. Shape: `(batch_size, seq_len, hidden_dim)`. \"\"\" old_shape = hidden_states . shape hidden_states = hidden_states . reshape ( - 1 , hidden_states . shape [ - 1 ]) expert_indices , expert_scores = self . router ( hidden_states ) self . _update_tokens_per_expert ( expert_indices ) hidden_states , expert_scores , expert_count = self . _communicator . dispatch ( hidden_states , expert_indices , expert_scores ) hidden_states = self . grouped_experts ( hidden_states , expert_scores , expert_count ) hidden_states = self . _communicator . combine ( hidden_states ) hidden_states = hidden_states . reshape ( * old_shape ) return hidden_states def reset_parameters ( self ): \"\"\"Resets module parameters.\"\"\" self . router . reset_parameters () self . grouped_experts . reset_parameters () nn . init . zeros_ ( self . tokens_per_expert ) __init__ ( hidden_dim , intermediate_dim_grouped , num_grouped_experts , top_k , router_renormalize_probabilities ) Constructs the MoELayer. Parameters: Name Type Description Default hidden_dim int Hidden size. required intermediate_dim_grouped int Intermediate dimension for the Expert FFNs. required num_grouped_experts int Total number of experts. required top_k int Number of experts to route each token to. required router_renormalize_probabilities bool Configures router probability normalization behavior. required Source code in d9d/module/block/moe/layer.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def __init__ ( self , hidden_dim : int , intermediate_dim_grouped : int , num_grouped_experts : int , top_k : int , router_renormalize_probabilities : bool , ): \"\"\" Constructs the MoELayer. Args: hidden_dim: Hidden size. intermediate_dim_grouped: Intermediate dimension for the Expert FFNs. num_grouped_experts: Total number of experts. top_k: Number of experts to route each token to. router_renormalize_probabilities: Configures router probability normalization behavior. \"\"\" super () . __init__ () self . router = TopKRouter ( dim = hidden_dim , num_experts = num_grouped_experts , top_k = top_k , renormalize_probabilities = router_renormalize_probabilities , ) self . grouped_experts = GroupedSwiGLU ( hidden_dim = hidden_dim , intermediate_dim = intermediate_dim_grouped , num_experts = num_grouped_experts ) self . _communicator : ExpertCommunicationHandler = NoCommunicationHandler ( num_grouped_experts ) self . _num_grouped_experts = num_grouped_experts self . _hidden_dim = hidden_dim self . tokens_per_expert = nn . Buffer ( torch . empty (( num_grouped_experts ,), dtype = torch . int64 ), persistent = False ) enable_distributed_communicator ( group ) Switches from local no-op communication to distributed DeepEP communication. This should be called during model initialization if the model is running in a distributed Expert Parallel environment. Parameters: Name Type Description Default group ProcessGroup The PyTorch process group spanning the expert parallel ranks. required Source code in d9d/module/block/moe/layer.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def enable_distributed_communicator ( self , group : ProcessGroup ): \"\"\" Switches from local no-op communication to distributed DeepEP communication. This should be called during model initialization if the model is running in a distributed Expert Parallel environment. Args: group: The PyTorch process group spanning the expert parallel ranks. \"\"\" communicator = DeepEpCommunicationHandler ( num_experts = self . _num_grouped_experts ) communicator . setup ( group , self . _hidden_dim , self . router . gate . weight . dtype ) self . _communicator = communicator forward ( hidden_states ) Routes tokens to experts, computes, and combines results. Parameters: Name Type Description Default hidden_states Tensor Input tensor. Shape: (batch_size, seq_len, hidden_dim) . required Returns: Type Description Tensor Output tensor combined from experts. Shape: (batch_size, seq_len, hidden_dim) . Source code in d9d/module/block/moe/layer.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def forward ( self , hidden_states : torch . Tensor ) -> torch . Tensor : \"\"\" Routes tokens to experts, computes, and combines results. Args: hidden_states: Input tensor. Shape: `(batch_size, seq_len, hidden_dim)`. Returns: Output tensor combined from experts. Shape: `(batch_size, seq_len, hidden_dim)`. \"\"\" old_shape = hidden_states . shape hidden_states = hidden_states . reshape ( - 1 , hidden_states . shape [ - 1 ]) expert_indices , expert_scores = self . router ( hidden_states ) self . _update_tokens_per_expert ( expert_indices ) hidden_states , expert_scores , expert_count = self . _communicator . dispatch ( hidden_states , expert_indices , expert_scores ) hidden_states = self . grouped_experts ( hidden_states , expert_scores , expert_count ) hidden_states = self . _communicator . combine ( hidden_states ) hidden_states = hidden_states . reshape ( * old_shape ) return hidden_states reset_parameters () Resets module parameters. Source code in d9d/module/block/moe/layer.py 114 115 116 117 118 119 def reset_parameters ( self ): \"\"\"Resets module parameters.\"\"\" self . router . reset_parameters () self . grouped_experts . reset_parameters () nn . init . zeros_ ( self . tokens_per_expert ) reset_stats () Resets the expert load balancing counters. Source code in d9d/module/block/moe/layer.py 85 86 87 88 @torch . no_grad () def reset_stats ( self ): \"\"\"Resets the expert load balancing counters.\"\"\" self . tokens_per_expert . zero_ () TopKRouter Bases: Module , ModuleLateInit Selects the top-K experts based on a learned gating mechanism. This router: Projects input tokens into expert space Applies softmax, optionally adds expert bias to influence selection Selects the experts with the highest probabilities Selected probabilities are then re-normalized to sum to 1 if needed. Source code in d9d/module/block/moe/router.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 class TopKRouter ( nn . Module , ModuleLateInit ): \"\"\" Selects the top-K experts based on a learned gating mechanism. This router: 1. Projects input tokens into expert space 2. Applies softmax, optionally adds expert bias to influence selection 3. Selects the experts with the highest probabilities 4. Selected probabilities are then re-normalized to sum to 1 if needed. \"\"\" def __init__ ( self , dim : int , num_experts : int , top_k : int , renormalize_probabilities : bool , enable_expert_bias : bool = False ): \"\"\" Constructs the TopKRouter. Args: dim: Input feature dimensionality. num_experts: Total number of experts to choose from. top_k: Number of experts to select for each token. renormalize_probabilities: If True, probabilities of selected experts will be renormalized to sum up to 1 enable_expert_bias: If True, adds a bias term to the routing scores before top-k selection. This can be used for loss-free load balancing. \"\"\" super () . __init__ () self . gate = nn . Linear ( dim , num_experts , bias = False ) self . expert_bias : nn . Buffer | None if enable_expert_bias : self . expert_bias = nn . Buffer ( torch . empty ( num_experts , dtype = torch . float32 ), persistent = True , ) else : self . expert_bias = None self . _num_experts = num_experts self . _top_k = top_k self . _renormalize_probabilities = renormalize_probabilities def forward ( self , hidden_states : torch . Tensor ) -> tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Calculates routing decisions for the input tokens. Args: hidden_states: Input tokens. Shape: `(num_tokens, dim)`. Returns: A tuple containing: - Selected expert indices. Shape: `(num_tokens, top_k)`. - Normalized routing weights for the selected experts. Shape: `(num_tokens, top_k)`. \"\"\" # scores shape (bs*slen, num_experts) # gate scores = self . gate ( hidden_states ) # and now do softmax (before top-k to be able to apply expert bias) scores = F . softmax ( scores , dim =- 1 , dtype = torch . float32 ) # select top-k if self . expert_bias is None : scores , selected_experts_indices = torch . topk ( scores , k = self . _top_k , dim =- 1 ) else : _ , selected_experts_indices = torch . topk ( scores + self . expert_bias , k = self . _top_k , dim =- 1 ) scores = scores . gather ( dim =- 1 , index = selected_experts_indices ) # re-normalize scores denominator = scores . sum ( dim =- 1 , keepdim = True ) + 1e-20 scores = scores / denominator return selected_experts_indices , scores def reset_parameters ( self ): \"\"\"Resets module parameters.\"\"\" if self . expert_bias is not None : nn . init . zeros_ ( self . expert_bias ) self . gate . reset_parameters () __init__ ( dim , num_experts , top_k , renormalize_probabilities , enable_expert_bias = False ) Constructs the TopKRouter. Parameters: Name Type Description Default dim int Input feature dimensionality. required num_experts int Total number of experts to choose from. required top_k int Number of experts to select for each token. required renormalize_probabilities bool If True, probabilities of selected experts will be renormalized to sum up to 1 required enable_expert_bias bool If True, adds a bias term to the routing scores before top-k selection. This can be used for loss-free load balancing. False Source code in d9d/module/block/moe/router.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def __init__ ( self , dim : int , num_experts : int , top_k : int , renormalize_probabilities : bool , enable_expert_bias : bool = False ): \"\"\" Constructs the TopKRouter. Args: dim: Input feature dimensionality. num_experts: Total number of experts to choose from. top_k: Number of experts to select for each token. renormalize_probabilities: If True, probabilities of selected experts will be renormalized to sum up to 1 enable_expert_bias: If True, adds a bias term to the routing scores before top-k selection. This can be used for loss-free load balancing. \"\"\" super () . __init__ () self . gate = nn . Linear ( dim , num_experts , bias = False ) self . expert_bias : nn . Buffer | None if enable_expert_bias : self . expert_bias = nn . Buffer ( torch . empty ( num_experts , dtype = torch . float32 ), persistent = True , ) else : self . expert_bias = None self . _num_experts = num_experts self . _top_k = top_k self . _renormalize_probabilities = renormalize_probabilities forward ( hidden_states ) Calculates routing decisions for the input tokens. Parameters: Name Type Description Default hidden_states Tensor Input tokens. Shape: (num_tokens, dim) . required Returns: Type Description Tensor A tuple containing: Tensor Selected expert indices. Shape: (num_tokens, top_k) . tuple [ Tensor , Tensor ] Normalized routing weights for the selected experts. Shape: (num_tokens, top_k) . Source code in d9d/module/block/moe/router.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def forward ( self , hidden_states : torch . Tensor ) -> tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Calculates routing decisions for the input tokens. Args: hidden_states: Input tokens. Shape: `(num_tokens, dim)`. Returns: A tuple containing: - Selected expert indices. Shape: `(num_tokens, top_k)`. - Normalized routing weights for the selected experts. Shape: `(num_tokens, top_k)`. \"\"\" # scores shape (bs*slen, num_experts) # gate scores = self . gate ( hidden_states ) # and now do softmax (before top-k to be able to apply expert bias) scores = F . softmax ( scores , dim =- 1 , dtype = torch . float32 ) # select top-k if self . expert_bias is None : scores , selected_experts_indices = torch . topk ( scores , k = self . _top_k , dim =- 1 ) else : _ , selected_experts_indices = torch . topk ( scores + self . expert_bias , k = self . _top_k , dim =- 1 ) scores = scores . gather ( dim =- 1 , index = selected_experts_indices ) # re-normalize scores denominator = scores . sum ( dim =- 1 , keepdim = True ) + 1e-20 scores = scores / denominator return selected_experts_indices , scores reset_parameters () Resets module parameters. Source code in d9d/module/block/moe/router.py 86 87 88 89 90 91 def reset_parameters ( self ): \"\"\"Resets module parameters.\"\"\" if self . expert_bias is not None : nn . init . zeros_ ( self . expert_bias ) self . gate . reset_parameters () d9d.module.block.moe.communications Provides communication strategies for Mixture-of-Experts routing operations. DeepEpCommunicationHandler Bases: ExpertCommunicationHandler Handles MoE communication using the high-performance DeepEP library. Source code in d9d/module/block/moe/communications/deepep.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 class DeepEpCommunicationHandler ( ExpertCommunicationHandler ): \"\"\"Handles MoE communication using the high-performance DeepEP library.\"\"\" def __init__ ( self , num_experts : int ): \"\"\"Constructs the DeepEpCommunicationHandler.\"\"\" self . _num_experts = num_experts self . _num_experts_per_shard = None # late-initialization # == fields saved for post-dispatch == self . _handle = None self . _hidden_shape_before_permute = None self . _unpermute_mapping = None def setup ( self , group : torch . distributed . ProcessGroup , hidden_size : int , hidden_dtype : torch . dtype ): \"\"\" Initializes the backend buffer and calculates expert sharding. Args: group: The process group containing all experts. hidden_size: Dimensionality of the hidden states. hidden_dtype: Data type of the hidden states. \"\"\" init_deepep_buffer ( group , hidden_size * hidden_dtype . itemsize ) if self . _num_experts % group . size () != 0 : raise ValueError ( \"num_experts must be divisible by distributed group size\" ) self . _num_experts_per_shard = self . _num_experts // group . size () def dispatch ( self , hidden_states : torch . Tensor , topk_ids : torch . Tensor , topk_weights : torch . Tensor ) -> tuple [ torch . Tensor , torch . Tensor , torch . Tensor ]: ( hidden_states , topk_ids , topk_weights , tokens_per_expert , handle ) = DeepEpDispatch . apply ( hidden_states , topk_ids , topk_weights , self . _num_experts ) routing_map , routing_probs = fused_indices_to_multihot ( topk_ids , topk_weights , self . _num_experts_per_shard ) self . _hidden_shape_before_permute = hidden_states . shape hidden_states , routing_probs , reverse_permute_map = moe_permute_with_probs ( hidden_states , routing_probs , routing_map , num_out_tokens = tokens_per_expert . sum () . item () ) self . _handle = handle self . _unpermute_mapping = reverse_permute_map return hidden_states , routing_probs , tokens_per_expert def combine ( self , hidden_states : torch . Tensor ) -> torch . Tensor : if self . _handle is None : raise ValueError ( \"you fucked up moe communication order: you should dispatch first and after that combine\" ) hidden_states = moe_unpermute_mask ( hidden_states , self . _unpermute_mapping , restore_shape = self . _hidden_shape_before_permute , ) hidden_states = DeepEpCombine . apply ( hidden_states , self . _handle ) self . _handle = None self . _unpermute_mapping = None self . _hidden_shape_before_permute = None return hidden_states __init__ ( num_experts ) Constructs the DeepEpCommunicationHandler. Source code in d9d/module/block/moe/communications/deepep.py 160 161 162 163 164 165 166 167 168 169 170 def __init__ ( self , num_experts : int ): \"\"\"Constructs the DeepEpCommunicationHandler.\"\"\" self . _num_experts = num_experts self . _num_experts_per_shard = None # late-initialization # == fields saved for post-dispatch == self . _handle = None self . _hidden_shape_before_permute = None self . _unpermute_mapping = None setup ( group , hidden_size , hidden_dtype ) Initializes the backend buffer and calculates expert sharding. Parameters: Name Type Description Default group ProcessGroup The process group containing all experts. required hidden_size int Dimensionality of the hidden states. required hidden_dtype dtype Data type of the hidden states. required Source code in d9d/module/block/moe/communications/deepep.py 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 def setup ( self , group : torch . distributed . ProcessGroup , hidden_size : int , hidden_dtype : torch . dtype ): \"\"\" Initializes the backend buffer and calculates expert sharding. Args: group: The process group containing all experts. hidden_size: Dimensionality of the hidden states. hidden_dtype: Data type of the hidden states. \"\"\" init_deepep_buffer ( group , hidden_size * hidden_dtype . itemsize ) if self . _num_experts % group . size () != 0 : raise ValueError ( \"num_experts must be divisible by distributed group size\" ) self . _num_experts_per_shard = self . _num_experts // group . size () ExpertCommunicationHandler Bases: ABC Abstract base class for Mixture-of-Experts communication strategies. Source code in d9d/module/block/moe/communications/base.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 class ExpertCommunicationHandler ( abc . ABC ): \"\"\"Abstract base class for Mixture-of-Experts communication strategies.\"\"\" @abc . abstractmethod def dispatch ( self , hidden_states : torch . Tensor , topk_ids : torch . Tensor , topk_weights : torch . Tensor ) -> tuple [ torch . Tensor , torch . Tensor , torch . Tensor ]: \"\"\" Prepares and routes local hidden states to their target experts (possibly on other workers). This process involves: 1. All-to-All Communication: Transfers hidden states to workers containing the assigned experts. States assigned to multiple experts are replicated. 2. Permutation: Sorts tokens by expert ID to prepare for Grouped GEMM. Args: hidden_states: Input tokens. Shape: `(num_tokens, hidden_size)`. topk_ids: Indices of the top-k experts selected for each token. Shape: `(num_tokens, k)`. topk_weights: Routing weights associated with the selected experts. Shape: `(num_tokens, k)`. Returns: A tuple containing: - Permuted hidden states received by this rank. Shape: `(num_received_tokens, hidden_size)`. - Permuted weights matching the hidden states order. Shape: `(num_received_tokens)`. - Expert count tensor indicating how many tokens each local expert received. Shape: `(num_local_experts)`. \"\"\" ... @abc . abstractmethod def combine ( self , hidden_states : torch . Tensor ) -> torch . Tensor : \"\"\" Restores hidden states to their original order and location. Undoes the permutation and performs the reverse All-to-All communication to return processed results to the workers that originated the requests. Args: hidden_states: The processed hidden states. Shape: `(num_received_tokens, hidden_size)`. Returns: The combined hidden states with the original shape and order. Shape: `(num_tokens, hidden_size)`. \"\"\" ... combine ( hidden_states ) abstractmethod Restores hidden states to their original order and location. Undoes the permutation and performs the reverse All-to-All communication to return processed results to the workers that originated the requests. Parameters: Name Type Description Default hidden_states Tensor The processed hidden states. Shape: (num_received_tokens, hidden_size) . required Returns: Type Description Tensor The combined hidden states with the original shape and order. Shape: (num_tokens, hidden_size) . Source code in d9d/module/block/moe/communications/base.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 @abc . abstractmethod def combine ( self , hidden_states : torch . Tensor ) -> torch . Tensor : \"\"\" Restores hidden states to their original order and location. Undoes the permutation and performs the reverse All-to-All communication to return processed results to the workers that originated the requests. Args: hidden_states: The processed hidden states. Shape: `(num_received_tokens, hidden_size)`. Returns: The combined hidden states with the original shape and order. Shape: `(num_tokens, hidden_size)`. \"\"\" ... dispatch ( hidden_states , topk_ids , topk_weights ) abstractmethod Prepares and routes local hidden states to their target experts (possibly on other workers). This process involves: All-to-All Communication: Transfers hidden states to workers containing the assigned experts. States assigned to multiple experts are replicated. Permutation: Sorts tokens by expert ID to prepare for Grouped GEMM. Parameters: Name Type Description Default hidden_states Tensor Input tokens. Shape: (num_tokens, hidden_size) . required topk_ids Tensor Indices of the top-k experts selected for each token. Shape: (num_tokens, k) . required topk_weights Tensor Routing weights associated with the selected experts. Shape: (num_tokens, k) . required Returns: Type Description Tensor A tuple containing: Tensor Permuted hidden states received by this rank. Shape: (num_received_tokens, hidden_size) . Tensor Permuted weights matching the hidden states order. Shape: (num_received_tokens) . tuple [ Tensor , Tensor , Tensor ] Expert count tensor indicating how many tokens each local expert received. Shape: (num_local_experts) . Source code in d9d/module/block/moe/communications/base.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 @abc . abstractmethod def dispatch ( self , hidden_states : torch . Tensor , topk_ids : torch . Tensor , topk_weights : torch . Tensor ) -> tuple [ torch . Tensor , torch . Tensor , torch . Tensor ]: \"\"\" Prepares and routes local hidden states to their target experts (possibly on other workers). This process involves: 1. All-to-All Communication: Transfers hidden states to workers containing the assigned experts. States assigned to multiple experts are replicated. 2. Permutation: Sorts tokens by expert ID to prepare for Grouped GEMM. Args: hidden_states: Input tokens. Shape: `(num_tokens, hidden_size)`. topk_ids: Indices of the top-k experts selected for each token. Shape: `(num_tokens, k)`. topk_weights: Routing weights associated with the selected experts. Shape: `(num_tokens, k)`. Returns: A tuple containing: - Permuted hidden states received by this rank. Shape: `(num_received_tokens, hidden_size)`. - Permuted weights matching the hidden states order. Shape: `(num_received_tokens)`. - Expert count tensor indicating how many tokens each local expert received. Shape: `(num_local_experts)`. \"\"\" ... NoCommunicationHandler Bases: ExpertCommunicationHandler Handles MoE routing within a single device or when no cross-device routing is needed. This handler does not perform network operations. It only permutes elements mostly for local logical grouping or debugging. Source code in d9d/module/block/moe/communications/naive.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 class NoCommunicationHandler ( ExpertCommunicationHandler ): \"\"\" Handles MoE routing within a single device or when no cross-device routing is needed. This handler does not perform network operations. It only permutes elements mostly for local logical grouping or debugging. \"\"\" def __init__ ( self , num_experts : int ): \"\"\"Constructs the NoCommunicationHandler.\"\"\" self . _num_experts = num_experts self . _hidden_shape_before_permute : Size | None = None self . _unpermute_mapping : torch . Tensor | None = None def dispatch ( self , hidden_states : torch . Tensor , topk_ids : torch . Tensor , topk_weights : torch . Tensor ) -> tuple [ torch . Tensor , torch . Tensor , torch . Tensor ]: with torch . no_grad (): tokens_per_expert = torch . bincount ( topk_ids . flatten (), minlength = self . _num_experts ) . cpu () routing_map , routing_probs = fused_indices_to_multihot ( topk_ids , topk_weights , self . _num_experts ) self . _hidden_shape_before_permute = hidden_states . shape hidden_states , routing_probs , reverse_permute_map = moe_permute_with_probs ( hidden_states , routing_probs , routing_map , num_out_tokens = cast ( int , tokens_per_expert . sum () . item ()) ) self . _unpermute_mapping = reverse_permute_map return hidden_states , routing_probs , tokens_per_expert def combine ( self , hidden_states : torch . Tensor ) -> torch . Tensor : if self . _unpermute_mapping is None : raise ValueError ( \"Cannot run combine before running dispatch!\" ) hidden_states = moe_unpermute_mask ( hidden_states , self . _unpermute_mapping , restore_shape = self . _hidden_shape_before_permute , ) self . _unpermute_mapping = None self . _hidden_shape_before_permute = None return hidden_states __init__ ( num_experts ) Constructs the NoCommunicationHandler. Source code in d9d/module/block/moe/communications/naive.py 22 23 24 25 26 27 def __init__ ( self , num_experts : int ): \"\"\"Constructs the NoCommunicationHandler.\"\"\" self . _num_experts = num_experts self . _hidden_shape_before_permute : Size | None = None self . _unpermute_mapping : torch . Tensor | None = None","title":"Mixture of Experts (MoE)"},{"location":"modules/moe/#about","text":"The d9d.module.block.moe package provides a complete, high-performance implementation of Sparse Mixture-of-Experts layers.","title":"About"},{"location":"modules/moe/#expert-parallelism","text":"For information on setting up Expert Parallelism, see this page .","title":"Expert Parallelism"},{"location":"modules/moe/#features","text":"","title":"Features"},{"location":"modules/moe/#sparse-expert-router","text":"TopKRouter is a learnable router implementation. It computes routing probabilities in FP32 to ensure numeric stability.","title":"Sparse Expert Router"},{"location":"modules/moe/#sparse-expert-token-dispatcher","text":"ExpertCommunicationHandler is the messaging layer. NoCommunicationHandler is used by default for single-GPU or Tensor Parallel setups where no token movement is needed. DeepEpCommunicationHandler is enabled if using Expert Parallelism. It uses the DeepEP library for highly optimized all-to-all communication over NVLink/RDMA, enabling scaling to thousands of experts.","title":"Sparse Expert Token Dispatcher"},{"location":"modules/moe/#sparse-experts","text":"GroupedSwiGLU provides a sparse SwiGLU experts module implementation. Instead of looping over experts, it uses Grouped GEMM kernels to execute all experts in parallel, regardless of how many tokens each expert received. Uses efficient fused SiLU-Mul kernel.","title":"Sparse Experts"},{"location":"modules/moe/#kernel-benchmarks-bf16-h100","text":"","title":"Kernel Benchmarks (BF16, H100)"},{"location":"modules/moe/#shared-experts","text":"Currently not supported, feel free to contribute :)","title":"Shared Experts"},{"location":"modules/moe/#d9d.module.block.moe","text":"Provides building blocks for Mixture-of-Experts (MoE) architectures.","title":"moe"},{"location":"modules/moe/#d9d.module.block.moe.GroupedLinear","text":"Bases: Module , ModuleLateInit Applies a linear transformation using Grouped GEMM (Generalized Matrix Multiplication). This module allows efficient execution of multiple linear layers (experts) in parallel, where each expert processes a variable number of tokens. It is the computational core of the Mixture-of-Experts layer. Source code in d9d/module/block/moe/grouped_linear.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 class GroupedLinear ( nn . Module , ModuleLateInit ): \"\"\" Applies a linear transformation using Grouped GEMM (Generalized Matrix Multiplication). This module allows efficient execution of multiple linear layers (experts) in parallel, where each expert processes a variable number of tokens. It is the computational core of the Mixture-of-Experts layer. \"\"\" def __init__ ( self , n_groups : int , in_features : int , out_features : int , device : torch . device | str | None = None , dtype : torch . dtype | None = None , ): \"\"\" Constructs the GroupedLinear layer. Args: n_groups: Number of groups (experts). in_features: Input hidden size. out_features: Output hidden size. device: Target device. dtype: Target data type. \"\"\" super () . __init__ () self . weight = nn . Parameter ( torch . empty ( n_groups , in_features , out_features , device = device , dtype = dtype )) self . n_groups = n_groups self . in_features = in_features self . out_features = out_features self . reset_parameters () def forward ( self , x : torch . Tensor , x_groups : torch . Tensor ) -> torch . Tensor : \"\"\" Performs the grouped matrix multiplication. Args: x: Flattened input tensor containing tokens for all groups. Shape: `(total_tokens, in_features)`. x_groups: CPU Tensor indicating the number of tokens assigned to each group. Must sum to `total_tokens`. Shape: `(n_groups,)`. Returns: The output tensor. Shape: `(total_tokens, out_features)`. \"\"\" weight : torch . Tensor = self . weight if isinstance ( weight , DTensor ): weight = weight . to_local () return gmm ( x , weight , x_groups , a_grad_direction = GradDirection . inputs , b_grad_direction = GradDirection . weight ) def reset_parameters ( self ): \"\"\"Initializes weights using a uniform distribution based on input features.\"\"\" nn . init . uniform_ ( self . weight , - 1 / math . sqrt ( self . in_features ), 1 / math . sqrt ( self . in_features ))","title":"GroupedLinear"},{"location":"modules/moe/#d9d.module.block.moe.GroupedLinear.__init__","text":"Constructs the GroupedLinear layer. Parameters: Name Type Description Default n_groups int Number of groups (experts). required in_features int Input hidden size. required out_features int Output hidden size. required device device | str | None Target device. None dtype dtype | None Target data type. None Source code in d9d/module/block/moe/grouped_linear.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def __init__ ( self , n_groups : int , in_features : int , out_features : int , device : torch . device | str | None = None , dtype : torch . dtype | None = None , ): \"\"\" Constructs the GroupedLinear layer. Args: n_groups: Number of groups (experts). in_features: Input hidden size. out_features: Output hidden size. device: Target device. dtype: Target data type. \"\"\" super () . __init__ () self . weight = nn . Parameter ( torch . empty ( n_groups , in_features , out_features , device = device , dtype = dtype )) self . n_groups = n_groups self . in_features = in_features self . out_features = out_features self . reset_parameters ()","title":"__init__"},{"location":"modules/moe/#d9d.module.block.moe.GroupedLinear.forward","text":"Performs the grouped matrix multiplication. Parameters: Name Type Description Default x Tensor Flattened input tensor containing tokens for all groups. Shape: (total_tokens, in_features) . required x_groups Tensor CPU Tensor indicating the number of tokens assigned to each group. Must sum to total_tokens . Shape: (n_groups,) . required Returns: Type Description Tensor The output tensor. Shape: (total_tokens, out_features) . Source code in d9d/module/block/moe/grouped_linear.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def forward ( self , x : torch . Tensor , x_groups : torch . Tensor ) -> torch . Tensor : \"\"\" Performs the grouped matrix multiplication. Args: x: Flattened input tensor containing tokens for all groups. Shape: `(total_tokens, in_features)`. x_groups: CPU Tensor indicating the number of tokens assigned to each group. Must sum to `total_tokens`. Shape: `(n_groups,)`. Returns: The output tensor. Shape: `(total_tokens, out_features)`. \"\"\" weight : torch . Tensor = self . weight if isinstance ( weight , DTensor ): weight = weight . to_local () return gmm ( x , weight , x_groups , a_grad_direction = GradDirection . inputs , b_grad_direction = GradDirection . weight )","title":"forward"},{"location":"modules/moe/#d9d.module.block.moe.GroupedLinear.reset_parameters","text":"Initializes weights using a uniform distribution based on input features. Source code in d9d/module/block/moe/grouped_linear.py 69 70 71 def reset_parameters ( self ): \"\"\"Initializes weights using a uniform distribution based on input features.\"\"\" nn . init . uniform_ ( self . weight , - 1 / math . sqrt ( self . in_features ), 1 / math . sqrt ( self . in_features ))","title":"reset_parameters"},{"location":"modules/moe/#d9d.module.block.moe.GroupedSwiGLU","text":"Bases: Module , ModuleLateInit Executes a collection of SwiGLU experts efficiently using Grouped GEMM. This module implements the architectural pattern: down_proj(SiLU(gate_proj(x)) * up_proj(x)) . It applies this operation across multiple discrete experts in parallel without padding or masking. Source code in d9d/module/block/moe/grouped_experts.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 class GroupedSwiGLU ( nn . Module , ModuleLateInit ): \"\"\" Executes a collection of SwiGLU experts efficiently using Grouped GEMM. This module implements the architectural pattern: `down_proj(SiLU(gate_proj(x)) * up_proj(x))`. It applies this operation across multiple discrete experts in parallel without padding or masking. \"\"\" def __init__ ( self , hidden_dim : int , intermediate_dim : int , num_experts : int ): \"\"\" Constructs the GroupedSwiGLU module. Args: hidden_dim: Dimensionality of the input and output hidden states. intermediate_dim: Dimensionality of the intermediate projection. num_experts: Total number of experts managed by this local instance. \"\"\" super () . __init__ () self . _num_experts = num_experts self . gate_proj = GroupedLinear ( num_experts , hidden_dim , intermediate_dim ) self . up_proj = GroupedLinear ( num_experts , hidden_dim , intermediate_dim ) self . down_proj = GroupedLinear ( num_experts , intermediate_dim , hidden_dim ) def forward ( self , permuted_x : torch . Tensor , permuted_probs : torch . Tensor , tokens_per_expert : torch . Tensor , ) -> torch . Tensor : \"\"\" Computes expert outputs for sorted input tokens. Args: permuted_x: Input tokens sorted by their assigned expert. Shape: `(total_tokens, hidden_dim)`. permuted_probs: Routing weights/probabilities corresponding to the sorted tokens. Shape: `(total_tokens)`. tokens_per_expert: Number of tokens assigned to each consecutive expert. It is a CPU tensor. Shape: `(num_experts)`. Returns: The computed and weighted output tokens (still permuted). Shape: `(total_tokens, hidden_dim)`. \"\"\" if permuted_x . numel () == 0 : # handle cases when there are no routed experts to this instance return permuted_x probs = permuted_probs [:, None ] . to ( permuted_x . dtype ) values = self . down_proj ( silu_mul ( self . gate_proj ( permuted_x , tokens_per_expert ), self . up_proj ( permuted_x , tokens_per_expert )), tokens_per_expert , ) return probs * values def reset_parameters ( self ): \"\"\"Resets parameters for all internal linear projections.\"\"\" self . gate_proj . reset_parameters () self . up_proj . reset_parameters () self . down_proj . reset_parameters ()","title":"GroupedSwiGLU"},{"location":"modules/moe/#d9d.module.block.moe.GroupedSwiGLU.__init__","text":"Constructs the GroupedSwiGLU module. Parameters: Name Type Description Default hidden_dim int Dimensionality of the input and output hidden states. required intermediate_dim int Dimensionality of the intermediate projection. required num_experts int Total number of experts managed by this local instance. required Source code in d9d/module/block/moe/grouped_experts.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def __init__ ( self , hidden_dim : int , intermediate_dim : int , num_experts : int ): \"\"\" Constructs the GroupedSwiGLU module. Args: hidden_dim: Dimensionality of the input and output hidden states. intermediate_dim: Dimensionality of the intermediate projection. num_experts: Total number of experts managed by this local instance. \"\"\" super () . __init__ () self . _num_experts = num_experts self . gate_proj = GroupedLinear ( num_experts , hidden_dim , intermediate_dim ) self . up_proj = GroupedLinear ( num_experts , hidden_dim , intermediate_dim ) self . down_proj = GroupedLinear ( num_experts , intermediate_dim , hidden_dim )","title":"__init__"},{"location":"modules/moe/#d9d.module.block.moe.GroupedSwiGLU.forward","text":"Computes expert outputs for sorted input tokens. Parameters: Name Type Description Default permuted_x Tensor Input tokens sorted by their assigned expert. Shape: (total_tokens, hidden_dim) . required permuted_probs Tensor Routing weights/probabilities corresponding to the sorted tokens. Shape: (total_tokens) . required tokens_per_expert Tensor Number of tokens assigned to each consecutive expert. It is a CPU tensor. Shape: (num_experts) . required Returns: Name Type Description Tensor The computed and weighted output tokens (still permuted). Shape Tensor (total_tokens, hidden_dim) . Source code in d9d/module/block/moe/grouped_experts.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def forward ( self , permuted_x : torch . Tensor , permuted_probs : torch . Tensor , tokens_per_expert : torch . Tensor , ) -> torch . Tensor : \"\"\" Computes expert outputs for sorted input tokens. Args: permuted_x: Input tokens sorted by their assigned expert. Shape: `(total_tokens, hidden_dim)`. permuted_probs: Routing weights/probabilities corresponding to the sorted tokens. Shape: `(total_tokens)`. tokens_per_expert: Number of tokens assigned to each consecutive expert. It is a CPU tensor. Shape: `(num_experts)`. Returns: The computed and weighted output tokens (still permuted). Shape: `(total_tokens, hidden_dim)`. \"\"\" if permuted_x . numel () == 0 : # handle cases when there are no routed experts to this instance return permuted_x probs = permuted_probs [:, None ] . to ( permuted_x . dtype ) values = self . down_proj ( silu_mul ( self . gate_proj ( permuted_x , tokens_per_expert ), self . up_proj ( permuted_x , tokens_per_expert )), tokens_per_expert , ) return probs * values","title":"forward"},{"location":"modules/moe/#d9d.module.block.moe.GroupedSwiGLU.reset_parameters","text":"Resets parameters for all internal linear projections. Source code in d9d/module/block/moe/grouped_experts.py 68 69 70 71 72 73 def reset_parameters ( self ): \"\"\"Resets parameters for all internal linear projections.\"\"\" self . gate_proj . reset_parameters () self . up_proj . reset_parameters () self . down_proj . reset_parameters ()","title":"reset_parameters"},{"location":"modules/moe/#d9d.module.block.moe.MoELayer","text":"Bases: Module , ModuleLateInit A complete Mixture-of-Experts (MoE) block comprising routing, communication, and computation. This layer integrates: Router : Selects experts for each token. Communicator : Handles token dispatch to local or remote experts (EP). Experts : Performs parallelized computation (Grouped SwiGLU). Source code in d9d/module/block/moe/layer.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 class MoELayer ( nn . Module , ModuleLateInit ): \"\"\" A complete Mixture-of-Experts (MoE) block comprising routing, communication, and computation. This layer integrates: 1. **Router**: Selects experts for each token. 2. **Communicator**: Handles token dispatch to local or remote experts (EP). 3. **Experts**: Performs parallelized computation (Grouped SwiGLU). \"\"\" def __init__ ( self , hidden_dim : int , intermediate_dim_grouped : int , num_grouped_experts : int , top_k : int , router_renormalize_probabilities : bool , ): \"\"\" Constructs the MoELayer. Args: hidden_dim: Hidden size. intermediate_dim_grouped: Intermediate dimension for the Expert FFNs. num_grouped_experts: Total number of experts. top_k: Number of experts to route each token to. router_renormalize_probabilities: Configures router probability normalization behavior. \"\"\" super () . __init__ () self . router = TopKRouter ( dim = hidden_dim , num_experts = num_grouped_experts , top_k = top_k , renormalize_probabilities = router_renormalize_probabilities , ) self . grouped_experts = GroupedSwiGLU ( hidden_dim = hidden_dim , intermediate_dim = intermediate_dim_grouped , num_experts = num_grouped_experts ) self . _communicator : ExpertCommunicationHandler = NoCommunicationHandler ( num_grouped_experts ) self . _num_grouped_experts = num_grouped_experts self . _hidden_dim = hidden_dim self . tokens_per_expert = nn . Buffer ( torch . empty (( num_grouped_experts ,), dtype = torch . int64 ), persistent = False ) def enable_distributed_communicator ( self , group : ProcessGroup ): \"\"\" Switches from local no-op communication to distributed DeepEP communication. This should be called during model initialization if the model is running in a distributed Expert Parallel environment. Args: group: The PyTorch process group spanning the expert parallel ranks. \"\"\" communicator = DeepEpCommunicationHandler ( num_experts = self . _num_grouped_experts ) communicator . setup ( group , self . _hidden_dim , self . router . gate . weight . dtype ) self . _communicator = communicator @torch . no_grad () def _update_tokens_per_expert ( self , expert_indices : torch . Tensor ): self . tokens_per_expert . add_ ( expert_indices . view ( - 1 ) . bincount ( minlength = self . _num_grouped_experts )) @torch . no_grad () def reset_stats ( self ): \"\"\"Resets the expert load balancing counters.\"\"\" self . tokens_per_expert . zero_ () def forward ( self , hidden_states : torch . Tensor ) -> torch . Tensor : \"\"\" Routes tokens to experts, computes, and combines results. Args: hidden_states: Input tensor. Shape: `(batch_size, seq_len, hidden_dim)`. Returns: Output tensor combined from experts. Shape: `(batch_size, seq_len, hidden_dim)`. \"\"\" old_shape = hidden_states . shape hidden_states = hidden_states . reshape ( - 1 , hidden_states . shape [ - 1 ]) expert_indices , expert_scores = self . router ( hidden_states ) self . _update_tokens_per_expert ( expert_indices ) hidden_states , expert_scores , expert_count = self . _communicator . dispatch ( hidden_states , expert_indices , expert_scores ) hidden_states = self . grouped_experts ( hidden_states , expert_scores , expert_count ) hidden_states = self . _communicator . combine ( hidden_states ) hidden_states = hidden_states . reshape ( * old_shape ) return hidden_states def reset_parameters ( self ): \"\"\"Resets module parameters.\"\"\" self . router . reset_parameters () self . grouped_experts . reset_parameters () nn . init . zeros_ ( self . tokens_per_expert )","title":"MoELayer"},{"location":"modules/moe/#d9d.module.block.moe.MoELayer.__init__","text":"Constructs the MoELayer. Parameters: Name Type Description Default hidden_dim int Hidden size. required intermediate_dim_grouped int Intermediate dimension for the Expert FFNs. required num_grouped_experts int Total number of experts. required top_k int Number of experts to route each token to. required router_renormalize_probabilities bool Configures router probability normalization behavior. required Source code in d9d/module/block/moe/layer.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def __init__ ( self , hidden_dim : int , intermediate_dim_grouped : int , num_grouped_experts : int , top_k : int , router_renormalize_probabilities : bool , ): \"\"\" Constructs the MoELayer. Args: hidden_dim: Hidden size. intermediate_dim_grouped: Intermediate dimension for the Expert FFNs. num_grouped_experts: Total number of experts. top_k: Number of experts to route each token to. router_renormalize_probabilities: Configures router probability normalization behavior. \"\"\" super () . __init__ () self . router = TopKRouter ( dim = hidden_dim , num_experts = num_grouped_experts , top_k = top_k , renormalize_probabilities = router_renormalize_probabilities , ) self . grouped_experts = GroupedSwiGLU ( hidden_dim = hidden_dim , intermediate_dim = intermediate_dim_grouped , num_experts = num_grouped_experts ) self . _communicator : ExpertCommunicationHandler = NoCommunicationHandler ( num_grouped_experts ) self . _num_grouped_experts = num_grouped_experts self . _hidden_dim = hidden_dim self . tokens_per_expert = nn . Buffer ( torch . empty (( num_grouped_experts ,), dtype = torch . int64 ), persistent = False )","title":"__init__"},{"location":"modules/moe/#d9d.module.block.moe.MoELayer.enable_distributed_communicator","text":"Switches from local no-op communication to distributed DeepEP communication. This should be called during model initialization if the model is running in a distributed Expert Parallel environment. Parameters: Name Type Description Default group ProcessGroup The PyTorch process group spanning the expert parallel ranks. required Source code in d9d/module/block/moe/layer.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def enable_distributed_communicator ( self , group : ProcessGroup ): \"\"\" Switches from local no-op communication to distributed DeepEP communication. This should be called during model initialization if the model is running in a distributed Expert Parallel environment. Args: group: The PyTorch process group spanning the expert parallel ranks. \"\"\" communicator = DeepEpCommunicationHandler ( num_experts = self . _num_grouped_experts ) communicator . setup ( group , self . _hidden_dim , self . router . gate . weight . dtype ) self . _communicator = communicator","title":"enable_distributed_communicator"},{"location":"modules/moe/#d9d.module.block.moe.MoELayer.forward","text":"Routes tokens to experts, computes, and combines results. Parameters: Name Type Description Default hidden_states Tensor Input tensor. Shape: (batch_size, seq_len, hidden_dim) . required Returns: Type Description Tensor Output tensor combined from experts. Shape: (batch_size, seq_len, hidden_dim) . Source code in d9d/module/block/moe/layer.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def forward ( self , hidden_states : torch . Tensor ) -> torch . Tensor : \"\"\" Routes tokens to experts, computes, and combines results. Args: hidden_states: Input tensor. Shape: `(batch_size, seq_len, hidden_dim)`. Returns: Output tensor combined from experts. Shape: `(batch_size, seq_len, hidden_dim)`. \"\"\" old_shape = hidden_states . shape hidden_states = hidden_states . reshape ( - 1 , hidden_states . shape [ - 1 ]) expert_indices , expert_scores = self . router ( hidden_states ) self . _update_tokens_per_expert ( expert_indices ) hidden_states , expert_scores , expert_count = self . _communicator . dispatch ( hidden_states , expert_indices , expert_scores ) hidden_states = self . grouped_experts ( hidden_states , expert_scores , expert_count ) hidden_states = self . _communicator . combine ( hidden_states ) hidden_states = hidden_states . reshape ( * old_shape ) return hidden_states","title":"forward"},{"location":"modules/moe/#d9d.module.block.moe.MoELayer.reset_parameters","text":"Resets module parameters. Source code in d9d/module/block/moe/layer.py 114 115 116 117 118 119 def reset_parameters ( self ): \"\"\"Resets module parameters.\"\"\" self . router . reset_parameters () self . grouped_experts . reset_parameters () nn . init . zeros_ ( self . tokens_per_expert )","title":"reset_parameters"},{"location":"modules/moe/#d9d.module.block.moe.MoELayer.reset_stats","text":"Resets the expert load balancing counters. Source code in d9d/module/block/moe/layer.py 85 86 87 88 @torch . no_grad () def reset_stats ( self ): \"\"\"Resets the expert load balancing counters.\"\"\" self . tokens_per_expert . zero_ ()","title":"reset_stats"},{"location":"modules/moe/#d9d.module.block.moe.TopKRouter","text":"Bases: Module , ModuleLateInit Selects the top-K experts based on a learned gating mechanism. This router: Projects input tokens into expert space Applies softmax, optionally adds expert bias to influence selection Selects the experts with the highest probabilities Selected probabilities are then re-normalized to sum to 1 if needed. Source code in d9d/module/block/moe/router.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 class TopKRouter ( nn . Module , ModuleLateInit ): \"\"\" Selects the top-K experts based on a learned gating mechanism. This router: 1. Projects input tokens into expert space 2. Applies softmax, optionally adds expert bias to influence selection 3. Selects the experts with the highest probabilities 4. Selected probabilities are then re-normalized to sum to 1 if needed. \"\"\" def __init__ ( self , dim : int , num_experts : int , top_k : int , renormalize_probabilities : bool , enable_expert_bias : bool = False ): \"\"\" Constructs the TopKRouter. Args: dim: Input feature dimensionality. num_experts: Total number of experts to choose from. top_k: Number of experts to select for each token. renormalize_probabilities: If True, probabilities of selected experts will be renormalized to sum up to 1 enable_expert_bias: If True, adds a bias term to the routing scores before top-k selection. This can be used for loss-free load balancing. \"\"\" super () . __init__ () self . gate = nn . Linear ( dim , num_experts , bias = False ) self . expert_bias : nn . Buffer | None if enable_expert_bias : self . expert_bias = nn . Buffer ( torch . empty ( num_experts , dtype = torch . float32 ), persistent = True , ) else : self . expert_bias = None self . _num_experts = num_experts self . _top_k = top_k self . _renormalize_probabilities = renormalize_probabilities def forward ( self , hidden_states : torch . Tensor ) -> tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Calculates routing decisions for the input tokens. Args: hidden_states: Input tokens. Shape: `(num_tokens, dim)`. Returns: A tuple containing: - Selected expert indices. Shape: `(num_tokens, top_k)`. - Normalized routing weights for the selected experts. Shape: `(num_tokens, top_k)`. \"\"\" # scores shape (bs*slen, num_experts) # gate scores = self . gate ( hidden_states ) # and now do softmax (before top-k to be able to apply expert bias) scores = F . softmax ( scores , dim =- 1 , dtype = torch . float32 ) # select top-k if self . expert_bias is None : scores , selected_experts_indices = torch . topk ( scores , k = self . _top_k , dim =- 1 ) else : _ , selected_experts_indices = torch . topk ( scores + self . expert_bias , k = self . _top_k , dim =- 1 ) scores = scores . gather ( dim =- 1 , index = selected_experts_indices ) # re-normalize scores denominator = scores . sum ( dim =- 1 , keepdim = True ) + 1e-20 scores = scores / denominator return selected_experts_indices , scores def reset_parameters ( self ): \"\"\"Resets module parameters.\"\"\" if self . expert_bias is not None : nn . init . zeros_ ( self . expert_bias ) self . gate . reset_parameters ()","title":"TopKRouter"},{"location":"modules/moe/#d9d.module.block.moe.TopKRouter.__init__","text":"Constructs the TopKRouter. Parameters: Name Type Description Default dim int Input feature dimensionality. required num_experts int Total number of experts to choose from. required top_k int Number of experts to select for each token. required renormalize_probabilities bool If True, probabilities of selected experts will be renormalized to sum up to 1 required enable_expert_bias bool If True, adds a bias term to the routing scores before top-k selection. This can be used for loss-free load balancing. False Source code in d9d/module/block/moe/router.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def __init__ ( self , dim : int , num_experts : int , top_k : int , renormalize_probabilities : bool , enable_expert_bias : bool = False ): \"\"\" Constructs the TopKRouter. Args: dim: Input feature dimensionality. num_experts: Total number of experts to choose from. top_k: Number of experts to select for each token. renormalize_probabilities: If True, probabilities of selected experts will be renormalized to sum up to 1 enable_expert_bias: If True, adds a bias term to the routing scores before top-k selection. This can be used for loss-free load balancing. \"\"\" super () . __init__ () self . gate = nn . Linear ( dim , num_experts , bias = False ) self . expert_bias : nn . Buffer | None if enable_expert_bias : self . expert_bias = nn . Buffer ( torch . empty ( num_experts , dtype = torch . float32 ), persistent = True , ) else : self . expert_bias = None self . _num_experts = num_experts self . _top_k = top_k self . _renormalize_probabilities = renormalize_probabilities","title":"__init__"},{"location":"modules/moe/#d9d.module.block.moe.TopKRouter.forward","text":"Calculates routing decisions for the input tokens. Parameters: Name Type Description Default hidden_states Tensor Input tokens. Shape: (num_tokens, dim) . required Returns: Type Description Tensor A tuple containing: Tensor Selected expert indices. Shape: (num_tokens, top_k) . tuple [ Tensor , Tensor ] Normalized routing weights for the selected experts. Shape: (num_tokens, top_k) . Source code in d9d/module/block/moe/router.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def forward ( self , hidden_states : torch . Tensor ) -> tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Calculates routing decisions for the input tokens. Args: hidden_states: Input tokens. Shape: `(num_tokens, dim)`. Returns: A tuple containing: - Selected expert indices. Shape: `(num_tokens, top_k)`. - Normalized routing weights for the selected experts. Shape: `(num_tokens, top_k)`. \"\"\" # scores shape (bs*slen, num_experts) # gate scores = self . gate ( hidden_states ) # and now do softmax (before top-k to be able to apply expert bias) scores = F . softmax ( scores , dim =- 1 , dtype = torch . float32 ) # select top-k if self . expert_bias is None : scores , selected_experts_indices = torch . topk ( scores , k = self . _top_k , dim =- 1 ) else : _ , selected_experts_indices = torch . topk ( scores + self . expert_bias , k = self . _top_k , dim =- 1 ) scores = scores . gather ( dim =- 1 , index = selected_experts_indices ) # re-normalize scores denominator = scores . sum ( dim =- 1 , keepdim = True ) + 1e-20 scores = scores / denominator return selected_experts_indices , scores","title":"forward"},{"location":"modules/moe/#d9d.module.block.moe.TopKRouter.reset_parameters","text":"Resets module parameters. Source code in d9d/module/block/moe/router.py 86 87 88 89 90 91 def reset_parameters ( self ): \"\"\"Resets module parameters.\"\"\" if self . expert_bias is not None : nn . init . zeros_ ( self . expert_bias ) self . gate . reset_parameters ()","title":"reset_parameters"},{"location":"modules/moe/#d9d.module.block.moe.communications","text":"Provides communication strategies for Mixture-of-Experts routing operations.","title":"communications"},{"location":"modules/moe/#d9d.module.block.moe.communications.DeepEpCommunicationHandler","text":"Bases: ExpertCommunicationHandler Handles MoE communication using the high-performance DeepEP library. Source code in d9d/module/block/moe/communications/deepep.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 class DeepEpCommunicationHandler ( ExpertCommunicationHandler ): \"\"\"Handles MoE communication using the high-performance DeepEP library.\"\"\" def __init__ ( self , num_experts : int ): \"\"\"Constructs the DeepEpCommunicationHandler.\"\"\" self . _num_experts = num_experts self . _num_experts_per_shard = None # late-initialization # == fields saved for post-dispatch == self . _handle = None self . _hidden_shape_before_permute = None self . _unpermute_mapping = None def setup ( self , group : torch . distributed . ProcessGroup , hidden_size : int , hidden_dtype : torch . dtype ): \"\"\" Initializes the backend buffer and calculates expert sharding. Args: group: The process group containing all experts. hidden_size: Dimensionality of the hidden states. hidden_dtype: Data type of the hidden states. \"\"\" init_deepep_buffer ( group , hidden_size * hidden_dtype . itemsize ) if self . _num_experts % group . size () != 0 : raise ValueError ( \"num_experts must be divisible by distributed group size\" ) self . _num_experts_per_shard = self . _num_experts // group . size () def dispatch ( self , hidden_states : torch . Tensor , topk_ids : torch . Tensor , topk_weights : torch . Tensor ) -> tuple [ torch . Tensor , torch . Tensor , torch . Tensor ]: ( hidden_states , topk_ids , topk_weights , tokens_per_expert , handle ) = DeepEpDispatch . apply ( hidden_states , topk_ids , topk_weights , self . _num_experts ) routing_map , routing_probs = fused_indices_to_multihot ( topk_ids , topk_weights , self . _num_experts_per_shard ) self . _hidden_shape_before_permute = hidden_states . shape hidden_states , routing_probs , reverse_permute_map = moe_permute_with_probs ( hidden_states , routing_probs , routing_map , num_out_tokens = tokens_per_expert . sum () . item () ) self . _handle = handle self . _unpermute_mapping = reverse_permute_map return hidden_states , routing_probs , tokens_per_expert def combine ( self , hidden_states : torch . Tensor ) -> torch . Tensor : if self . _handle is None : raise ValueError ( \"you fucked up moe communication order: you should dispatch first and after that combine\" ) hidden_states = moe_unpermute_mask ( hidden_states , self . _unpermute_mapping , restore_shape = self . _hidden_shape_before_permute , ) hidden_states = DeepEpCombine . apply ( hidden_states , self . _handle ) self . _handle = None self . _unpermute_mapping = None self . _hidden_shape_before_permute = None return hidden_states","title":"DeepEpCommunicationHandler"},{"location":"modules/moe/#d9d.module.block.moe.communications.DeepEpCommunicationHandler.__init__","text":"Constructs the DeepEpCommunicationHandler. Source code in d9d/module/block/moe/communications/deepep.py 160 161 162 163 164 165 166 167 168 169 170 def __init__ ( self , num_experts : int ): \"\"\"Constructs the DeepEpCommunicationHandler.\"\"\" self . _num_experts = num_experts self . _num_experts_per_shard = None # late-initialization # == fields saved for post-dispatch == self . _handle = None self . _hidden_shape_before_permute = None self . _unpermute_mapping = None","title":"__init__"},{"location":"modules/moe/#d9d.module.block.moe.communications.DeepEpCommunicationHandler.setup","text":"Initializes the backend buffer and calculates expert sharding. Parameters: Name Type Description Default group ProcessGroup The process group containing all experts. required hidden_size int Dimensionality of the hidden states. required hidden_dtype dtype Data type of the hidden states. required Source code in d9d/module/block/moe/communications/deepep.py 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 def setup ( self , group : torch . distributed . ProcessGroup , hidden_size : int , hidden_dtype : torch . dtype ): \"\"\" Initializes the backend buffer and calculates expert sharding. Args: group: The process group containing all experts. hidden_size: Dimensionality of the hidden states. hidden_dtype: Data type of the hidden states. \"\"\" init_deepep_buffer ( group , hidden_size * hidden_dtype . itemsize ) if self . _num_experts % group . size () != 0 : raise ValueError ( \"num_experts must be divisible by distributed group size\" ) self . _num_experts_per_shard = self . _num_experts // group . size ()","title":"setup"},{"location":"modules/moe/#d9d.module.block.moe.communications.ExpertCommunicationHandler","text":"Bases: ABC Abstract base class for Mixture-of-Experts communication strategies. Source code in d9d/module/block/moe/communications/base.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 class ExpertCommunicationHandler ( abc . ABC ): \"\"\"Abstract base class for Mixture-of-Experts communication strategies.\"\"\" @abc . abstractmethod def dispatch ( self , hidden_states : torch . Tensor , topk_ids : torch . Tensor , topk_weights : torch . Tensor ) -> tuple [ torch . Tensor , torch . Tensor , torch . Tensor ]: \"\"\" Prepares and routes local hidden states to their target experts (possibly on other workers). This process involves: 1. All-to-All Communication: Transfers hidden states to workers containing the assigned experts. States assigned to multiple experts are replicated. 2. Permutation: Sorts tokens by expert ID to prepare for Grouped GEMM. Args: hidden_states: Input tokens. Shape: `(num_tokens, hidden_size)`. topk_ids: Indices of the top-k experts selected for each token. Shape: `(num_tokens, k)`. topk_weights: Routing weights associated with the selected experts. Shape: `(num_tokens, k)`. Returns: A tuple containing: - Permuted hidden states received by this rank. Shape: `(num_received_tokens, hidden_size)`. - Permuted weights matching the hidden states order. Shape: `(num_received_tokens)`. - Expert count tensor indicating how many tokens each local expert received. Shape: `(num_local_experts)`. \"\"\" ... @abc . abstractmethod def combine ( self , hidden_states : torch . Tensor ) -> torch . Tensor : \"\"\" Restores hidden states to their original order and location. Undoes the permutation and performs the reverse All-to-All communication to return processed results to the workers that originated the requests. Args: hidden_states: The processed hidden states. Shape: `(num_received_tokens, hidden_size)`. Returns: The combined hidden states with the original shape and order. Shape: `(num_tokens, hidden_size)`. \"\"\" ...","title":"ExpertCommunicationHandler"},{"location":"modules/moe/#d9d.module.block.moe.communications.ExpertCommunicationHandler.combine","text":"Restores hidden states to their original order and location. Undoes the permutation and performs the reverse All-to-All communication to return processed results to the workers that originated the requests. Parameters: Name Type Description Default hidden_states Tensor The processed hidden states. Shape: (num_received_tokens, hidden_size) . required Returns: Type Description Tensor The combined hidden states with the original shape and order. Shape: (num_tokens, hidden_size) . Source code in d9d/module/block/moe/communications/base.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 @abc . abstractmethod def combine ( self , hidden_states : torch . Tensor ) -> torch . Tensor : \"\"\" Restores hidden states to their original order and location. Undoes the permutation and performs the reverse All-to-All communication to return processed results to the workers that originated the requests. Args: hidden_states: The processed hidden states. Shape: `(num_received_tokens, hidden_size)`. Returns: The combined hidden states with the original shape and order. Shape: `(num_tokens, hidden_size)`. \"\"\" ...","title":"combine"},{"location":"modules/moe/#d9d.module.block.moe.communications.ExpertCommunicationHandler.dispatch","text":"Prepares and routes local hidden states to their target experts (possibly on other workers). This process involves: All-to-All Communication: Transfers hidden states to workers containing the assigned experts. States assigned to multiple experts are replicated. Permutation: Sorts tokens by expert ID to prepare for Grouped GEMM. Parameters: Name Type Description Default hidden_states Tensor Input tokens. Shape: (num_tokens, hidden_size) . required topk_ids Tensor Indices of the top-k experts selected for each token. Shape: (num_tokens, k) . required topk_weights Tensor Routing weights associated with the selected experts. Shape: (num_tokens, k) . required Returns: Type Description Tensor A tuple containing: Tensor Permuted hidden states received by this rank. Shape: (num_received_tokens, hidden_size) . Tensor Permuted weights matching the hidden states order. Shape: (num_received_tokens) . tuple [ Tensor , Tensor , Tensor ] Expert count tensor indicating how many tokens each local expert received. Shape: (num_local_experts) . Source code in d9d/module/block/moe/communications/base.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 @abc . abstractmethod def dispatch ( self , hidden_states : torch . Tensor , topk_ids : torch . Tensor , topk_weights : torch . Tensor ) -> tuple [ torch . Tensor , torch . Tensor , torch . Tensor ]: \"\"\" Prepares and routes local hidden states to their target experts (possibly on other workers). This process involves: 1. All-to-All Communication: Transfers hidden states to workers containing the assigned experts. States assigned to multiple experts are replicated. 2. Permutation: Sorts tokens by expert ID to prepare for Grouped GEMM. Args: hidden_states: Input tokens. Shape: `(num_tokens, hidden_size)`. topk_ids: Indices of the top-k experts selected for each token. Shape: `(num_tokens, k)`. topk_weights: Routing weights associated with the selected experts. Shape: `(num_tokens, k)`. Returns: A tuple containing: - Permuted hidden states received by this rank. Shape: `(num_received_tokens, hidden_size)`. - Permuted weights matching the hidden states order. Shape: `(num_received_tokens)`. - Expert count tensor indicating how many tokens each local expert received. Shape: `(num_local_experts)`. \"\"\" ...","title":"dispatch"},{"location":"modules/moe/#d9d.module.block.moe.communications.NoCommunicationHandler","text":"Bases: ExpertCommunicationHandler Handles MoE routing within a single device or when no cross-device routing is needed. This handler does not perform network operations. It only permutes elements mostly for local logical grouping or debugging. Source code in d9d/module/block/moe/communications/naive.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 class NoCommunicationHandler ( ExpertCommunicationHandler ): \"\"\" Handles MoE routing within a single device or when no cross-device routing is needed. This handler does not perform network operations. It only permutes elements mostly for local logical grouping or debugging. \"\"\" def __init__ ( self , num_experts : int ): \"\"\"Constructs the NoCommunicationHandler.\"\"\" self . _num_experts = num_experts self . _hidden_shape_before_permute : Size | None = None self . _unpermute_mapping : torch . Tensor | None = None def dispatch ( self , hidden_states : torch . Tensor , topk_ids : torch . Tensor , topk_weights : torch . Tensor ) -> tuple [ torch . Tensor , torch . Tensor , torch . Tensor ]: with torch . no_grad (): tokens_per_expert = torch . bincount ( topk_ids . flatten (), minlength = self . _num_experts ) . cpu () routing_map , routing_probs = fused_indices_to_multihot ( topk_ids , topk_weights , self . _num_experts ) self . _hidden_shape_before_permute = hidden_states . shape hidden_states , routing_probs , reverse_permute_map = moe_permute_with_probs ( hidden_states , routing_probs , routing_map , num_out_tokens = cast ( int , tokens_per_expert . sum () . item ()) ) self . _unpermute_mapping = reverse_permute_map return hidden_states , routing_probs , tokens_per_expert def combine ( self , hidden_states : torch . Tensor ) -> torch . Tensor : if self . _unpermute_mapping is None : raise ValueError ( \"Cannot run combine before running dispatch!\" ) hidden_states = moe_unpermute_mask ( hidden_states , self . _unpermute_mapping , restore_shape = self . _hidden_shape_before_permute , ) self . _unpermute_mapping = None self . _hidden_shape_before_permute = None return hidden_states","title":"NoCommunicationHandler"},{"location":"modules/moe/#d9d.module.block.moe.communications.NoCommunicationHandler.__init__","text":"Constructs the NoCommunicationHandler. Source code in d9d/module/block/moe/communications/naive.py 22 23 24 25 26 27 def __init__ ( self , num_experts : int ): \"\"\"Constructs the NoCommunicationHandler.\"\"\" self . _num_experts = num_experts self . _hidden_shape_before_permute : Size | None = None self . _unpermute_mapping : torch . Tensor | None = None","title":"__init__"},{"location":"modules/positional/","text":"About The d9d.module.block.positional package manages positional encoding logic. Features Rotary Positional Encoding Rotary Positional Encoding from RoFormer. See RotaryEmbeddingProvider and RotaryEmbeddingApplicator classes. First one is typically bound to a model class and is used for providing (cos, sin) embedding tensors for specified position IDs. Second one is typically bound to attention module implementation and is used for modifying query and key states in runtime. d9d.module.block.positional Provides modules for positional embeddings, such as Rotary Positional Embeddings. RotaryEmbeddingApplicator Bases: Module Applies Rotary Positional Embeddings (RoPE) to Q and K projections. Source code in d9d/module/block/positional/rope.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 class RotaryEmbeddingApplicator ( nn . Module ): \"\"\"Applies Rotary Positional Embeddings (RoPE) to Q and K projections.\"\"\" def __init__ ( self ): \"\"\" Constructs RotaryEmbeddingApplicator object. \"\"\" super () . __init__ () def forward ( self , query_states : torch . Tensor , key_states : torch . Tensor , position_embedding_cos : torch . Tensor , position_embedding_sin : torch . Tensor , ) -> tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Rotates query and key states using provided cosine and sine embeddings. Args: query_states: Query tensor. Shape: `(batch, n_heads, seq_len, head_dim)`. key_states: Key tensor. Shape: `(batch, n_kv_heads, seq_len, head_dim)`. position_embedding_cos: Cosine values for positions. Shape: `(batch, seq_len, head_dim)`. position_embedding_sin: Sine values for positions. Shape: `(batch, seq_len, head_dim)`. Returns: A tuple containing the rotated query and key tensors. \"\"\" query_states , key_states = _apply_rotary_pos_emb ( query_states , key_states , position_embedding_cos , position_embedding_sin ) return query_states , key_states __init__ () Constructs RotaryEmbeddingApplicator object. Source code in d9d/module/block/positional/rope.py 111 112 113 114 115 116 def __init__ ( self ): \"\"\" Constructs RotaryEmbeddingApplicator object. \"\"\" super () . __init__ () forward ( query_states , key_states , position_embedding_cos , position_embedding_sin ) Rotates query and key states using provided cosine and sine embeddings. Parameters: Name Type Description Default query_states Tensor Query tensor. Shape: (batch, n_heads, seq_len, head_dim) . required key_states Tensor Key tensor. Shape: (batch, n_kv_heads, seq_len, head_dim) . required position_embedding_cos Tensor Cosine values for positions. Shape: (batch, seq_len, head_dim) . required position_embedding_sin Tensor Sine values for positions. Shape: (batch, seq_len, head_dim) . required Returns: Type Description tuple [ Tensor , Tensor ] A tuple containing the rotated query and key tensors. Source code in d9d/module/block/positional/rope.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def forward ( self , query_states : torch . Tensor , key_states : torch . Tensor , position_embedding_cos : torch . Tensor , position_embedding_sin : torch . Tensor , ) -> tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Rotates query and key states using provided cosine and sine embeddings. Args: query_states: Query tensor. Shape: `(batch, n_heads, seq_len, head_dim)`. key_states: Key tensor. Shape: `(batch, n_kv_heads, seq_len, head_dim)`. position_embedding_cos: Cosine values for positions. Shape: `(batch, seq_len, head_dim)`. position_embedding_sin: Sine values for positions. Shape: `(batch, seq_len, head_dim)`. Returns: A tuple containing the rotated query and key tensors. \"\"\" query_states , key_states = _apply_rotary_pos_emb ( query_states , key_states , position_embedding_cos , position_embedding_sin ) return query_states , key_states RotaryEmbeddingProvider Bases: Module , ModuleLateInit Module that manages and provides Rotary Positional Embeddings. Source code in d9d/module/block/positional/rope.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 class RotaryEmbeddingProvider ( nn . Module , ModuleLateInit ): \"\"\"Module that manages and provides Rotary Positional Embeddings.\"\"\" def __init__ ( self , rope_base : int , head_dim : int , max_position_ids : int ): \"\"\"Constructs the RotaryEmbeddingProvider.\"\"\" super () . __init__ () self . _rope_base = rope_base self . _head_dim = head_dim self . _max_position_ids = max_position_ids self . cos_emb = nn . Buffer ( torch . empty ( max_position_ids , head_dim ), persistent = False ) self . sin_emb = nn . Buffer ( torch . empty ( max_position_ids , head_dim ), persistent = False ) def forward ( self , position_ids : torch . Tensor ) -> tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Retrieves cached cosine and sine embeddings for specific positions. Args: position_ids: Tensor of position indices. Returns: A tuple of (cos, sin) tensors aligned with the input positions. \"\"\" return self . cos_emb [ position_ids ], self . sin_emb [ position_ids ] def reset_parameters ( self ): with torch . no_grad (): cos , sin = prepare_rotary_cos_sin_emb ( rope_base = self . _rope_base , head_dim = self . _head_dim , max_position_ids = self . _max_position_ids , device = self . cos_emb . device , dtype = self . cos_emb . dtype , ) self . cos_emb . data = cos self . sin_emb . data = sin __init__ ( rope_base , head_dim , max_position_ids ) Constructs the RotaryEmbeddingProvider. Source code in d9d/module/block/positional/rope.py 56 57 58 59 60 61 62 63 64 def __init__ ( self , rope_base : int , head_dim : int , max_position_ids : int ): \"\"\"Constructs the RotaryEmbeddingProvider.\"\"\" super () . __init__ () self . _rope_base = rope_base self . _head_dim = head_dim self . _max_position_ids = max_position_ids self . cos_emb = nn . Buffer ( torch . empty ( max_position_ids , head_dim ), persistent = False ) self . sin_emb = nn . Buffer ( torch . empty ( max_position_ids , head_dim ), persistent = False ) forward ( position_ids ) Retrieves cached cosine and sine embeddings for specific positions. Parameters: Name Type Description Default position_ids Tensor Tensor of position indices. required Returns: Type Description tuple [ Tensor , Tensor ] A tuple of (cos, sin) tensors aligned with the input positions. Source code in d9d/module/block/positional/rope.py 66 67 68 69 70 71 72 73 74 75 76 77 def forward ( self , position_ids : torch . Tensor ) -> tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Retrieves cached cosine and sine embeddings for specific positions. Args: position_ids: Tensor of position indices. Returns: A tuple of (cos, sin) tensors aligned with the input positions. \"\"\" return self . cos_emb [ position_ids ], self . sin_emb [ position_ids ]","title":"Positional Embeddings"},{"location":"modules/positional/#about","text":"The d9d.module.block.positional package manages positional encoding logic.","title":"About"},{"location":"modules/positional/#features","text":"","title":"Features"},{"location":"modules/positional/#rotary-positional-encoding","text":"Rotary Positional Encoding from RoFormer. See RotaryEmbeddingProvider and RotaryEmbeddingApplicator classes. First one is typically bound to a model class and is used for providing (cos, sin) embedding tensors for specified position IDs. Second one is typically bound to attention module implementation and is used for modifying query and key states in runtime.","title":"Rotary Positional Encoding"},{"location":"modules/positional/#d9d.module.block.positional","text":"Provides modules for positional embeddings, such as Rotary Positional Embeddings.","title":"positional"},{"location":"modules/positional/#d9d.module.block.positional.RotaryEmbeddingApplicator","text":"Bases: Module Applies Rotary Positional Embeddings (RoPE) to Q and K projections. Source code in d9d/module/block/positional/rope.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 class RotaryEmbeddingApplicator ( nn . Module ): \"\"\"Applies Rotary Positional Embeddings (RoPE) to Q and K projections.\"\"\" def __init__ ( self ): \"\"\" Constructs RotaryEmbeddingApplicator object. \"\"\" super () . __init__ () def forward ( self , query_states : torch . Tensor , key_states : torch . Tensor , position_embedding_cos : torch . Tensor , position_embedding_sin : torch . Tensor , ) -> tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Rotates query and key states using provided cosine and sine embeddings. Args: query_states: Query tensor. Shape: `(batch, n_heads, seq_len, head_dim)`. key_states: Key tensor. Shape: `(batch, n_kv_heads, seq_len, head_dim)`. position_embedding_cos: Cosine values for positions. Shape: `(batch, seq_len, head_dim)`. position_embedding_sin: Sine values for positions. Shape: `(batch, seq_len, head_dim)`. Returns: A tuple containing the rotated query and key tensors. \"\"\" query_states , key_states = _apply_rotary_pos_emb ( query_states , key_states , position_embedding_cos , position_embedding_sin ) return query_states , key_states","title":"RotaryEmbeddingApplicator"},{"location":"modules/positional/#d9d.module.block.positional.RotaryEmbeddingApplicator.__init__","text":"Constructs RotaryEmbeddingApplicator object. Source code in d9d/module/block/positional/rope.py 111 112 113 114 115 116 def __init__ ( self ): \"\"\" Constructs RotaryEmbeddingApplicator object. \"\"\" super () . __init__ ()","title":"__init__"},{"location":"modules/positional/#d9d.module.block.positional.RotaryEmbeddingApplicator.forward","text":"Rotates query and key states using provided cosine and sine embeddings. Parameters: Name Type Description Default query_states Tensor Query tensor. Shape: (batch, n_heads, seq_len, head_dim) . required key_states Tensor Key tensor. Shape: (batch, n_kv_heads, seq_len, head_dim) . required position_embedding_cos Tensor Cosine values for positions. Shape: (batch, seq_len, head_dim) . required position_embedding_sin Tensor Sine values for positions. Shape: (batch, seq_len, head_dim) . required Returns: Type Description tuple [ Tensor , Tensor ] A tuple containing the rotated query and key tensors. Source code in d9d/module/block/positional/rope.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def forward ( self , query_states : torch . Tensor , key_states : torch . Tensor , position_embedding_cos : torch . Tensor , position_embedding_sin : torch . Tensor , ) -> tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Rotates query and key states using provided cosine and sine embeddings. Args: query_states: Query tensor. Shape: `(batch, n_heads, seq_len, head_dim)`. key_states: Key tensor. Shape: `(batch, n_kv_heads, seq_len, head_dim)`. position_embedding_cos: Cosine values for positions. Shape: `(batch, seq_len, head_dim)`. position_embedding_sin: Sine values for positions. Shape: `(batch, seq_len, head_dim)`. Returns: A tuple containing the rotated query and key tensors. \"\"\" query_states , key_states = _apply_rotary_pos_emb ( query_states , key_states , position_embedding_cos , position_embedding_sin ) return query_states , key_states","title":"forward"},{"location":"modules/positional/#d9d.module.block.positional.RotaryEmbeddingProvider","text":"Bases: Module , ModuleLateInit Module that manages and provides Rotary Positional Embeddings. Source code in d9d/module/block/positional/rope.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 class RotaryEmbeddingProvider ( nn . Module , ModuleLateInit ): \"\"\"Module that manages and provides Rotary Positional Embeddings.\"\"\" def __init__ ( self , rope_base : int , head_dim : int , max_position_ids : int ): \"\"\"Constructs the RotaryEmbeddingProvider.\"\"\" super () . __init__ () self . _rope_base = rope_base self . _head_dim = head_dim self . _max_position_ids = max_position_ids self . cos_emb = nn . Buffer ( torch . empty ( max_position_ids , head_dim ), persistent = False ) self . sin_emb = nn . Buffer ( torch . empty ( max_position_ids , head_dim ), persistent = False ) def forward ( self , position_ids : torch . Tensor ) -> tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Retrieves cached cosine and sine embeddings for specific positions. Args: position_ids: Tensor of position indices. Returns: A tuple of (cos, sin) tensors aligned with the input positions. \"\"\" return self . cos_emb [ position_ids ], self . sin_emb [ position_ids ] def reset_parameters ( self ): with torch . no_grad (): cos , sin = prepare_rotary_cos_sin_emb ( rope_base = self . _rope_base , head_dim = self . _head_dim , max_position_ids = self . _max_position_ids , device = self . cos_emb . device , dtype = self . cos_emb . dtype , ) self . cos_emb . data = cos self . sin_emb . data = sin","title":"RotaryEmbeddingProvider"},{"location":"modules/positional/#d9d.module.block.positional.RotaryEmbeddingProvider.__init__","text":"Constructs the RotaryEmbeddingProvider. Source code in d9d/module/block/positional/rope.py 56 57 58 59 60 61 62 63 64 def __init__ ( self , rope_base : int , head_dim : int , max_position_ids : int ): \"\"\"Constructs the RotaryEmbeddingProvider.\"\"\" super () . __init__ () self . _rope_base = rope_base self . _head_dim = head_dim self . _max_position_ids = max_position_ids self . cos_emb = nn . Buffer ( torch . empty ( max_position_ids , head_dim ), persistent = False ) self . sin_emb = nn . Buffer ( torch . empty ( max_position_ids , head_dim ), persistent = False )","title":"__init__"},{"location":"modules/positional/#d9d.module.block.positional.RotaryEmbeddingProvider.forward","text":"Retrieves cached cosine and sine embeddings for specific positions. Parameters: Name Type Description Default position_ids Tensor Tensor of position indices. required Returns: Type Description tuple [ Tensor , Tensor ] A tuple of (cos, sin) tensors aligned with the input positions. Source code in d9d/module/block/positional/rope.py 66 67 68 69 70 71 72 73 74 75 76 77 def forward ( self , position_ids : torch . Tensor ) -> tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Retrieves cached cosine and sine embeddings for specific positions. Args: position_ids: Tensor of position indices. Returns: A tuple of (cos, sin) tensors aligned with the input positions. \"\"\" return self . cos_emb [ position_ids ], self . sin_emb [ position_ids ]","title":"forward"},{"location":"optimizer/stochastic/","text":"What is Stochastic Rounding and Why is It Useful For Optimization Standard floating-point casting (e.g., tensor.to(torch.bfloat16) ) typically utilizes Round-to-Nearest-Even . This method is statistically biased. When training models in reduced precision (like BF16), standard \"Round to Nearest\" operations can lead to stalling. If a weight update is smaller than the smallest representable difference for a given float value, the update disappears completely. Stochastic Rounding replaces rigid rounding with a probabilistic approach: for instance, if a value \\(x\\) is \\(30\\%\\) of the way between representable numbers \\(A\\) and \\(B\\) , it has a \\(30\\%\\) chance of rounding to \\(B\\) and \\(70\\%\\) chance of rounding to \\(A\\) . Over multiple updates, the statistical expectation matches the true high-precision value \\(E[Round(x)] = x\\) , allowing training to converge even when individual updates are technically \"too small\" for the format. For more information, please refer to: Zamirai, Pedram, et al. \u201cRevisiting BFloat16 Training.\u201d Version 2 Ozkara, Kaan, et al. \u201cStochastic Rounding for LLM Training: Theory and Practice.\u201d About This module provides optimizers for low precision training with stochastic rounding using highly optimized Triton kernels. Benchmarks All the benchmarks were performed on a single NVDIA H100 80GB GPU. copy_fp32_to_bf16_stochastic_ adamw_stochastic_bf16_ d9d.optim.stochastic StochasticAdamW Bases: Optimizer Implements the AdamW algorithm with Stochastic Rounding. This optimizer is designed to handle stochastic rounding primarily for BF16 training, leveraging a custom kernel. Parameters must be in BF16. Gradients could be both in BF16 and FP32. It natively supports PyTorch distributed DTensor parameters. It maintains its own random number generator state to ensure reproducibility. Source code in d9d/optim/stochastic/adamw.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 class StochasticAdamW ( Optimizer ): \"\"\"Implements the AdamW algorithm with Stochastic Rounding. This optimizer is designed to handle stochastic rounding primarily for BF16 training, leveraging a custom kernel. Parameters must be in BF16. Gradients could be both in BF16 and FP32. It natively supports PyTorch distributed ``DTensor`` parameters. It maintains its own random number generator state to ensure reproducibility. \"\"\" def __init__ ( self , params : ParamsT , lr : float , betas : tuple [ float , float ] = ( 0.9 , 0.999 ), eps : float = 1e-8 , weight_decay : float = 1e-2 , generator : torch . Generator | None = None , state_dtype : torch . dtype = torch . float32 , ): \"\"\"Constructs a new StochasticAdamW optimizer. Args: params: Iterable of parameters to optimize or dicts defining parameter groups. lr: Learning rate. betas: Coefficients used for computing running averages of gradient and its square. eps: Term added to the denominator to improve numerical stability. weight_decay: Weight decay coefficient. generator: Pseudorandom number generator for stochastic rounding. If None, a new generator is created and seeded from the main PyTorch generator. state_dtype: Data Type to use for the optimizer states. \"\"\" if lr <= 0 : raise ValueError ( f \"Invalid learning rate: { lr } \" ) if eps <= 0 : raise ValueError ( f \"Invalid epsilon value: { eps } \" ) if not 0.0 <= betas [ 0 ] < 1.0 : raise ValueError ( f \"Invalid beta parameter at index 0: { betas [ 0 ] } \" ) if not 0.0 <= betas [ 1 ] < 1.0 : raise ValueError ( f \"Invalid beta parameter at index 1: { betas [ 1 ] } \" ) if weight_decay < 0 : raise ValueError ( f \"Invalid weight_decay value: { weight_decay } \" ) if generator is None : generator = torch . Generator ( device = \"cpu\" ) # make the generator fork from pytorch's main generator seed = cast ( int , torch . randint ( 0 , 2 ** 32 , ( 1 ,)) . item ()) generator . manual_seed ( seed ) self . _generator = generator defaults = { \"lr\" : lr , \"betas\" : betas , \"eps\" : eps , \"weight_decay\" : weight_decay , \"state_dtype\" : state_dtype , } super () . __init__ ( params , defaults ) def state_dict ( self ) -> StateDict : state_dict = super () . state_dict () state_dict [ _GENERATOR_STATE_KEY ] = self . _generator . get_state () return state_dict def load_state_dict ( self , state_dict : StateDict ) -> None : if _GENERATOR_STATE_KEY in state_dict : self . _generator . set_state ( state_dict . pop ( _GENERATOR_STATE_KEY )) super () . load_state_dict ( state_dict ) @torch . no_grad () def step ( self , closure : None = None ) -> None : # type: ignore[override] if closure is not None : raise ValueError ( \"Closure is not supported\" ) for group in self . param_groups : lr = group [ \"lr\" ] beta1 , beta2 = group [ \"betas\" ] eps = group [ \"eps\" ] weight_decay = group [ \"weight_decay\" ] state_dtype = group [ \"state_dtype\" ] for p in group [ \"params\" ]: if p . grad is None : continue grad = p . grad if grad . is_sparse : raise RuntimeError ( \"StochasticAdamW does not support sparse gradients\" ) state = self . state [ p ] # State Initialization if len ( state ) == 0 : state [ \"step\" ] = 0 state [ \"exp_avg\" ] = _new_buffer ( p , dtype_override = state_dtype ) state [ \"exp_avg_sq\" ] = _new_buffer ( p , dtype_override = state_dtype ) state [ \"step\" ] += 1 exp_avg = state [ \"exp_avg\" ] exp_avg_sq = state [ \"exp_avg_sq\" ] adamw_stochastic_bf16_ ( params = _tensor_to_local ( p ), grads = _tensor_to_local ( grad ), exp_avg = _tensor_to_local ( exp_avg ), exp_avg_sq = _tensor_to_local ( exp_avg_sq ), lr = lr , beta1 = beta1 , beta2 = beta2 , eps = eps , weight_decay = weight_decay , step = state [ \"step\" ], generator = self . _generator , ) __init__ ( params , lr , betas = ( 0.9 , 0.999 ), eps = 1e-08 , weight_decay = 0.01 , generator = None , state_dtype = torch . float32 ) Constructs a new StochasticAdamW optimizer. Parameters: Name Type Description Default params ParamsT Iterable of parameters to optimize or dicts defining parameter groups. required lr float Learning rate. required betas tuple [ float , float ] Coefficients used for computing running averages of gradient and its square. (0.9, 0.999) eps float Term added to the denominator to improve numerical stability. 1e-08 weight_decay float Weight decay coefficient. 0.01 generator Generator | None Pseudorandom number generator for stochastic rounding. If None, a new generator is created and seeded from the main PyTorch generator. None state_dtype dtype Data Type to use for the optimizer states. float32 Source code in d9d/optim/stochastic/adamw.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def __init__ ( self , params : ParamsT , lr : float , betas : tuple [ float , float ] = ( 0.9 , 0.999 ), eps : float = 1e-8 , weight_decay : float = 1e-2 , generator : torch . Generator | None = None , state_dtype : torch . dtype = torch . float32 , ): \"\"\"Constructs a new StochasticAdamW optimizer. Args: params: Iterable of parameters to optimize or dicts defining parameter groups. lr: Learning rate. betas: Coefficients used for computing running averages of gradient and its square. eps: Term added to the denominator to improve numerical stability. weight_decay: Weight decay coefficient. generator: Pseudorandom number generator for stochastic rounding. If None, a new generator is created and seeded from the main PyTorch generator. state_dtype: Data Type to use for the optimizer states. \"\"\" if lr <= 0 : raise ValueError ( f \"Invalid learning rate: { lr } \" ) if eps <= 0 : raise ValueError ( f \"Invalid epsilon value: { eps } \" ) if not 0.0 <= betas [ 0 ] < 1.0 : raise ValueError ( f \"Invalid beta parameter at index 0: { betas [ 0 ] } \" ) if not 0.0 <= betas [ 1 ] < 1.0 : raise ValueError ( f \"Invalid beta parameter at index 1: { betas [ 1 ] } \" ) if weight_decay < 0 : raise ValueError ( f \"Invalid weight_decay value: { weight_decay } \" ) if generator is None : generator = torch . Generator ( device = \"cpu\" ) # make the generator fork from pytorch's main generator seed = cast ( int , torch . randint ( 0 , 2 ** 32 , ( 1 ,)) . item ()) generator . manual_seed ( seed ) self . _generator = generator defaults = { \"lr\" : lr , \"betas\" : betas , \"eps\" : eps , \"weight_decay\" : weight_decay , \"state_dtype\" : state_dtype , } super () . __init__ ( params , defaults ) d9d.kernel.stochastic Utilities for stochastic type casting (e.g., FP32 to BF16). adamw_stochastic_bf16_ ( params , grads , exp_avg , exp_avg_sq , lr , beta1 , beta2 , eps , weight_decay , step , generator = None ) Performs a single in-place AdamW optimization step. It is specifically designed for scenarios where parameters are stored in BFloat16. To mitigate precision loss during the parameter update, it utilizes stochastic rounding when casting FP32 calculation results back to BFloat16. This function supports mixed precision for gradients and optimizer states (they can be either FP32 or BFloat16). Parameters: Name Type Description Default params Tensor The tensor of model parameters to update. Must be BFloat16 and contiguous. required grads Tensor The gradient tensor. required exp_avg Tensor The exponential moving average of gradient values (first moment). required exp_avg_sq Tensor The exponential moving average of squared gradient values (second moment). required lr float The learning rate. required beta1 float Decay rate for the first moment estimate. required beta2 float Decay rate for the second moment estimate. required eps float Term added to the denominator to improve numerical stability. required weight_decay float Weight decay coefficient. required step int The current optimization step count, used for bias correction. required generator Generator | None PyTorch random number generator used to create the seed for stochastic rounding. None Raises: Type Description ValueError If main parameters are not BFloat16, if input tensor shapes do not match, if input tensors are not contiguous (for those that require in-place modification), if the optimizer states (exp_avg, exp_avg_sq) have different dtypes. Source code in d9d/kernel/stochastic/adamw_step.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 def adamw_stochastic_bf16_ ( # noqa: C901 params : torch . Tensor , grads : torch . Tensor , exp_avg : torch . Tensor , exp_avg_sq : torch . Tensor , lr : float , beta1 : float , beta2 : float , eps : float , weight_decay : float , step : int , generator : torch . Generator | None = None , ) -> None : \"\"\" Performs a single in-place AdamW optimization step. It is specifically designed for scenarios where parameters are stored in BFloat16. To mitigate precision loss during the parameter update, it utilizes stochastic rounding when casting FP32 calculation results back to BFloat16. This function supports mixed precision for gradients and optimizer states (they can be either FP32 or BFloat16). Args: params: The tensor of model parameters to update. Must be BFloat16 and contiguous. grads: The gradient tensor. exp_avg: The exponential moving average of gradient values (first moment). exp_avg_sq: The exponential moving average of squared gradient values (second moment). lr: The learning rate. beta1: Decay rate for the first moment estimate. beta2: Decay rate for the second moment estimate. eps: Term added to the denominator to improve numerical stability. weight_decay: Weight decay coefficient. step: The current optimization step count, used for bias correction. generator: PyTorch random number generator used to create the seed for stochastic rounding. Raises: ValueError: If main parameters are not BFloat16, if input tensor shapes do not match, if input tensors are not contiguous (for those that require in-place modification), if the optimizer states (exp_avg, exp_avg_sq) have different dtypes. \"\"\" # check shape equality if grads . shape != params . shape : raise ValueError ( \"Shape mismatch between grads and params.\" ) if exp_avg . shape != params . shape : raise ValueError ( \"Shape mismatch between exp_avg state and params.\" ) if exp_avg_sq . shape != params . shape : raise ValueError ( \"Shape mismatch between exp_avg_sq state and params.\" ) # check params if params . dtype != torch . bfloat16 : raise ValueError ( \"Params must be BFloat16 for this kernel.\" ) if not params . is_contiguous (): raise ValueError ( \"Params must be contiguous since it is an in-place kernel.\" ) # check grads if not grads . is_contiguous (): grads = grads . contiguous () # check states if not exp_avg . is_contiguous (): raise ValueError ( \"Exp_avg state must be contiguous since it is an in-place kernel.\" ) if not exp_avg_sq . is_contiguous (): raise ValueError ( \"Exp_avg_sq state must be contiguous since it is an in-place kernel.\" ) if exp_avg . dtype != exp_avg_sq . dtype : raise ValueError ( \"States have different dtypes.\" ) n_elements = params . numel () grad_is_bf16 = grads . dtype == torch . bfloat16 state_is_bf16 = exp_avg . dtype == torch . bfloat16 # Generate random seed seed = torch . randint ( 0 , 2 ** 31 - 1 , ( 1 ,), device = \"cpu\" , generator = generator ) . item () def _grid ( meta : dict [ str , int ]) -> tuple [ int , ... ]: return ( triton . cdiv ( n_elements , meta [ \"BLOCK_SIZE\" ]),) _adamw_stochastic_bf16_kernel [ _grid ]( params , grads , exp_avg , exp_avg_sq , n_elements , lr , beta1 , beta2 , eps , weight_decay , step , seed , GRAD_IS_BF16 = grad_is_bf16 , STATE_IS_BF16 = state_is_bf16 , ) copy_fp32_to_bf16_stochastic_ ( target , source , generator = None ) Copies elements from a Float32 tensor to a BFloat16 tensor using stochastic rounding. Unlike standard round-to-nearest casting, stochastic rounding probabilistically rounds numbers up or down based on the value of the bits being truncated. This preserves the expected value of the tensor (E[round(x)] = x), which is crucial for accumulating gradients or parameters in low precision without stagnation. This operation is performed in-place on the target tensor. Parameters: Name Type Description Default target Tensor The output tensor where results are written. Must be of type BFloat16 and contiguous. required source Tensor The input tensor containing values to copy. Must be of type Float32. required generator Generator | None An optional PyTorch RNG generator to strictly control the random noise used for rounding. None Returns: Type Description Tensor The target tensor, modified in-place. Raises: Type Description ValueError If target is not contiguous, if source/target shapes do not match, or if dtypes are not FP32 and BF16 respectively. Source code in d9d/kernel/stochastic/copy.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def copy_fp32_to_bf16_stochastic_ ( target : torch . Tensor , source : torch . Tensor , generator : torch . Generator | None = None ) -> torch . Tensor : \"\"\" Copies elements from a Float32 tensor to a BFloat16 tensor using stochastic rounding. Unlike standard round-to-nearest casting, stochastic rounding probabilistically rounds numbers up or down based on the value of the bits being truncated. This preserves the expected value of the tensor (E[round(x)] = x), which is crucial for accumulating gradients or parameters in low precision without stagnation. This operation is performed in-place on the target tensor. Args: target: The output tensor where results are written. Must be of type BFloat16 and contiguous. source: The input tensor containing values to copy. Must be of type Float32. generator: An optional PyTorch RNG generator to strictly control the random noise used for rounding. Returns: The target tensor, modified in-place. Raises: ValueError: If target is not contiguous, if source/target shapes do not match, or if dtypes are not FP32 and BF16 respectively. \"\"\" if not source . is_contiguous (): source = source . contiguous () if not target . is_contiguous (): raise ValueError ( \"Since this is an in-place operation, target should be a contiguous tensor!\" ) if source . shape != target . shape : raise ValueError ( \"Source and Target Tensors are of different shapes\" ) if source . dtype != torch . float32 : raise ValueError ( \"Source must be Float32\" ) if target . dtype != torch . bfloat16 : raise ValueError ( \"Target must be BFloat16\" ) n_elements = source . numel () # Generate a random seed for this specific kernel launch seed = torch . randint ( 0 , 2 ** 31 - 1 , ( 1 ,), device = \"cpu\" , generator = generator ) . item () def _grid ( meta : dict [ str , int ]) -> tuple [ int , ... ]: return ( triton . cdiv ( n_elements , meta [ \"BLOCK_SIZE\" ]),) _copy_fp32_to_bf16_kernel [ _grid ]( source , target , n_elements , seed ) return target","title":"Stochastic Optimizers"},{"location":"optimizer/stochastic/#what-is-stochastic-rounding-and-why-is-it-useful-for-optimization","text":"Standard floating-point casting (e.g., tensor.to(torch.bfloat16) ) typically utilizes Round-to-Nearest-Even . This method is statistically biased. When training models in reduced precision (like BF16), standard \"Round to Nearest\" operations can lead to stalling. If a weight update is smaller than the smallest representable difference for a given float value, the update disappears completely. Stochastic Rounding replaces rigid rounding with a probabilistic approach: for instance, if a value \\(x\\) is \\(30\\%\\) of the way between representable numbers \\(A\\) and \\(B\\) , it has a \\(30\\%\\) chance of rounding to \\(B\\) and \\(70\\%\\) chance of rounding to \\(A\\) . Over multiple updates, the statistical expectation matches the true high-precision value \\(E[Round(x)] = x\\) , allowing training to converge even when individual updates are technically \"too small\" for the format. For more information, please refer to: Zamirai, Pedram, et al. \u201cRevisiting BFloat16 Training.\u201d Version 2 Ozkara, Kaan, et al. \u201cStochastic Rounding for LLM Training: Theory and Practice.\u201d","title":"What is Stochastic Rounding and Why is It Useful For Optimization"},{"location":"optimizer/stochastic/#about","text":"This module provides optimizers for low precision training with stochastic rounding using highly optimized Triton kernels.","title":"About"},{"location":"optimizer/stochastic/#benchmarks","text":"All the benchmarks were performed on a single NVDIA H100 80GB GPU.","title":"Benchmarks"},{"location":"optimizer/stochastic/#copy_fp32_to_bf16_stochastic_","text":"","title":"copy_fp32_to_bf16_stochastic_"},{"location":"optimizer/stochastic/#adamw_stochastic_bf16_","text":"","title":"adamw_stochastic_bf16_"},{"location":"optimizer/stochastic/#d9d.optim.stochastic","text":"","title":"stochastic"},{"location":"optimizer/stochastic/#d9d.optim.stochastic.StochasticAdamW","text":"Bases: Optimizer Implements the AdamW algorithm with Stochastic Rounding. This optimizer is designed to handle stochastic rounding primarily for BF16 training, leveraging a custom kernel. Parameters must be in BF16. Gradients could be both in BF16 and FP32. It natively supports PyTorch distributed DTensor parameters. It maintains its own random number generator state to ensure reproducibility. Source code in d9d/optim/stochastic/adamw.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 class StochasticAdamW ( Optimizer ): \"\"\"Implements the AdamW algorithm with Stochastic Rounding. This optimizer is designed to handle stochastic rounding primarily for BF16 training, leveraging a custom kernel. Parameters must be in BF16. Gradients could be both in BF16 and FP32. It natively supports PyTorch distributed ``DTensor`` parameters. It maintains its own random number generator state to ensure reproducibility. \"\"\" def __init__ ( self , params : ParamsT , lr : float , betas : tuple [ float , float ] = ( 0.9 , 0.999 ), eps : float = 1e-8 , weight_decay : float = 1e-2 , generator : torch . Generator | None = None , state_dtype : torch . dtype = torch . float32 , ): \"\"\"Constructs a new StochasticAdamW optimizer. Args: params: Iterable of parameters to optimize or dicts defining parameter groups. lr: Learning rate. betas: Coefficients used for computing running averages of gradient and its square. eps: Term added to the denominator to improve numerical stability. weight_decay: Weight decay coefficient. generator: Pseudorandom number generator for stochastic rounding. If None, a new generator is created and seeded from the main PyTorch generator. state_dtype: Data Type to use for the optimizer states. \"\"\" if lr <= 0 : raise ValueError ( f \"Invalid learning rate: { lr } \" ) if eps <= 0 : raise ValueError ( f \"Invalid epsilon value: { eps } \" ) if not 0.0 <= betas [ 0 ] < 1.0 : raise ValueError ( f \"Invalid beta parameter at index 0: { betas [ 0 ] } \" ) if not 0.0 <= betas [ 1 ] < 1.0 : raise ValueError ( f \"Invalid beta parameter at index 1: { betas [ 1 ] } \" ) if weight_decay < 0 : raise ValueError ( f \"Invalid weight_decay value: { weight_decay } \" ) if generator is None : generator = torch . Generator ( device = \"cpu\" ) # make the generator fork from pytorch's main generator seed = cast ( int , torch . randint ( 0 , 2 ** 32 , ( 1 ,)) . item ()) generator . manual_seed ( seed ) self . _generator = generator defaults = { \"lr\" : lr , \"betas\" : betas , \"eps\" : eps , \"weight_decay\" : weight_decay , \"state_dtype\" : state_dtype , } super () . __init__ ( params , defaults ) def state_dict ( self ) -> StateDict : state_dict = super () . state_dict () state_dict [ _GENERATOR_STATE_KEY ] = self . _generator . get_state () return state_dict def load_state_dict ( self , state_dict : StateDict ) -> None : if _GENERATOR_STATE_KEY in state_dict : self . _generator . set_state ( state_dict . pop ( _GENERATOR_STATE_KEY )) super () . load_state_dict ( state_dict ) @torch . no_grad () def step ( self , closure : None = None ) -> None : # type: ignore[override] if closure is not None : raise ValueError ( \"Closure is not supported\" ) for group in self . param_groups : lr = group [ \"lr\" ] beta1 , beta2 = group [ \"betas\" ] eps = group [ \"eps\" ] weight_decay = group [ \"weight_decay\" ] state_dtype = group [ \"state_dtype\" ] for p in group [ \"params\" ]: if p . grad is None : continue grad = p . grad if grad . is_sparse : raise RuntimeError ( \"StochasticAdamW does not support sparse gradients\" ) state = self . state [ p ] # State Initialization if len ( state ) == 0 : state [ \"step\" ] = 0 state [ \"exp_avg\" ] = _new_buffer ( p , dtype_override = state_dtype ) state [ \"exp_avg_sq\" ] = _new_buffer ( p , dtype_override = state_dtype ) state [ \"step\" ] += 1 exp_avg = state [ \"exp_avg\" ] exp_avg_sq = state [ \"exp_avg_sq\" ] adamw_stochastic_bf16_ ( params = _tensor_to_local ( p ), grads = _tensor_to_local ( grad ), exp_avg = _tensor_to_local ( exp_avg ), exp_avg_sq = _tensor_to_local ( exp_avg_sq ), lr = lr , beta1 = beta1 , beta2 = beta2 , eps = eps , weight_decay = weight_decay , step = state [ \"step\" ], generator = self . _generator , )","title":"StochasticAdamW"},{"location":"optimizer/stochastic/#d9d.optim.stochastic.StochasticAdamW.__init__","text":"Constructs a new StochasticAdamW optimizer. Parameters: Name Type Description Default params ParamsT Iterable of parameters to optimize or dicts defining parameter groups. required lr float Learning rate. required betas tuple [ float , float ] Coefficients used for computing running averages of gradient and its square. (0.9, 0.999) eps float Term added to the denominator to improve numerical stability. 1e-08 weight_decay float Weight decay coefficient. 0.01 generator Generator | None Pseudorandom number generator for stochastic rounding. If None, a new generator is created and seeded from the main PyTorch generator. None state_dtype dtype Data Type to use for the optimizer states. float32 Source code in d9d/optim/stochastic/adamw.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def __init__ ( self , params : ParamsT , lr : float , betas : tuple [ float , float ] = ( 0.9 , 0.999 ), eps : float = 1e-8 , weight_decay : float = 1e-2 , generator : torch . Generator | None = None , state_dtype : torch . dtype = torch . float32 , ): \"\"\"Constructs a new StochasticAdamW optimizer. Args: params: Iterable of parameters to optimize or dicts defining parameter groups. lr: Learning rate. betas: Coefficients used for computing running averages of gradient and its square. eps: Term added to the denominator to improve numerical stability. weight_decay: Weight decay coefficient. generator: Pseudorandom number generator for stochastic rounding. If None, a new generator is created and seeded from the main PyTorch generator. state_dtype: Data Type to use for the optimizer states. \"\"\" if lr <= 0 : raise ValueError ( f \"Invalid learning rate: { lr } \" ) if eps <= 0 : raise ValueError ( f \"Invalid epsilon value: { eps } \" ) if not 0.0 <= betas [ 0 ] < 1.0 : raise ValueError ( f \"Invalid beta parameter at index 0: { betas [ 0 ] } \" ) if not 0.0 <= betas [ 1 ] < 1.0 : raise ValueError ( f \"Invalid beta parameter at index 1: { betas [ 1 ] } \" ) if weight_decay < 0 : raise ValueError ( f \"Invalid weight_decay value: { weight_decay } \" ) if generator is None : generator = torch . Generator ( device = \"cpu\" ) # make the generator fork from pytorch's main generator seed = cast ( int , torch . randint ( 0 , 2 ** 32 , ( 1 ,)) . item ()) generator . manual_seed ( seed ) self . _generator = generator defaults = { \"lr\" : lr , \"betas\" : betas , \"eps\" : eps , \"weight_decay\" : weight_decay , \"state_dtype\" : state_dtype , } super () . __init__ ( params , defaults )","title":"__init__"},{"location":"optimizer/stochastic/#d9d.kernel.stochastic","text":"Utilities for stochastic type casting (e.g., FP32 to BF16).","title":"stochastic"},{"location":"optimizer/stochastic/#d9d.kernel.stochastic.adamw_stochastic_bf16_","text":"Performs a single in-place AdamW optimization step. It is specifically designed for scenarios where parameters are stored in BFloat16. To mitigate precision loss during the parameter update, it utilizes stochastic rounding when casting FP32 calculation results back to BFloat16. This function supports mixed precision for gradients and optimizer states (they can be either FP32 or BFloat16). Parameters: Name Type Description Default params Tensor The tensor of model parameters to update. Must be BFloat16 and contiguous. required grads Tensor The gradient tensor. required exp_avg Tensor The exponential moving average of gradient values (first moment). required exp_avg_sq Tensor The exponential moving average of squared gradient values (second moment). required lr float The learning rate. required beta1 float Decay rate for the first moment estimate. required beta2 float Decay rate for the second moment estimate. required eps float Term added to the denominator to improve numerical stability. required weight_decay float Weight decay coefficient. required step int The current optimization step count, used for bias correction. required generator Generator | None PyTorch random number generator used to create the seed for stochastic rounding. None Raises: Type Description ValueError If main parameters are not BFloat16, if input tensor shapes do not match, if input tensors are not contiguous (for those that require in-place modification), if the optimizer states (exp_avg, exp_avg_sq) have different dtypes. Source code in d9d/kernel/stochastic/adamw_step.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 def adamw_stochastic_bf16_ ( # noqa: C901 params : torch . Tensor , grads : torch . Tensor , exp_avg : torch . Tensor , exp_avg_sq : torch . Tensor , lr : float , beta1 : float , beta2 : float , eps : float , weight_decay : float , step : int , generator : torch . Generator | None = None , ) -> None : \"\"\" Performs a single in-place AdamW optimization step. It is specifically designed for scenarios where parameters are stored in BFloat16. To mitigate precision loss during the parameter update, it utilizes stochastic rounding when casting FP32 calculation results back to BFloat16. This function supports mixed precision for gradients and optimizer states (they can be either FP32 or BFloat16). Args: params: The tensor of model parameters to update. Must be BFloat16 and contiguous. grads: The gradient tensor. exp_avg: The exponential moving average of gradient values (first moment). exp_avg_sq: The exponential moving average of squared gradient values (second moment). lr: The learning rate. beta1: Decay rate for the first moment estimate. beta2: Decay rate for the second moment estimate. eps: Term added to the denominator to improve numerical stability. weight_decay: Weight decay coefficient. step: The current optimization step count, used for bias correction. generator: PyTorch random number generator used to create the seed for stochastic rounding. Raises: ValueError: If main parameters are not BFloat16, if input tensor shapes do not match, if input tensors are not contiguous (for those that require in-place modification), if the optimizer states (exp_avg, exp_avg_sq) have different dtypes. \"\"\" # check shape equality if grads . shape != params . shape : raise ValueError ( \"Shape mismatch between grads and params.\" ) if exp_avg . shape != params . shape : raise ValueError ( \"Shape mismatch between exp_avg state and params.\" ) if exp_avg_sq . shape != params . shape : raise ValueError ( \"Shape mismatch between exp_avg_sq state and params.\" ) # check params if params . dtype != torch . bfloat16 : raise ValueError ( \"Params must be BFloat16 for this kernel.\" ) if not params . is_contiguous (): raise ValueError ( \"Params must be contiguous since it is an in-place kernel.\" ) # check grads if not grads . is_contiguous (): grads = grads . contiguous () # check states if not exp_avg . is_contiguous (): raise ValueError ( \"Exp_avg state must be contiguous since it is an in-place kernel.\" ) if not exp_avg_sq . is_contiguous (): raise ValueError ( \"Exp_avg_sq state must be contiguous since it is an in-place kernel.\" ) if exp_avg . dtype != exp_avg_sq . dtype : raise ValueError ( \"States have different dtypes.\" ) n_elements = params . numel () grad_is_bf16 = grads . dtype == torch . bfloat16 state_is_bf16 = exp_avg . dtype == torch . bfloat16 # Generate random seed seed = torch . randint ( 0 , 2 ** 31 - 1 , ( 1 ,), device = \"cpu\" , generator = generator ) . item () def _grid ( meta : dict [ str , int ]) -> tuple [ int , ... ]: return ( triton . cdiv ( n_elements , meta [ \"BLOCK_SIZE\" ]),) _adamw_stochastic_bf16_kernel [ _grid ]( params , grads , exp_avg , exp_avg_sq , n_elements , lr , beta1 , beta2 , eps , weight_decay , step , seed , GRAD_IS_BF16 = grad_is_bf16 , STATE_IS_BF16 = state_is_bf16 , )","title":"adamw_stochastic_bf16_"},{"location":"optimizer/stochastic/#d9d.kernel.stochastic.copy_fp32_to_bf16_stochastic_","text":"Copies elements from a Float32 tensor to a BFloat16 tensor using stochastic rounding. Unlike standard round-to-nearest casting, stochastic rounding probabilistically rounds numbers up or down based on the value of the bits being truncated. This preserves the expected value of the tensor (E[round(x)] = x), which is crucial for accumulating gradients or parameters in low precision without stagnation. This operation is performed in-place on the target tensor. Parameters: Name Type Description Default target Tensor The output tensor where results are written. Must be of type BFloat16 and contiguous. required source Tensor The input tensor containing values to copy. Must be of type Float32. required generator Generator | None An optional PyTorch RNG generator to strictly control the random noise used for rounding. None Returns: Type Description Tensor The target tensor, modified in-place. Raises: Type Description ValueError If target is not contiguous, if source/target shapes do not match, or if dtypes are not FP32 and BF16 respectively. Source code in d9d/kernel/stochastic/copy.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def copy_fp32_to_bf16_stochastic_ ( target : torch . Tensor , source : torch . Tensor , generator : torch . Generator | None = None ) -> torch . Tensor : \"\"\" Copies elements from a Float32 tensor to a BFloat16 tensor using stochastic rounding. Unlike standard round-to-nearest casting, stochastic rounding probabilistically rounds numbers up or down based on the value of the bits being truncated. This preserves the expected value of the tensor (E[round(x)] = x), which is crucial for accumulating gradients or parameters in low precision without stagnation. This operation is performed in-place on the target tensor. Args: target: The output tensor where results are written. Must be of type BFloat16 and contiguous. source: The input tensor containing values to copy. Must be of type Float32. generator: An optional PyTorch RNG generator to strictly control the random noise used for rounding. Returns: The target tensor, modified in-place. Raises: ValueError: If target is not contiguous, if source/target shapes do not match, or if dtypes are not FP32 and BF16 respectively. \"\"\" if not source . is_contiguous (): source = source . contiguous () if not target . is_contiguous (): raise ValueError ( \"Since this is an in-place operation, target should be a contiguous tensor!\" ) if source . shape != target . shape : raise ValueError ( \"Source and Target Tensors are of different shapes\" ) if source . dtype != torch . float32 : raise ValueError ( \"Source must be Float32\" ) if target . dtype != torch . bfloat16 : raise ValueError ( \"Target must be BFloat16\" ) n_elements = source . numel () # Generate a random seed for this specific kernel launch seed = torch . randint ( 0 , 2 ** 31 - 1 , ( 1 ,), device = \"cpu\" , generator = generator ) . item () def _grid ( meta : dict [ str , int ]) -> tuple [ int , ... ]: return ( triton . cdiv ( n_elements , meta [ \"BLOCK_SIZE\" ]),) _copy_fp32_to_bf16_kernel [ _grid ]( source , target , n_elements , seed ) return target","title":"copy_fp32_to_bf16_stochastic_"},{"location":"peft/0_index/","text":"About The d9d.peft package provides a flexible framework for fine-tuning models using parameter-efficient strategies or targeted full fine-tuning. Core Concepts Apply Before State Loading This package is deeply integrated with the model state mapping ecosystem. When you apply methods like LoRA, the model structure changes (e.g., a Linear layer becomes a LoRALinear wrapper). Consequently, the keys in your original checkpoint (e.g., layers.0.linear.weight ) no longer match the keys in the efficient model (e.g., layers.0.linear.base.weight ). d9d.peft automatically generates the necessary ModelStateMapper objects to load standard checkpoints into modified architectures. It is useful since framework user may apply a PEFT method to a model that was not initialized or horizontally distributed yet. Other PEFT frameworks usually want you to initialize model weights before applying PEFT which may break your horizontal parallelism setup logic or make it less reusable. Configuration All PEFT methods are driven by Pydantic configurations. This allows for custom validation of hyperparameters and easy serialization/deserialization. The Injection Lifecycle ( PeftMethod ) The framework operates on an Inject -> Train -> Merge lifecycle: Inject ( inject_peft_and_freeze ): The PeftMethod inspects the generic nn.Module . It locates target layers, replaces them with adapter layers (if necessary), and marks parameters that have to be trained with requires_grad=True . State Mapping : The injection process returns a ModelStateMapper object. This mapper describe how to map the original checkpoint keys to the new, injected model structure. Train : Here you train your model. Merge ( merge_peft ): Once training is complete, this method collapses the adapters back into the base weights, restoring the original architecture. d9d.peft Provides core logic for PEFT (Parameter-Efficient Fine-Tuning) application and base definitions. PeftInjectionResult dataclass Encapsulates the result of injecting a PEFT method into a model. Attributes: Name Type Description parameters_to_train list [ Parameter ] A list of parameters that should remain trainable. load_state_mappers list [ ModelStateMapper ] A list of mappers required to load pre-trained weights into the modified structure. Source code in d9d/peft/base.py 11 12 13 14 15 16 17 18 19 20 21 22 @dataclasses . dataclass ( slots = True ) class PeftInjectionResult : \"\"\" Encapsulates the result of injecting a PEFT method into a model. Attributes: parameters_to_train: A list of parameters that should remain trainable. load_state_mappers: A list of mappers required to load pre-trained weights into the modified structure. \"\"\" parameters_to_train : list [ nn . Parameter ] load_state_mappers : list [ ModelStateMapper ] PeftMethod Bases: ABC , Generic [ TConfig ] Abstract base class for all Parameter-Efficient Fine-Tuning methods. Source code in d9d/peft/base.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 class PeftMethod ( abc . ABC , Generic [ TConfig ]): \"\"\" Abstract base class for all Parameter-Efficient Fine-Tuning methods. \"\"\" @abc . abstractmethod def inject ( self , module : nn . Module ) -> PeftInjectionResult : \"\"\" Modifies the module in-place to apply the PEFT strategy. Args: module: The PyTorch module to modify. Returns: Result object containing trainable parameters and structure mappers. \"\"\" ... @abc . abstractmethod def merge ( self , module : nn . Module ): \"\"\" Merges the trained adapters back into the base model parameters. Args: module: The PyTorch module to update. \"\"\" ... @classmethod @abc . abstractmethod def from_config ( cls , config : TConfig ) -> Self : \"\"\" Creates an instance of the method from a configuration object. Args: config: The configuration object. Returns: An instance of the PeftMethod. \"\"\" ... from_config ( config ) abstractmethod classmethod Creates an instance of the method from a configuration object. Parameters: Name Type Description Default config TConfig The configuration object. required Returns: Type Description Self An instance of the PeftMethod. Source code in d9d/peft/base.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 @classmethod @abc . abstractmethod def from_config ( cls , config : TConfig ) -> Self : \"\"\" Creates an instance of the method from a configuration object. Args: config: The configuration object. Returns: An instance of the PeftMethod. \"\"\" ... inject ( module ) abstractmethod Modifies the module in-place to apply the PEFT strategy. Parameters: Name Type Description Default module Module The PyTorch module to modify. required Returns: Type Description PeftInjectionResult Result object containing trainable parameters and structure mappers. Source code in d9d/peft/base.py 33 34 35 36 37 38 39 40 41 42 43 44 @abc . abstractmethod def inject ( self , module : nn . Module ) -> PeftInjectionResult : \"\"\" Modifies the module in-place to apply the PEFT strategy. Args: module: The PyTorch module to modify. Returns: Result object containing trainable parameters and structure mappers. \"\"\" ... merge ( module ) abstractmethod Merges the trained adapters back into the base model parameters. Parameters: Name Type Description Default module Module The PyTorch module to update. required Source code in d9d/peft/base.py 46 47 48 49 50 51 52 53 54 55 @abc . abstractmethod def merge ( self , module : nn . Module ): \"\"\" Merges the trained adapters back into the base model parameters. Args: module: The PyTorch module to update. \"\"\" ... inject_peft_and_freeze ( method , module ) Applies a PEFT method to a module, freezes non-trained parameters, and prepares state mapping. This function performs three main steps: Sets requires_grad=False for all parameters in the module. Calls the method's inject to modify the model structure. Sets requires_grad=True for the parameters returned by the injection result. Parameters: Name Type Description Default method PeftMethod The PEFT method strategy to apply. required module Module The PyTorch module to modify. required Returns: Type Description ModelStateMapper A ModelStateMapper capable of loading checkpoint weights into the modified structure. Source code in d9d/peft/applicator.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def inject_peft_and_freeze ( method : PeftMethod , module : nn . Module ) -> ModelStateMapper : \"\"\" Applies a PEFT method to a module, freezes non-trained parameters, and prepares state mapping. This function performs three main steps: 1. Sets `requires_grad=False` for all parameters in the module. 2. Calls the method's `inject` to modify the model structure. 3. Sets `requires_grad=True` for the parameters returned by the injection result. Args: method: The PEFT method strategy to apply. module: The PyTorch module to modify. Returns: A ModelStateMapper capable of loading checkpoint weights into the modified structure. \"\"\" for param in module . parameters (): param . requires_grad = False result = method . inject ( module ) for param in result . parameters_to_train : param . requires_grad = True return ModelStateMapperParallel ( result . load_state_mappers ) merge_peft ( method , module ) Merges PEFT adaptations back into the base model weights. Parameters: Name Type Description Default method PeftMethod The PEFT method strategy originally applied. required module Module The PyTorch module to merge. required Source code in d9d/peft/applicator.py 38 39 40 41 42 43 44 45 46 47 def merge_peft ( method : PeftMethod , module : nn . Module ): \"\"\" Merges PEFT adaptations back into the base model weights. Args: method: The PEFT method strategy originally applied. module: The PyTorch module to merge. \"\"\" method . merge ( module )","title":"PEFT Overview"},{"location":"peft/0_index/#about","text":"The d9d.peft package provides a flexible framework for fine-tuning models using parameter-efficient strategies or targeted full fine-tuning.","title":"About"},{"location":"peft/0_index/#core-concepts","text":"","title":"Core Concepts"},{"location":"peft/0_index/#apply-before-state-loading","text":"This package is deeply integrated with the model state mapping ecosystem. When you apply methods like LoRA, the model structure changes (e.g., a Linear layer becomes a LoRALinear wrapper). Consequently, the keys in your original checkpoint (e.g., layers.0.linear.weight ) no longer match the keys in the efficient model (e.g., layers.0.linear.base.weight ). d9d.peft automatically generates the necessary ModelStateMapper objects to load standard checkpoints into modified architectures. It is useful since framework user may apply a PEFT method to a model that was not initialized or horizontally distributed yet. Other PEFT frameworks usually want you to initialize model weights before applying PEFT which may break your horizontal parallelism setup logic or make it less reusable.","title":"Apply Before State Loading"},{"location":"peft/0_index/#configuration","text":"All PEFT methods are driven by Pydantic configurations. This allows for custom validation of hyperparameters and easy serialization/deserialization.","title":"Configuration"},{"location":"peft/0_index/#the-injection-lifecycle-peftmethod","text":"The framework operates on an Inject -> Train -> Merge lifecycle: Inject ( inject_peft_and_freeze ): The PeftMethod inspects the generic nn.Module . It locates target layers, replaces them with adapter layers (if necessary), and marks parameters that have to be trained with requires_grad=True . State Mapping : The injection process returns a ModelStateMapper object. This mapper describe how to map the original checkpoint keys to the new, injected model structure. Train : Here you train your model. Merge ( merge_peft ): Once training is complete, this method collapses the adapters back into the base weights, restoring the original architecture.","title":"The Injection Lifecycle (PeftMethod)"},{"location":"peft/0_index/#d9d.peft","text":"Provides core logic for PEFT (Parameter-Efficient Fine-Tuning) application and base definitions.","title":"peft"},{"location":"peft/0_index/#d9d.peft.PeftInjectionResult","text":"Encapsulates the result of injecting a PEFT method into a model. Attributes: Name Type Description parameters_to_train list [ Parameter ] A list of parameters that should remain trainable. load_state_mappers list [ ModelStateMapper ] A list of mappers required to load pre-trained weights into the modified structure. Source code in d9d/peft/base.py 11 12 13 14 15 16 17 18 19 20 21 22 @dataclasses . dataclass ( slots = True ) class PeftInjectionResult : \"\"\" Encapsulates the result of injecting a PEFT method into a model. Attributes: parameters_to_train: A list of parameters that should remain trainable. load_state_mappers: A list of mappers required to load pre-trained weights into the modified structure. \"\"\" parameters_to_train : list [ nn . Parameter ] load_state_mappers : list [ ModelStateMapper ]","title":"PeftInjectionResult"},{"location":"peft/0_index/#d9d.peft.PeftMethod","text":"Bases: ABC , Generic [ TConfig ] Abstract base class for all Parameter-Efficient Fine-Tuning methods. Source code in d9d/peft/base.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 class PeftMethod ( abc . ABC , Generic [ TConfig ]): \"\"\" Abstract base class for all Parameter-Efficient Fine-Tuning methods. \"\"\" @abc . abstractmethod def inject ( self , module : nn . Module ) -> PeftInjectionResult : \"\"\" Modifies the module in-place to apply the PEFT strategy. Args: module: The PyTorch module to modify. Returns: Result object containing trainable parameters and structure mappers. \"\"\" ... @abc . abstractmethod def merge ( self , module : nn . Module ): \"\"\" Merges the trained adapters back into the base model parameters. Args: module: The PyTorch module to update. \"\"\" ... @classmethod @abc . abstractmethod def from_config ( cls , config : TConfig ) -> Self : \"\"\" Creates an instance of the method from a configuration object. Args: config: The configuration object. Returns: An instance of the PeftMethod. \"\"\" ...","title":"PeftMethod"},{"location":"peft/0_index/#d9d.peft.PeftMethod.from_config","text":"Creates an instance of the method from a configuration object. Parameters: Name Type Description Default config TConfig The configuration object. required Returns: Type Description Self An instance of the PeftMethod. Source code in d9d/peft/base.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 @classmethod @abc . abstractmethod def from_config ( cls , config : TConfig ) -> Self : \"\"\" Creates an instance of the method from a configuration object. Args: config: The configuration object. Returns: An instance of the PeftMethod. \"\"\" ...","title":"from_config"},{"location":"peft/0_index/#d9d.peft.PeftMethod.inject","text":"Modifies the module in-place to apply the PEFT strategy. Parameters: Name Type Description Default module Module The PyTorch module to modify. required Returns: Type Description PeftInjectionResult Result object containing trainable parameters and structure mappers. Source code in d9d/peft/base.py 33 34 35 36 37 38 39 40 41 42 43 44 @abc . abstractmethod def inject ( self , module : nn . Module ) -> PeftInjectionResult : \"\"\" Modifies the module in-place to apply the PEFT strategy. Args: module: The PyTorch module to modify. Returns: Result object containing trainable parameters and structure mappers. \"\"\" ...","title":"inject"},{"location":"peft/0_index/#d9d.peft.PeftMethod.merge","text":"Merges the trained adapters back into the base model parameters. Parameters: Name Type Description Default module Module The PyTorch module to update. required Source code in d9d/peft/base.py 46 47 48 49 50 51 52 53 54 55 @abc . abstractmethod def merge ( self , module : nn . Module ): \"\"\" Merges the trained adapters back into the base model parameters. Args: module: The PyTorch module to update. \"\"\" ...","title":"merge"},{"location":"peft/0_index/#d9d.peft.inject_peft_and_freeze","text":"Applies a PEFT method to a module, freezes non-trained parameters, and prepares state mapping. This function performs three main steps: Sets requires_grad=False for all parameters in the module. Calls the method's inject to modify the model structure. Sets requires_grad=True for the parameters returned by the injection result. Parameters: Name Type Description Default method PeftMethod The PEFT method strategy to apply. required module Module The PyTorch module to modify. required Returns: Type Description ModelStateMapper A ModelStateMapper capable of loading checkpoint weights into the modified structure. Source code in d9d/peft/applicator.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def inject_peft_and_freeze ( method : PeftMethod , module : nn . Module ) -> ModelStateMapper : \"\"\" Applies a PEFT method to a module, freezes non-trained parameters, and prepares state mapping. This function performs three main steps: 1. Sets `requires_grad=False` for all parameters in the module. 2. Calls the method's `inject` to modify the model structure. 3. Sets `requires_grad=True` for the parameters returned by the injection result. Args: method: The PEFT method strategy to apply. module: The PyTorch module to modify. Returns: A ModelStateMapper capable of loading checkpoint weights into the modified structure. \"\"\" for param in module . parameters (): param . requires_grad = False result = method . inject ( module ) for param in result . parameters_to_train : param . requires_grad = True return ModelStateMapperParallel ( result . load_state_mappers )","title":"inject_peft_and_freeze"},{"location":"peft/0_index/#d9d.peft.merge_peft","text":"Merges PEFT adaptations back into the base model weights. Parameters: Name Type Description Default method PeftMethod The PEFT method strategy originally applied. required module Module The PyTorch module to merge. required Source code in d9d/peft/applicator.py 38 39 40 41 42 43 44 45 46 47 def merge_peft ( method : PeftMethod , module : nn . Module ): \"\"\" Merges PEFT adaptations back into the base model weights. Args: method: The PEFT method strategy originally applied. module: The PyTorch module to merge. \"\"\" method . merge ( module )","title":"merge_peft"},{"location":"peft/full_tune/","text":"About The d9d.peft.full_tune package allows you to integrate standard fine-tuning into the PEFT workflow. It does not alter the model architecture. Instead, it uses regex patterns to identify specific modules (e.g., Norm layers or specific Heads) and unfreezes their parameters. This is particularly useful when combined with other PEFT methods via Stacking , allowing for hybrid training strategies (e.g., LoRA on Attention + Full Tune on LayerNorm). d9d.peft.full_tune Package for Full Fine-Tuning functionality within the PEFT framework. FullTune Bases: PeftMethod [ FullTuneConfig ] Implements Full Fine-Tuning as a 'PEFT' method. Instead of injecting adapters, this method simply identifies existing parameters that match the configuration pattern and marks them for training. Source code in d9d/peft/full_tune/method.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class FullTune ( PeftMethod [ FullTuneConfig ]): \"\"\" Implements Full Fine-Tuning as a 'PEFT' method. Instead of injecting adapters, this method simply identifies existing parameters that match the configuration pattern and marks them for training. \"\"\" def __init__ ( self , config : FullTuneConfig ): \"\"\" Constructs a FullTune object. Args: config: Configuration defining the module name patterns to fine-tune. \"\"\" self . _config = config def inject ( self , module : nn . Module ) -> PeftInjectionResult : params_to_train = [] for mod_name , mod in module . named_modules (): is_applicable = self . _config . module_name_pattern . fullmatch ( mod_name ) if is_applicable : params_to_train . extend ( mod . parameters ()) return PeftInjectionResult ( parameters_to_train = params_to_train , load_state_mappers = []) def merge ( self , module : nn . Module ): pass # do nothing here @classmethod def from_config ( cls , config : FullTuneConfig ) -> Self : return cls ( config ) __init__ ( config ) Constructs a FullTune object. Parameters: Name Type Description Default config FullTuneConfig Configuration defining the module name patterns to fine-tune. required Source code in d9d/peft/full_tune/method.py 17 18 19 20 21 22 23 24 25 def __init__ ( self , config : FullTuneConfig ): \"\"\" Constructs a FullTune object. Args: config: Configuration defining the module name patterns to fine-tune. \"\"\" self . _config = config FullTuneConfig Bases: BaseModel Configuration for Full Fine-Tuning. Allows specifying which modules should be fully fine-tuned using regex patterns. Attributes: Name Type Description kind Literal ['full_tune'] Discriminator field, always \"full_tune\". module_name_pattern Pattern Regular expression matching module names to unfreeze. Source code in d9d/peft/full_tune/config.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class FullTuneConfig ( BaseModel ): \"\"\" Configuration for Full Fine-Tuning. Allows specifying which modules should be fully fine-tuned using regex patterns. Attributes: kind: Discriminator field, always \"full_tune\". module_name_pattern: Regular expression matching module names to unfreeze. \"\"\" kind : Literal [ \"full_tune\" ] = \"full_tune\" module_name_pattern : Pattern ```","title":"Full Fine-Tuning"},{"location":"peft/full_tune/#about","text":"The d9d.peft.full_tune package allows you to integrate standard fine-tuning into the PEFT workflow. It does not alter the model architecture. Instead, it uses regex patterns to identify specific modules (e.g., Norm layers or specific Heads) and unfreezes their parameters. This is particularly useful when combined with other PEFT methods via Stacking , allowing for hybrid training strategies (e.g., LoRA on Attention + Full Tune on LayerNorm).","title":"About"},{"location":"peft/full_tune/#d9d.peft.full_tune","text":"Package for Full Fine-Tuning functionality within the PEFT framework.","title":"full_tune"},{"location":"peft/full_tune/#d9d.peft.full_tune.FullTune","text":"Bases: PeftMethod [ FullTuneConfig ] Implements Full Fine-Tuning as a 'PEFT' method. Instead of injecting adapters, this method simply identifies existing parameters that match the configuration pattern and marks them for training. Source code in d9d/peft/full_tune/method.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class FullTune ( PeftMethod [ FullTuneConfig ]): \"\"\" Implements Full Fine-Tuning as a 'PEFT' method. Instead of injecting adapters, this method simply identifies existing parameters that match the configuration pattern and marks them for training. \"\"\" def __init__ ( self , config : FullTuneConfig ): \"\"\" Constructs a FullTune object. Args: config: Configuration defining the module name patterns to fine-tune. \"\"\" self . _config = config def inject ( self , module : nn . Module ) -> PeftInjectionResult : params_to_train = [] for mod_name , mod in module . named_modules (): is_applicable = self . _config . module_name_pattern . fullmatch ( mod_name ) if is_applicable : params_to_train . extend ( mod . parameters ()) return PeftInjectionResult ( parameters_to_train = params_to_train , load_state_mappers = []) def merge ( self , module : nn . Module ): pass # do nothing here @classmethod def from_config ( cls , config : FullTuneConfig ) -> Self : return cls ( config )","title":"FullTune"},{"location":"peft/full_tune/#d9d.peft.full_tune.FullTune.__init__","text":"Constructs a FullTune object. Parameters: Name Type Description Default config FullTuneConfig Configuration defining the module name patterns to fine-tune. required Source code in d9d/peft/full_tune/method.py 17 18 19 20 21 22 23 24 25 def __init__ ( self , config : FullTuneConfig ): \"\"\" Constructs a FullTune object. Args: config: Configuration defining the module name patterns to fine-tune. \"\"\" self . _config = config","title":"__init__"},{"location":"peft/full_tune/#d9d.peft.full_tune.FullTuneConfig","text":"Bases: BaseModel Configuration for Full Fine-Tuning. Allows specifying which modules should be fully fine-tuned using regex patterns. Attributes: Name Type Description kind Literal ['full_tune'] Discriminator field, always \"full_tune\". module_name_pattern Pattern Regular expression matching module names to unfreeze. Source code in d9d/peft/full_tune/config.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class FullTuneConfig ( BaseModel ): \"\"\" Configuration for Full Fine-Tuning. Allows specifying which modules should be fully fine-tuned using regex patterns. Attributes: kind: Discriminator field, always \"full_tune\". module_name_pattern: Regular expression matching module names to unfreeze. \"\"\" kind : Literal [ \"full_tune\" ] = \"full_tune\" module_name_pattern : Pattern ```","title":"FullTuneConfig"},{"location":"peft/lora/","text":"About The d9d.peft.lora package implements Low-Rank Adaptation. It works by wrapping existing Linear layers (both standard nn.Linear and d9d's GroupedLinear ) with a container that holds the original frozen layer ( base ) and two low-rank trainable matrices ( lora_A and lora_B ). Because the original layer is moved to a submodule ( base ), the state keys change. The LoRA method automatically generates a ModelStateMapperRename to handle this transparently during checkpoint loading. Usage Example import torch import re from d9d.peft import inject_peft_and_freeze , merge_peft from d9d.peft.lora import LoRA , LoRAConfig , LoRAParameters # 1. Configuration config = LoRAConfig ( module_name_pattern = re . compile ( r \".*attention\\.q_proj.*\" ), # Target Attention Q projections params = LoRAParameters ( r = 16 , alpha = 32 , dropout = 0.1 ) ) # 2. Instantiate Method method = LoRA ( config ) # 3. Inject # This replaces nn.Linear with LoRALinear layers in-place. # 'mapper' knows how to route 'q_proj.weight' -> 'q_proj.base.weight' mapper = inject_peft_and_freeze ( method , model ) # ... pass 'mapper' object to d9d's Trainer or manually load a model checkpoint ... # ... train a model ... # 4. Merge - for exporting a model merge_peft ( method , model ) d9d.peft.lora Package for Low-Rank Adaptation (LoRA) implementation. LoRA Bases: PeftMethod [ LoRAConfig ] Implements the Low-Rank Adaptation (LoRA) injection strategy. It scans the module structure for nn.Linear or GroupedLinear layers matching the configured name pattern. Matched layers are replaced with LoRA wrappers. It also generates ModelStateMapperRename objects. Since the original weight layer.weight is now at layer.base.weight inside the wrapper, the mapper ensures that loading a standard checkpoint still works by redirecting the key. Source code in d9d/peft/lora/method.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 class LoRA ( PeftMethod [ LoRAConfig ]): \"\"\" Implements the Low-Rank Adaptation (LoRA) injection strategy. It scans the module structure for `nn.Linear` or `GroupedLinear` layers matching the configured name pattern. Matched layers are replaced with LoRA wrappers. It also generates `ModelStateMapperRename` objects. Since the original weight `layer.weight` is now at `layer.base.weight` inside the wrapper, the mapper ensures that loading a standard checkpoint still works by redirecting the key. \"\"\" def __init__ ( self , config : LoRAConfig ): \"\"\" Constructs a LoRA method. Args: config: LoRA configuration containing patterns and hyperparameters. \"\"\" self . _config = config def inject ( self , module : nn . Module ) -> PeftInjectionResult : params_to_train : list [ nn . Parameter ] = [] state_mappers : list [ ModelStateMapper ] = [] for mod_name , mod in named_modules_without_lora ( module ): if not isinstance ( mod , _CAN_APPLY_MODULES ): continue if not self . _config . module_name_pattern . fullmatch ( mod_name ): continue lora_mod : LoRALinear | LoRAGroupedLinear if isinstance ( mod , nn . Linear ): lora_mod = LoRALinear ( mod , self . _config . params ) elif isinstance ( mod , GroupedLinear ): lora_mod = LoRAGroupedLinear ( mod , self . _config . params ) else : raise ValueError ( f \"Unknown layer { type ( mod ) } for LoRA\" ) params_to_train . extend ( lora_mod . lora_A . parameters ()) params_to_train . extend ( lora_mod . lora_B . parameters ()) state_mappers . append ( ModelStateMapperRename ( name_from = f \" { mod_name } .weight\" , name_to = f \" { mod_name } .base.weight\" ) ) module . set_submodule ( mod_name , lora_mod ) return PeftInjectionResult ( parameters_to_train = params_to_train , load_state_mappers = state_mappers ) def merge ( self , module : nn . Module ): for mod_name , mod in module . named_modules (): if not isinstance ( mod , _LORA_MODULES ): continue if not self . _config . module_name_pattern . fullmatch ( mod_name ): continue with torch . no_grad (): orig_mod = mod . merge_with_base_ () module . set_submodule ( mod_name , orig_mod ) @classmethod def from_config ( cls , config : LoRAConfig ) -> Self : return cls ( config ) __init__ ( config ) Constructs a LoRA method. Parameters: Name Type Description Default config LoRAConfig LoRA configuration containing patterns and hyperparameters. required Source code in d9d/peft/lora/method.py 70 71 72 73 74 75 76 77 78 def __init__ ( self , config : LoRAConfig ): \"\"\" Constructs a LoRA method. Args: config: LoRA configuration containing patterns and hyperparameters. \"\"\" self . _config = config LoRAConfig Bases: BaseModel Configuration for LoRA application. Attributes: Name Type Description kind Literal ['lora'] Discriminator field, always \"lora\". module_name_pattern Pattern Regular expression matching module names to wrap with LoRA. params LoRAParameters Hyperparameters for the LoRA layers. Source code in d9d/peft/lora/config.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class LoRAConfig ( BaseModel ): \"\"\" Configuration for LoRA application. Attributes: kind: Discriminator field, always \"lora\". module_name_pattern: Regular expression matching module names to wrap with LoRA. params: Hyperparameters for the LoRA layers. \"\"\" kind : Literal [ \"lora\" ] = \"lora\" module_name_pattern : Pattern params : LoRAParameters LoRAGroupedLinear Bases: Module A LoRA wrapper around a GroupedLinear layer (commonly used in MoE or grouped query attention). Attributes: Name Type Description lora_A The A matrix (grouped linear). lora_B The B matrix (grouped linear). base The original base GroupedLinear layer. dropout Scaling dropout layer. Source code in d9d/peft/lora/layer.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 class LoRAGroupedLinear ( nn . Module ): \"\"\" A LoRA wrapper around a GroupedLinear layer (commonly used in MoE or grouped query attention). Attributes: lora_A: The A matrix (grouped linear). lora_B: The B matrix (grouped linear). base: The original base GroupedLinear layer. dropout: Scaling dropout layer. \"\"\" def __init__ ( self , base_layer : GroupedLinear , params : LoRAParameters ): \"\"\" Constructs a LoRAGroupedLinear layer. Args: base_layer: The original GroupedLinear layer to wrap. params: LoRA hyperparameters. \"\"\" super () . __init__ () self . lora_A = GroupedLinear ( base_layer . n_groups , base_layer . in_features , params . r , device = base_layer . weight . device , dtype = base_layer . weight . dtype , ) self . lora_B = GroupedLinear ( base_layer . n_groups , params . r , base_layer . out_features , device = base_layer . weight . device , dtype = base_layer . weight . dtype , ) self . base = base_layer self . dropout = nn . Dropout ( params . dropout ) self . _scale = params . alpha / params . r self . reset_parameters () def forward ( self , x : torch . Tensor , x_groups : torch . Tensor ) -> torch . Tensor : \"\"\" Computes forward pass for grouped inputs. Args: x: Input tensor. x_groups: A tensor indicating group indices for each input. Returns: Combined output of base and LoRA path. \"\"\" base_x = self . base ( x , x_groups ) adapt_x = self . _scale * self . lora_B ( self . lora_A ( self . dropout ( x ), x_groups ), x_groups ) return base_x + adapt_x @torch . no_grad () def merge_with_base_ ( self ) -> GroupedLinear : \"\"\" Collapse the LoRA weights into the base GroupedLinear layer. Returns: The modified GroupedLinear layer. \"\"\" mod = self . base mod . weight . data += ( torch . bmm ( self . lora_A . weight . data , self . lora_B . weight . data )) * self . _scale return mod def reset_parameters ( self ): \"\"\" Resets LoRA parameters. A is random, B is zeroed. \"\"\" self . lora_A . reset_parameters () nn . init . zeros_ ( self . lora_B . weight ) __init__ ( base_layer , params ) Constructs a LoRAGroupedLinear layer. Parameters: Name Type Description Default base_layer GroupedLinear The original GroupedLinear layer to wrap. required params LoRAParameters LoRA hyperparameters. required Source code in d9d/peft/lora/layer.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def __init__ ( self , base_layer : GroupedLinear , params : LoRAParameters ): \"\"\" Constructs a LoRAGroupedLinear layer. Args: base_layer: The original GroupedLinear layer to wrap. params: LoRA hyperparameters. \"\"\" super () . __init__ () self . lora_A = GroupedLinear ( base_layer . n_groups , base_layer . in_features , params . r , device = base_layer . weight . device , dtype = base_layer . weight . dtype , ) self . lora_B = GroupedLinear ( base_layer . n_groups , params . r , base_layer . out_features , device = base_layer . weight . device , dtype = base_layer . weight . dtype , ) self . base = base_layer self . dropout = nn . Dropout ( params . dropout ) self . _scale = params . alpha / params . r self . reset_parameters () forward ( x , x_groups ) Computes forward pass for grouped inputs. Parameters: Name Type Description Default x Tensor Input tensor. required x_groups Tensor A tensor indicating group indices for each input. required Returns: Type Description Tensor Combined output of base and LoRA path. Source code in d9d/peft/lora/layer.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def forward ( self , x : torch . Tensor , x_groups : torch . Tensor ) -> torch . Tensor : \"\"\" Computes forward pass for grouped inputs. Args: x: Input tensor. x_groups: A tensor indicating group indices for each input. Returns: Combined output of base and LoRA path. \"\"\" base_x = self . base ( x , x_groups ) adapt_x = self . _scale * self . lora_B ( self . lora_A ( self . dropout ( x ), x_groups ), x_groups ) return base_x + adapt_x merge_with_base_ () Collapse the LoRA weights into the base GroupedLinear layer. Returns: Type Description GroupedLinear The modified GroupedLinear layer. Source code in d9d/peft/lora/layer.py 152 153 154 155 156 157 158 159 160 161 162 163 @torch . no_grad () def merge_with_base_ ( self ) -> GroupedLinear : \"\"\" Collapse the LoRA weights into the base GroupedLinear layer. Returns: The modified GroupedLinear layer. \"\"\" mod = self . base mod . weight . data += ( torch . bmm ( self . lora_A . weight . data , self . lora_B . weight . data )) * self . _scale return mod reset_parameters () Resets LoRA parameters. A is random, B is zeroed. Source code in d9d/peft/lora/layer.py 165 166 167 168 169 170 171 def reset_parameters ( self ): \"\"\" Resets LoRA parameters. A is random, B is zeroed. \"\"\" self . lora_A . reset_parameters () nn . init . zeros_ ( self . lora_B . weight ) LoRALinear Bases: Module A LoRA wrapper around a standard PyTorch Linear layer. Wraps a base linear layer and adds low-rank adaptation matrices A and B. Attributes: Name Type Description lora_A The A matrix (in_features -> r). lora_B The B matrix (r -> out_features). base The original base Linear layer. dropout Dropout Scaling dropout layer. Source code in d9d/peft/lora/layer.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 class LoRALinear ( nn . Module ): \"\"\" A LoRA wrapper around a standard PyTorch Linear layer. Wraps a base linear layer and adds low-rank adaptation matrices A and B. Attributes: lora_A: The A matrix (in_features -> r). lora_B: The B matrix (r -> out_features). base: The original base Linear layer. dropout: Scaling dropout layer. \"\"\" def __init__ ( self , base_layer : nn . Linear , params : LoRAParameters ): \"\"\" Constructs a LoRALinear layer. Args: base_layer: The original Linear layer to wrap. params: LoRA hyperparameters (r, alpha, dropout). Raises: ValueError: If the base layer has a bias (currently unsupported). \"\"\" super () . __init__ () self . lora_A = nn . Linear ( base_layer . in_features , params . r , bias = False , device = base_layer . weight . device , dtype = base_layer . weight . dtype ) self . lora_B = nn . Linear ( params . r , base_layer . out_features , bias = False , device = base_layer . weight . device , dtype = base_layer . weight . dtype , ) self . base = base_layer if base_layer . bias is not None : raise ValueError ( \"LoRA is unsupported with biased linear layers\" ) self . dropout : nn . Dropout = nn . Dropout ( params . dropout ) self . _scale : float = params . alpha / params . r self . reset_parameters () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Takes input tensor, computes base output and LoRA adaptation, and returns the sum. Args: x: Input tensor. Returns: The output of base(x) + scale * (B @ A @ dropout(x)). \"\"\" base_x = self . base ( x ) adapt_x = self . _scale * self . lora_B ( self . lora_A ( self . dropout ( x ))) return base_x + adapt_x @torch . no_grad () def merge_with_base_ ( self ) -> nn . Linear : \"\"\" Collapse the LoRA weights into the base linear layer. Returns: The modified base linear layer with updated weights. \"\"\" mod = self . base mod . weight . data += ( self . lora_B . weight . data @ self . lora_A . weight . data ) * self . _scale return mod def reset_parameters ( self ): \"\"\" Resets LoRA parameters. A is random, B is zeroed. \"\"\" self . lora_A . reset_parameters () nn . init . zeros_ ( self . lora_B . weight ) __init__ ( base_layer , params ) Constructs a LoRALinear layer. Parameters: Name Type Description Default base_layer Linear The original Linear layer to wrap. required params LoRAParameters LoRA hyperparameters (r, alpha, dropout). required Raises: Type Description ValueError If the base layer has a bias (currently unsupported). Source code in d9d/peft/lora/layer.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def __init__ ( self , base_layer : nn . Linear , params : LoRAParameters ): \"\"\" Constructs a LoRALinear layer. Args: base_layer: The original Linear layer to wrap. params: LoRA hyperparameters (r, alpha, dropout). Raises: ValueError: If the base layer has a bias (currently unsupported). \"\"\" super () . __init__ () self . lora_A = nn . Linear ( base_layer . in_features , params . r , bias = False , device = base_layer . weight . device , dtype = base_layer . weight . dtype ) self . lora_B = nn . Linear ( params . r , base_layer . out_features , bias = False , device = base_layer . weight . device , dtype = base_layer . weight . dtype , ) self . base = base_layer if base_layer . bias is not None : raise ValueError ( \"LoRA is unsupported with biased linear layers\" ) self . dropout : nn . Dropout = nn . Dropout ( params . dropout ) self . _scale : float = params . alpha / params . r self . reset_parameters () forward ( x ) Takes input tensor, computes base output and LoRA adaptation, and returns the sum. Parameters: Name Type Description Default x Tensor Input tensor. required Returns: Type Description Tensor The output of base(x) + scale * (B @ A @ dropout(x)). Source code in d9d/peft/lora/layer.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Takes input tensor, computes base output and LoRA adaptation, and returns the sum. Args: x: Input tensor. Returns: The output of base(x) + scale * (B @ A @ dropout(x)). \"\"\" base_x = self . base ( x ) adapt_x = self . _scale * self . lora_B ( self . lora_A ( self . dropout ( x ))) return base_x + adapt_x merge_with_base_ () Collapse the LoRA weights into the base linear layer. Returns: Type Description Linear The modified base linear layer with updated weights. Source code in d9d/peft/lora/layer.py 71 72 73 74 75 76 77 78 79 80 81 82 @torch . no_grad () def merge_with_base_ ( self ) -> nn . Linear : \"\"\" Collapse the LoRA weights into the base linear layer. Returns: The modified base linear layer with updated weights. \"\"\" mod = self . base mod . weight . data += ( self . lora_B . weight . data @ self . lora_A . weight . data ) * self . _scale return mod reset_parameters () Resets LoRA parameters. A is random, B is zeroed. Source code in d9d/peft/lora/layer.py 84 85 86 87 88 89 90 def reset_parameters ( self ): \"\"\" Resets LoRA parameters. A is random, B is zeroed. \"\"\" self . lora_A . reset_parameters () nn . init . zeros_ ( self . lora_B . weight ) LoRAParameters Bases: BaseModel Hyperparameters for LoRA layers. Attributes: Name Type Description r int Rank of the low-rank adaptation matrices. alpha int Scaling factor for the learned weights. dropout float Dropout probability for the input to LoRA layers. Source code in d9d/peft/lora/config.py 7 8 9 10 11 12 13 14 15 16 17 18 19 class LoRAParameters ( BaseModel ): \"\"\" Hyperparameters for LoRA layers. Attributes: r: Rank of the low-rank adaptation matrices. alpha: Scaling factor for the learned weights. dropout: Dropout probability for the input to LoRA layers. \"\"\" r : int alpha : int dropout : float","title":"LoRA"},{"location":"peft/lora/#about","text":"The d9d.peft.lora package implements Low-Rank Adaptation. It works by wrapping existing Linear layers (both standard nn.Linear and d9d's GroupedLinear ) with a container that holds the original frozen layer ( base ) and two low-rank trainable matrices ( lora_A and lora_B ). Because the original layer is moved to a submodule ( base ), the state keys change. The LoRA method automatically generates a ModelStateMapperRename to handle this transparently during checkpoint loading.","title":"About"},{"location":"peft/lora/#usage-example","text":"import torch import re from d9d.peft import inject_peft_and_freeze , merge_peft from d9d.peft.lora import LoRA , LoRAConfig , LoRAParameters # 1. Configuration config = LoRAConfig ( module_name_pattern = re . compile ( r \".*attention\\.q_proj.*\" ), # Target Attention Q projections params = LoRAParameters ( r = 16 , alpha = 32 , dropout = 0.1 ) ) # 2. Instantiate Method method = LoRA ( config ) # 3. Inject # This replaces nn.Linear with LoRALinear layers in-place. # 'mapper' knows how to route 'q_proj.weight' -> 'q_proj.base.weight' mapper = inject_peft_and_freeze ( method , model ) # ... pass 'mapper' object to d9d's Trainer or manually load a model checkpoint ... # ... train a model ... # 4. Merge - for exporting a model merge_peft ( method , model )","title":"Usage Example"},{"location":"peft/lora/#d9d.peft.lora","text":"Package for Low-Rank Adaptation (LoRA) implementation.","title":"lora"},{"location":"peft/lora/#d9d.peft.lora.LoRA","text":"Bases: PeftMethod [ LoRAConfig ] Implements the Low-Rank Adaptation (LoRA) injection strategy. It scans the module structure for nn.Linear or GroupedLinear layers matching the configured name pattern. Matched layers are replaced with LoRA wrappers. It also generates ModelStateMapperRename objects. Since the original weight layer.weight is now at layer.base.weight inside the wrapper, the mapper ensures that loading a standard checkpoint still works by redirecting the key. Source code in d9d/peft/lora/method.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 class LoRA ( PeftMethod [ LoRAConfig ]): \"\"\" Implements the Low-Rank Adaptation (LoRA) injection strategy. It scans the module structure for `nn.Linear` or `GroupedLinear` layers matching the configured name pattern. Matched layers are replaced with LoRA wrappers. It also generates `ModelStateMapperRename` objects. Since the original weight `layer.weight` is now at `layer.base.weight` inside the wrapper, the mapper ensures that loading a standard checkpoint still works by redirecting the key. \"\"\" def __init__ ( self , config : LoRAConfig ): \"\"\" Constructs a LoRA method. Args: config: LoRA configuration containing patterns and hyperparameters. \"\"\" self . _config = config def inject ( self , module : nn . Module ) -> PeftInjectionResult : params_to_train : list [ nn . Parameter ] = [] state_mappers : list [ ModelStateMapper ] = [] for mod_name , mod in named_modules_without_lora ( module ): if not isinstance ( mod , _CAN_APPLY_MODULES ): continue if not self . _config . module_name_pattern . fullmatch ( mod_name ): continue lora_mod : LoRALinear | LoRAGroupedLinear if isinstance ( mod , nn . Linear ): lora_mod = LoRALinear ( mod , self . _config . params ) elif isinstance ( mod , GroupedLinear ): lora_mod = LoRAGroupedLinear ( mod , self . _config . params ) else : raise ValueError ( f \"Unknown layer { type ( mod ) } for LoRA\" ) params_to_train . extend ( lora_mod . lora_A . parameters ()) params_to_train . extend ( lora_mod . lora_B . parameters ()) state_mappers . append ( ModelStateMapperRename ( name_from = f \" { mod_name } .weight\" , name_to = f \" { mod_name } .base.weight\" ) ) module . set_submodule ( mod_name , lora_mod ) return PeftInjectionResult ( parameters_to_train = params_to_train , load_state_mappers = state_mappers ) def merge ( self , module : nn . Module ): for mod_name , mod in module . named_modules (): if not isinstance ( mod , _LORA_MODULES ): continue if not self . _config . module_name_pattern . fullmatch ( mod_name ): continue with torch . no_grad (): orig_mod = mod . merge_with_base_ () module . set_submodule ( mod_name , orig_mod ) @classmethod def from_config ( cls , config : LoRAConfig ) -> Self : return cls ( config )","title":"LoRA"},{"location":"peft/lora/#d9d.peft.lora.LoRA.__init__","text":"Constructs a LoRA method. Parameters: Name Type Description Default config LoRAConfig LoRA configuration containing patterns and hyperparameters. required Source code in d9d/peft/lora/method.py 70 71 72 73 74 75 76 77 78 def __init__ ( self , config : LoRAConfig ): \"\"\" Constructs a LoRA method. Args: config: LoRA configuration containing patterns and hyperparameters. \"\"\" self . _config = config","title":"__init__"},{"location":"peft/lora/#d9d.peft.lora.LoRAConfig","text":"Bases: BaseModel Configuration for LoRA application. Attributes: Name Type Description kind Literal ['lora'] Discriminator field, always \"lora\". module_name_pattern Pattern Regular expression matching module names to wrap with LoRA. params LoRAParameters Hyperparameters for the LoRA layers. Source code in d9d/peft/lora/config.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class LoRAConfig ( BaseModel ): \"\"\" Configuration for LoRA application. Attributes: kind: Discriminator field, always \"lora\". module_name_pattern: Regular expression matching module names to wrap with LoRA. params: Hyperparameters for the LoRA layers. \"\"\" kind : Literal [ \"lora\" ] = \"lora\" module_name_pattern : Pattern params : LoRAParameters","title":"LoRAConfig"},{"location":"peft/lora/#d9d.peft.lora.LoRAGroupedLinear","text":"Bases: Module A LoRA wrapper around a GroupedLinear layer (commonly used in MoE or grouped query attention). Attributes: Name Type Description lora_A The A matrix (grouped linear). lora_B The B matrix (grouped linear). base The original base GroupedLinear layer. dropout Scaling dropout layer. Source code in d9d/peft/lora/layer.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 class LoRAGroupedLinear ( nn . Module ): \"\"\" A LoRA wrapper around a GroupedLinear layer (commonly used in MoE or grouped query attention). Attributes: lora_A: The A matrix (grouped linear). lora_B: The B matrix (grouped linear). base: The original base GroupedLinear layer. dropout: Scaling dropout layer. \"\"\" def __init__ ( self , base_layer : GroupedLinear , params : LoRAParameters ): \"\"\" Constructs a LoRAGroupedLinear layer. Args: base_layer: The original GroupedLinear layer to wrap. params: LoRA hyperparameters. \"\"\" super () . __init__ () self . lora_A = GroupedLinear ( base_layer . n_groups , base_layer . in_features , params . r , device = base_layer . weight . device , dtype = base_layer . weight . dtype , ) self . lora_B = GroupedLinear ( base_layer . n_groups , params . r , base_layer . out_features , device = base_layer . weight . device , dtype = base_layer . weight . dtype , ) self . base = base_layer self . dropout = nn . Dropout ( params . dropout ) self . _scale = params . alpha / params . r self . reset_parameters () def forward ( self , x : torch . Tensor , x_groups : torch . Tensor ) -> torch . Tensor : \"\"\" Computes forward pass for grouped inputs. Args: x: Input tensor. x_groups: A tensor indicating group indices for each input. Returns: Combined output of base and LoRA path. \"\"\" base_x = self . base ( x , x_groups ) adapt_x = self . _scale * self . lora_B ( self . lora_A ( self . dropout ( x ), x_groups ), x_groups ) return base_x + adapt_x @torch . no_grad () def merge_with_base_ ( self ) -> GroupedLinear : \"\"\" Collapse the LoRA weights into the base GroupedLinear layer. Returns: The modified GroupedLinear layer. \"\"\" mod = self . base mod . weight . data += ( torch . bmm ( self . lora_A . weight . data , self . lora_B . weight . data )) * self . _scale return mod def reset_parameters ( self ): \"\"\" Resets LoRA parameters. A is random, B is zeroed. \"\"\" self . lora_A . reset_parameters () nn . init . zeros_ ( self . lora_B . weight )","title":"LoRAGroupedLinear"},{"location":"peft/lora/#d9d.peft.lora.LoRAGroupedLinear.__init__","text":"Constructs a LoRAGroupedLinear layer. Parameters: Name Type Description Default base_layer GroupedLinear The original GroupedLinear layer to wrap. required params LoRAParameters LoRA hyperparameters. required Source code in d9d/peft/lora/layer.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def __init__ ( self , base_layer : GroupedLinear , params : LoRAParameters ): \"\"\" Constructs a LoRAGroupedLinear layer. Args: base_layer: The original GroupedLinear layer to wrap. params: LoRA hyperparameters. \"\"\" super () . __init__ () self . lora_A = GroupedLinear ( base_layer . n_groups , base_layer . in_features , params . r , device = base_layer . weight . device , dtype = base_layer . weight . dtype , ) self . lora_B = GroupedLinear ( base_layer . n_groups , params . r , base_layer . out_features , device = base_layer . weight . device , dtype = base_layer . weight . dtype , ) self . base = base_layer self . dropout = nn . Dropout ( params . dropout ) self . _scale = params . alpha / params . r self . reset_parameters ()","title":"__init__"},{"location":"peft/lora/#d9d.peft.lora.LoRAGroupedLinear.forward","text":"Computes forward pass for grouped inputs. Parameters: Name Type Description Default x Tensor Input tensor. required x_groups Tensor A tensor indicating group indices for each input. required Returns: Type Description Tensor Combined output of base and LoRA path. Source code in d9d/peft/lora/layer.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def forward ( self , x : torch . Tensor , x_groups : torch . Tensor ) -> torch . Tensor : \"\"\" Computes forward pass for grouped inputs. Args: x: Input tensor. x_groups: A tensor indicating group indices for each input. Returns: Combined output of base and LoRA path. \"\"\" base_x = self . base ( x , x_groups ) adapt_x = self . _scale * self . lora_B ( self . lora_A ( self . dropout ( x ), x_groups ), x_groups ) return base_x + adapt_x","title":"forward"},{"location":"peft/lora/#d9d.peft.lora.LoRAGroupedLinear.merge_with_base_","text":"Collapse the LoRA weights into the base GroupedLinear layer. Returns: Type Description GroupedLinear The modified GroupedLinear layer. Source code in d9d/peft/lora/layer.py 152 153 154 155 156 157 158 159 160 161 162 163 @torch . no_grad () def merge_with_base_ ( self ) -> GroupedLinear : \"\"\" Collapse the LoRA weights into the base GroupedLinear layer. Returns: The modified GroupedLinear layer. \"\"\" mod = self . base mod . weight . data += ( torch . bmm ( self . lora_A . weight . data , self . lora_B . weight . data )) * self . _scale return mod","title":"merge_with_base_"},{"location":"peft/lora/#d9d.peft.lora.LoRAGroupedLinear.reset_parameters","text":"Resets LoRA parameters. A is random, B is zeroed. Source code in d9d/peft/lora/layer.py 165 166 167 168 169 170 171 def reset_parameters ( self ): \"\"\" Resets LoRA parameters. A is random, B is zeroed. \"\"\" self . lora_A . reset_parameters () nn . init . zeros_ ( self . lora_B . weight )","title":"reset_parameters"},{"location":"peft/lora/#d9d.peft.lora.LoRALinear","text":"Bases: Module A LoRA wrapper around a standard PyTorch Linear layer. Wraps a base linear layer and adds low-rank adaptation matrices A and B. Attributes: Name Type Description lora_A The A matrix (in_features -> r). lora_B The B matrix (r -> out_features). base The original base Linear layer. dropout Dropout Scaling dropout layer. Source code in d9d/peft/lora/layer.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 class LoRALinear ( nn . Module ): \"\"\" A LoRA wrapper around a standard PyTorch Linear layer. Wraps a base linear layer and adds low-rank adaptation matrices A and B. Attributes: lora_A: The A matrix (in_features -> r). lora_B: The B matrix (r -> out_features). base: The original base Linear layer. dropout: Scaling dropout layer. \"\"\" def __init__ ( self , base_layer : nn . Linear , params : LoRAParameters ): \"\"\" Constructs a LoRALinear layer. Args: base_layer: The original Linear layer to wrap. params: LoRA hyperparameters (r, alpha, dropout). Raises: ValueError: If the base layer has a bias (currently unsupported). \"\"\" super () . __init__ () self . lora_A = nn . Linear ( base_layer . in_features , params . r , bias = False , device = base_layer . weight . device , dtype = base_layer . weight . dtype ) self . lora_B = nn . Linear ( params . r , base_layer . out_features , bias = False , device = base_layer . weight . device , dtype = base_layer . weight . dtype , ) self . base = base_layer if base_layer . bias is not None : raise ValueError ( \"LoRA is unsupported with biased linear layers\" ) self . dropout : nn . Dropout = nn . Dropout ( params . dropout ) self . _scale : float = params . alpha / params . r self . reset_parameters () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Takes input tensor, computes base output and LoRA adaptation, and returns the sum. Args: x: Input tensor. Returns: The output of base(x) + scale * (B @ A @ dropout(x)). \"\"\" base_x = self . base ( x ) adapt_x = self . _scale * self . lora_B ( self . lora_A ( self . dropout ( x ))) return base_x + adapt_x @torch . no_grad () def merge_with_base_ ( self ) -> nn . Linear : \"\"\" Collapse the LoRA weights into the base linear layer. Returns: The modified base linear layer with updated weights. \"\"\" mod = self . base mod . weight . data += ( self . lora_B . weight . data @ self . lora_A . weight . data ) * self . _scale return mod def reset_parameters ( self ): \"\"\" Resets LoRA parameters. A is random, B is zeroed. \"\"\" self . lora_A . reset_parameters () nn . init . zeros_ ( self . lora_B . weight )","title":"LoRALinear"},{"location":"peft/lora/#d9d.peft.lora.LoRALinear.__init__","text":"Constructs a LoRALinear layer. Parameters: Name Type Description Default base_layer Linear The original Linear layer to wrap. required params LoRAParameters LoRA hyperparameters (r, alpha, dropout). required Raises: Type Description ValueError If the base layer has a bias (currently unsupported). Source code in d9d/peft/lora/layer.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def __init__ ( self , base_layer : nn . Linear , params : LoRAParameters ): \"\"\" Constructs a LoRALinear layer. Args: base_layer: The original Linear layer to wrap. params: LoRA hyperparameters (r, alpha, dropout). Raises: ValueError: If the base layer has a bias (currently unsupported). \"\"\" super () . __init__ () self . lora_A = nn . Linear ( base_layer . in_features , params . r , bias = False , device = base_layer . weight . device , dtype = base_layer . weight . dtype ) self . lora_B = nn . Linear ( params . r , base_layer . out_features , bias = False , device = base_layer . weight . device , dtype = base_layer . weight . dtype , ) self . base = base_layer if base_layer . bias is not None : raise ValueError ( \"LoRA is unsupported with biased linear layers\" ) self . dropout : nn . Dropout = nn . Dropout ( params . dropout ) self . _scale : float = params . alpha / params . r self . reset_parameters ()","title":"__init__"},{"location":"peft/lora/#d9d.peft.lora.LoRALinear.forward","text":"Takes input tensor, computes base output and LoRA adaptation, and returns the sum. Parameters: Name Type Description Default x Tensor Input tensor. required Returns: Type Description Tensor The output of base(x) + scale * (B @ A @ dropout(x)). Source code in d9d/peft/lora/layer.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Takes input tensor, computes base output and LoRA adaptation, and returns the sum. Args: x: Input tensor. Returns: The output of base(x) + scale * (B @ A @ dropout(x)). \"\"\" base_x = self . base ( x ) adapt_x = self . _scale * self . lora_B ( self . lora_A ( self . dropout ( x ))) return base_x + adapt_x","title":"forward"},{"location":"peft/lora/#d9d.peft.lora.LoRALinear.merge_with_base_","text":"Collapse the LoRA weights into the base linear layer. Returns: Type Description Linear The modified base linear layer with updated weights. Source code in d9d/peft/lora/layer.py 71 72 73 74 75 76 77 78 79 80 81 82 @torch . no_grad () def merge_with_base_ ( self ) -> nn . Linear : \"\"\" Collapse the LoRA weights into the base linear layer. Returns: The modified base linear layer with updated weights. \"\"\" mod = self . base mod . weight . data += ( self . lora_B . weight . data @ self . lora_A . weight . data ) * self . _scale return mod","title":"merge_with_base_"},{"location":"peft/lora/#d9d.peft.lora.LoRALinear.reset_parameters","text":"Resets LoRA parameters. A is random, B is zeroed. Source code in d9d/peft/lora/layer.py 84 85 86 87 88 89 90 def reset_parameters ( self ): \"\"\" Resets LoRA parameters. A is random, B is zeroed. \"\"\" self . lora_A . reset_parameters () nn . init . zeros_ ( self . lora_B . weight )","title":"reset_parameters"},{"location":"peft/lora/#d9d.peft.lora.LoRAParameters","text":"Bases: BaseModel Hyperparameters for LoRA layers. Attributes: Name Type Description r int Rank of the low-rank adaptation matrices. alpha int Scaling factor for the learned weights. dropout float Dropout probability for the input to LoRA layers. Source code in d9d/peft/lora/config.py 7 8 9 10 11 12 13 14 15 16 17 18 19 class LoRAParameters ( BaseModel ): \"\"\" Hyperparameters for LoRA layers. Attributes: r: Rank of the low-rank adaptation matrices. alpha: Scaling factor for the learned weights. dropout: Dropout probability for the input to LoRA layers. \"\"\" r : int alpha : int dropout : float","title":"LoRAParameters"},{"location":"peft/stack/","text":"About Complex fine-tuning often requires hybrid approaches. The d9d.peft.all package facilitates this by grouping multiple PEFT configurations into a single PeftStack . Usage Example Applying LoRA to attention layers while fully fine-tuning normalization layers. import re from d9d.peft.all import PeftStackConfig , peft_method_from_config from d9d.peft.lora import LoRAConfig , LoRAParameters from d9d.peft.full_tune import FullTuneConfig from d9d.peft import inject_peft_and_freeze , merge_peft # 1. Define your Strategy config = PeftStackConfig ( methods = [ # Method A: LoRA on attention projections LoRAConfig ( module_name_pattern = re . compile ( r \".*attention\\..*_proj.*\" ), params = LoRAParameters ( r = 8 , alpha = 16 , dropout = 0.05 ) ), # Method B: Full Tune on LayerNorms FullTuneConfig ( module_name_pattern = re . compile ( r \".*norm.*\" ) ) ] ) # 2. Create Factory # This automatically creates a PeftStack containing the sub-methods method = peft_method_from_config ( config ) # 3. Inject mapper = inject_peft_and_freeze ( method , model ) # ... pass 'mapper' object to d9d's Trainer or manually load a model checkpoint ... # ... train a model ... # 4. Merge - for exporting a model merge_peft ( method , model ) d9d.peft.all Package for composing multiple PEFT methods into a stack. PeftStack Bases: PeftMethod [ PeftStackConfig ] A composite PEFT method that applies a list of methods sequentially. Source code in d9d/peft/all/method.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 class PeftStack ( PeftMethod [ PeftStackConfig ]): \"\"\" A composite PEFT method that applies a list of methods sequentially. \"\"\" def __init__ ( self , methods : list [ PeftMethod ]): \"\"\" Constructs a PeftStack object. Args: methods: A list of instantiated PEFT methods to apply in order. \"\"\" self . _methods = methods def inject ( self , module : nn . Module ) -> PeftInjectionResult : params_to_train = [] state_mappers = [] for method in self . _methods : result = method . inject ( module ) params_to_train . extend ( result . parameters_to_train ) state_mappers . extend ( result . load_state_mappers ) return PeftInjectionResult ( parameters_to_train = params_to_train , load_state_mappers = state_mappers ) def merge ( self , module : nn . Module ): for method in self . _methods [:: - 1 ]: method . merge ( module ) @classmethod def from_config ( cls , config : PeftStackConfig ) -> Self : methods = [] for method in config . methods : methods . append ( peft_method_from_config ( method )) return cls ( methods ) __init__ ( methods ) Constructs a PeftStack object. Parameters: Name Type Description Default methods list [ PeftMethod ] A list of instantiated PEFT methods to apply in order. required Source code in d9d/peft/all/method.py 19 20 21 22 23 24 25 26 27 def __init__ ( self , methods : list [ PeftMethod ]): \"\"\" Constructs a PeftStack object. Args: methods: A list of instantiated PEFT methods to apply in order. \"\"\" self . _methods = methods PeftStackConfig Bases: BaseModel Configuration for applying a stack of multiple PEFT methods sequentially. Attributes: Name Type Description kind Literal ['stack'] Discriminator field, always \"stack\". methods list [ AnyPeftConfig ] A list of specific PEFT configurations (e.g., LoRA, FullTune) to apply in order. Source code in d9d/peft/all/config.py 9 10 11 12 13 14 15 16 17 18 19 20 class PeftStackConfig ( BaseModel ): \"\"\" Configuration for applying a stack of multiple PEFT methods sequentially. Attributes: kind: Discriminator field, always \"stack\". methods: A list of specific PEFT configurations (e.g., LoRA, FullTune) to apply in order. \"\"\" kind : Literal [ \"stack\" ] = \"stack\" methods : list [ \"AnyPeftConfig\" ] peft_method_from_config ( config ) Factory function to instantiate the correct PeftMethod based on the configuration type. Parameters: Name Type Description Default config TConfig A specific PEFT configuration object (e.g., LoRAConfig). required Returns: Type Description PeftMethod [ TConfig ] The corresponding method instance. Source code in d9d/peft/all/method.py 61 62 63 64 65 66 67 68 69 70 71 72 73 def peft_method_from_config ( config : TConfig ) -> PeftMethod [ TConfig ]: \"\"\" Factory function to instantiate the correct PeftMethod based on the configuration type. Args: config: A specific PEFT configuration object (e.g., LoRAConfig). Returns: The corresponding method instance. \"\"\" method_cls = cast ( type [ PeftMethod [ TConfig ]], _PEFT_CONFIG_MAP [ type ( config )]) return method_cls . from_config ( config )","title":"Method Stacking"},{"location":"peft/stack/#about","text":"Complex fine-tuning often requires hybrid approaches. The d9d.peft.all package facilitates this by grouping multiple PEFT configurations into a single PeftStack .","title":"About"},{"location":"peft/stack/#usage-example","text":"Applying LoRA to attention layers while fully fine-tuning normalization layers. import re from d9d.peft.all import PeftStackConfig , peft_method_from_config from d9d.peft.lora import LoRAConfig , LoRAParameters from d9d.peft.full_tune import FullTuneConfig from d9d.peft import inject_peft_and_freeze , merge_peft # 1. Define your Strategy config = PeftStackConfig ( methods = [ # Method A: LoRA on attention projections LoRAConfig ( module_name_pattern = re . compile ( r \".*attention\\..*_proj.*\" ), params = LoRAParameters ( r = 8 , alpha = 16 , dropout = 0.05 ) ), # Method B: Full Tune on LayerNorms FullTuneConfig ( module_name_pattern = re . compile ( r \".*norm.*\" ) ) ] ) # 2. Create Factory # This automatically creates a PeftStack containing the sub-methods method = peft_method_from_config ( config ) # 3. Inject mapper = inject_peft_and_freeze ( method , model ) # ... pass 'mapper' object to d9d's Trainer or manually load a model checkpoint ... # ... train a model ... # 4. Merge - for exporting a model merge_peft ( method , model )","title":"Usage Example"},{"location":"peft/stack/#d9d.peft.all","text":"Package for composing multiple PEFT methods into a stack.","title":"all"},{"location":"peft/stack/#d9d.peft.all.PeftStack","text":"Bases: PeftMethod [ PeftStackConfig ] A composite PEFT method that applies a list of methods sequentially. Source code in d9d/peft/all/method.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 class PeftStack ( PeftMethod [ PeftStackConfig ]): \"\"\" A composite PEFT method that applies a list of methods sequentially. \"\"\" def __init__ ( self , methods : list [ PeftMethod ]): \"\"\" Constructs a PeftStack object. Args: methods: A list of instantiated PEFT methods to apply in order. \"\"\" self . _methods = methods def inject ( self , module : nn . Module ) -> PeftInjectionResult : params_to_train = [] state_mappers = [] for method in self . _methods : result = method . inject ( module ) params_to_train . extend ( result . parameters_to_train ) state_mappers . extend ( result . load_state_mappers ) return PeftInjectionResult ( parameters_to_train = params_to_train , load_state_mappers = state_mappers ) def merge ( self , module : nn . Module ): for method in self . _methods [:: - 1 ]: method . merge ( module ) @classmethod def from_config ( cls , config : PeftStackConfig ) -> Self : methods = [] for method in config . methods : methods . append ( peft_method_from_config ( method )) return cls ( methods )","title":"PeftStack"},{"location":"peft/stack/#d9d.peft.all.PeftStack.__init__","text":"Constructs a PeftStack object. Parameters: Name Type Description Default methods list [ PeftMethod ] A list of instantiated PEFT methods to apply in order. required Source code in d9d/peft/all/method.py 19 20 21 22 23 24 25 26 27 def __init__ ( self , methods : list [ PeftMethod ]): \"\"\" Constructs a PeftStack object. Args: methods: A list of instantiated PEFT methods to apply in order. \"\"\" self . _methods = methods","title":"__init__"},{"location":"peft/stack/#d9d.peft.all.PeftStackConfig","text":"Bases: BaseModel Configuration for applying a stack of multiple PEFT methods sequentially. Attributes: Name Type Description kind Literal ['stack'] Discriminator field, always \"stack\". methods list [ AnyPeftConfig ] A list of specific PEFT configurations (e.g., LoRA, FullTune) to apply in order. Source code in d9d/peft/all/config.py 9 10 11 12 13 14 15 16 17 18 19 20 class PeftStackConfig ( BaseModel ): \"\"\" Configuration for applying a stack of multiple PEFT methods sequentially. Attributes: kind: Discriminator field, always \"stack\". methods: A list of specific PEFT configurations (e.g., LoRA, FullTune) to apply in order. \"\"\" kind : Literal [ \"stack\" ] = \"stack\" methods : list [ \"AnyPeftConfig\" ]","title":"PeftStackConfig"},{"location":"peft/stack/#d9d.peft.all.peft_method_from_config","text":"Factory function to instantiate the correct PeftMethod based on the configuration type. Parameters: Name Type Description Default config TConfig A specific PEFT configuration object (e.g., LoRAConfig). required Returns: Type Description PeftMethod [ TConfig ] The corresponding method instance. Source code in d9d/peft/all/method.py 61 62 63 64 65 66 67 68 69 70 71 72 73 def peft_method_from_config ( config : TConfig ) -> PeftMethod [ TConfig ]: \"\"\" Factory function to instantiate the correct PeftMethod based on the configuration type. Args: config: A specific PEFT configuration object (e.g., LoRAConfig). Returns: The corresponding method instance. \"\"\" method_cls = cast ( type [ PeftMethod [ TConfig ]], _PEFT_CONFIG_MAP [ type ( config )]) return method_cls . from_config ( config )","title":"peft_method_from_config"}]}